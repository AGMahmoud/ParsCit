<?xml version="1.0" encoding="UTF-8"?>
<file>
  <entry no="0">
    <variant no="0" confidence="1.0"><title>Building Contextual Representations of +L+ Word Senses Using Statistical Models +L+ </title><author>Claudia Leacock, l Geoffrey Towdl ~ and Ellen Voorhees 2 +L+ </author><affiliation>I Peinceton University +L+ </affiliation><email>leacockOclarity.princeton, edu +L+ </email><affiliation>2Siemens Corporate Research, Inc. +L+ </affiliation><email>towellOlearning.scr.siemenJ, com, ellenOlearning.scr.siemenJ, com +L+ </email><abstract>Abstract +L+ Automatic corpus-based sense resolution, or sense dlsambiguation, techniques tend to +L+ focus either on very local context or on topical context. Both components axe needed +L+ for word sense resolution. A contextual representation of a word sense consists of top- +L+ ical context and local context. Our goal is to construct contextual representations by +L+ automatically extracting topical and local information from textual corpora. We re- +L+ view an experiment evaluating three statistical classifiers that automatically extract +L+ topical context. An experiment designed to examine human subject performance +L+ with similar input is described. Finally, we investigate a method for automatically +L+ extracting local context from a corpus. Preliminary results show improved perfor- +L+ ms, nce. +L+ 1 Contextual Representations +L+ The goal of automatic sense resolution is to acquire a conteztual representation of word +L+ senses. A contextual representation, as defined by Miller and Charles \[7\], is a characteri- +L+ zation of the linguistic contexts in which a word can be used. We look at two components +L+ of contextual representations that can be automatically extracted from textual corpora +L+ using statistical methods. These are topical contezt and local contezt. +L+ Topical contezt is comprised of substantive words that are likely to co-occur with a +L+ given sense of a target word. If, for example, the polysemous word line occurs in a sentence +L+ with poetry and we/re, it is probably being used to express a different sense of line than +L+ if it occurred with stand and wait. Topical context is relatively insensitive to the order +L+ of words or their grammatical inflections; the focus is on the meanings of the open-class +L+ words that are used together in the same sentences. +L+ Local contezt includes information on word order, distance and syntactic structure. For +L+ example, a line from does not suggest the same sense as in line .for. Order and inflection +L+ are critical clues for local information, which is not restricted to open-class words. +L+ In the next section, we briefly review an experiment using three statistical classifiers +L+ designed for sense resolution, and show that they are effective in extracting topical con- +L+ text. Section 3 describes an experiment that was performed to establish the upper bound +L+ of performance for these classifiers. Section 4 presents some techniques that we are devel- +L+ oping to extract local context. +L+ 2 Acquiring Topical Context +L+ Of the two types of context features, topical ones seem easier to identify. The idea is +L+ simple: for any topic there is a sub-vocabulary of terms that are appropriate for discussing +L+ 10 +L+ it. The task is to identify the topic, then to select that sense of the polysemous word that +L+ best fits the topic. For example, if the topic is writing, then sheet probably refers to a +L+ piece of paper; if the topic is sleeping, then it probably refers to bed linen; if the topic is +L+ sailing, it could refer to a sail; and so on. +L+ Instead of using topics to discover senses, one can use senses to discover topics. That +L+ is to say, if the senses are known in advance for a textual corpus, it is possible to search +L+ for words that are likely to co-occur with each sense. This strategy requires two steps. +L+ It is necessary (1) to partition a sizeable number of occurrences of a polysemous word +L+ according to its senses, and then (2) to use the resulting sets of instances to search for +L+ co-occurring words that are diagnostic of each sense. That was the strategy followed with +L+ considerable success by Gale, Church, and Yarowsky \[1\], who used a bilingual corpus for +L+ (1), and a Bayesian decision system for (2). +L+ To understand this and other statistical systems better, we posed a very specific prob- +L+ lem: given a set of contexts, each containing the noun line in a known sense, construct +L+ a classifier that selects the correct sense of line for new contexts. To see how the degree +L+ of polysemy affects performance, we ran three- and six-sense tasks. A full description +L+ of the three-sense task is reported in Voorhees, et. al. \[11\], and the six-sense task in +L+ Leacock, et. al \[5\]. These experiments are reviewed briefly below. +L+ We tested three corpus-based statistical sense resolution methods which attempt to +L+ infer the correct sense of a polysemous word by using knowledge about patterns of word +L+ co-occurrences. The first technique, developed by Gale et. al. \[1\] at AT&amp;T Bell Labora- +L+ tories, is based on Bayesian decision theory, the second is based on neural network with +L+ back propagation \[9\], and the third is based on content vectors as used in information +L+ retrieval \[10\]. The only information used by the three classifiers is co-occurrence of char- +L+ acter strings in the contexts. They use no other cues, such as syntactic tags or word +L+ order, nor do they require any augmentation of the training and testing data that is not +L+ fully automatic. The Bayesian classifier uses all of the information in the sentence except +L+ word order. That is, it uses punctuation, upper/lower case distinctions, and inflectional +L+ endings. The other two classifiers remove punctuation and convert all characters to lower +L+ case. In addition, they remove a list of stop words, a set of about 570 very high frequency +L+ words that includes most function words as well as some content words. The remaining +L+ strings are stemmed." suffixes are removed to conflate across morphological distinctions. +L+ For example, the strings computer(s), computing, computcdion(al), etc. are conflated to +L+ the stem comput. +L+ 2.1 Methodology +L+ The training and testing contexts were taken from the 1987-89 Wall Street Journal corpus +L+ and from the APHB corpus. 1 Sentences containing line(s) and Line(s) were extracted +L+ and manually assigned a single sense from WordNet. 2 Sentences with proper names con- +L+ taining Line, such as Japan Air Lines, were removed from the set of sentences. Sentences +L+ containing collocations that have a single sense in WordNet, such as product line and line +L+ of products, were also excluded since the collocations are not ambiguous. +L+ 1 The 25 million word corpus, obtained from the American Printing House for the Blind, is arc.hlved at +L+ IBM's T.J. Watson Research Center; it consists of stories and articles from books and general circulation +L+ magazines. +L+ 2WordNet is a lexical database developed by George Miller and his colleagues at Princeton Univer- +L+ sity \[6\]. +L+ 11 +L+ Typically, experiments have used a fixed number of words or characters on either +L+ side of the target word as the context. In these experiments, we used linguistic units - +L+ sentences - instead. Since the target word is often used anaphorically to refer back to the +L+ previous sentence, as in: +L+ That was the last time Bell ever talked on the phone. He couldn't get his wife +L+ off the line. +L+ we chose to use two-sentence contexts: the sentence containing line and the preceding +L+ sentence. However, if the sentence containing line was the first sentence in the article, +L+ then the context consists of one sentence. If the preceding sentence also contained line in +L+ the same sense, then an additional preceding sentence was added to the context, creating +L+ contexts three or more sentences long. The average size of the training and testing contexts +L+ was 44.5 words. +L+ The sense resolution task used the following six senses of the noun line: +L+ 1. a product: ... a new line of midsized cars ... +L+ 2. a formation of people or things: People waited patiently in long lines ... +L+ 3. spoken or written tezt: One winning line from that speech ... +L+ 4. a thin, flexible object; cord: With a line tied to his foot, ... +L+ 5. an abstract division: ... the Amish draw no line between work and religion and life. +L+ 6. a telephone connection: One key to WordPerfect's growth was its toll-free help line +L+ The classifiers were run three times each on randomly selected training sets. The set of +L+ contexts for each sense was randomly permuted, with each permutation corresponding +L+ to one trial. For each trial, the first 200 contexts of each sense were selected as training +L+ contexts. The next 149 contexts were selected as test contexts. The remaining contexts +L+ were not used in that trial. The 200 training contexts for each sense were combined to +L+ form a final training set of size 1200. The final test set contained the 149 test contexts +L+ from each sense, for a total of 894 contexts. To test the effect that the number of training +L+ examples has on classifier performance, smaller training sets of 50 and 100 contexts were +L+ extracted from the 200 context training set. +L+ 2.2 Results +L+ All of the classifiers performed best with the largest number (200) of training contexts, +L+ and the percent correct results reported here are averaged over the three trials with 200 +L+ training contexts. On the six-sense task, the Bayesian classifier averaged 71% correct +L+ answers, the content vector classifier 72%, and the neural networks 76%. None of these +L+ differences are statistically significant due to the limited sample size of three trials. +L+ The ten most heavily weighted tokens for each sense for each classifier appear in +L+ Table 1. The words on the list seem, for the most part, indicative of the target sense +L+ and are reasonable indicators of topical context. However, there are some consistent +L+ differences among the methods. For example, while the Bayesian method is sensitive +L+ to proper nouns, the neural network appears to have no such preference. To test the +L+ hypothesis that the methods have different response patterns, we performed the X 2 test for +L+ correlated proportions. This test measures how consistently the methods treat individual +L+ test contexts by determining whether the classifiers are making the same classification +L+ errors in each of the senses. +L+ 12 +L+ </abstract><degree>Bayesian +L+ "Chrysler +L+ workstations +L+ Digital +L+ introduced +L+ models +L+ IBM +L+ Compacl +L+ sell +L+ agreement +L+ ,,,computers +L+ Product Formation +L+ Vector Network Bayesian Vector Network +L+ </degree><email>comput +L+ ibm +L+ produc +L+ corp +L+ flale +L+ model +L+ sell +L+ introduc +L+ brand +L+ mainframe +L+ comput +L+ sell +L+ minicomput +L+ model +L+ introduc +L+ extend +L+ acquir +L+ launch +L+ continu +L+ quak +L+ night +L+ checkout +L+ wait +L+ gasoline +L+ outside +L+ waiting +L+ food +L+ hours +L+ long +L+ driver +L+ walt +L+ long +L+ checkout +L+ park +L+ mr +L+ airport +L+ shop +L+ count +L+ peopl +L+ canad +L+ walt +L+ long +L+ stand +L+ checkout +L+ park +L+ hour +L+ form +L+ short +L+ custom +L+ shop +L+ </email><author>Text Cord +L+ Bayesian Vector Network Bayesian Vector Network +L+ </author><email>speech +L+ writ +L+ mr +L+ bush +L+ ad +L+ speak +L+ read +L+ dukak +L+ biden +L+ poem +L+ fish +L+ fishing +L+ bow +L+ deck +L+ sea +L+ boat +L+ water +L+ clothes +L+ fastened +L+ ship +L+ Biden +L+ ad +L+ Bush +L+ opening +L+ famous +L+ Dole +L+ speech +L+ Dukakis +L+ funny +L+ speeches +L+ fish +L+ boat +L+ wat +L+ hook +L+ wash +L+ float +L+ men +L+ dive +L+ cage +L+ rod +L+ familiar +L+ writ +L+ ad +L+ rememb +L+ deliv +L+ fame +L+ speak +L+ funny +L+ movie +L+ read +L+ hap +L+ fish +L+ wash +L+ pull +L+ boat +L+ rope +L+ break +L+ hook +L+ exercis +L+ cry +L+ </email><affiliation>Division Phone +L+ Bayesian Vector Network Bayesian Vector Network +L+ phones telephon +L+ toll phon +L+ porn call +L+ Bellsouth </affiliation><email>access +L+ gab dial +L+ telephone gab +L+ </email><affiliation>Bell bell +L+ billion servic +L+ Pacific </affiliation><email>toll +L+ calls porn +L+ blurred +L+ walking +L+ crossed +L+ ethics +L+ narrow +L+ fine +L+ class +L+ between +L+ walk +L+ draw +L+ draw draw +L+ fine priv +L+ blur hug +L+ cross blur +L+ walk cross +L+ narrow fine +L+ mr thin +L+ tread funct +L+ faction genius +L+ thin narrow +L+ telephon +L+ phon +L+ dead +L+ cheer +L+ hear +L+ henderson +L+ minut +L+ call +L+ bill +L+ silent +L+ Table 1: </email><abstract>Topical Context. The ten most heavily weighted tokens for each sense of line for +L+ the Bayesian, content vector and neural network classifiers. +L+ 13 +L+ The results of the X 2 test for a three-sense resolution task (product, formation and +L+ tezt), 3 indicate that the response pattern of the content vector classifier is significantly +L+ different from the patterns of both the Bayesian and neural network classifiers, but the +L+ Bayesian response pattern is significantly different from the neural network pattern for +L+ the product sense only. In the six-sense disambiguation task, the X 2 results indicate +L+ that the Bayesian and neural network classifiers' response patterns are not significantly +L+ different for any sense. The neural network and Bayesian classifiers' response patterns +L+ are significantly different from the content vector classifier only in the formation and tezt +L+ senses. Therefore, with the addition of three senses, the classifiers' response patterns +L+ appear to be converging. +L+ A pilot two-sense distinction task (between product and formation) yielded over 90% +L+ correct answers. 4 In the three-sense distinction task, the three classifiers had a mean of +L+ 76% correct, yielding a sharp degradation with the addition of a third sense. Therefore, we +L+ hypothesized degree of polysemy to be a major factor for performance. We were surprised +L+ to find that in the six-sense task, all three classifiers degraded only slightly from the three- +L+ sense task, with a mean of 73% correct. Although the addition of three new senses to +L+ the task caused consistent degradation, the degradation is relatively slight. Hence, we +L+ conclude that some senses are harder to resolve than others, and it appears that overall +L+ accuracy is a function of the difficulty of the sense rather than being strictly a function of +L+ the degree of polysemy. The hardest sense for all three classifiers to learn was tezt, followed +L+ by formation, followed by division. The difficulty in training for the product, phone, and +L+ cord senses varied among the classifiers, but they were the three 'easiest' senses across +L+ the classifiers. To test our conclusion that the difficulty involved in learning individual +L+ senses is a greater factor for performance than degree of polysemy, we ran a three-way +L+ experiment on the three 'easy' senses. On this task, the content vector classifier achieved +L+ 90% accuracy and neural network classifier 92% accuracy. +L+ The convergence of the response patterns for the three methods suggests that each +L+ of the classifiers is extracting as much data as is available in word co-occurrences in +L+ the training contexts. If this is the case, any technique that uses only word counts will +L+ not be significantly more accurate than the techniques tested here. Although the degree +L+ of polysemy does affect the difficulty of the sense resolution task, a greater factor ~ for +L+ performance is the difficulty of resolving individual senses. From inspection of the contexts +L+ for the various senses, it appears that the senses of line that were easy to learn tend to +L+ be surrounded by a lot of topical context. With the senses that were hard to learn, the +L+ crucial dissmbiguating information tends to be very local, so that a greater proportion +L+ of the context is noise. Although it is recognized that local information is more reliable +L+ than distant information, the classifiers make no use of locality. Figure 1 shows some +L+ representative contexts for each sense of line used in the study. The product, phone +L+ and cord senses contain a lot of topical context, while the other senses have little or no +L+ information that is not very local. +L+ The three classifiers are doing a good job finding topical context. However, simply +L+ knowing which words are likely to co-occur in the same sentences when a particular topic +L+ is under discussion is not sufficient for sense resolution. +L+ 3'Prainlng and test sets for these senses are identical to those in the six-sense resolution task. +L+ aThls task was only run with the content vector and neural network clarsifiers. +L+ 14 +L+ 1. text: In a warmly received speech that seemingly sought to distance him from +L+ Reagan administration civil-rights policies, Mr. Bush outlined what he called a +L+ "positive civil-rights agenda," and promised to have "minority men and women +L+ of excellence as full-scale partners" during his presidency. One winning line from +L+ that speech: "Whenever racism rears its ugly head-Howard Beach, Forsyth County, +L+ wherever-we must be there to cut it off." +L+ 2. formation: </abstract><degree>On the way to work one morning, he stops at the building to tell Mr. +L+ Arkhipov: "Don't forget the drains today." Back in his office, the llne of people +L+ waiting to see him has dwindled, so Mr. Goncharov stops in to see the mayor, Yuri +L+ Khivrich. </degree><abstract>+L+ 3. division: Thus, some families are probably buying take-out food from grocery +L+ stores-such as barbecued chicken-but aren't classifying it as such. The line between +L+ groceries and take-out food may have become blurred. +L+ 4. cord: Larry ignored the cries and came swooping in. The fisherman's nylon llne, +L+ taut and glistening with drops of seawater, suddenly went slack as Larry's board +L+ rode over it. +L+ 5. phone: "Hello, Weaver," he said and then to put her on the defensive, "what's all +L+ the gabbing on the house phones? I couldn't get an open llne to you." +L+ 6. product: International Business Machines Corp., seeking to raise the return on +L+ its massive research and development investments, said it will start charging more +L+ money to license its 32,000 patents around the world. In announcing the change, +L+ IBM also said that it's willing to license patents for its PS/2 line of personal com- +L+ puters. +L+ Figure i: Representative contexts for the six senses of line used in the study. +L+ 15 +L+ 3 An Upper Bound For Classifier Performance +L+ In an effort to establish an upper bound for performance on corpus-based statistical sense +L+ resolution methods, we decided to see how humans would perform on a sense resolution +L+ task using the same input that drives the statistical classifiers \[4\]. An experiment was +L+ designed to answer the following questions: +L+ 1. How do humans perform in a sense resolution task when given the same testing +L+ input as the statistical classifiers? +L+ 2. Are the contexts that are hard/easy for the statistical classifiers also hard/easy for +L+ people? +L+ The three-sense task was replicated using human subjects. For each of the three senses +L+ of line (product, tezt, and formation), we selected 10 easy contexts (contexts that were +L+ correctly classified by the three statistical methods) and 10 haed contexts (contexts that +L+ were misclassified by the three methods), for a total of 60 contexts. These contexts were +L+ prepared in three formats: (1) a sentential form (as they originally appeared in the corpus), +L+ (2) a long list format (as was used by the Bayesian classifier), and (3) a shor~ list format +L+ (as was used by the content vector and neural network classifiers). In order to mimic +L+ the fact that the classifiers do not use word order, collocations, or syntactic structure, +L+ the latter two contexts were presented to the subjects as word lists in reverse alphabetical +L+ order. 36 subjects each saw 60 contexts, 20 in each of the three formats, and were asked to +L+ choose the appropriate sense of line. The order in which the formats were presented was +L+ counter-balanced across subjects. No subject saw the same context twice. The subjects +L+ were Princeton undergraduates who were paid for their participation. +L+ Human subjects performed almost perfectly on the sentential formats and had about +L+ a 32% error rate on the list formats. There was no significant difference between the two +L+ list formats - indicating that function words are of no use for sense resolution when word +L+ order is lost. They made significantly more errors on the contexts that were hard for the +L+ statistical classifiers, and fewer errors on the contexts that were easy for the classifiers. +L+ Not all the senses were equally difficult for human subjects: there were significantly fewer +L+ errors for the product sense of line than for the tczt and fformgtion senses. Error rates for +L+ the subjects on the list formats were almost 50% for the hard contexts (contexts where +L+ the classifiers performed with 100% error), so subjects performed much better than the +L+ classifiers on these contexts. However, on the easy contexts, where the classifiers made no +L+ errors, the students showed an error rate of approximately 15%. +L+ When subjects see the original sentences and therefore have access to all cues, both +L+ topical and local, they resolve the senses of line with 98% accuracy. When they are given +L+ the contexts in a list format, and are getting only topical cues, their performance drops +L+ to about 70% accuracy. Although their performance was significantly better than the +L+ classifiers (which all performed at 50% accuracy on this sample) human subjecrts are not +L+ able to disamhiguate effectively using only topical context. From this result we conclude +L+ that in order to improve the performance of automatic classifiers, we need to incorporate +L+ local information into the statistical methods. +L+ 16 +L+ 4 Acquiring Local Context +L+ Kelly and Stone \[3\] pioneered research in finding local context by creating algorithms for +L+ automatic sense resolution. Over a period of seven years in the early 1970s, they (and +L+ some 30 students) hand coded sets of ordered rules for disambiguating 671 words. The +L+ rules include syntactic markers (part of speech, position within the sentence, punctuation, +L+ inflection), semantic markers and selectional restrictions, and words occurring within a +L+ specified distance before and/or after the target. An obvious shortcoming of this approach +L+ is the amount of work involved. +L+ Recently there has been much interest in automatic and semi-automatic acquisition of +L+ local context (Hearst \[2\], Resnik \[8\], Yarowsky \[13\]). These systems are all plagued with +L+ the same problem, excellent precision but low recall. That is, if the local information that +L+ the methods learn is also present in a novel context, then that information is very reliable. +L+ However, quite frequently no local context match is found in a novel context. Given the +L+ sparseness of the local data, we hope to look for both local and topical context, and we +L+ have begun experimenting with various ways of acquiring the local context. +L+ Local context can be derived from a variety of sources, including WordNet. The nouns +L+ in WordNet are organized in a hierarchical tree structure based on hypernomy/hyponomy. +L+ The hypernym of a noun is its superordinate, and the/s a kind o/relation exists between +L+ a noun and its hypernym. For example, line is a hypernym of conga line, which is to +L+ say that a conga line is a kind of line. Conversely, cong~ line is a hyponym of line. +L+ Polysemous words tend to have hyponyms that are monosemous collocations incorporating +L+ the polysemous word: product line is a monosemous hyponym of the merchandise sense of +L+ line; any occurrence of product line can be recognized immediately as an instance of that +L+ sense. Similarly, phone line is a hyponym of the telephone connection sense of line, actor's +L+ line is a hyponym of the text sense of line, etc. These collocational hyponyms provide a +L+ convenient starting point for the construction of local contexts for polysemous words. +L+ We are also experimenting with template matching, suggested by Weiss as one ap- +L+ proach to using local context to resolve word senses \[12\]. In template matching, specific +L+ word patterns recognized as being indicative of a particular sense (the templates) are +L+ used to select a sense when a template is contained in the novel context; otherwise word +L+ co-occurrence within the context (topical context) is used to select a sense. Weiss initially +L+ used templates that were created by hand, and later derived templates automatically from +L+ his dataset. Unfortunately, the datasets available to Weiss at the time were very small, +L+ and his results are inconclusive. We are investigating a similar approach using the line +L+ data: training contexts are used to both automatically extract indicative templates and +L+ create topical sense vectors. +L+ To create the templates, the system extracts contiguous subsets of tokens including +L+ the target word and up to two tokens on either side of the target as candidate templates, s +L+ The system keeps a count of the number of times each candidate template occurs in all +L+ of the training contexts. A candidate is selected as a template if it occurs in at least n of +L+ the training contexts and one sense accounts for at least m% of its total occurrences. For +L+ example, Figure 2 shows the templates formed when this process is used on a training set +L+ of 200 contexts for each of six senses when n = 10 and m = 75. The candidate template +L+ blurs the line is not selected as a template with these parameter settings because it does +L+ not occur frequently enough in the training corpus; the candidate template line o/is not +L+ Sin the templ6te learning phase, tokens include ptmetuation and Jiop wordm. No stemmingls performed +L+ and case distinctions are significant. +L+ 17 +L+ cord +L+ his line +L+ division +L+ a fine line between, line line between, a line line, line line +L+ line between the, the line between, line between +L+ draw the line, over the line +L+ formation +L+ a long line of, long line of, a long line, long line, long lines +L+ in line for, wait in line, in line +L+ phone +L+ telephone lines +L+ access lines +L+ lille wa4l +L+ product +L+ a new line of, a new line, new line of, new line +L+ Figure 2: Templates formed for a training set of 200 contexts for each of six senses when +L+ a template must occur at least 10 times and at least 75% of the occurrences must be for +L+ one sense. No templates were learned for the tezt sense. +L+ selected because it appears too frequently in both the product line and formation contexts. +L+ With the exception of his line (cord) and line was (phone), these templates readily +L+ suggest their corresponding sense. The n = 10 and m = 75 parameter settings are +L+ relatively stringent criteria for template formation, so not many templates are formed, +L+ but those templates that are formed tend to be highly indicative of the sense. +L+ Preliminary results show template matching improves the performance of the content +L+ vector classifier. The six-sense experiment was repeated using a simple decision tree to +L+ incorporate the templates: The sense corresponding to the longest template contained in a +L+ test context was selected for that context; if the context contained no template, the sense +L+ chosen by the vector classifer was selected. The templates were automatically created +L+ from the same training set as was used to create the content vectors. To be selected as a +L+ template, a candidate had to appear at least 3 times for the training sets that included +L+ 50 of each sense, 5 times for the 100 each training sets, and 10 times for the 200 each +L+ training sets. In all cases, a single sense had to account for at least 75% of a candidate's +L+ occurrences. This hybrid approach was more accurate than the content vector classifier +L+ alone on each of the 9 trials. The average accuracy when trained using 200 contexts of +L+ each sense was 75% for the hybrid approach compared to 72% for the content vectors +L+ alone. +L+ Other researchers have also suggested methods for incorporating local information into +L+ a classifier. Yarowsky found collocations 8 to be such powerful sense indicators that he +L+ suggests choosing a sense by matching on a set of collocations and choosing the most +L+ frequent sense if no collocation matches \[13\]. To resolve syntactic ambiguities, Resnlk +L+ eyarowsky uses the term collocation to denote constructs similar to what we have called templates. +L+ 18 +L+ investigated four different methods for combining three sources of information \[8\]. The +L+ "backing off" strategy, in which the three sources of information were tried in order from +L+ most reliable to least reliable until some match was found (no resolution was done if no +L+ method matched), maintained high precision (81%) and produced substantially higher +L+ recall (95%) than any single method. +L+ Our plans for incorporating templates into the content vector classifier include inves- +L+ tigating the significance of the tradeoff between the reliability of the templates and the +L+ number of templates that are formed. When stringent criteria are used for template for- +L+ mation, and the templates are thought to be highly reliable sense indicators, the sense +L+ corresponding to a matched template will always be selected, and the sense vectors will +L+ be used only when no template match occurs. When the templates are thought to be less +L+ reliable, the choice of sense will be a function of the uniqueness of a matched template +L+ (if any) and the sense vector similarities. By varying the relative importance of a tem- +L+ plate match and sense vector similarity we will be able to incorporate different amounts +L+ of topical and local information into the template classifier. +L+ 5 Conclusion +L+ The capacity to determine the intended sense of an ambiguous word is an important +L+ component of any general system for language understanding. We believe that, in order +L+ to accomplish this task, we need contextual representations of word senses containing both +L+ topical and local context. Initial experiments focused on methods that are able to extract +L+ topical context. These methods are effective, but topical context alone is not sufficient +L+ for sense resolution tasks. The human subject experiment shows that even people are not +L+ very good at resolving senses when given only topical context. Currently we are testing +L+ methods for learning local context for word senses. Preliminary results show that the +L+ addition of template matching on local context improves performance. +L+ Acknowledgments +L+ </abstract><note>This work was supported in part by Grant No. N00014-91-1634 from the Defense Ad- +L+ vanced Research Projects Agency, Information and Technology Office, by the Office of +L+ Naval Research, and by the James S. McDonnell Foundation. We are indebted to George +L+ A. Miller and Martin S. Chodorow for valuable comments on an earlier version of this +L+ paper. +L+ References +L+ \[1\] William Gale, Kenneth W. Church, and David Yarowsky. A method for disam- +L+ biguating word senses in a large corpus. Statistical Research Report 104, AT&amp;T Bell +L+ Laboratories, 1992. +L+ \[2\] Marti A. Hearst. Noun homograph disambiguation using local context in large text +L+ corpora. In Seventh Annual Conference of the UW Centre for the New OED and +L+ Tezt Research: Using Corpora, pages 1-22, Oxford, 1991. UW Centre for the New +L+ OED and Text Research. +L+ 19 +L+ \[3\] Edward Kelly and Philip Stone. Computer Recognition of English Word Senses. +L+ North-Holland, Amsterdam, 1975. +L+ \[4\] Claudia Leacock, Shari Landes, and Martin Chodorow. Comparison of sense reso- +L+ lution by statistical classifiers and human subjects. Cognitive Science Laboratory +L+ Report, Princeton University, in preparation. +L+ \[5\] Claudia Leacock, Geoffrey Towell, and Ellen M. Voorhees. Corpus-based statisti- +L+ cal sense resolution. In Proceedings off the ARPA Workshop on Human Language +L+ Technology, 1993. +L+ \[6\] George Miller. Special Issue, WordNet: An on-line lexical database. International +L+ Journal off Lezicography, 3(4), 1990. +L+ \[7\] George A. Miller and Walter G. Charles. Contextual correlates of semantic similarity. +L+ Language and Cognitive Processes, 6(1), 1991. +L+ \[8\] Philip Resnik. Semantic classes and syntactic ambiguity. In Proceedings of the ARPA +L+ Workshop on Human Language Technology, 1993. +L+ \[9\] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations +L+ by error propagation. In D. E. Rumelhart and J. L. McClelland, editors, Parallel +L+ Distributed Processing: Ezplorations in the Microstrncture of Cognition, Volume 1: +L+ Foundatior~s, pages 318-363. MIT Press, Cambridge, 1986. +L+ \[10\] G. Salton, A. Wong, and C.S. Yang. A vector space model for automatic indexing. +L+ Communications of the ACM, 18(11):613-620, 1975. +L+ \[11\] Ellen M. Voorhees, Claudia Leacock, and Geoffrey Towell. Learning context to dis- +L+ smbiguate word senses. In Proceedings off the 3rd Computational Leaerdng Theory +L+ and Natural Learning Systems Conference-lags, Cambridge, to appear. MIT Press. +L+ \[12\] Stephen Weiss. Learning to disambiguate. Infformation Storage and Retrieval, 9:33- +L+ 41, 1973. +L+ \[13\] David Yarowsky. One sense per collocation. In Proceedings off the ARPA Workshop +L+ on Human Language Technology, 1993. +L+ 20 +L+ THIS PAGE INTENTION~T,LY LEFT BLANK +L+ 21 +L+ </note></variant>
  </entry>
  <entry no="1">
  </entry>
</file>
