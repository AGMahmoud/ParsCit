<algorithm name="ParsCit" version="1.0">
<citationList>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process</title>
<date>1972</date>
<journal>Inequalities</journal>
<volume>3</volume>
<contexts>
<context>s for such commands rather than struggle with mouse menus or dialog boxes during class. 3 The Subject Matter Among topics in natural language processing, the forward-backward or Baum-Welch algorithm (Baum, 1972) is particularly difficult to teach. The algorithm estimates the parameters of a Hidden Markov Model (HMM) by ExpectationMaximization (EM), using dynamic programming to carry out the expectation step</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>L. E. Baum. 1972. An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. Inequalities, 3.</rawString>
</citation>
<citation valid="true">
<title>TnT: A statistical part-of-speech tagger</title>
<booktitle>In Proc. of ANLP</booktitle>
<location>Seattle</location>
<marker></marker>
<rawString>Thorsten Brants. 2000. TnT: A statistical part-of-speech tagger. In Proc. of ANLP, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text</title>
<date>1988</date>
<booktitle>Proc. of ANLP</booktitle>
<contexts>
<context>mization (EM), using dynamic programming to carry out the expectation steps efficiently. HMMs have long been central in speech recognition (Rabiner, 1989). Their application to partof-speech tagging (Church, 1988; DeRose, 1988) kicked off the era of statistical NLP, and they have found additional NLP applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction. The</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>K. W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. Proc. of ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization</title>
<date>1988</date>
<journal>Computational Linguistics</journal>
<volume>14</volume>
<contexts>
<context> using dynamic programming to carry out the expectation steps efficiently. HMMs have long been central in speech recognition (Rabiner, 1989). Their application to partof-speech tagging (Church, 1988; DeRose, 1988) kicked off the era of statistical NLP, and they have found additional NLP applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction. The algorithm is </context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>Steven J. DeRose. 1988. Grammatical category disambiguation by statistical optimization. Computational Linguistics, 14(1):31–39.</rawString>
</citation>
<citation valid="true">
<title>Parameter estimation for probabilistic finite-state transducers</title>
<booktitle>In Proc. of ACL</booktitle>
<marker></marker>
<rawString>Jason Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen L Lauritzen</author>
</authors>
<title>The EM algorithm for graphical association models with missing data. Computational Statistics and Data Analysis</title>
<date>1995</date>
<contexts>
<context>case of (1) the inside-outside algorithm for estimation of probabilistic context-free grammars; (2) belief propagation for training singly-connected Bayesian networks and junction trees (Pearl, 1988; Lauritzen, 1995); (3) algorithms for learning alignment models such as weighted edit distance; (4) general finitestate parameter estimation (Eisner, 2002). Before studying the algorithm, students should first have w</context>
</contexts>
<marker>Lauritzen, 1995</marker>
<rawString>Steffen L. Lauritzen. 1995. The EM algorithm for graphical association models with missing data. Computational Statistics and Data Analysis, 19:191–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model</title>
<date>1994</date>
<journal>Comp. Ling</journal>
<volume>20</volume>
<contexts>
<context>to predict the word sequence. 39Advanced students might also want to read about a modern supervised trigram tagger (Brants, 2000), or the mixed results when one actually trains trigram taggers by EM (Merialdo, 1994). Furthermore, students could check their ice cream output against the spreadsheet, and track down basic bugs by comparing their intermediate results to the spreadsheet’s. They reported this to be ve</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Comp. Ling., 20(2):155–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
<date>1988</date>
<publisher>Morgan Kaufmann</publisher>
<location>San Mateo, California</location>
<contexts>
<context>tive special case of (1) the inside-outside algorithm for estimation of probabilistic context-free grammars; (2) belief propagation for training singly-connected Bayesian networks and junction trees (Pearl, 1988; Lauritzen, 1995); (3) algorithms for learning alignment models such as weighted edit distance; (4) general finitestate parameter estimation (Eisner, 2002). Before studying the algorithm, students sh</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, California.</rawString>
</citation>
</citationList>
</algorithm>
