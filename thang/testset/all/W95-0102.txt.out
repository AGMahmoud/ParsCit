<algorithm name="ParsCit" version="1.0">
<citationList>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition</title>
<date>1979</date>
<booktitle>In Proceedings of the 97th Meeting of the Acoustical Society of America</booktitle>
<pages>547--550</pages>
<contexts>
<context>rs have turned to su-spervised training from bracketed corpora. We examine why previous approaches havesfailed to acquire desired grammars, concentrating our analysis on the inside-outsidesalgorithm (Baker, 1979), and propose that with a representation of phrase structurescentered on head relations such supervision may not be necessary.s1. INTRODUCTIONsResearchers investigating the acquisition of phrase-stru</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In Proceedings of the 97th Meeting of the Acoustical Society of America, pages 547-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Brent</author>
</authors>
<title>Minimal generative explanations: A middle ground between neurons and triggers</title>
<date>1993</date>
<booktitle>In Proc. of the 15th Annual Meeting of the Cognitive Science Society</booktitle>
<pages>28--36</pages>
<marker>Brent, 1993</marker>
<rawString>Michael Brent. 1993. Minimal generative explanations: A middle ground between neurons and triggers. In Proc. of the 15th Annual Meeting of the Cognitive Science Society, pages 28-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Automatic grammar induction and parsing free text: A transformation based approach</title>
<date>1993</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop</booktitle>
<contexts>
<context>reebank (Marcus,s1991) to train the learner: (Pereira and Schabes, 1992) demonstrate that the inside-outside algo-srithm can learn grammars effectively given such constraint; from a bracketed corpus (Brill, 1993)ssuccessfully learns rules that iteratively transform a default phrase-structure into a better one forsa particular sentence.sThe necessity of bracketed corpora for training is grating to our sensibi</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993. Automatic grammar induction and parsing free text: A transformation based approach. In Proceedings of the DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>Nick Waegner</author>
</authors>
<title>Robust stochastic parsing using the inside-outside algorithm</title>
<date>1992</date>
<booktitle>In Proc. of ~he AAAI Workshop on Probabilistic-Based Natural Language Processing Techniques</booktitle>
<pages>39--52</pages>
<marker>Briscoe, Waegner, 1992</marker>
<rawString>Ted Briscoe and Nick Waegner. 1992. Robust stochastic parsing using the inside-outside algorithm. In Proc. of ~he AAAI Workshop on Probabilistic-Based Natural Language Processing Techniques, pages 39-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Andrew Cartwright</author>
<author>Michael R Brent</author>
</authors>
<title>Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition</title>
<date>1994</date>
<booktitle>In Proc. of the 16th Annual Meeting of the Cognitive Science Society</booktitle>
<location>Hillsdale, New Jersey</location>
<contexts>
<context>ition of syntax, similar or identical statistical models to those discussedshere have been used to acquiring words and morphemes from sequences of characters (Olivier, 1968; Wolff, 1982; Brent,s1993; Cartwright and Brent, 1994) and syllables from phonemes (Ellison, 1992), among other language appfications.s16sexplore ways of fixing them.sLet us look again at (A), reproduced below, and center discussion on an extended stoch</context>
</contexts>
<marker>Cartwright, Brent, 1994</marker>
<rawString>Timothy Andrew Cartwright and Michael R. Brent. 1994. Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition. In Proc. of the 16th Annual Meeting of the Cognitive Science Society, Hillsdale, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl de Marcken</author>
</authors>
<title>Parsing with stochastic, feature-based RTNs</title>
<date>1995</date>
<journal>Memo A.I. Memo, MIT Artificial Intelligence Lab</journal>
<location>Cambridge, Massachusetts</location>
<marker>de Marcken, 1995</marker>
<rawString>Carl de Marcken. 1995. Parsing with stochastic, feature-based RTNs. Memo A.I. Memo, MIT Artificial Intelligence Lab., Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della-Pietra</author>
<author>V Della-Pietra</author>
<author>J Gillett</author>
<author>J Lafferty</author>
<author>H Printz</author>
<author>L Ure§</author>
</authors>
<title>Inference and estimation of a long-range trigram model</title>
<date>1994</date>
<booktitle>In International Colloquium on Grammatical Inference</booktitle>
<pages>78--92</pages>
<location>Alicante, Spain</location>
<marker>Della-Pietra, Della-Pietra, Gillett, Lafferty, Printz, Ure§, 1994</marker>
<rawString>S. Della-Pietra, V. Della-Pietra, J. Gillett, J. Lafferty, H. Printz, and L. Ure§. 1994. Inference and estimation of a long-range trigram model. In International Colloquium on Grammatical Inference, pages 78-92, Alicante, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mark Ellison</author>
</authors>
<title>The Machine Learning of Phonological Structure</title>
<date>1992</date>
<tech>Ph.D. thesis</tech>
<institution>University of Western Australia</institution>
<contexts>
<context>to those discussedshere have been used to acquiring words and morphemes from sequences of characters (Olivier, 1968; Wolff, 1982; Brent,s1993; Cartwright and Brent, 1994) and syllables from phonemes (Ellison, 1992), among other language appfications.s16sexplore ways of fixing them.sLet us look again at (A), reproduced below, and center discussion on an extended stochasticscontext-free grammar model in which a </context>
</contexts>
<marker>Ellison, 1992</marker>
<rawString>T. Mark Ellison. 1992. The Machine Learning of Phonological Structure. Ph.D. thesis, University of Western Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Grammatical trigrams: A probabalistic model of link grammar</title>
<date>1992</date>
<tech>Technical Report CMU-CS-92-181</tech>
<institution>Carnegie Mellon University</institution>
<location>Pittsburgh, Pennsylvania</location>
<contexts>
<context>sWe have implemented a statistical parser and training mechanism based on the above notions,sbut results are too preliminary to include here. Stochastic link-grammar based models have beensdiscussed (Lafferty et al., 1992) but the only test results we have seen (Della-Pietra et ai., 1994)sassume a very restricted subset of the model and do not explore the &amp;quot;phrase structures&amp;quot; that resultsfrom training on English text.s</context>
</contexts>
<marker>Lafferty, Sleator, Temperley, 1992</marker>
<rawString>John Lafferty, Daniel Sleator, and Davy Temperley. 1992. Grammatical trigrams: A probabalistic model of link grammar. Technical Report CMU-CS-92-181, Carnegie Mellon University, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context>ar by starting with an exhaustive set ofsstochastic context-free rules of a certain form, and estimate probabilities for these rules from astest corpus. This is the same general procedure as used by (Lari and Young, 1990; Briscoe andsWaegner, 1992; Pereira and Schabes, 1992) and others. For parts-of-speech Y and Z, the rules wesinclude in our base grammar aresS:=~ZP ZP:=~ZPYP ZP:=~YP ZPsZP~ ZYP ZP ~YPZ ZP~Zswhere S i</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Parsing a natural language using mutual information statistics</title>
<date>1990</date>
<booktitle>In Proc. of the American Association for Artificial Intelligence</booktitle>
<pages>984--989</pages>
<contexts>
<context>ic context free grammar trained on part-of-speech sequencessfrom English text can have an entropy as low or lower than another but bracket the text muchsmore poorly (tested on hand-annotations). And (Magerman and Marcus, 1990) provide evidencesthat grouping sub-sequences of events with high mutual information is not always a good heuristic;sthey must include in their parsing algorithm a list of event sequences (such as no</context>
<context>cur together not because they are partsof a common word, but because Engfish syntax and semantics places these two morphemes side-sby-side. At a syntactic level, this is exactly why the algorithm of (Magerman and Marcus, 1990)shas problems: English places prepositions after nouns not because they are in the same phrase,sbut because prepositional phrases often adjoin to noun phrases. Any greedy algorithm (such ass(Magerman</context>
</contexts>
<marker>Magerman, Marcus, 1990</marker>
<rawString>David M. Magerman and Mitchell P. Marcus. 1990. Parsing a natural language using mutual information statistics. In Proc. of the American Association for Artificial Intelligence, pages 984-989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
</authors>
<title>Very large annotated databse of American English</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop</booktitle>
<marker>Marcus, 1991</marker>
<rawString>Mitchell Marcus. 1991. Very large annotated databse of American English. In Proceedings of the DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A MelEuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice</title>
<date>1988</date>
<publisher>Press</publisher>
<institution>State University of New York</institution>
<marker>MelEuk, 1988</marker>
<rawString>I. A. MelEuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Cort Olivier</author>
</authors>
<title>Stochastic Grammars and Language Acquisition Mechanisms</title>
<date>1968</date>
<tech>Ph.D. thesis</tech>
<institution>Harvard University</institution>
<location>Cambridge, Massachusetts</location>
<contexts>
<context>ile this paper concentrates on the acquisition of syntax, similar or identical statistical models to those discussedshere have been used to acquiring words and morphemes from sequences of characters (Olivier, 1968; Wolff, 1982; Brent,s1993; Cartwright and Brent, 1994) and syllables from phonemes (Ellison, 1992), among other language appfications.s16sexplore ways of fixing them.sLet us look again at (A), reprod</context>
</contexts>
<marker>Olivier, 1968</marker>
<rawString>Donald Cort Olivier. 1968. Stochastic Grammars and Language Acquisition Mechanisms. Ph.D. thesis, Harvard University, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Sehabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora</title>
<date>1992</date>
<booktitle>In Proc. 29th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>128--135</pages>
<location>Berkeley, California</location>
<marker>Pereira, Sehabes, 1992</marker>
<rawString>Fernando Pereira and Yves Sehabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proc. 29th Annual Meeting of the Association for Computational Linguistics, pages 128-135, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D K Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing english with a link grammar</title>
<date>1991</date>
<tech>Technical Report CMU-CS-91-196</tech>
<institution>Carnegie Mellon University</institution>
<location>Pittsburgh, Pennsylvania</location>
<contexts>
<context>use for learning human language, especially when combined with the inside-outside algorithm. Wesargue that head-driven grammatical formalisms like dependency grammars (Mel~uk, 1988) or linksgrammars (Sleator and Temperley, 1991) are better suited to the task.s14s2. LINGUISTIC AND STATISTICAL BASIS OF PHRASE STRUCTUREsLet us look at a particular example. In English, the word sequence &amp;quot;walking on ice&amp;quot; is generallysassumed to </context>
<context> is essentially incapable of finding an optimal grammar withoutsbracketing help.sWe now suggest that a representation that explicitly represents relations between phrase heads,ssuch as link grammars (Sleator and Temperley, 1991), is far more amenable to language acquisitionsproblems. Let us look one final time at the sequence V P N. There are only three words here,sand therefore three heads. Assuming a head-driven bigram mo</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Daniel D. K. Sleator and Davy Temperley. 1991. Parsing english with a link grammar. Technical Report CMU-CS-91-196, Carnegie Mellon University, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Bayesian Learning of Probabalistic Language Models</title>
<date>1994</date>
<tech>Ph.D. thesis</tech>
<institution>University of California at Berkeley</institution>
<location>Berkeley, CA</location>
<contexts>
<context>hey are in the same phrase,sbut because prepositional phrases often adjoin to noun phrases. Any greedy algorithm (such ass(Magerman and Marcus, 1990) and the context-free grammar induction method of (Stolcke, 1994))sthat builds phrases by grouping events with high mutual information will consequently fail to deriveslinguistically-plausible phrase structure in many situations.s3. INCORPORATING HEADEDNESS INTO L</context>
</contexts>
<marker>Stolcke, 1994</marker>
<rawString>Andreas Stolcke. 1994. Bayesian Learning of Probabalistic Language Models. Ph.D. thesis, University of California at Berkeley, Berkeley, CA.</rawString>
</citation>
</citationList>
</algorithm>
