table Integration of Speech and Vision in a small mobile robot
table Dominique ESTIVAL
table Department of Linguistics and Applied Linguistics
table University of Melbourne
table Parkville VIC 3052, Australia
email D.Estival@linguistics.unimelb.edu.au
sectionHeader Abstract
bodyText This paper reports on the integration of a
bodyText speech recognition component into a small
bodyText robot, J. Edgar, which was developed in the
bodyText Al Vision Lab at the University of
bodyText Melbourne. While the use of voice
bodyText commands was fairly easy to implement
bodyText the interaction of the voice commands with
bodyText the existing navigation system of the robot
bodyText turned out to pose a number of problems
bodyText Introduction
bodyText J. Edgar is a small autonomous mobile robot
bodyText developed in the AT Vision Lab at the
bodyText University of Melbourne, which is primarily
bodyText used as a platform for research in vision and
bodyText navigation. The project which we describe in
bodyText this paper consists in the addition of some
bodyText language capabilities to the existing system, in
bodyText particular the recognition of voice commands
bodyText and the integration of the speech recognition
bodyText component with the navigation system
bodyText While the vision and navigation work is mainly
bodyText carried out by Ph.D. students in Computer
bodyText Science, adding speech and language
bodyText capabilities to the J.Edgar robot has been a
bodyText collaborative project between the two
bodyText Departments of Computer Science and of
bodyText Linguistics and Applied Linguistics, and the
bodyText work has been performed by several linguistics
bodyText students hosted by the Computer Science
bodyText department and working in tandem with CS
bodyText students
bodyText The paper is organized as follows: section
bodyText ldescribes the capabilities and restrictions of
bodyText the robot J. Edgar, section 2 is an overview of
bodyText the speech recognition and language
bodyText understanding system we have added to the
bodyText robot, section 3 goes through the different
bodyText stages of the integration and section 4 briefly
bodyText describes the generation component
sectionHeader 1 Description of J. Edgar
subsectionHeader 1.1 Moving around
bodyText The J.Edgar robot is rather limited in the types
bodyText of movement it can perform. Its twin wheels
bodyText allow it to move forward in a straight line, and
bodyText to turn around, either right or left, up to 3600
bodyText but it cannot move backwards. Its speed can be
bodyText varied, but is usually kept very low to avoid
bodyText accidents
equation 1 . 2 Vision and Navigation
equation 1.2.1 Vision
bodyText The vision system of J.Edgar consists in a one
bodyText eye monochrome camera mounted on a small
bodyText frame with two independent drive wheels and a
bodyText pan head. Its spatial representation is two
bodyText dimensional and relies on edge detection
bodyText More specifically, it interprets discontinuities
bodyText as boundaries between surfaces, which
bodyText constitute obstacles
subsubsectionHeader 1.2.2 Navigation
bodyText The J.Edgar robot uses MYNORCA, a vision
bodyText based navigation system developed in the
listItem University Melbourne Al Vision Lab (Howard
listItem and Kitchen, 1997a, 1997b). This navigation
listItem system is divided into two levels
listItem The local navigation system uses visual
bodyText clues for obstacle detection and to form
bodyText local maps. It allows the robot to navigate
bodyText in its immediate environment and to reach
bodyText local goals without colliding with
bodyText obstacles. Most solid objects are
bodyText recognized as obstacles, but obstacles can
bodyText also be recognized as walls, corners or
bodyText doorways (see section 3.3
listItem The global navigation system detects
bodyText significant landmarks and uses a global
bodyText map to determine its location in the
bodyText environment. It allows the robot to reach
bodyText distant goals specified according to the
bodyText global map. The detection of landmarks
bodyText also requires a level of object recognition
page 109
bodyText and the interpretation of visual cues
bodyText needed at the local level
bodyText Figure 1 shows a series of snapshots for the
bodyText local and global navigation systems during a
bodyText given time period. Both systems are based on
bodyText the production of occupancy maps generated
bodyText by a visual mapping system based on the
bodyText detection of boundaries
bodyText This project has so far been able to interface
bodyText only with the vision-based navigation system at
bodyText the local level, but we hope we will soon be
bodyText able to extend it to the object recognition
bodyText aspect and interact with the global level
figure r--rrrrvrrrr-i
figure 
figure 4 Ini4-----i
figure 110 i
figure h 10
figure r
figure amp;apos;;.f
figure 4
figure 4
figure Ir. 44
figure II
figure 5 i
figure Nec
figure L,.........i..........,; .. .. t. .,..,.L
figure r .n 0 vK
figure F.----11.itrITirrrrx
figure 
figure amp;quot;,.,,,,..•,.........,..,,,,,,.-...,......,4
figure e r ,..ro.. r
figure amp;apos;4U3 ....K.,L4&amp;apos;;i&amp;apos;. 4
figure amp;amp;ol
figure t• r., r.,./..r.r.......„,..Ar.,&amp;quot;„,4
figure 
figure 4
figure 1
figureCaption Figure 1: The upper set of images is a series of snapshots of the local occupancy
bodyText map indicating the robots current location and path. The lower set of images is a
bodyText series of snapshots showing the evolution of the estimated global position
bodyText global pose estimate). The cross-hatched region indicates possible robot
bodyText locations in the global model. [from Howard &amp;amp; Kitchen 19974
page 110
bodyText The vision and navigation systems are installed
bodyText on a base-station which communicates via a
bodyText UHF data-link with the on-board computer
bodyText The on-board computer performs the low
bodyText level hardware functions and supports the
bodyText obstacle detection agent (see below
sectionHeader 2 Speech and Language
bodyText The first step towards integrating some sort of
bodyText Natural Language capabilities into the robot
bodyText was to install a speech recognition component
bodyText The second step was to develop a grammar to
bodyText analyze voice commands and to map those
bodyText commands onto the actual actions which the
bodyText robot can perform
bodyText In the next stage of the project, we are now
bodyText working towards the development of a
bodyText dialogue system, with which J.Edgar can
bodyText respond according to its internal status and
bodyText make appropriate answers to the voice
bodyText commands it recognizes. Until the speech
bodyText synthesizer component is fully incorporated
bodyText into the system, we are using canned speech
bodyText for the answers
bodyText The speech recognition system is installed on
bodyText the base-station and communicates with the
bodyText robot via the UHF modem
subsectionHeader 2.1 Speech Recognition
bodyText The main factor taken into consideration in
bodyText choosing an off-the-shelf speech recognition
bodyText system was the possibility of building an
bodyText application on top of it, and the IBM
bodyText VoiceType system was first chosen because of
bodyText the availability of development tools. Despite
bodyText some initial problems, these tools have proven
bodyText useful and have allowed us to develop our own
bodyText grammar and interface with the robot. We
bodyText have now migrated to the IBM ViaVoice Gold
bodyText system, which provides better speech
bodyText recognition performance and the same
bodyText development tools as VoiceType. In addition
bodyText ViaVoice includes a speech synthesizer, which
bodyText we are currently incorporating in our system
bodyText In the remainder of this paper, I will describe
bodyText the work that has been carried out using the
bodyText IBM VoiceType system and ported to the
bodyText ViaVoice system
bodyText The system is speaker-independent and so far
bodyText has been trained with more than 15 people
bodyText Care has been taken not to overtrain it with
bodyText any one particular person in order to maintain
bodyText speaker-independence
bodyText In general terms, the lexicon used in the
bodyText system maps onto the actions which the robot
bodyText can perform and the entities it can recognize
bodyText The lexicon is thus as limited as the world of
bodyText the robot, but it includes as many variant
bodyText lexical items as might be plausibly used (e.g
bodyText turn, rotate, spin etc. for TURN). These
bodyText actions and entities are described in section 3
bodyText The IBM VoiceType or ViaVoice system can
bodyText be used either as a dictation system with
bodyText discrete words, or in continuous speech mode
bodyText Taking advantage of the grammar
bodyText development tools, we are using it in
bodyText continuous mode, and the voice commands are
bodyText parsed by the grammar described in section
bodyText 2.2
subsectionHeader 2.2. Commands Grammar
bodyText In addition to the baseline word recognition
bodyText capability, the development tools in the IBM
bodyText VoiceType or ViaVoice systems all the
bodyText developer to write a BNF grammar for parsing
bodyText input strings of recognized words. We have
bodyText thus developed a grammar mapping voice
bodyText commands to the actions J. Edgar is capable of
bodyText performing
subsubsectionHeader 2.2.1. Semantics
bodyText Each item in the lexicon is annotated with an
bodyText amp;quot;annodata&amp;quot;, which can be thought of as its
bodyText semantic interpretation for this domain
bodyText Recognized input strings are thus transformed
bodyText into strings of &amp;quot;annodata&amp;quot;, which are further
bodyText parsed and sent to the communication
bodyText protocol. A command such as (1) will be
bodyText recognized as (2) and the string of annodata
listItem 3) will be then parsed to produce the
listItem sequence of commands (4
listItem 1) J. Edgar before turning left and
listItem moving forward please turn around
listItem 2) J.Edgar:&amp;quot;INITIALIZE&amp;quot; before:&amp;quot;INIT2&amp;quot
equation turning:&amp;quot;TURN&amp;quot; /eft:&amp;quot;LEFT&amp;quot
equation and:&amp;quot;SEQUENCE&amp;quot; moving :&amp;quot;MOVE&amp;quot
equation forward :&amp;quot;FORWARD&amp;quot; please :&amp;quot;INIT1&amp;quot
equation turn:&amp;quot;TURN&amp;quot; around:&amp;quot;BACKW ARDS&amp;quot
table 3) INITIALIZE INIT2 TURN LEFT
table SEQUENCE MOVE FORWARD
table INIT1 TURN BACKWARDS
table 4) INITIALIZE INIT1 TURN
table BACKWARDS INIT2 TURN LEFT
table SEQUENCE MOVE FORWARD
page 111
subsubsectionHeader 2.2.2. Syntactic analysis
bodyText All commands to the robot are in the
bodyText imperative. However, some structures for
bodyText complex commands have been implemented
bodyText These concern mainly the coordination of
bodyText commands and temporal sequence. As shown
bodyText in the example above, conjunctions such as
bodyText before and after will trigger the recognition of
bodyText a temporal sequence and the possible
bodyText reordering of the commands. Other
bodyText recognized constructions include
listItem 5) IF „.. COMMAND
listItem If there is a wall to your left, turn
listItem right and move forward
listItem 6) WHEN .... COMMAND
listItem When you get to a all, go along it
listItem 3. Integration
subsectionHeader 3.1. Movements only
bodyText In the first stage of this project, the natural
bodyText language system was only interfacing with the
bodyText movement commands of the robot, and not
bodyText with the navigation system (either local or
bodyText global). That is, the robot was either
bodyText performing in the voice command modality
bodyText or in the navigation modality. The main
bodyText reason for this limitation was that the
bodyText navigation system was still under development
bodyText and not robust enough to ensure safe
bodyText manoeuvering in case of voice commands
bodyText leading to potentially damaging situations
bodyText As a result, only commands relating to
bodyText movements (MOVE or TURN), and their
bodyText specifications (FORWARD, LEFT, RIGHT, and
bodyText specific distances) were understood and there
bodyText was no need for representing objects or
bodyText entities
bodyText 3.2. Low-level vision
bodyText In the second stage of the project, we only
bodyText integrated the language capabilities with the
bodyText low-level vision system of the local navigation
bodyText system. In practical terms this means that while
bodyText the robot can both accept spoken commands
bodyText and scan its environment, it can only recognize
bodyText local movement commands and will only obey
bodyText them if they do not lead to a collision
bodyText Thus, this stage also did not require the
bodyText addition of any semantic representation for
bodyText objects. However, to avoid a collision with an
bodyText obstacle, we need the local vision system for
bodyText obstacle recognition. We use the
bodyText amp;quot;careForward&amp;quot; function, which overrides the
bodyText default distance of 1 meter if there is an
bodyText obstacle in the path of the robot and ensures
bodyText that the robot will only move to a safe
bodyText distance from it
subsectionHeader 3.3. Local navigation
bodyText Further integration consists in issuing
bodyText commands that involve locations and objects
bodyText the robot knows about, as in (7
bodyText 7) Go down the corridor and go through
bodyText the first doorway on the right
bodyText This stage involves referring to objects and
bodyText entities recognized by the robot
bodyText There are five types of primitive objects in the
bodyText world which the robot can identify
listItem WALL
listItem a straight line
listItem DOORWAY
listItem a gap between two walls
listItem INSIDE CORNER (&amp;quot;in the corner&amp;quot
listItem two lines meeting at an angle and
listItem enclosing the robot
listItem OUTSIDE CORNER (&amp;quot;around the corner&amp;quot
listItem two lines meeting at an angle and
listItem going away from the robot
listItem LUMP
listItem a bounded solid object
listItem From combining these primitive objects, the
listItem robot can also create representations far
listItem complex objects
listItem INTERSECTION
listItem two outside corners that form an
listItem opening
listItem CORRIDOR
bodyText two parallel walls
bodyText Both types of objects can be used as referents
bodyText in commands and can be queried
bodyText It is worth emphasising that obstacles are not
bodyText recognized as a separate categorie, but are
bodyText either walls, lumps, corners, or doorways which
bodyText are not wide enough for the robot to pass
bodyText through
bodyText For instance, in Figure 2, the robot recognizes
bodyText an opening in the wall on its right and might
bodyText later recognize an outside corner to its left
bodyText i.12
bodyText The white area corresponds to the area the
bodyText robot has already recognized as being empty
bodyText and the black areas to recognized walls
bodyText 3.4. Global navigation
bodyText The next stage of the project is the integration
bodyText with the whole navigation system, including
bodyText the recognition of objects and locations. In
bodyText this mode, the robot will not only stop when
bodyText there is an obstacle, but will be able to decide
bodyText whether to try to go around it. The objects to
bodyText be used as referents will include locations such
bodyText as Office 214, Andrew&amp;apos;s office, or Corridor A
bodyText which have specific coordinates on the robot&amp;apos;s
bodyText global map. This is on-going work and we
bodyText hope to have achieved this level of integration
bodyText in the next few months
sectionHeader 4. Generation
bodyText In the meantime, the robot can return
bodyText information about its perception of the
bodyText environment, including the obstacles which
bodyText were recognized, and can ask for further
bodyText instructions. We have identified four situations
bodyText for the generation of questions by the robot
bodyText 1. when a command is not recognized
bodyText 2, when a command is incomplete
bodyText 3. when a command cannot be completed
bodyText 4. when an object referred to in a command
bodyText cannot be located
bodyText The first and second situations only require
bodyText input from the speech recognition system
bodyText including the mapping to robot commands
bodyText However, the third situation requires access to
bodyText the local navigation system, or at least to
bodyText obstacle detection, and the fourth situation
bodyText requires access to either the local or global
bodyText navigation system, depending on whether the
bodyText object is a primitive object or whether it
bodyText requires coordinates on the global map. In
bodyText these last two situations, the generation of
bodyText questions by the robot involves a mapping
bodyText between the robot&amp;apos;s internal representations of
bodyText the recognized environment and the actual
bodyText expressions used both in the commands and in
bodyText returning answers
bodyText Conclusion
bodyText While this project has been a successful
bodyText collaboration between vision-based navigation
bodyText and natural language processing, the J.Edgar
bodyText robot is still far from having achieved a
bodyText convincing level of speech understanding
bodyText Some of the challenges of such a project
bodyText reside in the successful communication
bodyText between the speech recognition system and the
bodyText robot, but the more interesting aspect is that of
bodyText the correspondence between the entities used
bodyText by the navigation system and the phrases
bodyText recognized by the speech system
bodyText Since the speech system is independent of the
bodyText physical robot, it can be interfaced with a
bodyText number of robots. One of the extensions of
bodyText this project is to install a natural language
bodyText interface for some of the other robots being
bodyText built in the Al lab and eventually to use the
bodyText same natural language interface with more
bodyText than one robot at a time
sectionHeader Acknowledgments
bodyText We thank Leon Sterling and Liz Sonnenberg
bodyText for the support of the Computer Science
bodyText Department for this project, Andrew Howard
bodyText for letting us use J.Edgar and for his help and
bodyText advice throughout, Elise Dettman, Meladel
bodyText Mistica and John Moore for their enthusiasm
bodyText and dedication, and all the people in the AI
bodyText Vision Lab for their help
sectionHeader References
reference Colleen Crangle and Patrick Suppes (1994
reference Language and learning for robots. CSLI lecture
reference notes 41. Stanford: CSLI
reference Andrew Howard and Les Kitchen (1997a). Vision
reference Based navigation Using Natural Landmarks
reference FSR&amp;apos;97 International Conference on Field and
reference Service Robotics. Canberra, Australia
reference Andrew Howard and Les Kitchen (1997b). Fast Visual
reference mapping for Mobile Robot Navigation, ICIPS&amp;apos;97
reference IEEE International Conference on Intelligent
reference Processing Systems, Beijing
page 113
