<?xml version="1.0" encoding="UTF-8"?>
<result>
<algorithm name="SectLabel" version="090625" confidence="0.131751">
<title confidence="0.995095">
A Similarity Measure for Motion Stream
Segmentation and Recognition*
</title>
<author confidence="0.995704">
Chuanjun Li B. Prabhakaran
</author>
<affiliation confidence="0.999644">
Department of Computer Science
The University of Texas at Dallas, Richardson, TX 75083
</affiliation>
<email confidence="0.977671">
{chuanjun, praba}@utdallas.edu
</email>
<sectionHeader confidence="0.792243">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999820733333333">
Recognition of motion streams such as data streams gener-
ated by different sign languages or various captured human
body motions requires a high performance similarity mea-
sure. The motion streams have multiple attributes, and mo-
tion patterns in the streams can have different lengths from
those of isolated motion patterns and different attributes
can have different temporal shifts and variations. To ad-
dress these issues, this paper proposes a similarity measure
based on singular value decomposition (SVD) of motion ma-
trices. Eigenvector differences weighed by the corresponding
eigenvalues are considered for the proposed similarity mea-
sure. Experiments with general hand gestures and human
motion streams show that the proposed similarity measure
gives good performance for recognizing motion patterns in
the motion streams in real time.
</bodyText>
<sectionHeader confidence="0.970362333333333">
Categories and Subject Descriptors: H.2.8 [Database
Management]: Database Applications – Data Mining
General Terms: Algorithm
Keywords: Pattern recognition, gesture, data streams, seg-
mentation, singular value decomposition.
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.998481">
Motion streams can be generated by continuously per-
formed sign language words [14] or captured human body
motions such as various dances. Captured human motions
can be applied to the movie and computer game industries
by reconstructing various motions from video sequences [10]
or images [15] or from motions captured by motion capture
systems [4]. Recognizing motion patterns in the streams
with unsupervised methods requires no training process, and
is very convenient when new motions are expected to be
added to the known pattern pools. A similarity measure
with good performance is thus necessary for segmenting and
recognizing the motion streams. Such a similarity measure
needs to address some new challenges posed by real world
</bodyText>
<footnote confidence="0.952497">
*Work supported partially by the National Science Founda-
tion under Grant No. 0237954 for the project CAREER:
Animation Databases.
</footnote>
<copyright confidence="0.997132571428572">
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$5.00.
</copyright>
<bodyText confidence="0.99972">
motion streams: first, the motion patterns have dozens of at-
tributes, and similar patterns can have different lengths due
to different motion durations; second, different attributes of
similar motions have different variations and different tem-
poral shifts due to motion variations; and finally, motion
streams are continuous, and there are no obvious ”pauses”
between neighboring motions in a stream. A good similarity
measure not only needs to capture the similarity of complete
motion patterns, but also needs to capture the differences
between complete motion patterns and incomplete motion
patterns or sub-patterns in order to segment a stream for
motion recognition.
As the main contribution of this paper, we propose a sim-
ilarity measure to address the above issues. The proposed
similarity measure is defined based on singular value decom-
position of the motion matrices. The first few eigenvectors
are compared for capturing the similarity of two matrices,
and the inner products of the eigenvectors are given differ-
ent weights for their different contributions. We propose to
use only the eigenvalues corresponding to the involved eigen-
vectors of the two motion matrices as weights. This simple
and intuitive weighing strategy gives the same importance to
eigenvalues of the two matrices. We also show that the 95%
variance rule for choosing the number of eigenvectors [13] is
not sufficient for recognizing both isolated patterns and mo-
tion streams. Our experiments demonstrate that at least the
first 6 eigenvectors need to be considered for motion streams
of either 22 attribute or 54 attributes, and the first 6 eigen-
values accounts for more than 99.5% of the total variance in
the motion matrices.
</bodyText>
<sectionHeader confidence="0.999927">
2. RELATED WORK
</sectionHeader>
<bodyText confidence="0.9992420625">
Multi-attribute pattern similarity search, especially in con-
tinuous motion streams, has been widely studied for sign
language recognition and for motion synthesis in computer
animation. The recognition methods usually include tem-
plate matching by distance measures and hidden Markov
models (HMM).
Template matching by using similarity/distance measures
has been employed for multi-attribute pattern recognition.
Joint angles are extracted in [11] as features to represent dif-
ferent human body static poses for the Mahalanobis distance
measure of two joint angle features. Similarly, momentum,
kinetic energy and force are constructed in [2,5] as activ-
ity measure and prediction of gesture boundaries for various
segments of the human body, and the Mahalanobis distance
function of two composite features are solved by dynamic
programming.
</bodyText>
<page confidence="0.998646">
89
</page>
<bodyText confidence="0.999882">
Similarity measures are defined for multi-attribute data
in [6,12,16] based on principal component analysis (PCA).
Inner products or angular differences of principal compo-
nents (PCs) are considered for similarity measure defini-
tions, with different weighted strategies for different PCs.
Equal weights are considered for different combinations of
PCs in [6], giving different PCs equal contributions to the
similarity measure. The similarity measure in [12] takes the
minimum of two weighted sums of PC inner products, and
the two sums are respectively weighted by different weights.
A global weight vector is obtained by taking into account all
available isolated motion patterns in [16], and this weight
vector is used for specifying different contributions from dif-
ferent PC inner products to the similarity measure Eros.
The dominating first PC and a normalized eigenvalue vector
are considered in [7,8] for pattern recognition. In contrast,
this paper propose to consider the first few PCs, and the
angular differences or inner products of different PCs are
weighted by different weights which depends on the data
variances along the corresponding PCs.
The HMM technique has been widely used for sign lan-
guage recognition, and different recognition rates have been
reported for different sign languages and different feature se-
lection approaches. Starner et al. [14] achieved 92% and 98%
word accuracy respectively for two systems, the first of the
systems used a camera mounted on a desk and the second
one used a camera in a user’s cap for extracting features
as the input of HMM. Similarly Liang and Ouhyoung [9]
used HMM for postures, orientations and motion primitives
as features extracted from continuous Taiwan sign language
streams and an average 80.4% recognition rate was achieved.
In contrast, the approach proposed in this paper is an un-
supervised approach, and no training as required for HMM
recognizers is needed.
</bodyText>
<sectionHeader confidence="0.9995155">
3. SIMILARITY MEASURE FOR MOTION
STREAM RECOGNITION
</sectionHeader>
<bodyText confidence="0.999820857142857">
The joint positional coordinates or joint angular values of
a subject in motion can be represented by a matrix: the
columns or attributes of the matrix are for different joints,
and the rows or frames of the matrix are for different time
instants. Similarity of two motions is the similarity of the
resulting motion matrices, which have the same number of
attributes or columns, and yet can have different number
of rows due to different motion durations. To capture the
similarity of two matrices of different lengths, we propose
to apply singular value decomposition (SVD) to the motion
matrices in order to capture the similarity of the matrix
geometric structures. Hence we briefly present SVD and its
associated properties below before proposing the similarity
measure based on SVD in this section.
</bodyText>
<subsectionHeader confidence="0.998794">
3.1 Singular Value Decomposition
</subsectionHeader>
<bodyText confidence="0.999279">
The geometric structure of a matrix can be revealed by
the SVD of the matrix. As shown in [3], any real m x n
matrix A can be decomposed into A = UEVT , where U =
[u1, u2, ... , um] E Rm×m and V = [v1, v2, ... , vn] E RnXn
are two orthogonal matrices, and E is a diagonal matrix with
diagonal entries being the singular values of A: v1 &amp;gt; v2 &amp;gt;
. . . &amp;gt; Qmin(m,n) &amp;gt; 0. Column vectors ui and vi are the ith
left and right singular vectors of A, respectively.
It can be shown that the right singular vectors of the sym-
metric n x n matrix M = AT A are identical to the corre-
sponding right singular vectors of A, referred to as eigenvec-
tors of M. The singular values of M, or eigenvalues of M,
are squares of the corresponding singular values of A. The
eigenvector with the largest eigenvalue gives the first prin-
cipal component. The eigenvector with the second largest
eigenvalue is the second principal component and so on.
</bodyText>
<subsectionHeader confidence="0.999275">
3.2 Similarity Measure
</subsectionHeader>
<bodyText confidence="0.998884136363636">
Since SVD exposes the geometric structure of a matrix, it
can be used for capturing the similarity of two matrices. We
can compute the SVD of M = AT A instead of computing
the SVD of A to save computational time. The reasons are
that the eigenvectors of M are identical to the corresponding
right singular vectors of A, the eigenvalues of M are the
squares of the corresponding singular values of A, and SVD
takes O(n 3) time for the n x n M and takes O(mn2) time
with a large constant for the m x n A, and usually m &amp;gt; n.
Ideally, if two motions are similar, their corresponding
eigenvectors should be parallel to each other, and their cor-
responding eigenvalues should also be proportional to each
other. This is because the eigenvectors are the correspond-
ing principal components, and the eigenvalues reflect the
variances of the matrix data along the corresponding prin-
cipal components. But due to motion variations, all corre-
sponding eigenvectors cannot be parallel as shown in Fig-
ure 1. The parallelness or angular differences of two eigen-
vectors u and v can be described by the absolute value of
their inner products: l cosOl = lu • vl/(lullvl) = lu • vl, where
lul = lvl = 1. We consider the absolute value of the in-
ner products because eigenvectors can have different signs
as shown in [8].
Since eigenvalues are numerically related to the variances
of the matrix data along the associated eigenvectors, the im-
portance of the eigenvector parallelness can be described by
the corresponding eigenvalues. Hence, eigenvalues are to be
used to give different weights to different eigenvector pairs.
Figure 2 shows that the first eigenvalues are the dominat-
ing components of all the eigenvalues, and other eigenval-
ues become smaller and smaller and approach zero. As the
eigenvalues are close to zero, their corresponding eigenvec-
tors can be very different even if two matrices are similar.
Hence not all the eigenvectors need to be incorporated into
the similarity measure.
Since two matrices have two eigenvalues for the corre-
sponding eigenvector pair, these two eigenvalues should have
equal contributions or weights to the eigenvector parallel-
ness. In addition, the similarity measure of two matrices
should be independent to other matrices, hence only eigen-
vectors and eigenvalues of the two matrices should be con-
sidered.
Based on the above discussions, we propose the following
similarity measure for two matrices Q and P:
</bodyText>
<equation confidence="0.99452025">
k
1
(Q, P) =
2 i=1
</equation>
<bodyText confidence="0.998525666666667">
where vi and Ai are the ith eigenvalues corresponding to the
ith eigenvectors ui and vi of square matrices of Q and P,
respectively, and 1 &amp;lt; k &amp;lt; n. Integer k determines how many
eigenvectors are considered and it depends on the number
of attributes n of motion matrices. Experiments with hand
gesture motions (n = 22) and human body motions (n =
</bodyText>
<equation confidence="0.992538428571429">
n
((�i/
i=1
n
Qi+Ai/
i=1
Ai)lui •vil)
</equation>
<page confidence="0.994436">
90
</page>
<figureCaption confidence="0.9768215">
Figure 1: Eigenvectors of similar patterns. The first
eigenvectors are similar to each other, while other
eigenvectors, such as the second vectors shown in
the bottom, can be quite different.
</figureCaption>
<bodyText confidence="0.995860636363636">
54) in Section 4 show that k = 6 is large enough without
loss of pattern recognition accuracy in streams. We refer to
this non-metric similarity measure as k Weighted Angular
Similarity (kWAS) , which captures the angular similarities
of the first k corresponding eigenvector pairs weighted by
the corresponding eigenvalues.
It can be easily verified that the value of kWAS ranges over
[0,1]. When all corresponding eigenvectors are normal to
each other, the similarity measure will be zero, and when two
matrices are identical, the similarity measure approaches the
maximum value one if k approaches n.
</bodyText>
<subsectionHeader confidence="0.999515">
3.3 Stream Segmentation Algorithm
</subsectionHeader>
<bodyText confidence="0.98990925">
In order to recognize motion streams, we assume one mo-
tion in a stream has a minimum length l and a maximum
length L. The following steps can be applied to incremen-
tally segment a stream for motion recognition:
</bodyText>
<listItem confidence="0.999508666666666">
1. SVD is applied to all isolated motion patterns P to
obtain their eigenvectors and eigenvalues. Let S be
the incremented stream length for segmentation, and
let L be the location for segmentation. Initially L = l.
2. Starting from the beginning of the stream or the end of
the previously recognized motion, segment the stream
at location L. Compute the eigenvectors and eigenval-
ues of the motion segment Q.
3. Compute kWAS between Q and all motion patterns
</listItem>
<bodyText confidence="0.953602">
P. Update T... to be the highest similarity after the
previous motion’s recognition.
</bodyText>
<listItem confidence="0.937651">
4. If L+S &amp;lt; L, update L = L+S and go to step 2. Other-
</listItem>
<bodyText confidence="0.9922445">
wise, the segment corresponding to T,r,.. is recognized
to be the motion pattern which gives the highest simi-
larity T..., update L = l starting from the end of the
last recognized motion pattern and go to step 2.
</bodyText>
<figure confidence="0.994985666666667">
100
99
95
93
91
89
87
1 2 3 4 5 6 7 8
Number of Eigenvalues
</figure>
<figureCaption confidence="0.98678575">
Figure 2: Accumulated eigenvalue percentages in
total eigenvalues for CyberGlove data and captured
human body motion data. There are 22 eigenvalues
for the CyberGlove data and 54 eigenvalues for the
captured motion data. The sum of the first 2 eigen-
values is more than 95% of the corresponding total
eigenvalues, and the sum of the first 6 eigenvalues is
almost 100% of the total eigenvalues.
</figureCaption>
<sectionHeader confidence="0.994749">
4. PERFORMANCE EVALUATION
</sectionHeader>
<bodyText confidence="0.999741666666667">
This section evaluates experimentally the performances
of the similarity measure kWAS proposed in this paper. It
has been shown in [16] that Eros [16] outperforms other
similarity measures mentioned in Section 2 except MAS [8].
Hence in this section, we compare the performances of the
proposed kWAS with Eros and MAS for recognizing similar
isolated motion patterns and for segmenting and recognizing
motion streams from hand gesture capturing CyberGlove
and human body motion capture system.
</bodyText>
<subsectionHeader confidence="0.998841">
4.1 Data Generation
</subsectionHeader>
<bodyText confidence="0.999626142857143">
A similarity measure should be able to be used not only
for recognizing isolated patterns with high accuracy, but also
for recognizing patterns in continuous motions or motion
streams. Recognizing motion streams is more challenging
than recognizing isolated patterns. This is because many
very similar motion segments or sub-patterns needs to be
compared in order to find appropriate segmentation loca-
tions, and a similarity measure should capture the difference
between a complete motion or pattern and its sub-patterns.
Hence, both isolated motion patterns and motion streams
were generated for evaluating the performance of kWAS.
Two data sources are considered for data generation: a Cy-
berGlove for capturing hand gestures and a Vicon motion
capture system for capturing human body motions.
</bodyText>
<subsubsectionHeader confidence="0.99888">
4.1.1 CyberGlove Data
</subsubsectionHeader>
<bodyText confidence="0.997439769230769">
A CyberGlove is a fully instrumented data glove that pro-
vides 22 sensors for measuring hand joint angular values to
capture motions of a hand, such as American Sign Language
(ASL) words for hearing impaired. The data for a hand ges-
ture contain 22 angular values for each time instant/frame,
one value for a joint of one degree of freedom. The mo-
tion data are extracted at around 120 frames per second.
Data matrices thus have 22 attributes for the CyberGlove
motions.
One hundred and ten different isolated motions were gen-
erated as motion patterns, and each motion was repeated
for three times, resulting in 330 isolated hand gesture mo-
tions. Some motions have semantic meanings. For example,
</bodyText>
<figure confidence="0.999270347826087">
2 4 6 8 10 12 14 16 18 20 22
Motion341
Motion342
Motion341
Motion342
2 4 6 8 10 12 14 16 18 20 22
Component of Second Eigenvector
0.6 0.4 0.2 0 −0.2 −0.4
0.2
0.1
0
−0.1
−0.2
−0.3
−0.4
−0.5
−0.6
−0.7
Component of First Eigenvector
97
85
CyberGlove Data
MoCap Data
</figure>
<page confidence="0.998826">
91
</page>
<bodyText confidence="0.996644857142857">
the motion for BUS as shown in Table 1 is for the ASL sign
”bus”. Yet for segmentation and recognition, we only re-
quire that each individual motion be different from others,
and thus some motions are general motions, and do not have
any particular semantic meanings, such as the THUMBUP
motion in Table 1.
The following 18 motions shown in Table 1 were used to
generate continuous motions or streams. Twenty four dif-
ferent motion streams were generated for segmentation and
recognition purpose. There are 5 to 10 motions in a stream
and 150 motions in total in 24 streams, with 6.25 motions in
a stream on average. It should be noted that variable-length
transitional noises occur between successive motions in the
generated streams.
</bodyText>
<tableCaption confidence="0.996027">
Table 1: Individual motions used for streams
</tableCaption>
<table confidence="0.968861333333333">
35 60 70 80 90 BUS GOODBYE
HALF IDIOM JAR JUICE KENNEL KNEE
MILK TV SCISSOR SPREAD THUMBUP
</table>
<subsubsectionHeader confidence="0.994957">
4.1.2 Motion Capture Data
</subsubsectionHeader>
<bodyText confidence="0.998034285714286">
The motion capture data come from various motions cap-
tured collectively by using 16 Vicon cameras and the Vicon
iQ Workstation software. A dancer wears a suit of non-
reflective material and 44 markers are attached to the body
suit. After system calibration and subject calibration, global
coordinates and rotation angles of 19 joints/segments can
be obtained at about 120 frames per second for any mo-
tion. Similarity of patterns with global 3D positional data
can be disguised by different locations, orientations or differ-
ent paths of motion execution as illustrated in Figure 3(a).
Since two patterns are similar to each other because of sim-
ilar relative positions of corresponding body segments at
corresponding time, and the relative positions of different
segments are independent of locations or orientations of the
body, we can transform the global position data into local
position data as follows.
Let Xp, Yp, Zp be the global coordinates of one point on
pelvis, the selected origin of the ”moving” local coordinate
system, and a,,3, -y be the rotation angles of the pelvis seg-
ment relative to the global coordinate system axes, respec-
tively. The translation matrix is T as follows:
</bodyText>
<equation confidence="0.995540615384616">
1 0 0 0
0 1 0 0
0 0 1 0
—Xp —Yp —Zp 1
The rotation matrix R = R. x Ry x Rz, where
1 0 0 0
0 cos a — sin a 0
0 sin a cos a 0
0 0 0 1
cos,3 0 sin,3 0
0 1 0 0
—sin,3 0 cos,3 0
0 0 0 1
</equation>
<figure confidence="0.9927465">
Motion Capture Frames Motion Capture Frames
(a) (b)
</figure>
<figureCaption confidence="0.990241">
Figure 3: 3D motion capture data for similar motions
executed at different locations and in different orien-
tations: (a) before transformation; (b) after transfor-
mation.
</figureCaption>
<equation confidence="0.979892">
cos-y —sin -y 0 0
sin -y cos -y 0 0
0 0 1 0
0 0 0 1
</equation>
<bodyText confidence="0.976833666666667">
Let X, Y, Z be the global coordinates of one point on any
segments, and x, y, z be the corresponding transformed local
coordinates. x, y and z can be computed as follows:
</bodyText>
<equation confidence="0.935201">
[x y z 1]=[X Y Z 1] x T x R
</equation>
<bodyText confidence="0.99886252631579">
The transformed data are positions of different segments
relative to a moving coordinate system with the origin at
some fixed point of the body, for example the pelvis. The
moving coordinate system is not necessarily aligned with
the global system, and it can rotate with the body. So data
transformation includes both translation and rotation, and
the transformed data would be translation and rotation in-
variant as shown in Figure 3(b). The coordinates of the
origin pelvis are not included, thus the transformed matri-
ces have 54 columns.
Sixty two isolated motions including Taiqi, Indian dances,
and western dances were performed for generating motion
capture data, and each motion was repeated 5 times, yield-
ing 310 isolated human motions. Every repeated motion has
a different location and different durations, and can face
different orientations. Twenty three motion streams were
generated for segmentation. There are 3 to 5 motions in
a stream, and 93 motions in total in 23 streams, with 4.0
motions in a stream on average.
</bodyText>
<subsectionHeader confidence="0.997467">
4.2 Performance of kWAS for Capturing Sim-
ilarities and Segmenting Streams
</subsectionHeader>
<bodyText confidence="0.9998221">
We first apply kWAS to isolated motion patterns to show
that the proposed similarity measure kWAS can capture the
similarities of isolated motion patterns. Then kWAS is ap-
plied to motion streams for segmenting streams and recog-
nizing motion patterns in the streams. We experimented
with different k values in order to find out the smallest k
without loss of good performance.
Figure 2 shows the accumulated eigenvalue percentages
averaged on 330 hand gestures and 310 human motions, re-
spectively. Although the first two eigenvalues account for
</bodyText>
<figure confidence="0.995311875">
1500
1000
500
0
−500
−1000
−1500
0 50 100 150 200 250 300 350 400 450
−1000
0 50 100 150 200 250 300 350 400 450
1000
500
0
−500
2000
1500
1000
500
0
0 50 100 150 200 250 300 350 400 450
−1000
0 50 100 150 200 250 300 350 400 450
1000
500
0
−500
T=
R, =
Ry =
Rz =
92
Number of Nearest Neghbors (Most Simlar Patterns)
</figure>
<figureCaption confidence="0.9924826">
Figure 4: Recognition rate of similar CyberGlove
motion patterns. When k is 3, kWAS can find the
most similar motions for about 99.7% of 330 mo-
tions, and can find the second most similar motions
for 97.5% of the them.
</figureCaption>
<figure confidence="0.956223">
Number of Nearest Neighbors (Most aimilar Patterns1
</figure>
<figureCaption confidence="0.986774">
Figure 5: Recognition rate of similar captured mo-
tion patterns. When k is 5, by using kWAS, the most
similar motions of all 310 motions can be found, and
the second most similar motions of 99.8% of the 310
motions can also be found.
</figureCaption>
<bodyText confidence="0.99718804">
more than 95% of the respective sums of all eigenvalues,
considering only the first two eigenvectors for kWAS is not
sufficient as shown in Figure 4 and Figure 5. For Cyber-
Glove data with 22 attributes, kWAS with k = 3 gives the
same performance as kWAS with k = 22, and for motion
capture data with 54 attributes, kWAS with k = 5 gives the
same performance as kWAS with k = 54. Figure 4 and Fig-
ure 5 illustrate that kWAS can be used for finding similar
motion patterns and outperforms MAS and Eros for both
hand gesture and human body motion data.
The steps in Section 3.3 are used for segmenting streams
and recognizing motions in streams. The recognition accu-
racy as defined in [14] is used for motion stream recognition.
The motion recognition accuracies are shown in Table 2. For
both CyberGlove motion and captured motion data, k = 6
is used for kWAS, which gives the same accuracy as k = 22
for CyberGlove data and k = 54 for motion capture data,
respectively.
Figure 6 shows the time taken for updating the candi-
date segment, including updating the matrix, computing the
SVD of the updated matrix, and computing the similarities
of the segment and all motion patterns. The code imple-
mented in C++ was run on one 2.70 GHz Intel processor
of a GenuineIntel Linux box. There are 22 attributes for
the CyberGlove streams, and 54 attributes for the captured
</bodyText>
<figureCaption confidence="0.987666">
Figure 6: Computation time for stream segment up-
date and similarity computation.
</figureCaption>
<tableCaption confidence="0.981573">
Table 2: Stream Pattern Recognition Accuracy (%)
</tableCaption>
<table confidence="0.9976814">
Similarity Measures CyberGlove Motion Capture
Streams Streams
Eros 68.7 78.5
MAS 93.3 78.5
kWAS (k=6) 94.0 94.6
</table>
<bodyText confidence="0.996042">
motion streams. Hence updating captured motion segments
takes longer than updating CyberGlove motion segments as
shown in Figure 6. The time required by kWAS is close to
the time required by MAS, and is less than half of the time
taken by using Eros.
</bodyText>
<subsectionHeader confidence="0.997928">
4.3 Discussions
</subsectionHeader>
<bodyText confidence="0.998949956521739">
kWAS captures the similarity of square matrices of two
matrices P and Q, yet the temporal order of pattern execu-
tion is not revealed in the square matrices. As shown in [7],
two matrices with the identical row vectors in different or-
ders have identical eigenvectors and identical eigenvalues. If
different temporal orders of pattern execution yield patterns
with different semantic meanings, we need to further con-
sider the temporal execution order, which is not reflected in
the eigenvectors and eigenvalues and has not been consid-
ered previously in [6,12,16].
Since the first eigenvectors are close or parallel for similar
patterns, we can project pattern A onto its first eigenvector
ul by Aul. Then similar patterns would have similar projec-
tions (called projection vectors hereafter), showing similar
temporal execution orders while the projection variations
for each pattern can be maximized. The pattern projection
vectors can be compared by computing their dynamic time
warping (DTW) distances, for DTW can align sequences
of different lengths and can be solved easily by dynamic
programming [1]. Incorporating temporal order information
into the similarity measure can be done as for MAS in [7]
if motion temporal execution orders cause motion pattern
ambiguity to kWAS.
</bodyText>
<sectionHeader confidence="0.999959">
5. CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.9983472">
This paper has proposed a similarity measure kWAS for
motion stream segmentation and motion pattern recogni-
tion. kWAS considers the first few k eigenvectors and com-
putes their angular similarities/differences, and weighs con-
tributions of different eigenvector pairs by their correspond-
</bodyText>
<figure confidence="0.999500215686275">
100
99
98
97
96
95
94
93
92
91
90
1 2
kWAS (k = 22)
kWAS (k = 5)
kWAS (k = 3)
kWAS (k = 2)
MAS
EROS
99.5
98.5
97.5
96.5
95.5
100
99
98
97
96
95
123 4
kWna (k = 541
kWna (k = 51
kWna (k = 41
kWna (k = 31
Mna
EROa
20
18
16
14
12
10
8
6
4
2
0
CyberGlove Streams Motion Capture Streams
MAS
kWAS (k = 6)
EROS
</figure>
<page confidence="0.998647">
93
</page>
<bodyText confidence="0.9966485">
ing eigenvalues. Eigenvalues from two motion matrices are
given equal importance to the weights. Experiments with
CyberGlove hand gesture streams and captured human body
motions such as Taiqi and dances show that kWAS can rec-
ognize 100% most similar isolated patterns and can recog-
nize 94% motion patterns in continuous motion streams.
</bodyText>
<sectionHeader confidence="0.997614">
6. REFERENCES
</sectionHeader>
<reference confidence="0.999748968253968">
[1] D. Berndt and J. Clifford. Using dynamic time
warping to find patterns in time series. In AAAI-94
Workshop on Knowledge Discovery in Databases,
pages 229–248, 1994.
[2] V. M. Dyaberi, H. Sundaram, J. James, and G. Qian.
Phrase structure detection in dance. In Proceedings of
the ACM Multimedia Conference 2004, pages 332–335,
Oct. 2004.
[3] G. H. Golub and C. F. V. Loan. Matrix Computations.
The Johns Hopkins University Press,
Baltimore,Maryland, 1996.
[4] L. Ikemoto and D. A. Forsyth. Enriching a motion
collection by transplanting limbs. In Proceedings of the
2004 ACM SIGGRAPH/Eurographics symposium on
Computer animation, pages 99 – 108, 2004.
[5] K. Kahol, P. Tripathi, S. Panchanathan, and
T. Rikakis. Gesture segmentation in complex motion
sequences. In Proceedings of IEEE International
Conference on Image Processing, pages II – 105–108,
Sept. 2003.
[6] W. Krzanowski. Between-groups comparison of
principal components. J. Amer. Stat. Assoc.,
74(367):703–707, 1979.
[7] C. Li, B. Prabhakaran, and S. Zheng. Similarity
measure for multi-attribute data. In Proceedings of the
2005 IEEE International Conference on Acoustics,
Speach, and Signal Processing (ICASSP), Mar. 2005.
[8] C. Li, P. Zhai, S.-Q. Zheng, and B. Prabhakaran.
Segmentation and recognition of multi-attribute
motion sequences. In Proceedings of the ACM
Multimedia Conference 2004, pages 836–843, Oct.
2004.
[9] R. H. Liang and M. Ouhyoung. A real-time continuous
gesture recognition system for sign language. In
Proceedings of the 3rd. International Conference on
Face and Gesture Recognition, pages 558–565, 1998.
[10] K. Pullen and C. Bregler. Motion capture assisted
animation: texturing and synthesis. In SIGGRAPH,
pages 501–508, 2002.
[11] G. Qian, F. Guo, T. Ingalls, L. Olson, J. James, and
T. Rikakis. A gesture-driven multimodal interactive
dance system. In Proceedings of IEEE International
Conference on Multimedia and Expo, June 2004.
[12] C. Shahabi and D. Yan. Real-time pattern isolation
and recognition over immersive sensor data streams.
In Proceedings of the 9th International Conference on
Multi-Media Modeling, pages 93–113, Jan 2003.
[13] A. Singhal and D. E. Seborg. Clustering of
multivariate time-series data. In Proceedings of the
American Control Conference, pages 3931–3936, 2002.
[14] T. Starner, J. Weaver, and A. Pentland. Real-time
american sign language recognition using desk and
wearable computer based video. IEEE Transactions
on Pattern Analysis and Machine Intelligence,
20(12):1371–1375, 1998.
[15] C. J. Taylor. Reconstruction of articulated objects
from point correspondences in a single image.
Computer Vision and Image Understanding,
80(3):349–363, 2000.
[16] K. Yang and C. Shahabi. A PCA-based similarity
measure for multivariate time series. In Proceedings of
the Second ACM International Workshop on
Multimedia Databases, pages 65–74, Nov. 2004.
</reference>
<page confidence="0.999801">
94
</page>
</algorithm>
</result>