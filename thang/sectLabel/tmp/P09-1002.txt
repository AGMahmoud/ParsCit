title ||| Investigations on Word Senses and Word Usages
author ||| Katrin Erk	Diana McCarthy	Nicholas Gaylord
affiliation ||| University of Texas at Austin	University of Sussex	University of Texas at Austin
email ||| katrin.erk@mail.utexas.edu	dianam@sussex.ac.uk	nlgaylord@mail.utexas.edu
sectionHeader ||| Abstract
bodyText ||| The vast majority of work on word senses
bodyText ||| has relied on predefined sense invento-
bodyText ||| ries and an annotation schema where each
bodyText ||| word instance is tagged with the best fit-
bodyText ||| ting sense. This paper examines the case
bodyText ||| for a graded notion of word meaning in
bodyText ||| two experiments, one which uses WordNet
bodyText ||| senses in a graded fashion, contrasted with
bodyText ||| the “winner takes all” annotation, and one
bodyText ||| which asks annotators to judge the similar-
bodyText ||| ity of two usages. We find that the graded
bodyText ||| responses correlate with annotations from
bodyText ||| previous datasets, but sense assignments
bodyText ||| are used in a way that weakens the case for
bodyText ||| clear cut sense boundaries. The responses
bodyText ||| from both experiments correlate with the
bodyText ||| overlap of paraphrases from the English
bodyText ||| lexical substitution task which bodes well
bodyText ||| for the use of substitutes as a proxy for
bodyText ||| word sense. This paper also provides two
bodyText ||| novel datasets which can be used for eval-
bodyText ||| uating computational systems.
sectionHeader ||| 1 Introduction
bodyText ||| The vast majority of work on word sense tag-
bodyText ||| ging has assumed that predefined word senses
bodyText ||| from a dictionary are an adequate proxy for the
bodyText ||| task, although of course there are issues with
bodyText ||| this enterprise both in terms of cognitive valid-
bodyText ||| ity (Hanks, 2000; Kilgarriff, 1997; Kilgarriff,
bodyText ||| 2006) and adequacy for computational linguis-
bodyText ||| tics applications (Kilgarriff, 2006). Furthermore,
bodyText ||| given a predefined list of senses, annotation efforts
bodyText ||| and computational approaches to word sense dis-
bodyText ||| ambiguation (WSD) have usually assumed that one
bodyText ||| best fitting sense should be selected for each us-
bodyText ||| age. While there is usually some allowance made
bodyText ||| for multiple senses, this is typically not adopted by
bodyText ||| annotators or computational systems.
bodyText ||| Research on the psychology of concepts (Mur-
bodyText ||| phy, 2002; Hampton, 2007) shows that categories
bodyText ||| in the human mind are not simply sets with clear-
bodyText ||| cut boundaries: Some items are perceived as
bodyText ||| more typical than others (Rosch, 1975; Rosch and
bodyText ||| Mervis, 1975), and there are borderline cases on
bodyText ||| which people disagree more often, and on whose
bodyText ||| categorization they are more likely to change their
bodyText ||| minds (Hampton, 1979; McCloskey and Glucks-
bodyText ||| berg, 1978). Word meanings are certainly related
bodyText ||| to mental concepts (Murphy, 2002). This raises
bodyText ||| the question of whether there is any such thing as
bodyText ||| the one appropriate sense for a given occurrence.
bodyText ||| In this paper we will explore using graded re-
bodyText ||| sponses for sense tagging within a novel annota-
bodyText ||| tion paradigm. Modeling the annotation frame-
bodyText ||| work after psycholinguistic experiments, we do
bodyText ||| not train annotators to conform to sense distinc-
bodyText ||| tions; rather we assess individual differences by
bodyText ||| asking annotators to produce graded ratings in-
bodyText ||| stead of making a binary choice. We perform two
bodyText ||| annotation studies. In the first one, referred to
bodyText ||| as WSsim (Word Sense Similarity), annotators
bodyText ||| give graded ratings on the applicability of Word-
bodyText ||| Net senses. In the second one, Usim (Usage Sim-
bodyText ||| ilarity), annotators rate the similarity of pairs of
bodyText ||| occurrences (usages) of a common target word.
bodyText ||| Both studies explore whether users make use of
bodyText ||| a graded scale or persist in making binary deci-
bodyText ||| sions even when there is the option for a graded
bodyText ||| response. The first study additionally tests to what
bodyText ||| extent the judgments on WordNet senses fall into
bodyText ||| clear-cut clusters, while the second study allows
bodyText ||| us to explore meaning similarity independently of
bodyText ||| any lexicon resource.
page ||| 10
note ||| Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 10–18,
note ||| Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
sectionHeader ||| 2 Related Work
bodyText ||| Manual word sense assignment is difficult for
bodyText ||| human annotators (Krishnamurthy and Nicholls,
bodyText ||| 2000). Reported inter-annotator agreement (ITA)
bodyText ||| for fine-grained word sense assignment tasks has
bodyText ||| ranged between 69% (Kilgarriff and Rosenzweig,
bodyText ||| 2000) for a lexical sample using the HECTOR dic-
bodyText ||| tionary and 78.6.% using WordNet (Landes et al.,
bodyText ||| 1998) in all-words annotation. The use of more
bodyText ||| coarse-grained senses alleviates the problem: In
bodyText ||| OntoNotes (Hovy et al., 2006), an ITA of 90% is
bodyText ||| used as the criterion for the construction of coarse-
bodyText ||| grained sense distinctions. However, intriguingly,
bodyText ||| for some high-frequency lemmas such as leave
bodyText ||| this ITA threshold is not reached even after mul-
bodyText ||| tiple re-partitionings of the semantic space (Chen
bodyText ||| and Palmer, 2009). Similarly, the performance
bodyText ||| of WSD systems clearly indicates that WSD is not
bodyText ||| easy unless one adopts a coarse-grained approach,
bodyText ||| and then systems tagging all words at best perform
bodyText ||| a few percentage points above the most frequent
bodyText ||| sense heuristic (Navigli et al., 2007). Good perfor-
bodyText ||| mance on coarse-grained sense distinctions may
bodyText ||| be more useful in applications than poor perfor-
bodyText ||| mance on fine-grained distinctions (Ide and Wilks,
bodyText ||| 2006) but we do not know this yet and there is
bodyText ||| some evidence to the contrary (Stokoe, 2005).
bodyText ||| Rather than focus on the granularity of clus-
bodyText ||| ters, the approach we will take in this paper
bodyText ||| is to examine the phenomenon of word mean-
bodyText ||| ing both with and without recourse to predefined
bodyText ||| senses by focusing on the similarity of uses of a
bodyText ||| word. Human subjects show excellent agreement
bodyText ||| on judging word similarity out of context (Ruben-
bodyText ||| stein and Goodenough, 1965; Miller and Charles,
bodyText ||| 1991), and human judgments have previously been
bodyText ||| used successfully to study synonymy and near-
bodyText ||| synonymy (Miller and Charles, 1991; Bybee and
bodyText ||| Eddington, 2006). We focus on polysemy rather
bodyText ||| than synonymy. Our aim will be to use WSsim
bodyText ||| to determine to what extent annotations form co-
bodyText ||| hesive clusters. In principle, it should be possi-
bodyText ||| ble to use existing sense-annotated data to explore
bodyText ||| this question: almost all sense annotation efforts
bodyText ||| have allowed annotators to assign multiple senses
bodyText ||| to a single occurrence, and the distribution of these
bodyText ||| sense labels should indicate whether annotators
bodyText ||| viewed the senses as disjoint or not. However,
bodyText ||| the percentage of markables that received multi-
bodyText ||| ple sense labels in existing corpora is small, and it
bodyText ||| varies massively between corpora: In the SemCor
bodyText ||| corpus (Landes et al., 1998), only 0.3% of all
bodyText ||| markables received multiple sense labels. In the
bodyText ||| SENSEVAL-3 English lexical task corpus (Mihal-
bodyText ||| cea et al., 2004) (hereafter referred to as SE-3), the
bodyText ||| ratio is much higher at 8% of all markables1. This
bodyText ||| could mean annotators feel that there is usually a
bodyText ||| single applicable sense, or it could point to a bias
bodyText ||| towards single-sense assignment in the annotation
bodyText ||| guidelines and/or the annotation tool. The WSsim
bodyText ||| experiment that we report in this paper is designed
bodyText ||| to eliminate such bias as far as possible and we
bodyText ||| conduct it on data taken from SemCor and SE-3 so
bodyText ||| that we can compare the annotations. Although we
bodyText ||| use WordNet for the annotation, our study is not a
bodyText ||| study of WordNet per se. We choose WordNet be-
bodyText ||| cause it is sufficiently fine-grained to examine sub-
bodyText ||| tle differences in usage, and because traditionally
bodyText ||| annotated datasets exist to which we can compare
bodyText ||| our results.
bodyText ||| Predefined dictionaries and lexical resources are
bodyText ||| not the only possibilities for annotating lexical
bodyText ||| items with meaning. In cross-lingual settings, the
bodyText ||| actual translations of a word can be taken as the
bodyText ||| sense labels (Resnik and Yarowsky, 2000). Re-
bodyText ||| cently, McCarthy and Navigli (2007) proposed
bodyText ||| the English Lexical Substitution task (hereafter
bodyText ||| referred to as LEXSUB) under the auspices of
bodyText ||| SemEval-2007. It uses paraphrases for words in
bodyText ||| context as a way of annotating meaning. The task
bodyText ||| was proposed following a background of discus-
bodyText ||| sions in the WSD community as to the adequacy
bodyText ||| of predefined word senses. The LEXSUB dataset
bodyText ||| comprises open class words (nouns, verbs, adjec-
bodyText ||| tives and adverbs) with token instances of each
bodyText ||| word appearing in the context of one sentence
bodyText ||| taken from the English Internet Corpus (Sharoff,
bodyText ||| 2006). The methodology can only work where
bodyText ||| there are paraphrases, so the dataset only contains
bodyText ||| words with more than one meaning where at least
bodyText ||| two different meanings have near synonyms. For
bodyText ||| meanings without obvious substitutes the annota-
bodyText ||| tors were allowed to use multiword paraphrases or
bodyText ||| words with slightly more general meanings. This
bodyText ||| dataset has been used to evaluate automatic sys-
bodyText ||| tems which can find substitutes appropriate for the
bodyText ||| context. To the best of our knowledge there has
bodyText ||| been no study of how the data collected relates to
bodyText ||| word sense annotations or judgments of semantic
bodyText ||| similarity. In this paper we examine these relation-
footnote ||| 1 This is even though both annotation efforts use balanced
footnote ||| corpora, the Brown corpus in the case of SemCor, the British
footnote ||| National Corpus for SE-3.
page ||| 11
bodyText ||| ships by re-using data from LEXSUB in both new
bodyText ||| annotation experiments and testing the results for
bodyText ||| correlation.
sectionHeader ||| 3 Annotation
bodyText ||| We conducted two experiments through an on-
bodyText ||| line annotation interface. Three annotators partic-
bodyText ||| ipated in each experiment; all were native British
bodyText ||| English speakers. The first experiment, WSsim,
bodyText ||| collected annotator judgments about the applica-
bodyText ||| bility of dictionary senses using a 5-point rating
bodyText ||| scale. The second, Usim, also utilized a 5-point
bodyText ||| scale but collected judgments on the similarity in
bodyText ||| meaning between two uses of a word. 2 The scale
bodyText ||| was 1 – completely different, 2 – mostly different,
bodyText ||| 3 – similar, 4 – very similar and 5 – identical. In
bodyText ||| Usim, this scale rated the similarity of the two uses
bodyText ||| of the common target word; in WSsim it rated the
bodyText ||| similarity between the use of the target word and
bodyText ||| the sense description. In both experiments, the an-
bodyText ||| notation interface allowed annotators to revisit and
bodyText ||| change previously supplied judgments, and a com-
bodyText ||| ment box was provided alongside each item.
bodyText ||| WSsim. This experiment contained a total of
bodyText ||| 430 sentences spanning 11 lemmas (nouns, verbs
bodyText ||| and adjectives). For 8 of these lemmas, 50 sen-
bodyText ||| tences were included, 25 of them randomly sam-
bodyText ||| pled from SemCor 3 and 25 randomly sampled
bodyText ||| from SE-3 .4 The remaining 3 lemmas in the ex-
bodyText ||| periment each had 10 sentences taken from the
bodyText ||| LEXSUB data.
bodyText ||| WSsim is a word sense annotation task using
bodyText ||| WordNet senses.5 Unlike previous word sense an-
bodyText ||| notation projects, we asked annotators to provide
bodyText ||| judgments on the applicability of every WordNet
bodyText ||| sense of the target lemma with the instruction: 6
footnote ||| 2Throughout this paper, a target word is assumed to be a
footnote ||| word in a given PoS.
footnote ||| 3The SemCor dataset was produced alongside WordNet,
footnote ||| so it can be expected to support the WordNet sense distinc-
footnote ||| tions. The same cannot be said for SE-3.
footnote ||| 4Sentence fragments and sentences with 5 or fewer words
footnote ||| were excluded from the sampling. Annotators were given
footnote ||| the sentences, but not the original annotation from these re-
footnote ||| sources.
footnote ||| 5WordNet 1.7.1 was used in the annotation of both SE-3
footnote ||| and SemCor; we used the more current WordNet 3.0 after
footnote ||| verifying that the lemmas included in this experiment had the
footnote ||| same senses listed in both versions. Care was taken addition-
footnote ||| ally to ensure that senses were not presented in an order that
footnote ||| reflected their frequency of occurrence.
footnote ||| 6The guidelines for both experiments are avail-
footnote ||| able	at	http://comp.ling.utexas.edu/
footnote ||| people/katrin erk/graded sense and usage
footnote ||| annotation
construct ||| Your task is to rate, for each of these descriptions,
construct ||| how well they reflect the meaning of the boldfaced
construct ||| word in the sentence.
bodyText ||| Applicability judgments were not binary, but were
bodyText ||| instead collected using the five-point scale given
bodyText ||| above which allowed annotators to indicate not
bodyText ||| only whether a given sense applied, but to what
bodyText ||| degree. Each annotator annotated each of the 430
bodyText ||| items. By having multiple annotators per item and
bodyText ||| a graded, non-binary annotation scheme we al-
bodyText ||| low for and measure differences between annota-
bodyText ||| tors, rather than training annotators to conform to
bodyText ||| a common sense distinction guideline. By asking
bodyText ||| annotators to provide ratings for each individual
bodyText ||| sense, we strive to eliminate all bias towards either
bodyText ||| single-sense or multiple-sense assignment. In tra-
bodyText ||| ditional word sense annotation, such bias could be
bodyText ||| introduced directly through annotation guidelines
bodyText ||| or indirectly, through tools that make it easier to
bodyText ||| assign fewer senses. We focus not on finding the
bodyText ||| best fitting sense but collect judgments on the ap-
bodyText ||| plicability of all senses.
bodyText ||| Usim. This experiment used data from LEXSUB.
bodyText ||| For more information on LEXSUB, see McCarthy
bodyText ||| and Navigli (2007). 34 lemmas (nouns, verbs, ad-
bodyText ||| jectives and adverbs) were manually selected, in-
bodyText ||| cluding the 3 lemmas also used in WSsim. We se-
bodyText ||| lected lemmas which exhibited a range of mean-
bodyText ||| ings and substitutes in the LEXSUB data, with
bodyText ||| as few multiword substitutes as possible. Each
bodyText ||| lemma is the target in 10 LEXSUB sentences. For
bodyText ||| our experiment, we took every possible pairwise
bodyText ||| comparison of these 10 sentences for a lemma. We
bodyText ||| refer to each such pair of sentences as an SPAIR.
bodyText ||| The resulting dataset comprised 45 SPAIRs per
bodyText ||| lemma, adding up to 1530 comparisons per anno-
bodyText ||| tator overall.
bodyText ||| In this annotation experiment, annotators saw
bodyText ||| SPAIRs with a common target word and rated the
bodyText ||| similarity in meaning between the two uses of the
bodyText ||| target word with the instruction:
construct ||| Your task is to rate, for each pair of sentences, how
construct ||| similar in meaning the two boldfaced words are on
construct ||| a five -point scale.
bodyText ||| In addition annotators had the ability to respond
bodyText ||| with “Cannot Decide”, indicating that they were
bodyText ||| unable to make an effective comparison between
bodyText ||| the two contexts, for example because the mean-
bodyText ||| ing of one usage was unclear. This occurred in
bodyText ||| 9 paired occurrences during the course of anno-
bodyText ||| tation, and these items (paired occurrences) were
page ||| 12
bodyText ||| excluded from further analysis.
bodyText ||| The purpose of Usim was to collect judgments
bodyText ||| about degrees of similarity between a word’s
bodyText ||| meaning in different contexts. Unlike WSsim,
bodyText ||| Usim does not rely upon any dictionary resource
bodyText ||| as a basis for the judgments.
sectionHeader ||| 4 Analyses
bodyText ||| This section reports on analyses on the annotated
bodyText ||| data. In all the analyses we use Spearman’s rank
bodyText ||| correlation coefficient (p), a nonparametric test,
bodyText ||| because the data does not seem to be normally
bodyText ||| distributed. We used two-tailed tests in all cases,
bodyText ||| rather than assume the direction of the relation-
bodyText ||| ship. As noted above, we have three annotators
bodyText ||| per task, and each annotator gave judgments for
bodyText ||| every sentence (WSsim) or sentence pair (Usim).
bodyText ||| Since the annotators may vary as to how they use
bodyText ||| the ordinal scale, we do not use the mean of judg-
bodyText ||| ments7 but report all individual correlations. All
bodyText ||| analyses were done using the R package.8
subsectionHeader ||| 4.1 WSsim analysis
bodyText ||| In the WSsim experiment, annotators rated the ap-
bodyText ||| plicability of each WordNet 3.0 sense for a given
bodyText ||| target word occurrence. Table 1 shows a sample
bodyText ||| annotation for the target argument.n. 9
bodyText ||| Pattern of annotation and annotator agree-
bodyText ||| ment. Figure 1 shows how often each of the five
bodyText ||| judgments on the scale was used, individually and
bodyText ||| summed over all annotators. (The y-axis shows
bodyText ||| raw counts of each judgment.) We can see from
bodyText ||| this figure that the extreme ratings 1 and 5 are used
bodyText ||| more often than the intermediate ones, but annota-
bodyText ||| tors make use of the full ordinal scale when judg-
bodyText ||| ing the applicability of a sense. Also, the figure
bodyText ||| shows that annotator 1 used the extreme negative
bodyText ||| rating 1 much less than the other two annotators.
bodyText ||| Figure 2 shows the percentage of times each judg-
bodyText ||| ment was used on senses of three lemmas, differ-
bodyText ||| ent.a, interest.n, and win.v. In WordNet, they have
bodyText ||| 5, 7, and 4 senses, respectively. The pattern for
bodyText ||| win.v resembles the overall distribution of judg-
bodyText ||| ments, with peaks at the extreme ratings 1 and 5.
bodyText ||| The lemma interest.n has a single peak at rating
bodyText ||| 1, partly due to the fact that senses 5 (financial
footnote ||| 7We have also performed several of our calculations us-
footnote ||| ing the mean judgment, and they also gave highly significant
footnote ||| results in all the cases we tested.
footnote ||| 8http://www.r-project.org/
footnote ||| 9We use word.PoS to denote a target word (lemma).
footnote ||| Annotator 1 Annotator 2 Annotator 3 overall
figureCaption ||| Figure 1: WSsim experiment: number of times
figureCaption ||| each judgment was used, by annotator and
figureCaption ||| summed over all annotators. The y-axis shows raw
figureCaption ||| counts of each judgment.
figureCaption ||| different.a	interest.n	win.v
figureCaption ||| Figure 2: WSsim experiment: percentage of times
figureCaption ||| each judgment was used for the lemmas differ-
figureCaption ||| ent.a, interest.n and win.v. Judgment counts were
figureCaption ||| summed over all three annotators.
bodyText ||| involvement) and 6 (interest group) were rarely
bodyText ||| judged to apply. For the lemma different.a, all
bodyText ||| judgments have been used with approximately the
bodyText ||| same frequency.
bodyText ||| We measured the level of agreement between
bodyText ||| annotators using Spearman’s p between the judg-
bodyText ||| ments of every pair of annotators. The pairwise
bodyText ||| correlations were p = 0.506, p = 0.466 and p =
bodyText ||| 0.540, all highly significant with p < 2.2e-16.
bodyText ||| Agreement with previous annotation in
bodyText ||| SemCor and SE-3. 200 of the items in WSsim
bodyText ||| had been previously annotated in SemCor, and
bodyText ||| 200 in SE-3. This lets us compare the annotation
bodyText ||| results across annotation efforts. Table 2 shows
bodyText ||| the percentage of items where more than one
bodyText ||| sense was assigned in the subset of WSsim from
bodyText ||| SemCor (first row), from SE-3 (second row), and
figure ||| 1
figure ||| 2
figure ||| 3
figure ||| 5
page ||| 13
table ||| Sentence	1	2	Senses	5	6	7	Annotator
table ||| 			3	4
table ||| This question provoked arguments in America about the	1	4	4	2	1	1	3	Ann. 1
table ||| Norton Anthology of Literature by Women, some of the	4	5	4	2	1	1	4	Ann. 2
table ||| contents of which were said to have had little value as literature.	1	4	5	1	1	1	1	Ann. 3
tableCaption ||| Table 1: A sample annotation in the WSsim experiment. The senses are: 1:statement, 2:controversy,
tableCaption ||| 3:debate, 4:literary argument, 5:parameter, 6:variable, 7:line of reasoning
table ||| WSsim judgment
table ||| >3 >4 5
table ||| 80.2 57.5 28.3
table ||| 78.0 58.3 27.1
table ||| 78.8 57.4 27.7
table ||| Data	Orig.
table ||| W Ssim/SemCor	0.0
table ||| WSsim/SE-3	24.0
table ||| All WSsim
table ||| 	p<0.05 pos	neg		p<0.01 pos	neg
table ||| Ann. 1	30.8	11.4	23.2	5.9
table ||| Ann. 2	22.2	24.1	19.6	19.6
table ||| Ann. 3	12.7	12.0	10.0	6.0
tableCaption ||| Table 2: Percentage of items with multiple senses
tableCaption ||| assigned. Orig: in the original SemCor/SE-3 data.
tableCaption ||| WSsim judgment: items with judgments at or
tableCaption ||| above the specified threshold. The percentages for
tableCaption ||| WSsim are averaged over the three annotators.
bodyText ||| all of WSsim (third row). The Orig. column
bodyText ||| indicates how many items had multiple labels in
bodyText ||| the original annotation (SemCor or SE-3)10. Note
bodyText ||| that no item had more than one sense label in
bodyText ||| SemCor. The columns under WSsim judgment
bodyText ||| show the percentage of items (averaged over
bodyText ||| the three annotators) that had judgments at or
bodyText ||| above the specified threshold, starting from rating
bodyText ||| 3 – similar. Within WSsim, the percentage of
bodyText ||| multiple assignments in the three rows is fairly
bodyText ||| constant. WSsim avoids the bias to one sense
bodyText ||| by deliberately asking for judgments on the
bodyText ||| applicability of each sense rather than asking
bodyText ||| annotators to find the best one.
bodyText ||| To compute the Spearman’s correlation between
bodyText ||| the original sense labels and those given in the
bodyText ||| WSsim annotation, we converted SemCor and
bodyText ||| SE-3 labels to the format used within WSsim: As-
bodyText ||| signed senses were converted to a judgment of 5,
bodyText ||| and unassigned senses to a judgment of 1. For the
bodyText ||| WSsim/SemCor dataset, the correlation between
bodyText ||| original and WSsim annotation was p = 0.234,
bodyText ||| p = 0.448, and p = 0.390 for the three anno-
bodyText ||| tators, each highly significant with p < 2.2e-16.
bodyText ||| For the WSsim/SE-3 dataset, the correlations were
bodyText ||| p = 0.346, p = 0.449 and p = 0.338, each of them
bodyText ||| again highly significant at p < 2.2e-16.
bodyText ||| Degree of sense grouping. Next we test to what
bodyText ||| extent the sense applicability judgments in the
footnote ||| 10Overall, 0.3% of tokens in SemCor have multiple labels,
footnote ||| and 8% of tokens in SE-3, so the multiple label assignment in
footnote ||| our sample is not an underestimate.
tableCaption ||| Table 3: Percentage of sense pairs that were sig-
tableCaption ||| nificantly positively (pos) or negatively (neg) cor-
tableCaption ||| related at p < 0.05 and p < 0.01, shown by anno-
tableCaption ||| tator.
table ||| 	j>3	j>4	j=5
table ||| Ann. 1	71.9	49.1	8.1
table ||| Ann. 2	55.3	24.7	8.1
table ||| Ann. 3	42.8	24.0	4.9
tableCaption ||| Table 4: Percentage of sentences in which at least
tableCaption ||| two uncorrelated (p > 0.05) or negatively corre-
tableCaption ||| lated senses have been annotated with judgments
tableCaption ||| at the specified threshold.
bodyText ||| WSsim task could be explained by more coarse-
bodyText ||| grained, categorial sense assignments. We first
bodyText ||| test how many pairs of senses for a given lemma
bodyText ||| show similar patterns in the ratings that they re-
bodyText ||| ceive. Table 3 shows the percentage of sense pairs
bodyText ||| that were significantly correlated for each anno-
bodyText ||| tator.11 Significantly positively correlated senses
bodyText ||| can possibly be reduced to more coarse-grained
bodyText ||| senses. Would annotators have been able to des-
bodyText ||| ignate a single appropriate sense given these more
bodyText ||| coarse-grained senses? Call two senses groupable
bodyText ||| if they are significantly positively correlated; in or-
bodyText ||| der not to overlook correlations that are relatively
bodyText ||| weak but existent, we use a cutoff of p = 0.05 for
bodyText ||| significant correlation. We tested how often anno-
bodyText ||| tators gave ratings of at least similar, i.e. ratings
bodyText ||| > 3, to senses that were not groupable. Table 4
bodyText ||| shows the percentages of items where at least two
bodyText ||| non-groupable senses received ratings at or above
bodyText ||| the specified threshold. The table shows that re-
bodyText ||| gardless of which annotator we look at, over 40%
bodyText ||| of all items had two or more non-groupable senses
bodyText ||| receive judgments of at least 3 (similar). There
footnote ||| 11 We exclude senses that received a uniform rating of 1 on
footnote ||| all items. This concerned 4 senses for annotator 2 and 6 for
footnote ||| annotator 3.
page ||| 14
figure ||| 1) We study the methods and concepts that each writer uses to
figure ||| defend the cogency of legal, deliberative, or more generally
figure ||| political prudence against explicit or implicit charges that
figure ||| practical thinking is merely a knack or form of cleverness.
figure ||| 2) Eleven CIRA members have been convicted of criminal
figure ||| charges and others are awaiting trial.
figureCaption ||| Figure 3: An SPAIR for charge.n. Annotator judg-
figureCaption ||| ments: 2,3,4
bodyText ||| were even several items where two or more non-
bodyText ||| groupable senses each got a judgment of 5. The
bodyText ||| sentence in table 1 is a case where several non-
bodyText ||| groupable senses got ratings > 3. This is most
bodyText ||| pronounced for Annotator 2, who along with sense
bodyText ||| 2 (controversy) assigned senses 1 (statement), 7
bodyText ||| (line of reasoning), and 3 (debate), none of which
bodyText ||| are groupable with sense 2.
subsectionHeader ||| 4.2 Usim analysis
bodyText ||| In this experiment, ratings between 1 and 5 were
bodyText ||| given for every pairwise combination of sentences
bodyText ||| for each target lemma. An example of an SPAIR
bodyText ||| for charge.n is shown in figure 3. In this case the
bodyText ||| verdicts from the annotators were 2, 3 and 4.
bodyText ||| Pattern of Annotations and Annotator Agree-
bodyText ||| ment Figure 4 gives a bar chart of the judgments
bodyText ||| for each annotator and summed over annotators.
bodyText ||| We can see from this figure that the annotators
bodyText ||| use the full ordinal scale when judging the simi-
bodyText ||| larity of a word’s usages, rather than sticking to
bodyText ||| the extremes. There is variation across words, de-
bodyText ||| pending on the relatedness of each word’s usages.
bodyText ||| Figure 5 shows the judgments for the words bar.n,
bodyText ||| work.v and raw.a. We see that bar.n has predom-
bodyText ||| inantly different usages with a peak for category
bodyText ||| 1, work.v has more similar judgments (category 5)
bodyText ||| compared to any other category and raw.a has a
bodyText ||| peak in the middle category (3). 12 There are other
bodyText ||| words, like for example fresh.a, where the spread
bodyText ||| is more uniform.
bodyText ||| To gauge the level of agreement between anno-
bodyText ||| tators, we calculated Spearman’s p between the
bodyText ||| judgments of every pair of annotators as in sec-
bodyText ||| tion 4.1. The pairwise correlations are all highly
bodyText ||| significant (p < 2.2e-16) with Spearman’s p =
bodyText ||| 0.502, 0.641 and 0.501 giving an average corre-
bodyText ||| lation of 0.548. We also perform leave-one-out re-
bodyText ||| sampling following Lapata (2006) which gave us
bodyText ||| a Spearman’s correlation of 0.630.
footnote ||| 12For figure 5 we sum the judgments over annotators.
figureCaption ||| Figure 4: Usim experiment: number of times each
figureCaption ||| judgment was used, by annotator and summed
figureCaption ||| over all annotators
figure ||| bar.n	raw.a	work.v
figureCaption ||| Figure 5: Usim experiment: number of times each
figureCaption ||| judgment was used for bar.n, work.v and raw. a
bodyText ||| Comparison with LEXSUB substitutions Next
bodyText ||| we look at whether the Usim judgments on sen-
bodyText ||| tence pairs (SPAIRs) correlate with LEXSUB sub-
bodyText ||| stitutes. To do this we use the overlap of substi-
bodyText ||| tutes provided by the five LEXSUB annotators be-
bodyText ||| tween two sentences in an SPAIR. In LEXSUB the
bodyText ||| annotators had to replace each item (a target word
bodyText ||| within the context of a sentence) with a substitute
bodyText ||| that fitted the context. Each annotator was permit-
bodyText ||| ted to supply up to three substitutes provided that
bodyText ||| they all fitted the context equally. There were 10
bodyText ||| sentences per lemma. For our analyses we take
bodyText ||| every SPAIR for a given lemma and calculate the
bodyText ||| overlap (inter) of the substitutes provided by the
bodyText ||| annotators for the two usages under scrutiny. Let
bodyText ||| s1 and s2 be a pair of sentences in an SPAIR and
bodyText ||| Annotator 4 Annotator 5 Annotator 6	overall
figure ||| 1
figure ||| 2
figure ||| 3
figure ||| 4
figure ||| 5
figure ||| 1
figure ||| 2
figure ||| 3
figure ||| 4
figure ||| 5
page ||| 15
bodyText ||| x1 and x2 be the multisets of substitutes for the
bodyText ||| respective sentences. Let freq(w,x) be the fre-
bodyText ||| quency of a substitute w in a multiset x of sub-
bodyText ||| stitutes for a given sentence. 13 INTER(s1,s2) =
equation ||| �wEx1f1x2 min (freq(w,x1), freq(w,x2))
equation ||| max(1x1 1, 1x2 1)
bodyText ||| Using this calculation for each SPAIR we can
bodyText ||| now compute the correlation between the Usim
bodyText ||| judgments for each annotator and the INTER val-
bodyText ||| ues, again using Spearman’s. The figures are
bodyText ||| shown in the leftmost block of table 5. The av-
bodyText ||| erage correlation for the 3 annotators was 0.488
bodyText ||| and the p-values were all < 2.2e-16. This shows
bodyText ||| a highly significant correlation of the Usim judg-
bodyText ||| ments and the overlap of substitutes.
bodyText ||| We also compare the WSsim judgments against
bodyText ||| the LEXSUB substitutes, again using the INTER
bodyText ||| measure of substitute overlap. For this analysis,
bodyText ||| we only use those WSsim sentences that are origi-
bodyText ||| nally from LEXSUB. In WSsim, the judgments for
bodyText ||| a sentence comprise judgments for each WordNet
bodyText ||| sense of that sentence. In order to compare against
bodyText ||| INTER, we need to transform these sentence-wise
bodyText ||| ratings in WSsim to a WSsim-based judgment of
bodyText ||| sentence similarity. To this end, we compute the
bodyText ||| Euclidean Distance14 (ED) between two vectors J1
bodyText ||| and J2 of judgments for two sentences s1, s2 for the
bodyText ||| same lemma E. Each of the n indexes of the vector
bodyText ||| represent one of the n different WordNet senses
bodyText ||| for E. The value at entry i of the vector J1 is the
bodyText ||| judgment that the annotator in question (we do not
bodyText ||| average over annotators here) provided for sense i
bodyText ||| of E for sentence s1.
equation ||| (J1[i]—J2[i])2) (1)
bodyText ||| We correlate the Euclidean distances with
bodyText ||| INTER. We can only test correlation for the subset
bodyText ||| of WSsim that overlaps with the LEXSUB data: the
bodyText ||| 30 sentences for investigator.n, function.n and or-
bodyText ||| der.v, which together give 135 unique SPAIRs. We
bodyText ||| refer to this subset as Wf1U. The results are given
bodyText ||| in the third block of table 5. Note that since we are
bodyText ||| measuring distance between SPAIRs for WSsim
footnote ||| 13The frequency of a substitute in a multiset depends on
footnote ||| the number of LEXSUB annotators that picked the substitute
footnote ||| for this item.
footnote ||| 14We use Euclidean Distance rather than a normalizing
footnote ||| measure like Cosine because a sentence where all ratings are
footnote ||| 5 should be very different from a sentence where all senses
footnote ||| received a rating of 1.
table ||| Usim All		Usim Wf1U	WSsim Wf1U
table ||| ann.	p	p	ann.	p
table ||| 4	0.383	0.330	1	-0.520
table ||| 5	0.498	0.635	2	-0.503
table ||| 6	0.584	0.631	3	-0.463
tableCaption ||| Table 5: Annotator correlation with LEXSUB sub-
tableCaption ||| stitute overlap (inter)
bodyText ||| whereas INTER is a measure of similarity, the cor-
bodyText ||| relation is negative. The results are highly signif-
bodyText ||| icant with individual p-values from < 1.067e-10
bodyText ||| to < 1.551e-08 and a mean correlation of -0.495.
bodyText ||| The results in the first and third block of table 5 are
bodyText ||| not directly comparable, as the results in the first
bodyText ||| block are for all Usim data and not the subset of
bodyText ||| LEXSUB with WSsim annotations. We therefore
bodyText ||| repeated the analysis for Usim on the subset of
bodyText ||| data in WSsim and provide the correlation in the
bodyText ||| middle section of table 5. The mean correlation
bodyText ||| for Usim on this subset of the data is 0.532, which
bodyText ||| is a stronger relationship compared to WSsim, al-
bodyText ||| though there is more discrepancy between individ-
bodyText ||| ual annotators, with the result for annotator 4 giv-
bodyText ||| ing a p-value = 9.139e-05 while the other two an-
bodyText ||| notators had p-values < 2.2e-16.
bodyText ||| The LEXSUB substitute overlaps between dif-
bodyText ||| ferent usages correlate well with both Usim and
bodyText ||| WSsim judgments, with a slightly stronger rela-
bodyText ||| tionship to Usim, perhaps due to the more compli-
bodyText ||| cated representation of word meaning in WSsim
bodyText ||| which uses the full set of WordNet senses.
subsectionHeader ||| 4.3 Correlation between WSsim and Usim
bodyText ||| As we showed in section 4.1, WSsim correlates
bodyText ||| with previous word sense annotations in SemCor
bodyText ||| and SE-3 while allowing the user a more graded
bodyText ||| response to sense tagging. As we saw in sec-
bodyText ||| tion 4.2, Usim and WSsim judgments both have a
bodyText ||| highly significant correlation with similarity of us-
bodyText ||| ages as measured using the overlap of substitutes
bodyText ||| from LEXSUB. Here, we look at the correlation
bodyText ||| of WSsim and Usim, considering again the sub-
bodyText ||| set of data that is common to both experiments.
bodyText ||| We again transform WSsim sense judgments for
bodyText ||| individual sentences to distances between SPAIRs
bodyText ||| using Euclidean Distance (ED). The Spearman’s
bodyText ||| p range between —0.307 and —0.671, and all re-
bodyText ||| sults are highly significant with p-values between
bodyText ||| 0.0003 and < 2.2e-16. As above, the correla-
bodyText ||| tion is negative because ED is a distance measure
bodyText ||| between sentences in an SPAIR, whereas the judg-
equation ||| ED(J1,J2) = V1(
equation ||| n
equation ||| �
equation ||| i=1
page ||| 16
bodyText ||| ments for Usim are similarity judgments. We see
bodyText ||| that there is highly significant correlation for every
bodyText ||| pairing of annotators from the two experiments.
sectionHeader ||| 5 Discussion
bodyText ||| Validity of annotation scheme. Annotator rat-
bodyText ||| ings show highly significant correlation on both
bodyText ||| tasks. This shows that the tasks are well-defined.
bodyText ||| In addition, there is a strong correlation between
bodyText ||| WSsim and Usim, which indicates that the poten-
bodyText ||| tial bias introduced by the use of dictionary senses
bodyText ||| in WSsim is not too prominent. However, we note
bodyText ||| that WSsim only contained a small portion of 3
bodyText ||| lemmas (30 sentences and 135 SPAIRs) in com-
bodyText ||| mon with Usim, so more annotation is needed to
bodyText ||| be certain of this relationship. Given the differ-
bodyText ||| ences between annotator 1 and the other annota-
bodyText ||| tors in Fig. 1, it would be interesting to collect
bodyText ||| judgments for additional annotators.
bodyText ||| Graded judgments of use similarity and sense
bodyText ||| applicability. The annotators made use of the
bodyText ||| full spectrum of ratings, as shown in Figures 1 and
bodyText ||| 4. This may be because of a graded perception of
bodyText ||| the similarity of uses as well as senses, or because
bodyText ||| some uses and senses are very similar. Table 4
bodyText ||| shows that for a large number of WSsim items,
bodyText ||| multiple senses that were not significantly posi-
bodyText ||| tively correlated got high ratings. This seems to
bodyText ||| indicate that the ratings we obtained cannot sim-
bodyText ||| ply be explained by more coarse-grained senses. It
bodyText ||| may hence be reasonable to pursue computational
bodyText ||| models of word meaning that are graded, maybe
bodyText ||| even models that do not rely on dictionary senses
bodyText ||| at all (Erk and Pado, 2008).
bodyText ||| Comparison to previous word sense annotation.
bodyText ||| Our graded WSsim annotations do correlate with
bodyText ||| traditional “best fitting sense” annotations from
bodyText ||| SemCor and SE-3; however, if annotators perceive
bodyText ||| similarity between uses and senses as graded, tra-
bodyText ||| ditional word sense annotation runs the risk of in-
bodyText ||| troducing bias into the annotation.
bodyText ||| Comparison to lexical substitutions. There is a
bodyText ||| strong correlation between both Usim and WSsim
bodyText ||| and the overlap in paraphrases that annotators gen-
bodyText ||| erated for LEXSUB. This is very encouraging, and
bodyText ||| especially interesting because LEXSUB annotators
bodyText ||| freely generated paraphrases rather than selecting
bodyText ||| them from a list.
sectionHeader ||| 6 Conclusions
bodyText ||| We have introduced a novel annotation paradigm
bodyText ||| for word sense annotation that allows for graded
bodyText ||| judgments and for some variation between anno-
bodyText ||| tators. We have used this annotation paradigm
bodyText ||| in two experiments, WSsim and Usim, that shed
bodyText ||| some light on the question of whether differences
bodyText ||| between word usages are perceived as categorial
bodyText ||| or graded. Both datasets will be made publicly
bodyText ||| available. There was a high correlation between
bodyText ||| annotator judgments within and across tasks, as
bodyText ||| well as with previous word sense annotation and
bodyText ||| with paraphrases proposed in the English Lex-
bodyText ||| ical Substitution task. Annotators made ample
bodyText ||| use of graded judgments in a way that cannot
bodyText ||| be explained through more coarse-grained senses.
bodyText ||| These results suggest that it may make sense to
bodyText ||| evaluate WSD systems on a task of graded rather
bodyText ||| than categorial meaning characterization, either
bodyText ||| through dictionary senses or similarity between
bodyText ||| uses. In that case, it would be useful to have more
bodyText ||| extensive datasets with graded annotation, even
bodyText ||| though this annotation paradigm is more time con-
bodyText ||| suming and thus more expensive than traditional
bodyText ||| word sense annotation.
bodyText ||| As a next step, we will automatically cluster the
bodyText ||| judgments we obtained in the WSsim and Usim
bodyText ||| experiments to further explore the degree to which
bodyText ||| the annotation gives rise to sense grouping. We
bodyText ||| will also use the ratings in both experiments to
bodyText ||| evaluate automatically induced models of word
bodyText ||| meaning. The SemEval-2007 word sense induc-
bodyText ||| tion task (Agirre and Soroa, 2007) already allows
bodyText ||| for evaluation of automatic sense induction sys-
bodyText ||| tems, but compares output to gold-standard senses
bodyText ||| from OntoNotes. We hope that the Usim dataset
bodyText ||| will be particularly useful for evaluating methods
bodyText ||| which relate usages without necessarily producing
bodyText ||| hard clusters. Also, we will extend the current
bodyText ||| dataset using more annotators and exploring ad-
bodyText ||| ditional lexicon resources.
sectionHeader ||| Acknowledgments. We acknowledge support
sectionHeader ||| from the UK Royal Society for a Dorothy Hodkin
sectionHeader ||| Fellowship to the second author. We thank Sebas-
sectionHeader ||| tian Pado for many helpful discussions, and An-
sectionHeader ||| drew Young for help with the interface.
sectionHeader ||| References
reference ||| E. Agirre and A. Soroa. 2007. SemEval-2007
reference ||| task 2: Evaluating word sense induction and dis-
page ||| 17
reference ||| crimination systems. In Proceedings of the 4th
reference ||| International Workshop on Semantic Evaluations
reference ||| (SemEval-2007), pages 7–12, Prague, Czech Repub-
reference ||| lic.
reference ||| J. Bybee and D. Eddington. 2006. A usage-based ap-
reference ||| proach to Spanish verbs of ’becoming’. Language,
reference ||| 82(2):323–355.
reference ||| J. Chen and M. Palmer. 2009. Improving English
reference ||| verb sense disambiguation performance with lin-
reference ||| guistically motivated features and clear sense dis-
reference ||| tinction boundaries. Journal of Language Resources
reference ||| and Evaluation, Special Issue on SemEval-2007. in
reference ||| press.
reference ||| K. Erk and S. Pado. 2008. A structured vector space
reference ||| model for word meaning in context. In Proceedings
reference ||| of EMNLP-08, Waikiki, Hawaii.
reference ||| J. A. Hampton. 1979. Polymorphous concepts in se-
reference ||| mantic memory. Journal of Verbal Learning and
reference ||| Verbal Behavior, 18:441–461.
reference ||| J. A. Hampton. 2007. Typicality, graded membership,
reference ||| and vagueness. Cognitive Science, 31:355–384.
reference ||| P. Hanks. 2000. Do word meanings exist? Computers
reference ||| and the Humanities, 34(1-2):205–215(11).
reference ||| E. H. Hovy, M. Marcus, M. Palmer, S. Pradhan,
reference ||| L. Ramshaw, and R. Weischedel. 2006. OntoNotes:
reference ||| The 90% solution. In Proceedings of the Hu-
reference ||| man Language Technology Conference of the North
reference ||| American Chapter of the ACL (NAACL-2006), pages
reference ||| 57–60, New York.
reference ||| N. Ide and Y. Wilks. 2006. Making sense about
reference ||| sense. In E. Agirre and P. Edmonds, editors,
reference ||| Word Sense Disambiguation, Algorithms and Appli-
reference ||| cations, pages 47–73. Springer.
reference ||| A. Kilgarriff and J. Rosenzweig. 2000. Framework
reference ||| and results for English Senseval. Computers and the
reference ||| Humanities, 34(1-2):15–48.
reference ||| A. Kilgarriff. 1997. I don’t believe in word senses.
reference ||| Computers and the Humanities, 31(2):91–113.
reference ||| A. Kilgarriff. 2006. Word senses. In E. Agirre
reference ||| and P. Edmonds, editors, Word Sense Disambigua-
reference ||| tion, Algorithms and Applications, pages 29–46.
reference ||| Springer.
reference ||| R. Krishnamurthy and D. Nicholls. 2000. Peeling
reference ||| an onion: the lexicographers’ experience of man-
reference ||| ual sense-tagging. Computers and the Humanities,
reference ||| 34(1-2).
reference ||| S. Landes, C. Leacock, and R. Tengi. 1998. Build-
reference ||| ing semantic concordances. In C. Fellbaum, editor,
reference ||| WordNet: An Electronic Lexical Database. The MIT
reference ||| Press, Cambridge, MA.
reference ||| M. Lapata. 2006. Automatic evaluation of information
reference ||| ordering. Computational Linguistics, 32(4):471–
reference ||| 484.
reference ||| D. McCarthy and R. Navigli. 2007. SemEval-2007
reference ||| task 10: English lexical substitution task. In Pro-
reference ||| ceedings of the 4th International Workshop on Se-
reference ||| mantic Evaluations (SemEval-2007), pages 48–53,
reference ||| Prague, Czech Republic.
reference ||| M. McCloskey and S. Glucksberg. 1978. Natural cat-
reference ||| egories: Well defined or fuzzy sets? Memory &
reference ||| Cognition, 6:462–472.
reference ||| R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
reference ||| The Senseval-3 English lexical sample task. In
reference ||| 3rd International Workshop on Semantic Evalua-
reference ||| tions (SensEval-3) atACL-2004, Barcelona, Spain.
reference ||| G. Miller and W. Charles. 1991. Contextual correlates
reference ||| of semantic similarity. Language and cognitive pro-
reference ||| cesses, 6(1):1–28.
reference ||| G. L. Murphy. 2002. The Big Book of Concepts. MIT
reference ||| Press.
reference ||| R. Navigli, K. C. Litkowski, and O. Hargraves.
reference ||| 2007. SemEval-2007 task 7: Coarse-grained En-
reference ||| glish all-words task. In Proceedings of the 4th
reference ||| International Workshop on Semantic Evaluations
reference ||| (SemEval-2007), pages 30–35, Prague, Czech Re-
reference ||| public.
reference ||| P. Resnik and D. Yarowsky. 2000. Distinguishing
reference ||| systems and distinguishing senses: New evaluation
reference ||| methods for word sense disambiguation. Natural
reference ||| Language Engineering, 5(3):113–133.
reference ||| E. Rosch and C. B. Mervis. 1975. Family resem-
reference ||| blance: Studies in the internal structure of cate-
reference ||| gories. Cognitive Psychology, 7:573–605.
reference ||| E. Rosch. 1975. Cognitive representations of seman-
reference ||| tic categories. Journal of Experimental Psychology:
reference ||| General, 104:192–233.
reference ||| H. Rubenstein and J. Goodenough. 1965. Contextual
reference ||| correlates of synonymy. Computational Linguistics,
reference ||| 8:627–633.
reference ||| S. Sharoff. 2006. Open-source corpora: Using the net
reference ||| to fish for linguistic data. International Journal of
reference ||| Corpus Linguistics, 11(4):435–462.
reference ||| C. Stokoe. 2005. Differentiating homonymy and pol-
reference ||| ysemy in information retrieval. In Proceedings of
reference ||| HLT/EMNLP-05, pages 403–410, Vancouver, B.C.,
reference ||| Canada.
page ||| 18
