title ||| A New Statistical Formula for Chinese Text Segmentation
title ||| Incorporating Contextual Information
author ||| Yubin Dai
author ||| Christopher S.G. Khoo
affiliation ||| Division of Information Studies
affiliation ||| School of Applied Science
affiliation ||| Nanyang Technological University
address ||| Singapore 639798
address ||| (65) 790-4602
email ||| dyb_lte@hotmail.com
email ||| assgkhoo@ntu.edu.sg
sectionHeader ||| ABSTRACT
bodyText ||| A new statistical formula for identifying 2-character words in
bodyText ||| Chinese text, called the contextual information formula, was
bodyText ||| developed empirically by performing stepwise logistic regression
bodyText ||| using a sample of sentences that had been manually segmented.
bodyText ||| Contextual information in the form of the frequency of characters
bodyText ||| that are adjacent to the bigram being processed as well as the
bodyText ||| weighted document frequency of the overlapping bigrams were
bodyText ||| found to be significant factors for predicting the probablity that
bodyText ||| the bigram constitutes a word. Local information (the number of
bodyText ||| times the bigram occurs in the document being segmented) and
bodyText ||| the position of the bigram in the sentence were not found to be
bodyText ||| useful in determining words. The contextual information formula
bodyText ||| was found to be significantly and substantially better than the
bodyText ||| mutual information formula in identifying 2-character words.
bodyText ||| The method can also be used for identifying multi-word terms in
bodyText ||| English text.
sectionHeader ||| Keywords
keyword ||| Chinese text segmentation, word boundary identification, logistic
keyword ||| regression, multi-word terms
sectionHeader ||| 1. INTRODUCTION
bodyText ||| Chinese text is different from English text in that there is no
bodyText ||| explicit word boundary. In English text, words are separated by
bodyText ||| spaces. Chinese text (as well as text of other Oriental languages)
bodyText ||| is made up of ideographic characters, and a word can comprise
bodyText ||| one, two or more such characters, without explicit indication
bodyText ||| where one word ends and another begins.
bodyText ||| This has implications for natural language processing and
bodyText ||| information retrieval with Chinese text. Text processing
bodyText ||| techniques that have been developed for Western languages deal
bodyText ||| with words as meaningful text units and assume that words are
bodyText ||| easy to identify. These techniques may not work well for Chinese
bodyText ||| text without some adjustments. To apply these techniques to
author ||| Teck Ee Loh
address ||| 10 Kent Ridge Crescent
affiliation ||| Data Storage Institute
address ||| Singapore 119260
address ||| (65) 874-8413
email ||| dsilohte@dsi.nus.edu.sg
bodyText ||| Chinese text, automatic methods for identifying word boundaries
bodyText ||| accurately have to be developed. The process of identifying word
bodyText ||| boundaries has been referred to as text segmentation or, more
bodyText ||| accurately, word segmentation.
bodyText ||| Several techniques have been developed for Chinese text
bodyText ||| segmentation. They can be divided into:
listItem ||| 1. statistical methods, based on statistical properties and
listItem ||| frequencies of characters and character strings in a corpus
listItem ||| (e.g. [13] and [16]).
listItem ||| 2. dictionary-based methods, often complemented with
listItem ||| grammar rules. This approach uses a dictionary of words to
listItem ||| identify word boundaries. Grammar rules are often used to
listItem ||| resolve conflicts (choose between alternative segmentations)
listItem ||| and to improve the segmentation (e.g. [4], [8], [19] and [20]).
listItem ||| 3. syntax-based methods, which integrate the word
listItem ||| segmentation process with syntactic parsing or part-of-speech
listItem ||| tagging (e.g. [1]).
listItem ||| 4. conceptual methods, that make use of some kind of semantic
listItem ||| processing to extract information and store it in a knowledge
listItem ||| representation scheme. Domain knowledge is used for
listItem ||| disambiguation (e.g. [9]).
bodyText ||| Many researchers use a combination of methods (e.g. [14]).
bodyText ||| The objective of this study was to empirically develop a
bodyText ||| statistical formula for Chinese text segmentation. Researchers
bodyText ||| have used different statistical methods in segmentation, most of
bodyText ||| which were based on theoretical considerations or adopted from
bodyText ||| other fields. In this study, we developed a statistical formula
bodyText ||| empirically by performing stepwise logistic regression using a
bodyText ||| sample of sentences that had been manually segmented. This
bodyText ||| paper reports the new formula developed for identifying 2-
bodyText ||| character words, and the effectiveness of this formula compared
bodyText ||| with the mutual information formula.
bodyText ||| This study has the following novel aspects:
listItem ||| •	The statistical formula was derived empirically using
listItem ||| regression analysis.
listItem ||| •	The manual segmentation was performed to identify
bodyText ||| meaningful	words rather than simple words.
bodyText ||| Meaningful words include phrasal words and multi-
bodyText ||| word terms.
listItem ||| •	In addition to the relative frequencies of bigrams and
bodyText ||| characters often used in other studies, our study also
bodyText ||| investigated the use of document frequencies and weighted
page ||| 82
bodyText ||| document frequencies. Weighted document frequencies are
bodyText ||| similar to document frequencies but each document is
bodyText ||| weighted by the square of the number of times the character
bodyText ||| or bigram occurs in the document.
listItem ||| •	Contextual information was included in the study. To predict
listItem ||| whether the bigram BC in the character string	A B C D
listItem ||| constitutes a word, we investigated whether the
listItem ||| frequencies for AB, CD, A and D should be included in the
listItem ||| formula.
listItem ||| •	Local frequencies were included in the study. We
listItem ||| investigated character and bigram frequencies within the
listItem ||| document in which the sentence occurs (i.e. the number of
listItem ||| times the character or bigram appears in the document being
listItem ||| segmented).
listItem ||| •	We investigated whether the position of the bigram (at the
listItem ||| beginning of the sentence, before a punctuation mark, or after
listItem ||| a punctuation mark) had a significant effect.
listItem ||| •	We developed a segmentation algorithm to apply the
listItem ||| statistical formula to segment sentences and resolve conflicts.
bodyText ||| In this study, our objective was to segment text into
bodyText ||| meaningful words rather than simple words . A simple
bodyText ||| word is the smallest independent unit of a sentence that has
bodyText ||| meaning on its own. A meaningful word can be a simple word or
bodyText ||| a compound word comprising 2 or more simple words –
bodyText ||| depending on the context. In many cases, the meaning of a
bodyText ||| compound word is more than just a combination of the meanings
bodyText ||| of the constituent simple words, i.e. some meaning is lost when
bodyText ||| the compound word is segmented into simple words.
bodyText ||| Furthermore, some phrases are used so often that native speakers
bodyText ||| perceive them and use them as a unit. Admittedly, there is some
bodyText ||| subjectivity in the manual segmentation of text. But the fact that
bodyText ||| statistical models can be developed to predict the manually
bodyText ||| segmented words substantially better than chance indicates some
bodyText ||| level of consistency in the manual segmentation.
bodyText ||| The problem of identifying meaningful words is not limited to
bodyText ||| Chinese and oriental languages. Identifying multi-word terms is
bodyText ||| also a problem in text processing with English and other Western
bodyText ||| languages, and researchers have used the mutual information
bodyText ||| formula and other statistical approaches for identifying such
bodyText ||| terms (e.g. [3], [6] and [7]).
sectionHeader ||| 2. PREVIOUS STUDIES
bodyText ||| There are few studies using a purely statistical approach to
bodyText ||| Chinese text segmentation. One statistical formula that has been
bodyText ||| used by other researchers (e.g. [11] and [16]) is the mutual
bodyText ||| information formula. Given a character string A B C D
bodyText ||| , the mutual information for the bigram BC is given by the
bodyText ||| formula:
equation ||| freq(BC)
equation ||| log2 freq(B) * freq(C)
equation ||| = log2 freq(BC) – log2 freq(B) – log2 freq(C)
bodyText ||| where freq refers to the relative frequency of the character or
bodyText ||| bigram in the corpus (i.e. the number of times the character or
bodyText ||| bigram occurs in the corpus divided by the number of characters
bodyText ||| in the corpus).
bodyText ||| Mutual information is a measure of how strongly the two
bodyText ||| characters are associated, and can be used as a measure of how
bodyText ||| likely the pair of characters constitutes a word. Sproat & Shih
bodyText ||| [16] obtained recall and precision values of 94% using mutual
bodyText ||| information to identify words. This study probably segmented
bodyText ||| text into simple words rather than meaningful words. In our
bodyText ||| study, text was segmented into meaningful words and we
bodyText ||| obtained much poorer results for the mutual information
bodyText ||| formula.
bodyText ||| Lua [12] and Lua & Gan [13] applied information theory to the
bodyText ||| problem of Chinese text segmentation. They calculated the
bodyText ||| information content of characters and words using the
bodyText ||| information entropy formula I = - log2 P, where P is the
bodyText ||| probability of occurrence of the character or word. If the
bodyText ||| information content of a character string is less than the sum of
bodyText ||| the information content of the constituent characters, then the
bodyText ||| character string is likely to constitute a word. The formula for
bodyText ||| calculating this loss of information content when a word is
bodyText ||| formed is identical to the mutual information formula. Lua &
bodyText ||| Gan [13] obtained an accuracy of 99% (measured in terms of the
bodyText ||| number of errors per 100 characters).
bodyText ||| Tung & Lee [18] also used information entropy to identify
bodyText ||| unknown words in a corpus. However, instead of calculating the
bodyText ||| entropy value for the character string that is hypothesized to be a
bodyText ||| word (i.e. the candidate word), they identified all the characters
bodyText ||| that occurred to the left of the candidate word in the corpus. For
bodyText ||| each left character, they calculated the probability and entropy
bodyText ||| value for that character given that it occurs to the left of the
bodyText ||| candidate word. The same is done for the characters to the right
bodyText ||| of the candidate word. If the sum of the entropy values for the
bodyText ||| left characters and the sum of the entropy values for the right
bodyText ||| characters are both high, than the candidate word is considered
bodyText ||| likely to be a word. In other words, a character string is likely to
bodyText ||| be a word if it has several different characters to the left and to
bodyText ||| the right of it in the corpus, and none of the left and right
bodyText ||| characters predominate (i.e. not strongly associated with the
bodyText ||| character string).
bodyText ||| Ogawa & Matsuda [15] developed a statistical method to
bodyText ||| segment Japanese text. Instead of attempting to identify words
bodyText ||| directly, they developed a formula to estimate the probability that
bodyText ||| a bigram straddles a word boundary. They referred to this as the
bodyText ||| segmentation probability. This was complemented with some
bodyText ||| syntactic information about which class of characters could be
bodyText ||| combined with which other class.
bodyText ||| All the above mathematical formulas used for identifying words
bodyText ||| and word boundaries were developed based on theoretical
bodyText ||| considerations and not derived empirically.
bodyText ||| Other researchers have developed statistical methods to find the
bodyText ||| best segmentation for the whole sentence rather than focusing on
bodyText ||| identifying individual words. Sproat et al. [17] developed a
bodyText ||| stochastic finite state model for segmenting text. In their model,
bodyText ||| a word dictionary is represented as a weighted finite state
bodyText ||| transducer. Each weight represents the estimated cost of the
bodyText ||| word (calculated using the negative log probability). Basically,
bodyText ||| the system selects the sentence segmentation that has the
bodyText ||| smallest total cost. Chang & Chen [1] developed a method for
bodyText ||| word segmentation and part-of-speech tagging based on a first-
bodyText ||| order hidden Markov model.
equation ||| MI(BC) =
page ||| 83
sectionHeader ||| 3. RESEARCH METHOD
bodyText ||| The purpose of this study was to empirically develop a statistical
bodyText ||| formula for identifying 2-character words as well as to
bodyText ||| investigate the usefulness of various factors for identifying the
bodyText ||| words. A sample of 400 sentences was randomly selected from 2
bodyText ||| months (August and September 1995) of news articles from the
bodyText ||| Xin Hua News Agency, comprising around 2.3 million characters.
bodyText ||| The sample sentences were manually segmented. The
bodyText ||| segmentation rules described in [10] were followed fairly closely.
bodyText ||| More details of the manual segmentation process, especially with
bodyText ||| regard to identifying meaningful words will be given in [5].
bodyText ||| 300 sentences were used for model building, i.e. using regression
bodyText ||| analysis to develop a statistical formula. 100 sentences were set
bodyText ||| aside for model validation to evaluate the formula developed in
bodyText ||| the regression analysis. The sample sentences were broken up
bodyText ||| into overlapping bigrams. In the regression analysis, the
bodyText ||| dependent variable was whether a bigram was a two-character
bodyText ||| word according to the manual segmentation. The independent
bodyText ||| variables were various corpus statistics derived from the corpus
bodyText ||| (2 months of news articles).
bodyText ||| The types of frequency information investigated were:
listItem ||| 1. Relative frequency of individual characters and bigrams
listItem ||| (character pairs) in the corpus, i.e. the number of times the
listItem ||| character or bigram occurs in the corpus divided by the total
listItem ||| number of characters in the corpus.
listItem ||| 2. Document frequency of characters and bigrams, i.e. the
listItem ||| number of documents in the corpus containing the character
listItem ||| or bigram divided by the total number of documents in the
listItem ||| corpus.
listItem ||| 3. Weighted document frequency of characters and bigrams. To
listItem ||| calculate the weighted document frequency of a character
listItem ||| string, each document containing the character string is
listItem ||| assigned a score equal to the square of the number of times
listItem ||| the character string occurs in the document. The scores for all
listItem ||| the documents containing the character string are then
listItem ||| summed and divided by the total number of documents in the
listItem ||| corpus to obtain the weighted document frequency for the
listItem ||| character string. The rationale is that if a character string
listItem ||| occurs several times within the same document, this is
listItem ||| stronger evidence that the character string constitutes a word,
listItem ||| than if the character string occurs once in several documents.
listItem ||| Two or more characters can occur together by chance in
listItem ||| several different documents. It is less likely for two
listItem ||| characters to occur together several times within the same
listItem ||| document by chance.
listItem ||| 4. Local frequency in the form of within-document frequency of
listItem ||| characters and bigrams, i.e. the number of times the character
listItem ||| or bigram occurs in the document being segmented.
listItem ||| 5. Contextual information. Frequency information of characters
listItem ||| adjacent to a bigram is used to help determine whether the
listItem ||| bigram is a word. For the character string	A B C D
listItem ||| , to determine whether the bigram BC is a word,
listItem ||| frequency information for the adjacent characters A and D, as
listItem ||| well as the overlapping bigrams AB and BC were considered.
listItem ||| 6. Positional information. We studied whether the position of a
listItem ||| character string (at the beginning, middle or end of a
listItem ||| sentence) gave some indication of whether the character
listItem ||| string was a word.
bodyText ||| The statistical model was developed using forward stepwise
bodyText ||| logistic regression, using the Proc Logistic function in the SAS
bodyText ||| v.6.12 statistical package for Windows. Logistic regression is an
bodyText ||| appropriate regression technique when the dependent variable is
bodyText ||| binary valued (takes the value 0 or 1). The formula developed
bodyText ||| using logistic regression predicts the probability (more
bodyText ||| accurately, the log of the odds) that a bigram is a meaningful
bodyText ||| word.
bodyText ||| In the stepwise regression, the threshold for a variable to enter
bodyText ||| the model was set at the 0.001 significance level and the
bodyText ||| threshold for retaining a variable in the model was set at 0.01. In
bodyText ||| addition, preference was given to relative frequencies and local
bodyText ||| frequencies because they are easier to calculate than document
bodyText ||| frequencies and weighted document frequencies. Also, relative
bodyText ||| frequencies are commonly used in previous studies.
bodyText ||| Furthermore, a variable was entered in a model only if it gave a
bodyText ||| noticeable improvement to the effectiveness of the model. During
bodyText ||| regression analysis, the effectiveness of the model was estimated
bodyText ||| using the measure of concordance that was automatically output
bodyText ||| by the SAS statistical program. A variable was accepted into the
bodyText ||| model only if the measure of concordance improved by at least
bodyText ||| 2% when the variable was entered into the model.
bodyText ||| We evaluated the accuracy of the segmentation using measures of
bodyText ||| recall and precision. Recall and precision in this context are
bodyText ||| defined as follows:
equation ||| Recall = No. of 2-character words identified in the automatic
equation ||| segmentation that are correct
equation ||| No. of 2-character words identified in the manual
equation ||| segmentation
equation ||| Precision = No. of 2-character words identified in the automatic
equation ||| segmentation that are correct
equation ||| No. of 2-character words identified in the automatic
equation ||| segmentation
sectionHeader ||| 4. STATISTICAL FORMULAS
sectionHeader ||| DEVELOPED
subsectionHeader ||| 4.1 The Contextual Information Formula
bodyText ||| The formula that was developed for 2-character words is as
bodyText ||| follows. Given a character string A B C D , the
bodyText ||| association strength for bigram BC is:
equation ||| Assoc(BC) = 0.35 * log2 freq(BC) + 0.37 * log2 freq(A) +
equation ||| 0.32 log2 freq(D) – 0.36 * log2 docfreqwt(AB) –
equation ||| 0.29 * log2 docfreqwt(CD) + 5.91
bodyText ||| where freq refers to the relative frequency in the corpus and
bodyText ||| docfreqwt refers to the weighted document frequency. We refer to
bodyText ||| this formula as the contextual information formula. More details
bodyText ||| of the regression model are given in Table 1.
bodyText ||| The formula indicates that contextual information is helpful in
bodyText ||| identifying word boundaries. A in the formula refers to the
bodyText ||| character preceding the bigram that is being processed, whereas
bodyText ||| D is the character following the bigram. The formula indicates
bodyText ||| that if the character preceding and the character following the
bodyText ||| bigram have high relative frequencies, then the bigram is more
bodyText ||| likely to be a word.
page ||| 84
table ||| 		Parameter	Standard	Wald		Pr >	Standardized
table ||| Variable	DF	Estimate	Error	Chi-Square		Chi-Square	Estimate
table ||| INTERCPT	1	5.9144	0.1719	1184.0532	0.0001	.
table ||| Log freq(BC)	1	0.3502	0.0106	1088.7291	0.0001	0.638740
table ||| Log freq(A)	1	0.3730	0.0113	1092.1382	0.0001	0.709621
table ||| Log freq(D)	1	0.3171	0.0107	886.4446	0.0001	0.607326
table ||| Log docfreqwt(AB)	1	-0.3580	0.0111	1034.0948	0.0001	-0.800520
table ||| Log docfreqwt(CD)	1	-0.2867	0.0104	754.2276	0.0001	-0.635704
table ||| Note: freq refers to the relative frequency, and docfreqwt refers to the
table ||| weighted document frequency.
table ||| Association of Predicted Probabilities and Observed Responses
table ||| Somers' D = 0.803
table ||| Gamma	= 0.803
table ||| Tau-a	= 0.295
table ||| (23875432 pairs)	c	= 0.901
table ||| Concordant	=	90.1%
table ||| Discordant	=	9.8%
table ||| Tied	=	0.1%
tableCaption ||| Table 1. Final regression model for 2-character words
bodyText ||| Contextual information involving the weighted document
bodyText ||| frequency was also found to be significant. The formula indicates
bodyText ||| that if the overlapping bigrams AB and CD have high weighted
bodyText ||| document frequencies, then the bigram BC is less likely to be a
bodyText ||| word. We tried replacing the weighted document frequencies
bodyText ||| with the unweighted document frequencies as well as the relative
bodyText ||| frequencies. These were found to give a lower concordance score.
bodyText ||| Even with docfreq (AB) and docfreq (CD) in the model, docfreqwt
bodyText ||| (AB) and docfreqwt (CD) were found to improve the model
bodyText ||| significantly. However, local frequencies were surprisingly not
bodyText ||| found to be useful in predicting 2-character words.
bodyText ||| We investigated whether the position of the bigram in the
bodyText ||| sentence was a significant factor. We included a variable to
bodyText ||| indicate whether the bigram occurred just after a punctuation
bodyText ||| mark or at the beginning of the sentence, and another variable to
bodyText ||| indicate whether the bigram occurred just before a punctuation
bodyText ||| mark or at the end of a sentence. The interaction between each of
bodyText ||| the position variables and the various relative frequencies
bodyText ||| were not significant. However, it was found that whether or not
bodyText ||| the bigram was at the end of a sentence or just before a
bodyText ||| punctuation mark was a significant factor. Bigrams at the end of
bodyText ||| a sentence or just before a punctuation mark tend to be words.
bodyText ||| However, since this factor did not improve the concordance score
bodyText ||| by 2%, the effect was deemed too small to be included in the
bodyText ||| model.
bodyText ||| It should be noted that the contextual information used in the
bodyText ||| study already incorporates some positional information. The
bodyText ||| frequency of character A (the character preceding the bigram)
bodyText ||| was given the value 0 if the bigram was preceded by a
bodyText ||| punctuation mark or was at the beginning of a sentence.
bodyText ||| Similarly, the frequency of character D (the character following
bodyText ||| the bigram) was given the value 0 if the bigram preceded a
bodyText ||| punctuation mark.
bodyText ||| We also investigated whether the model would be different for
bodyText ||| high and low frequency words. We included in the regression
bodyText ||| analysis the interaction between the relative frequency of the
bodyText ||| bigram and the other relative frequencies. The interaction terms
bodyText ||| were not found to be significant. Finally, it is noted that the
bodyText ||| coefficients for the various factors are nearly the same, hovering
bodyText ||| around 0.34.
subsectionHeader ||| 4.2 Improved Mutual Information Formula
bodyText ||| In this study, the contextual information formula (CIF) was
bodyText ||| evaluated by comparing it with the mutual information formula
bodyText ||| (MIF). We wanted to find out whether the segmentation results
bodyText ||| using the CIF was better than the segmentation results using the
bodyText ||| MIF.
bodyText ||| In the CIF model, the coefficients of the variables were
bodyText ||| determined using regression analysis. If CIF was found to give
bodyText ||| better results than MIF, it could be because the coefficients for
bodyText ||| the variables in CIF had been determined empirically – and not
bodyText ||| because of the types of variables in the formula. To reject this
bodyText ||| explanation, regression analysis was used to determine the
bodyText ||| coefficients for the factors in the mutual information formula.
bodyText ||| We refer to this new version of the formula as the improved
bodyText ||| mutual information formula.
bodyText ||| Given a character string	A B C D	, the improved
bodyText ||| mutual information formula is:
equation ||| Improved MI(BC) = 0.39 * log2 freq(BC) - 0.28 * log2 freq(B) -
equation ||| 0.23 log2 freq(C) - 0.32
bodyText ||| The coefficients are all close to 0.3. The formula is thus quite
bodyText ||| similar to the mutual information formula, except for a
bodyText ||| multiplier of 0.3.
sectionHeader ||| 5. SEGMENTATION ALGORITHMS
bodyText ||| The automatic segmentation process has the following steps:
listItem ||| 1. The statistical formula is used to calculate a score for each
listItem ||| bigram to indicate its association strength (or how likely the
listItem ||| bigram is a word).
listItem ||| 2. A threshold value is then set and used to decide which
listItem ||| bigram is a word. If a bigram obtains a score above the
listItem ||| threshold value, then it is selected as a word. Different
listItem ||| threshold values can be used, depending on whether the user
listItem ||| prefers high recall or high precision.
listItem ||| 3. A segmentation algorithm is used to resolve conflict. If two
listItem ||| overlapping bigrams both have association scores above the
page ||| 85
table ||| 	Precision
table ||| Recall	Comparative Forward Match	Forward Match	Improvement
table ||| Mutual Information		-	-
table ||| 90%	51%
table ||| 80%	52%	47%	5%
table ||| 70%	53%	51%	2%
table ||| 60%	54%	52%	2%
table ||| Improved Mutual Information
table ||| 90%	51%		-	-
table ||| 80%	53%	46%	7%
table ||| 70%	54%	52%	2%
table ||| 60%	55%	54%	1%
table ||| Contextual Information Formula
table ||| 90%	55%	54%	1%
table ||| 80%	62%	62%	0%
table ||| 70%	65%	65%	0%
table ||| 60%	68%	68%	0%
tableCaption ||| Table 2. Recall and precision values for the comparative
tableCaption ||| forward match segmentation algorithm vs. forward match
bodyText ||| threshold value, then there is conflict or ambiguity. The
bodyText ||| frequency of such conflicts will rise as the threshold value is
bodyText ||| lowered. The segmentation algorithm resolves the conflict
bodyText ||| and selects one of the bigrams as a word.
bodyText ||| One simple segmentation algorithm is the forward match
bodyText ||| algorithm. Consider the sentence A B C D E . The
bodyText ||| segmentation process proceeds from the beginning of the
bodyText ||| sentence to the end. First the bigram AB is considered. If the
bodyText ||| association score is above the threshold, then AB is taken as a
bodyText ||| word, and the bigram CD is next considered. If the association
bodyText ||| score of AB is below the threshold, the character A is taken as a
bodyText ||| 1-character word. And the bigram BC is next considered. In
bodyText ||| effect, if the association score of both AB and BC are above
bodyText ||| threshold, the forward match algorithm selects AB as a word and
bodyText ||| not BC.
bodyText ||| The forward match method for resolving ambiguity is somewhat
bodyText ||| arbitrary and not satisfactory. When overlapping bigrams exceed
bodyText ||| the threshold value, it simply decides in favour of the earlier
bodyText ||| bigram. Another segmentation algorithm was developed in this
bodyText ||| study which we refer to as the comparative forward match
bodyText ||| algorithm. This has an additional step:
bodyText ||| If 2 overlapping bigrams AB and BC both have scores above
bodyText ||| the threshold value then their scores are compared. If AB has a
bodyText ||| higher value, then it is selected as a word, and the program
bodyText ||| next considers the bigrams CD and DE. On the other hand, if
bodyText ||| AB has a lower value, then character A is selected as a 1-
bodyText ||| character word, and the program next considers bigrams BC
bodyText ||| and CD.
bodyText ||| The comparative forward match method (CFM) was compared
bodyText ||| with the forward match method (FM) by applying them to the 3
bodyText ||| statistical formulas (the contextual information formula, the
bodyText ||| mutual information formula and the improved mutual
bodyText ||| information formula). One way to compare the effectiveness of
bodyText ||| the 2 segmentation algorithms is by comparing their precision
bodyText ||| figures at the same recall levels. The precision figures for
table ||| Precision
table ||| Recall	Mutual	Improved Mutual Contextual
table ||| Information	Information	Information
table ||| 90%	57%	(0.0)	57%	(-2.5)	61%	(-1.5)
table ||| 80%	59%	(3.7)	59%	(-1.5)	66%	(-0.8)
table ||| 70%	59%	(4.7)	60%	(-1.0)	70%	(-0.3)
table ||| 60%	60%	(5.6)	62%	(-0.7)	74%	(0.0)
table ||| * Threshold values are given in parenthesis.
tableCaption ||| Table 3. Recall and precision for three statistical formulas
bodyText ||| selected recall levels are given in Table 2. The results are based
bodyText ||| on the sample of 300 sentences.
bodyText ||| The comparative forward match algorithm gave better results for
bodyText ||| the mutual information and improved mutual information
bodyText ||| formulas – especially at low threshold values when a large
bodyText ||| number of conflicts are likely. Furthermore, for the forward
bodyText ||| match method, the recall didn t go substantially higher than
bodyText ||| 80% even at low threshold values.
bodyText ||| For the contextual information formula, the comparative forward
bodyText ||| match method did not perform better than forward match, except
bodyText ||| at very low threshold values when the recall was above 90%.
bodyText ||| This was expected because the contextual information formula
bodyText ||| already incorporates information about neighboring characters
bodyText ||| within the formula. The formula gave very few conflicting
bodyText ||| segmentations. There were very few cases of overlapping
bodyText ||| bigrams both having association scores above the threshold –
bodyText ||| except when threshold values were below –1.5.
sectionHeader ||| 6. EVALUATION
subsectionHeader ||| 6.1 Comparing the Contextual Information
subsectionHeader ||| Formula with the Mutual Information
subsectionHeader ||| Formula
bodyText ||| In this section we compare the effectiveness of the contextual
bodyText ||| information formula with the mutual information formula and
bodyText ||| the improved mutual information formula using the 100
bodyText ||| sentences that had been set aside for evaluation purposes. For the
bodyText ||| contextual information formula, the forward match segmentation
bodyText ||| algorithm was used. The comparative forward match algorithm
bodyText ||| was used for the mutual information and the improved mutual
bodyText ||| information formulas.
bodyText ||| The three statistical formulas were compared by comparing their
bodyText ||| precision figures at 4 recall levels – at 60%, 70%, 80% and 90%.
bodyText ||| For each of the three statistical formulas, we identified the
bodyText ||| threshold values that would give a recall of 60%, 70%, 80% and
bodyText ||| 90%. We then determined the precision values at these threshold
bodyText ||| values to find out whether the contextual information formula
bodyText ||| gave better precision than the other two formulas at 60%, 70%,
bodyText ||| 80% and 90% recall. These recall levels were selected because a
bodyText ||| recall of 50% or less is probably unacceptable for most
bodyText ||| applications.
bodyText ||| The precision figures for the 4 recall levels are given in Table 3.
bodyText ||| The recall-precision graphs for the 3 formulas are given in Fig. 1.
bodyText ||| The contextual information formula substantially outperforms
bodyText ||| the mutual information and the improved mutual information
bodyText ||| formulas. At the 90% recall level, the contextual information
page ||| 86
table ||| Avg Precision
table ||| Avg	Mutual	Improved Mutual Contextual
table ||| Recall Information	Information	Information
table ||| 90%	57%	(1.0)	58%	(-2.3)	61%	(-1.5)
table ||| 80%	60%	(3.8)	60%	(-1.4)	67%	(-0.7)
table ||| 70%	59%	(4.8)	60%	(-1.0)	70%	(-0.3)
table ||| 60%	60%	(5.6)	63%	(-0.6)	73%	(0.0)
table ||| * Threshold values are given in parenthesis.
tableCaption ||| Table 4. Average recall and average precision for the three
tableCaption ||| statistical formulas
figure ||| 60	65	70	75	80	85	90	95
figure ||| Recall(%)
figureCaption ||| Fig. 1. Recall-precision graph for the three statistical
bodyText ||| formula was better by about 4%. At the 60% recall level, it
bodyText ||| outperformed the mutual information formula by 14% (giving a
bodyText ||| relative improvement of 23%). The results also indicate that the
bodyText ||| improved mutual information formula does not perform better
bodyText ||| than the mutual information formula.
subsectionHeader ||| 6.2 Statistical Test of Significance
bodyText ||| In order to perform a statistical test, recall and precision figures
bodyText ||| were calculated for each of the 100 sentences used in the
bodyText ||| evaluation. The average recall and the average precision across
bodyText ||| the 100 sentences were then calculated for the three statistical
bodyText ||| formulas. In the previous section, recall and precision were
bodyText ||| calculated for all the 100 sentences combined. Here, recall and
bodyText ||| precision were obtained for individual sentences and then the
bodyText ||| average across the 100 sentences was calculated. The average
bodyText ||| precision for 60%, 70%, 80% and 90% average recall are given
bodyText ||| in Table 4.
bodyText ||| For each recall level, an analysis of variance with repeated
bodyText ||| measures was carried out to find out whether the differences in
bodyText ||| precision were significant. Pairwise comparisons using Tukey s
bodyText ||| HSD test was also carried out. The contextual information
bodyText ||| formula was significantly better (a=0.001) than the mutual
bodyText ||| information and the improved mutual information formulas at all
bodyText ||| 4 recall levels. The improved mutual information formula was
bodyText ||| not found to be significantly better than mutual information.
table ||| Association Score>1.0 (definite errors)
table ||| (	)	university (agricultural
table ||| university)
table ||| (	)	geology (geologic age)
table ||| (	)	plant (upland plant)
table ||| (	)	sovereignty (sovereign state)
table ||| Association Score Between –1.0 and 1.0
table ||| (borderline errors)
table ||| (	)	statistics (statistical data)
table ||| (	)	calamity (natural calamity)
table ||| (	)	resources (manpower resources)
table ||| (	)	professor (associate professor)
table ||| (	)	poor (pauperization)
table ||| (	)	fourteen (the 14th day)
table ||| (	)	twenty (twenty pieces)
tableCaption ||| Table 5. Simple words that are part of a longer
tableCaption ||| meaningful word
table ||| Association Score >1.0 (definite errors)
table ||| will through
table ||| telegraph [on the] day [31 July]
table ||| Association Score Between –1.0 and 1.0
table ||| (borderline errors)
table ||| still	to
table ||| will be
table ||| people etc.
table ||| I want
table ||| Person's name
table ||| (	)	Wan Wen Ju
table ||| Place name
table ||| (	)	a village name in China
table ||| (	)	Canada
table ||| Name of an organization/institution
table ||| (	)	Xin Hua Agency
table ||| (	)	The State Department
tableCaption ||| Table 6. Bigrams incorrectly identified as words
sectionHeader ||| 7. ANALYSIS OF ERRORS
bodyText ||| The errors that arose from using the contextual information
bodyText ||| formula were analyzed to gain insights into the weaknesses of
bodyText ||| the model and how the model can be improved. There are two
bodyText ||| types of errors: errors of commission and errors of omission.
bodyText ||| Errors of commission are bigrams that are identified by the
bodyText ||| automatic segmentation to be words when in fact they are not
bodyText ||| (according to the manual segmentation). Errors of omission are
bodyText ||| bigrams that are not identified by the automatic segmentation to
bodyText ||| be words but in fact they are.
bodyText ||| The errors depend of course on the threshold values used. A high
bodyText ||| threshold (e.g. 1.0) emphasizes precision and a low threshold
bodyText ||| (e.g. –1.0) emphasizes recall. 50 sentences were selected from
bodyText ||| the 100 sample sentences to find the distribution of errors at
bodyText ||| different regions of threshold values.
figure ||| Contextual information
figure ||| Mutual information
figure ||| Improved mutual
figure ||| information
figure ||| 	75 70 65 60 55
page ||| 87
table ||| Association Score between -1.0 and -2.0
table ||| the northern section of a construction project
table ||| fragments of ancient books
table ||| Association Score < -2.0
table ||| September
table ||| 3rd day
table ||| (name of a district in China )
table ||| (name of an institution)
table ||| the Book of Changes
tableCaption ||| Table 7. 2-character words with association score
tableCaption ||| below -1.0
bodyText ||| We divide the errors of commission (bigrams that are incorrectly
bodyText ||| identified as words by the automatic segmentation) into 2 groups:
listItem ||| 1. Definite errors: bigrams with association scores above 1.0 but
listItem ||| are not words
listItem ||| 2. Borderline errors: bigrams with association scores between –
listItem ||| 1.0 and 1.0 and are not words
bodyText ||| We also divide the errors of omission (bigrams that are words
bodyText ||| but are not identified by the automatic segmentation) into 2
bodyText ||| groups:
listItem ||| 1. Definite errors: bigrams with association scores below –1.0
listItem ||| but are words
listItem ||| 2. Borderline errors: bigrams with association scores between –
listItem ||| 1.0 and 1.0 and are words.
subsectionHeader ||| 7.1 Errors of Commission
bodyText ||| Errors of commission can be divided into 2 types:
listItem ||| 1. The bigram is a simple word that is part of a longer
listItem ||| meaningful word.
listItem ||| 2. The bigram is not a word (neither simple word nor
listItem ||| meaningful word).
bodyText ||| Errors of the first type are illustrated in Table 5. The words
bodyText ||| within parenthesis are actually meaningful words but segmented
bodyText ||| as simple words (words on the left). The words lose part of the
bodyText ||| meaning when segmented as simple words. These errors
bodyText ||| occurred mostly with 3 or 4-character meaningful words.
bodyText ||| Errors of the second type are illustrated in Table 6. Many of the
bodyText ||| errors are caused by incorrectly linking a character with a
bodyText ||| function word or pronoun. Some of the errors can easily be
bodyText ||| removed by using a list of function words and pronouns to
bodyText ||| identify these characters.
subsectionHeader ||| 7.2 Errors of Omission
bodyText ||| Examples of definite errors of omission (bigrams with
bodyText ||| association scores below –1.0 but are words) are given in Table
bodyText ||| 7. Most of the errors are rare words and time words. Some are
bodyText ||| ancient names, rare and unknown place names, as well as
bodyText ||| technical terms. Since our corpus comprises general news
bodyText ||| articles, these types of words are not frequent in the corpus. Time
bodyText ||| words like dates usually have low association values because
bodyText ||| they change everyday! These errors can be reduced by
bodyText ||| incorporating a separate algorithm for recognizing them.
bodyText ||| The proportion of errors of the various types are given in Table 8.
sectionHeader ||| 8. CONCLUSION
bodyText ||| A new statistical formula for identifying 2-character words in
bodyText ||| Chinese text, called the contextual information formula, was
bodyText ||| developed empirically using regression analysis. The focus was
bodyText ||| on identifying meaningful words (including multi-word terms
bodyText ||| and idioms) rather than simple words. The formula was found to
bodyText ||| give significantly and substantially better results than the mutual
bodyText ||| information formula.
bodyText ||| Contextual information in the form of the frequency of characters
bodyText ||| that are adjacent to the bigram being processed as well as the
bodyText ||| weighted document frequency of the overlapping bigrams were
bodyText ||| found to be significant factors for predicting the probablity that
bodyText ||| the bigram constitutes a word. Local information (e.g. the
bodyText ||| number of times the bigram occurs in the document being
bodyText ||| segmented) and the position of the bigram in the sentence were
bodyText ||| not found to be useful in determining words.
bodyText ||| Of the bigrams that the formula erroneously identified as words,
bodyText ||| about 80% of them were actually simple words. Of the rest,
bodyText ||| many involved incorrect linking with a function words. Of the
bodyText ||| words that the formula failed to identify as words, more than a
bodyText ||| third of them were rare words or time words. The proportion of
bodyText ||| rare words increased as the threshold value used was lowered.
bodyText ||| These rare words cannot be identified using statistical
bodyText ||| techniques.
bodyText ||| This study investigated a purely statistical approach to text
table ||| Errors of Commission	Borderline Cases	Errors of Omission
table ||| Association score > 1.0	Association score: –1.0 to1.0	Association score < –1.0
table ||| (No. of errors=34)	(No. of cases: 210)
table ||| Simple words	Not words	Simple words	Not words	Meaning- ful words	Association score:	Association score
table ||| 82.3%	17.7%	55.2%	20.5%	24.3%	–1.0 to –2.0	< –2.0
table ||| 					(No. of errors=43)	(No. of errors=22)
table ||| 					Rare words	Others	Rare words	Others
table ||| 					& time	76.8%	& time	36.4%
table ||| 					words		words
table ||| 					23.2%		63.6%
tableCaption ||| Table 8. Proportion of errors of different types
page ||| 88
bodyText ||| segmentation. The advantage of the statistical approach is that it
bodyText ||| can be applied to any domain, provided that the document
bodyText ||| collection is sufficiently large to provide frequency information.
bodyText ||| A domain-specific dictionary of words is not required. In fact, the
bodyText ||| statistical formula can be used to generate a shortlist of candidate
bodyText ||| words for such a dictionary. On the other hand, the statistical
bodyText ||| method cannot identify rare words and proper names. It is also
bodyText ||| fooled by combinations of function words that occur frequently
bodyText ||| and by function words that co-occur with other words.
bodyText ||| It is well-known that a combination of methods is needed to give
bodyText ||| the best segmentation results. The segmentation quality in this
bodyText ||| study can be improved by using a list of function words and
bodyText ||| segmenting the function words as single character words. A
bodyText ||| dictionary of common and well-known names (including names
bodyText ||| of persons, places, institutions, government bodies and classic
bodyText ||| books) could be used by the system to identify proper names that
bodyText ||| occur infrequently in the corpus. Chang et al. [2] developed a
bodyText ||| method for recognizing proper nouns using a dictionary of family
bodyText ||| names in combination with a statistical method for identifying
bodyText ||| the end of the name. An algorithm for identifying time and dates
bodyText ||| would also be helpful. It is not clear whether syntactic processing
bodyText ||| can be used to improve the segmentation results substantially.
bodyText ||| Our current work includes developing statistical formulas for
bodyText ||| identifying 3 and 4-character words, as well as investigating
bodyText ||| whether the statistical formula developed here can be used with
bodyText ||| other corpora. The approach adopted in this study can also be
bodyText ||| used to develop statistical models for identifying multi-word
bodyText ||| terms in English text. It would be interesting to see whether the
bodyText ||| regression model developed for English text is similar to the one
bodyText ||| developed in this study for Chinese text. Frantzi, Ananiadou &
bodyText ||| Tsujii [7], using a different statistical approach, found that
bodyText ||| contextual information could be used to improve the
bodyText ||| identification of multi-word terms in English text.
sectionHeader ||| 9. REFERENCES
reference ||| [1] Chang, C.-H., and Chen, C.-D. A study of integrating
reference ||| Chinese word segmentation and part-of-speech tagging.
reference ||| Communications of COLIPS, 3, 1 (1993), 69-77.
reference ||| [2] Chang, J.-S., Chen, S.-D., Ker, S.-J., Chen, Y., and Liu, J.S.
reference ||| A multiple-corpus approach to recognition of proper names
reference ||| in Chinese texts. Computer Processing of Chinese and
reference ||| Oriental Languages, 8, 1 (June 1994), 75-85.
reference ||| [3] Church, K.W., and Hanks, P. Word association norms,
reference ||| mutual information and lexicography. In Proceedings of the
reference ||| 27th Annual Meeting of the Association for Computational
reference ||| Linguistics (Vancouver, June 1989), 76-83.
reference ||| [4] Dai, J.C., and Lee, H.J. A generalized unification-based LR
reference ||| parser for Chinese. Computer Processing of Chinese and
reference ||| Oriental Languages, 8, 1 (1994), 1-18.
reference ||| [5] Dai, Y. Developing a new statistical method for Chinese
reference ||| text segmentation. (Master s thesis in preparation)
reference ||| [6] Damerau, F.J. Generating and evaluating domain-oriented
reference ||| multi-word terms from texts. Information Processing &
reference ||| Management, 29, 4 (1993), 433-447.
reference ||| [7] Frantzi, K.T., Ananiadou, S., and Tsujii, J. The C-
reference ||| value/NC-value method of automatic recognition for multi-
reference ||| word terms. In C. Nikolaou and C. Stephanidis (eds.),
reference ||| Research and Advanced Technology for Digital Libraries,
reference ||| 2nd European Conference, ECDL 98 (Heraklion, Crete,
reference ||| September 1998), Springer-Verlag, 585-604.
reference ||| [8] Liang, N.Y. The knowledge of Chinese words segmentation
reference ||| [in Chinese]. Journal of Chinese Information Processing, 4,
reference ||| 2 (1990), 42-49.
reference ||| [9] Liu, I.M. Descriptive-unit analysis of sentences: Toward a
reference ||| model natural language processing. Computer Processing of
reference ||| Chinese & Oriental Languages, 4, 4 (1990), 314-355.
reference ||| [10] Liu, Y., Tan, Q., and Shen, X.K. Xin xi chu li yong xian dai
reference ||| han yu fen ci gui fan ji zi dong fen ci fang fa [ Modern
reference ||| Chinese Word Segmentation Rules and Automatic Word
reference ||| Segmentation Methods for Information Processing ]. Qing
reference ||| Hua University Press, Beijing, 1994.
reference ||| [11] Lua, K.T. Experiments on the use of bigram mutual
reference ||| information in Chinese natural language processing.
reference ||| Presented at the 1995 International Conference on Computer
reference ||| Processing of Oriental Languages (ICCPOL) (Hawaii,
reference ||| November 1995). Available: http://137.132.89.143/luakt/
reference ||| publication.html
reference ||| [12] Lua, K.T. From character to word - An application of
reference ||| information theory. Computer Processing of Chinese &
reference ||| Oriental Languages, 4, 4 (1990), 304-312.
reference ||| [13] Lua, K.T., and Gan, G.W. An application of information
reference ||| theory in Chinese word segmentation. Computer Processing
reference ||| of Chinese & Oriental Languages, 8, 1 (1994), 115-124.
reference ||| [14] Nie, J.Y., Hannan, M.L., and Jin, W.Y. Unknown word
reference ||| detection and segmentation of Chinese using statistical and
reference ||| heuristic knowledge. Communications of COLIPS, 5, 1&2
reference ||| (1995), 47-57.
reference ||| [15] Ogawa, Y., and Matsuda, T. Overlapping statistical word
reference ||| indexing: A new indexing method for Japanese text. In
reference ||| Proceedings of the 20th Annual International ACM SIGIR
reference ||| Conference on Research and Development in Information
reference ||| Retrieval (Philadelphia, July 1997), ACM, 226-234.
reference ||| [16] Sproat, R., and Shih, C.L. A statistical method for finding
reference ||| word boundaries in Chinese text. Computer Processing of
reference ||| Chinese & Oriental Languages, 4, 4 (1990), 336-351.
reference ||| [17] Sproat, R., Shih, C., Gale, W., and Chang, N. A stochastic
reference ||| finite-state word-segmentation algorithm for Chinese.
reference ||| Computational Lingustics, 22, 3 (1996), 377-404.
reference ||| [18] Tung, C.-H., and Lee, H.-J. Identification of unknown words
reference ||| from a corpus. Computer Processing of Chinese and
reference ||| Oriental Languages, 8 (Supplement, Dec. 1994), 131-145.
reference ||| [19] Wu, Z., and Tseng, G. ACTS: An automatic Chinese text
reference ||| segmentation system for full text retrieval. Journal of the
reference ||| American Society for Information Science, 46, 2 (1995), 83-
reference ||| 96.
reference ||| [20] Yeh, C.L., and Lee, H.J. Rule-based word identification for
reference ||| mandarin Chinese sentences: A unification approach.
reference ||| Computer Processing of Chinese and Oriental Languages, 5,
reference ||| 2 (1991), 97-118.
page ||| 89
