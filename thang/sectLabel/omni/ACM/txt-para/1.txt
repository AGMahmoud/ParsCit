# Para 0 1
2-Source Dispersers for Sub-Polynomial Entropy and
# Para 1 1
Ramsey Graphs Beating the Frankl-Wilson Construction
# Para 2 1
∗
# Para 3 1
Boaz Barak
# Para 4 2
Department of Computer Science
Princeton University
# Para 6 2
boaz@cs.princeton.edu
Ronen Shaltiel ‡
# Para 8 3
University of Haifa
Mount Carmel
Haifa, Israel
# Para 11 1
ronen@cs.haifa.ac.il
# Para 12 1
ABSTRACT
# Para 13 7
The main result of this paper is an explicit disperser for two 
independent sources on n bits, each of entropy k = no(1). 
Put differently, setting N = 2n and K = 2k, we construct 
explicit N × N Boolean matrices for which no K × K sub- 
matrix is monochromatic. Viewed as adjacency matrices of 
bipartite graphs, this gives an explicit construction of K- 
Ramsey bipartite graphs of size N.
# Para 20 5
This greatly improves the previous bound of k = o(n) of 
Barak, Kindler, Shaltiel, Sudakov and Wigderson [4]. It also 
significantly improves the 25-year record of k = ~O(√n) on 
the special case of Ramsey graphs, due to Frankl and Wilson 
[9].
# Para 25 3
The construction uses (besides ”classical” extractor ideas) 
almost all of the machinery developed in the last couple of 
years for extraction from independent sources, including:
# Para 28 2
•	Bourgain’s extractor for 2 independent sources of some 
entropy rate &lt; 1/2 [5]
# Para 30 2
•	Raz’s extractor for 2 independent sources, one of which 
has any entropy rate &gt; 1/2 [18]
# Para 32 1
∗Supported by a Princeton University startup grant.
# Para 33 4
†Most of this work was done while the author was visiting 
Princeton University and the Institute for Advanced Study. 
Supported in part by an MCD fellowship from UT Austin 
and NSF Grant CCR-0310960.
# Para 37 2
‡This research was supported by the United States-Israel 
Binational Science Foundation (BSF) grant 2004329.
# Para 39 1
§This research was supported by NSF Grant CCR-0324906.
# Para 40 6
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, to 
republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee.
# Para 46 1
STOC’06, May 21–23, 2006, Seattle, Washington, USA.
# Para 47 1
Copyright 2006 ACM 1-59593-134-1/06/0005 ...$5.00.
# Para 48 1
Anup Rao †
# Para 49 2
Department of Computer Science
University of Texas at Austin
# Para 51 2
arao@cs.utexas.edu
Avi Wigderson §
# Para 53 3
Institute for Advanced Study
Princeton
New Jersey
# Para 56 1
avi@math.ias.edu
# Para 57 2
•	Rao’s extractor for 2 independent block-sources of en-
tropy no(1) [17]
# Para 59 2
•	The “Challenge-Response” mechanism for detecting 
“entropy concentration” of [4].
# Para 61 9
The main novelty comes in a bootstrap procedure which 
allows the Challenge-Response mechanism of [4] to be used 
with sources of less and less entropy, using recursive calls 
to itself. Subtleties arise since the success of this mecha-
nism depends on restricting the given sources, and so re-
cursion constantly changes the original sources. These are 
resolved via a new construct, in between a disperser and 
an extractor, which behaves like an extractor on sufficiently 
large subsources of the given ones.
# Para 70 3
This version is only an extended abstract, please see the 
full version, available on the authors’ homepages, for more 
details.
# Para 73 1
Categories and Subject Descriptors
# Para 74 2
G.2.2 [Mathematics of Computing]: Discrete Mathe-
matics—Graph algorithms
# Para 76 1
General Terms
# Para 77 1
Theory, Algorithms
# Para 78 1
Keywords
# Para 79 2
Dispersers, Ramsey Graphs, Independent Sources, Extrac-
tors
# Para 81 1
1. INTRODUCTION
# Para 82 8
This paper deals with randomness extraction from weak 
random sources. Here a weak random source is a distribu-
tion which contains some entropy. The extraction task is to 
design efficient algorithms (called extractors) to convert this 
entropy into useful form, namely a sequence of independent 
unbiased bits. Beyond the obvious motivations (potential 
use of physical sources in pseudorandom generators and in 
derandomization), extractors have found applications in a
# Para 90 1
671
# Para 91 4
variety of areas in theoretical computer science where ran-
domness does not seem an issue, such as in efficient con-
structions of communication networks [24, 7], error correct-
ing codes [22, 12], data structures [14] and more.
# Para 95 6
Most work in this subject over the last 20 years has fo-
cused on what is now called seeded extraction, in which the 
extractor is given as input not only the (sample from the) 
defective random source, but also a few truly random bits 
(called the seed). A comprehensive survey of much of this 
body of work is [21].
# Para 101 10
Another direction, which has been mostly dormant till 
about two years ago, is (seedless, deterministic) extraction 
from a few independent weak sources. This kind of extrac-
tion is important in several applications where it is unrealis-
tic to have a short random seed or deterministically enumer-
ate over its possible values. However, it is easily shown to be 
impossible when only one weak source is available. When at 
least 2 independent sources are available extraction becomes 
possible in principle. The 2-source case is the one we will 
focus on in this work.
# Para 111 12
The rest of the introduction is structured as follows. We’ll 
start by describing our main result in the context of Ramsey 
graphs. We then move to the context of extractors and dis-
perser, describing the relevant background and stating our 
result in this language. Then we give an overview of the 
construction of our dispersers, describing the main building 
blocks we construct along the way. As the construction is 
quite complex and its analysis quite subtle, in this proceed-
ings version we try to abstract away many of the technical 
difficulties so that the main ideas, structure and tools used 
are highlighted. For that reason we also often state defini-
tions and theorems somewhat informally.
# Para 123 1
1.1 Ramsey Graphs
# Para 124 3
DefInItIOn 1.1. A graph on N vertices is called a K- 
Ramsey Graph if it contains no clique or independent set of 
size K.
# Para 127 5
In 1947 Erd}os published his paper inaugurating the Prob-
abilistic Method with a few examples, including a proof that 
most graphs on N = 2n vertices are 2n-Ramsey. The quest 
for constructing such graphs explicitly has existed ever since 
and lead to some beautiful mathematics.
# Para 132 19
The best record to date was obtained in 1981 by Frankl 
and Wilson [9], who used intersection theorems for set sys-
tems to construct N-vertex graphs which are 21�n log n-Ramsey. 
This bound was matched by Alon [1] using the Polynomial 
Method, by Grolmusz [11] using low rank matrices over rings, 
and also by Barak [2] boosting Abbot’s method with almost 
k-wise independent random variables (a construction that 
was independently discovered by others as well). Remark-
ably all of these different approaches got stuck at essentially 
the same bound. In recent work, Gopalan [10] showed that 
other than the last construction, all of these can be viewed 
as coming from low-degree symmetric representations of the 
OR function. He also shows that any such symmetric rep-
resentation cannot be used to give a better Ramsey graph, 
which gives a good indication of why these constructions 
had similar performance. Indeed, as we will discuss in a 
later section, the √n entropy bound initially looked like a 
natural obstacle even for our techniques, though eventually 
we were able to surpass it.
# Para 151 2
The analogous question for bipartite graphs seemed much 
harder.
# Para 153 3
DefInItIOn 1.2. A bipartite graph on two sets of N ver-
tices is a K-Ramsey Bipartite Graph if it has no K × K 
complete or empty bipartite subgraph.
# Para 156 7
While Erd}os’ result on the abundance of 2n-Ramsey graphs 
holds as is for bipartite graphs, until recently the best ex-
plicit construction of bipartite Ramsey graphs was 2n/2- 
Ramsey, using the Hadamard matrix. This was improved 
last year, first to o(2n/2) by Pudlak and R}odl [16] and then 
to 2o(n) by Barak, Kindler, Shaltiel, Sudakov and Wigderson 
[4] .
# Para 163 3
It is convenient to view such graphs as functions f : 
({0, 1}n)2 → {0, 1}. This then gives exactly the definition 
of a disperser.
# Para 166 4
DefInItIOn 1.3. A function f : ({0, 1}n)2 → {0, 1} is 
called a 2-source disperser for entropy k if for any two sets 
X, Y ⊂ {0, 1}n with | X | = |Y| = 2k, we have that the image 
f (X, Y) is {0, 1}.
# Para 170 4
This allows for a more formal definition of explicitness: we 
simply demand that the function f is computable in polyno-
mial time. Most of the constructions mentioned above are 
explicit in this sense.&apos;
# Para 174 2
Our main result (stated informally) significantly improves 
the bounds in both the bipartite and non-bipartite settings:
# Para 176 5
TheOrem 1.4. For every N we construct polynomial time 
computable bipartite graphs which are 2n&apos;(1)-Ramsey. A stan-
dard transformation of these graphs also yields polynomial 
time computable ordinary Ramsey Graphs with the same pa-
rameters.
# Para 181 2
1.2 Extractors and Dispersers from indepen-
dent sources
# Para 183 6
Now we give a brief review of past relevant work (with the 
goal of putting this paper in proper context) and describe 
some of the tools from these past works that we will use. 
We start with the basic definitions of k-sources by Nisan 
and Zuckerman [15] and of extractors and dispersers for in-
dependent sources by Santha and Vazirani [20].
# Para 189 5
DefInItIOn 1.5 ([15], See alSO [8]). The min-entropy 
of a distribution X is the maximum k such that for every 
element x in its support, Pr[X = x] ≤ 2-k. If X is a dis-
tribution on strings with min-entropy at least k, we will call 
X a k-source 2.
# Para 194 3
To simplify the presentation, in this version of the paper 
we will assume that we are working with entropy as opposed 
to min-entropy.
# Para 197 3
DefInItIOn 1.6 ([20]). A function f : ({0,1}n)c → 
{0, 1}m is a c-source (k, ǫ) extractor if for every family of c 
independent k-sources X&apos;, • • • , Xc, the output f (X&apos;, • • • , Xc)
# Para 200 3
&apos;The Abbot’s product based Ramsey-graph construction of 
[3] and the bipartite Ramsey construction of [16] only satisfy 
a weaker notion of explicitness.
# Para 203 1
2It is no loss of generality to imagine that X is uniformly
# Para 204 1
distributed over some (unknown) set of size 2k.
# Para 205 1
672
# Para 206 3
is a ǫ-close 3 to uniformly distributed on m bits. f is a dis-
perser for the same parameters if the output is simply re-
quired to have a support of relative size (1 − ǫ).
# Para 209 2
To simplify the presentation, in this version of the paper, 
we will assume that ǫ = 0 for all of our constructions.
# Para 211 7
In this language, Erd}os’ theorem says that most functions 
f : ({0, 1}n)2 → {0, 1} are dispersers for entropy 1 + logn 
(treating f as the characteristic function for the set of edges 
of the graph). The proof easily extends to show that indeed 
most such functions are in fact extractors. This naturally 
challenges us to find explicit functions f that are 2-source 
extractors.
# Para 218 3
Until one year ago, essentially the only known explicit 
construction was the Hadamard extractor Had defined by 
Had(x,y)
# Para 221 7
k &gt; n/2 as observed by Chor and Goldreich [8] and can 
be extended to give m = Q(n) output bits as observed by 
Vazirani [23]. Over 20 years later, a recent breakthrough 
of Bourgain [5] broke this “1/2 barrier” and can handle 2 
sources of entropy .4999n, again with linear output length 
m = 0(n). This seemingly minor improvement will be cru-
cial for our work!
# Para 228 3
TheOrem 1.7 ([5] ). There is a polynomial time com-
putable 2-source extractor f : ({0, 1}n)2 → {0, 1}m for en-
tropy .4999n and m = 0(n).
# Para 231 16
No better bounds are known for 2-source extractors. Now 
we turn our attention to 2-source dispersers. It turned out 
that progress for building good 2-source dispersers came via 
progress on extractors for more than 2 sources, all happening 
in fast pace in the last 2 years. The seminal paper of Bour-
gain, Katz and Tao [6] proved the so-called ”sum-product 
theorem” in prime fields, a result in arithmetic combina-
torics. This result has already found applications in diverse 
areas of mathematics, including analysis, number theory, 
group theory and ... extractor theory. Their work implic-
itly contained dispersers for c = O(log(n/k)) independent 
sources of entropy k (with output m = Q(k)). The use of 
the ”sum-product” theorem was then extended by Barak et 
al. [3] to give extractors with similar parameters. Note that 
for linear entropy k = 0(n), the number of sources needed 
for extraction c is a constant!
# Para 247 5
Relaxing the independence assumptions via the idea of 
repeated condensing, allowed the reduction of the number 
of independent sources to c = 3, for extraction from sources 
of any linear entropy k = 0(n), by Barak et al. [4] and 
independently by Raz [18].
# Para 252 9
For 2 sources Barak et al. [4] were able to construct dis-
persers for sources of entropy o(n). To do this, they first 
showed that if the sources have extra structure (block-source 
structure, defined below), even extraction is possible from 2 
sources. The notion of block-sources, capturing ”semi inde-
pendence” of parts of the source, was introduced by Chor 
and Goldreich [8]. It has been fundamental in the develop-
ment of seeded extractors and as we shall see, is essential 
for us as well.
# Para 261 4
DefInItIOn 1.8 ([8] ). A distribution X = X1, ... , Xc 
is a c-block-source of (block) entropy k if every block Xi 
has entropy k even conditioned on fixing the previous blocks 
X1, • • • , Xi_1 to arbitrary constants.
# Para 265 2
3The error is usually measured in terms of ℓ1 distance or 
variation distance.
# Para 267 4
This definition allowed Barak et al. [4] to show that their 
extractor for 4 independent sources, actually performs as 
well with only 2 independent sources, as long as both are 
2-block-sources.
# Para 271 3
TheOrem 1.9 ([4] ). There exists a polynomial time com-
putable extractor f : ({0, 1}n)2 → {0, 1} for 2 independent 
2-block-sources with entropy o(n).
# Para 274 12
There is no reason to assume that the given sources are 
block-sources, but it is natural to try and reduce to this 
case. This approach has been one of the most successful in 
the extractor literature. Namely try to partition a source 
X into two blocks X = X1, X2 such that X1, X2 form a 
2-block-source. Barak et al. introduced a new technique to 
do this reduction called the Challenge-Response mechanism, 
which is crucial for this paper. This method gives a way to 
“find” how entropy is distributed in a source X, guiding the 
choice of such a partition. This method succeeds only with 
small probability, dashing the hope for an extractor, but still 
yielding a disperser.
# Para 286 3
TheOrem 1.10 ([4] ). There exists a polynomial time 
computable 2-source disperser f : ({0, 1}n)2 → {0, 1} for 
entropy o(n).
# Para 289 12
Reducing the entropy requirement of the above 2-source 
disperser, which is what we achieve in this paper, again 
needed progress on achieving a similar reduction for extrac-
tors with more independent sources. A few months ago Rao 
[?] was able to significantly improve all the above results 
for c ≥ 3 sources. Interestingly, his techniques do not use 
arithmetic combinatorics, which seemed essential to all the 
papers above. He improves the results of Barak et al. [3] to 
give c = O((logn)/(logk))-source extractors for entropy k. 
Note that now the number c of sources needed for extraction 
is constant, even when the entropy is as low as nδ for any 
constant δ!
# Para 301 4
Again, when the input sources are block-sources with suf-
ficiently many blocks, Rao proves that 2 independent sources 
suffice (though this result does rely on arithmetic combina-
torics, in particular, on Bourgain’s extractor).
# Para 305 4
TheOrem 1.11 ([?] ). There is a polynomial time com-
putable extractor f : ({0, 1}n)2 → {0, 1}m for 2 independent 
c-block-sources with block entropy k and m = 0(k), as long 
as c = O((log n)/(log k)).
# Para 309 9
In this paper (see Theorem 2.7 below) we improve this 
result to hold even when only one of the 2 sources is a c-
block-source. The other source can be an arbitrary source 
with sufficient entropy. This is a central building block in 
our construction. This extractor, like Rao’s above, critically 
uses Bourgain’s extractor mentioned above. In addition it 
uses a theorem of Raz [18] allowing seeded extractors to have 
”weak” seeds, namely instead of being completely random 
they work as long as the seed has entropy rate &gt; 1/2.
# Para 318 1
2. MAIN NOTIONS AND NEW RESULTS
# Para 319 5
The main result of this paper is a polynomial time com-
putable disperser for 2 sources of entropy no(1), significantly 
improving both the results of Barak et al. [4] (o(n) entropy). 
It also improves on Frankl and Wilson [9], who only built 
Ramsey Graphs and only for entropy ~O(√n).
# Para 324 1
= (x, y)( mod 2). It is an extractor for entropy
# Para 325 1
673
# Para 326 3
ThEOREm 2.1 (MaIn thEOREm, REStatEd). There ex-
ists a polynomial time computable 2-source disperser D : 
({0, 1}n)2 → {0, 1} for entropy no(1).
# Para 329 9
The construction of this disperser will involve the con-
struction of an object which in some sense is stronger and 
in another weaker than a disperser: a subsource somewhere 
extractor. We first define a related object: a somewhere ex-
tractor, which is a function producing several outputs, one of 
which must be uniform. Again we will ignore many technical 
issues such as error, min-entropy vs. entropy and more, in 
definitions and results, which are deferred to the full version 
of this paper.
# Para 338 5
DEfInItIOn 2.2. A function f : ({0, 1}n)2 → ({0,1}m)ℓ 
is a 2-source somewhere extractor with ℓ outputs, for entropy 
k, if for every 2 independent k-sources X, Y there exists an 
i ∈ [ℓ] such the ith output f (X, Y)i is a uniformly distributed 
string of m bits.
# Para 343 9
Here is a simple construction of such a somewhere extrac-
tor with ℓ as large as poly(n) (and the p in its name will 
stress the fact that indeed the number of outputs is that 
large). It will nevertheless be useful to us (though its de-
scription in the next sentence may be safely skipped). Define 
pSE(x, y)i = V(E(x, i), E(y, i)) where E is a ”strong” loga-
rithmic seed extractor, and V is the Hadamard/Vazirani 2- 
source extractor. Using this construction, it is easy to see 
that:
# Para 352 4
PROPOSItIOn 2.3. For every n, k there is a polynomial 
time computable somewhere extractor pSE : ({0, 1}n)2 → 
({0, 1}m)ℓ with ℓ = poly(n) outputs, for entropy k, and m = 
Q(k).
# Para 356 2
Before we define subsource somewhere extractor, we must 
first define a subsource.
# Para 358 4
DEfInItIOn 2.4 (SUBSOURCES). Given random variables 
Z and Z^ on {0, 1}n we say that Z^ is a deficiency d subsource 
of Z and write Z^ ⊆ Z if there exists a set A ⊆ {0,1}n such 
that (Z|Z ∈ A) = Z^ and Pr[Z ∈ A] ≥ 2-d.
# Para 362 1
A subsource somewhere extractor guarantees the ”some-
# Para 363 1
where extractor” property only on subsources X&apos;, Y&apos; of the
# Para 364 7
original input distributions X, Y (respectively). It will be 
extremely important for us to make these subsources as large 
as possible (i.e. we have to lose as little entropy as possible). 
Controlling these entropy deficiencies is a major technical 
complication we have to deal with. However we will be in-
formal with it here, mentioning it only qualitatively when 
needed. We discuss this issue a little more in Section 6.
# Para 371 6
DEfInItIOn 2.5. A function f : ({0, 1}n)2 → ({0,1}m)ℓ 
is a 2-source subsource somewhere extractor with ℓ outputs 
for entropy k, if for every 2 independent k-sources X, Y there 
exists a subsource X^ of X, a subsource Y^ of Y and an i ∈ [ℓ] 
such the ith output f (^X, Y^)i is a uniformly distributed string 
of m bits.
# Para 377 4
A central technical result for us is that with this ”sub- 
source” relaxation, we can have much fewer outputs – in-
deed we’ll replace poly(n) outputs in our first construction 
above with no(1) outputs.
# Para 381 5
ThEOREm 2.6 (SUBSOURCE SOmEWhERE ExtRaCtOR). 
For every δ &gt; 0 there is a polynomial time computable sub- 
source somewhere extractor SSE : ({0, 1}n)2 → ({0, 1}m)ℓ 
with ℓ = no(1) outputs, for entropy k = nδ, with output 
m=√k.
# Para 386 8
We will describe the ideas used for constructing this im-
portant object and analyzing it in the next section, where 
we will also indicate how it is used in the construction of 
the final disperser. Here we state a central building block, 
mentioned in the previous section (as an improvement of the 
work of Rao [?]). We construct an extractor for 2 indepen-
dent sources one of which is a block-sources with sufficient 
number of blocks.
# Para 394 5
ThEOREm 2.7 (BlOCK SOURCE ExtRaCtOR). There is 
a polynomial time computable extractor B : ({0, 1}n)2 → 
{0,1}m for 2 independent sources, one of which is a c-block-
sources with block entropy k and the other a source of en-
tropy k, with m = 0(k), and c = O((log n)/(log k)).
# Para 399 7
A simple corollary of this block-source extractor B, is the 
following weaker (though useful) somewhere block-source 
extractor SB. A source Z = Z1, Z2, • • • , Zt is a somewhere 
c-block-source of block entropy k if for some c indices i1 &lt; 
i2 &lt; • • • &lt; ic the source Zi1, Zi2, • • • , Zic is a c-block-source. 
Collecting the outputs of B on every c-subset of blocks re-
sults in that somewhere extractor.
# Para 406 6
COROllaRY 2.8. There is a polynomial time computable 
somewhere extractorSB : ({0, 1}n)2 → ({0, 1}m)ℓ for2 inde-
pendent sources, one of which is a somewhere c-block-sources 
with block entropy k and t blocks total and the other a source 
of entropy k, with m = 0(k), c = O((log n)/(log k)), and 
ℓ ≤ tc.
# Para 412 3
In both the theorem and corollary above, the values of 
entropy k we will be interested in are k = no(1). It follows 
that a block-source with a constant c = O(1) suffices.
# Para 415 2
3. THE CHALLENGE-RESPONSE MECH-
ANISM
# Para 417 9
We now describe abstractly a mechanism which will be 
used in the construction of the disperser as well as the sub- 
source somewhere extractor. Intuitively, this mechanism al-
lows us to identify parts of a source which contain large 
amounts of entropy. One can hope that using such a mech-
anism one can partition a given source into blocks in a way 
which make it a block-source, or alternatively focus on a part 
of the source which is unusually condensed with entropy - 
two cases which may simplify the extraction problem.
# Para 426 6
The reader may decide, now or in the middle of this 
section, to skip ahead to the next section which describes 
the construction of the subsource somewhere extractor SSE, 
which extensively uses this mechanism. Then this section 
may seem less abstract, as it will be clearer where this mech-
anism is used.
# Para 432 5
This mechanism was introduced by Barak et al. [4], and 
was essential in their 2-source disperser. Its use in this paper 
is far more involved (in particular it calls itself recursively, 
a fact which creates many subtleties). However, at a high 
level, the basic idea behind the mechanism is the same:
# Para 437 1
Let Z be a source and Z&apos; a part of Z (Z projected on a
# Para 438 1
subset of the coordinates). We know that Z has entropy k,
# Para 439 1
674
# Para 440 4
and want to distinguish two possibilities: Z′ has no entropy 
(it is fixed) or it has at least k′ entropy. Z′ will get a pass 
or fail grade, hopefully corresponding to the cases of high or 
no entropy in Z′.
# Para 444 5
Anticipating the use of this mechanism, it is a good idea 
to think of Z as a ”parent” of Z′, which wants to check if 
this ”child” has sufficient entropy. Moreover, in the context 
of the initial 2 sources X, Y we will operate on, think of Z 
as a part of X, and thus that Y is independent of Z and Z′.
# Para 449 4
To execute this ”test” we will compute two sets of strings 
(all of length m, say): the Challenge C = C(Z′,Y) and 
the Response R = R(Z, Y). Z′ fails if C C R and passes 
otherwise.
# Para 453 5
The key to the usefulness of this mechanism is the follow-
ing lemma, which states that what ”should” happen, indeed 
happens after some restriction of the 2 sources Z and Y. 
We state it and then explain how the functions C and R are 
defined to accommodate its proof.
# Para 458 1
Lemma 3.1. Assume Z, Y are sources of entropy k.
# Para 459 2
1. If Z′ has entropy k′+O(m), then there are subsources 
Z^ of Z and Y^ of Y, such that
# Para 461 1
Pr[^Z′ passes] = Pr[C(^Z′, Y^) C R(^Z, Y^)] &gt; 1—nO(1)2−m
# Para 462 2
2. If Z′ is fixed (namely, has zero entropy), then for some 
subsources Z^ of Z and Y^ of Y, we have
# Para 464 1
Pr[Z′ fails] = Pr[C(^Z′, Y^) C R(^Z, Y^)] = 1
# Para 465 6
Once we have such a mechanism, we will design our dis-
perser algorithm assuming that the challenge response mech-
anism correctly identifies parts of the source with high or 
low levels of entropy. Then in the analysis, we will ensure 
that our algorithm succeeds in making the right decisions, 
at least on subsources of the original input sources.
# Para 471 3
Now let us explain how to compute the sets C and R. We 
will use some of the constructs above with parameters which 
don’t quite fit.
# Para 474 4
The response set R(Z, Y) = pSE(Z, Y) is chosen to be the 
output of the somewhere extractor of Proposition 2.3. The 
challenge set C(Z′, Y) = SSE(Z′, Y) is chosen to be the out-
put of the subsource somewhere extractor of Theorem 2.6.
# Para 478 4
Why does it work? We explain each of the two claims 
in the lemma in turn (and after each comment on the im-
portant parameters and how they differ from Barak et al. 
[4]).
# Para 482 16
1. Z′ has entropy. We need to show that Z′ passes the 
test with high probability. We will point to the out-
put string in C(^Z′, Y^′) which avoids R(^Z, Y^) with high 
probability as follows. In the analysis we will use the 
union bound on several events, one associated with 
each (poly(n) many) string in pSE(^Z, Y^). We note 
that by the definition of the response function, if we 
want to fix a particular element in the response set to 
a particular value, we can do this by fixing E(Z, i) and 
E(Y, i). This fixing keeps the restricted sources inde-
pendent and loses only O(m) entropy. In the subsource 
of Z′ guaranteed to exist by Theorem 2.6 we can afford 
to lose this entropy in Z′. Thus we conclude that one 
of its outputs is uniform. The probability that this 
output will equal any fixed value is thus 2−m, com-
pleting the argument. We note that we can handle
# Para 498 4
the polynomial output size of pSE, since the uniform 
string has length m = no(1) (something which could 
not be done with the technology available to Barak et 
al. [4]).
# Para 502 22
2. Z′ has no entropy. We now need to guarantee that 
in the chosen subsources (which we choose) ^Z, Y^, all 
strings in C = C(^Z′, Y^) are in R(^Z, Y^). First notice 
that as Z′ is fixed, C is only a function of Y. We 
set Y~ to be the subsource of Y that fixes all strings 
in C = C(Y) to their most popular values (losing 
only ℓm entropy from Y). We take care of includ-
ing these fixed strings in R(Z, Y~) one at a time, by 
restricting to subsources assuring that. Let σ be any 
m-bit string we want to appear in R(Z, Y~). Recall that 
R(z, y) = V(E(z, i), E(y, i)). We pick a ”good” seed i, 
and restrict Z, Y~ to subsources with only O(m) less 
entropy by fixing E(Z, i) = a and E(Y~, i) = b to values 
(a, b) for which V(a, b) = σ. This is repeated suc-
cessively ℓ times, and results in the final subsources 
^Z, Y^ on which ^Z′ fails with probability 1. Note that 
we keep reducing the entropy of our sources ℓ times, 
which necessitates that this ℓ be tiny (here we could 
not tolerate poly(n), and indeed can guarantee no(1), 
at least on a subsource – this is one aspect of how cru-
cial the subsource somewhere extractor SSE is to the 
construction.
# Para 524 10
We note that initially it seemed like the Challenge-Response 
mechanism as used in [4] could not be used to handle en-
tropy that is significantly less than -,/n (which is approxi-
mately the bound that many of the previous constructions 
got stuck at). The techniques of [4] involved partitioning 
the sources into t pieces of length n/t each, with the hope 
that one of those parts would have a significant amount of 
entropy, yet there’d be enough entropy left over in the rest 
of the source (so that the source can be partitioned into a 
block source).
# Para 534 8
However it is not clear how to do this when the total 
entropy is less than -,/n. On the one hand we will have 
to partition our sources into blocks of length significantly 
more than -,/n (or the adversary could distribute a negligible 
fraction of entropy in all blocks). On the other hand, if 
our blocks are so large, a single block could contain all the 
entropy. Thus it was not clear how to use the challenge 
response mechanism to find a block source.
# Para 542 2
4. THE SUBSOURCE SOMEWHERE 
EXTRACTOR SSE
# Para 544 14
We now explain some of the ideas behind the construction 
of the subsource somewhere extractor SSE of Theorem 2.6. 
Consider the source X. We are seeking to find in it a some-
where c-block-source, so that we can use it (together with Y) 
in the block-source extractor of Theorem 2.8. Like in previ-
ous works in the extractor literature (e.g. [19, 13]) we use a 
”win-win” analysis which shows that either X is already a 
somewhere c-block-source, or it has a condensed part which 
contains a lot of the entropy of the source. In this case we 
proceed recursively on that part. Continuing this way we 
eventually reach a source so condensed that it must be a 
somewhere block source. Note that in [4], the challenge re-
sponse mechanism was used to find a block source also, but 
there the entropy was so high that they could afford to use
# Para 558 1
675
# Para 559 1
Not Somewhere block source	n bits total
# Para 560 1
		t blocks			Outputs
# Para 561 1
&lt; k’
# Para 562 2
Challenge Challenge 
responded responded
# Para 564 1
X
# Para 565 1
low
# Para 566 1
med
# Para 567 1
high
# Para 568 1
n/t bits total
# Para 569 1
t blocks
# Para 570 1
Challenge Unresponded
# Para 571 1
SB
# Para 572 1
Somewhere Block Source!
# Para 573 1
med
# Para 574 1
med
# Para 575 1
low
# Para 576 3
0&lt; low &lt; k’/t 
k’/t &lt; med &lt; k’/c 
k’/c &lt; high &lt; k’
# Para 579 1
high
# Para 580 1
med
# Para 581 1
Random Row
# Para 582 1
med
# Para 583 1
SB
# Para 584 1
Figure 1: Analysis of the subsource somewhere extractor.
# Para 585 2
a tree of depth 1. They did not need to recurse or condense 
the sources.
# Para 587 15
Consider the tree of parts of the source X evolved by 
such recursion. Each node in the tree corresponds to some 
interval of bit locations of the source, with the root node 
corresponding to the entire source. A node is a child of an-
other if its interval is a subinterval of the parent. It can be 
shown that some node in the tree is ”good”; it corresponds 
to a somewhere c-source, but we don’t know which node is 
good. Since we only want a somewhere extractor, we can 
apply to each node the somewhere block-source extractor of 
Corollary 2.8 – this will give us a random output in every 
”good” node of the tree. The usual idea is output all these 
values (and in seeded extractors, merge them using the ex-
ternally given random seed). However, we cannot afford to 
do that here as there is no external seed and the number of 
these outputs (the size of the tree) is far too large.
# Para 602 12
Our aim then will be to significantly prune this number 
of candidates and in fact output only the candidates on one 
path to a canonical”good” node. First we will give a very in-
formal description of how to do this (Figure 1). Before call-
ing SSE recursively on a subpart of a current part of X, we’ll 
use the ”Challenge-Response” mechanism described above 
to check if ”it has entropy”.4 We will recurse only with the 
first (in left-to-right order) part which passes the ”entropy 
test”. Thus note that we will follow a single path on this 
tree. The algorithm SSE will output only the sets of strings 
produced by applying the somewhere c-block-extractor SB 
on the parts visited along this path.
# Para 614 3
Now let us describe the algorithm for SSE. SSE will be 
initially invoked as SSE(x, y), but will recursively call itself 
with different inputs z which will always be substrings of x.
# Para 617 3
4We note that we ignore the additional complication that 
SSE will actually use recursion also to compute the challenge 
in the challenge-response mechanism.
# Para 620 1
Algorithm: SSE(z, y)
# Para 621 2
Let pSE(., .) be the somewhere extractor with a polyno-
mial number of outputs of Proposition 2.3.
# Para 623 2
Let SB be the somewhere block source extractor of Corol-
lary 2.8.
# Para 625 2
Global Parameters: t, the branching factor of the tree. k 
the original entropy of the sources.
# Para 627 1
Output will be a set of strings.
# Para 628 2
1. If z is shorter than √k, return the empty set, else 
continue.
# Para 630 1
2. Partition z into t equal parts z = z1, z2, ... ,zt.
# Para 631 2
3. Compute the response set R(z, y) which is the set of 
strings output by pSE(z, y).
# Para 633 2
4. For i E [t], compute the challenge set C(zi, y), which 
is the set of outputs of SSE(zi, y).
# Para 635 3
5. Let h be the smallest index for which the challenge set 
C(zh, y) is not contained in the response set (set h = t 
if no such index exists).
# Para 638 1
6. Output SB(z, y) concatenated with SSE(zh, y).
# Para 639 9
Proving that indeed there are subsources on which SSE 
will follow a path to a ”good” (for these subsources) node, 
is the heart of the analysis. It is especially complex due 
to the fact that the recursive call to SSE on subparts of 
the current part is used to generate the Challenges for the 
Challenge-Response mechanism. Since SSE works only on 
a subsources we have to guarantee that restriction to these 
does not hamper the behavior of SSE in past and future calls 
to it.
# Para 648 1
Let us turn to the highlights of the analysis, for the proof
# Para 649 1
of Theorem 2.6. Let k&apos; be the entropy of the source Z at
# Para 650 1
some place in this recursion. Either one of its blocks Zi has
# Para 651 1
676
# Para 652 2
entropy k&apos;/c, in which case it is very condensed, since its 
size is n/t for t ≫ c), or it must be that c of its blocks form
# Para 654 1
a c-block source with block entropy k&apos;/t (which is sufficient
# Para 655 14
for the extractor B used by SB). In the 2nd case the fact 
that SB(z, y) is part of the output of of our SSE guarantees 
that we are somewhere random. If the 2nd case doesn’t hold, 
let Zi be the leftmost condensed block. We want to ensure 
that (on appropriate subsources) SSE calls itself on that ith 
subpart. To do so, we fix all Zj for j &lt; i to constants zj. We 
are now in the position described in the Challenge-Response 
mechanism section, that (in each of the first i parts) there 
is either no entropy or lots of entropy. We further restrict 
to subsources as explained there which make all first i − 1 
blocks fail the ”entropy test”, and the fact that Zi still has 
lots of entropy after these restrictions (which we need to 
prove) ensures that indeed SSE will be recursively applied 
to it.
# Para 669 4
We note that while the procedure SSE can be described re-
cursively, the formal analysis of fixing subsources is actually 
done globally, to ensure that indeed all entropy requirements 
are met along the various recursive calls.
# Para 673 7
Let us remark on the choice of the branching parameter t. 
On the one hand, we’d like to keep it small, as it dominates 
the number of outputs tc of SB, and thus the total number of 
outputs (which is tc logt n). For this purpose, any t = no(1) 
will do. On the other hand, t should be large enough so that 
condensing is faster than losing entropy. Here note that if 
Z is of length n, its child has length n/t, while the entropy
# Para 680 1
shrinks only from k&apos; to k&apos;/c. A simple calculation shows that
# Para 681 5
if k(lo9t)/lo9c) &gt; n2 then a c block-source must exist along 
such a path before the length shrinks to √k. Note that for 
k = nΩ(1) a (large enough) constant t suffices (resulting in 
only logarithmic number of outputs of SSE). This analysis 
is depicted pictorially in Figure 1.
# Para 686 1
5. THE FINAL DISPERSER D
# Para 687 8
Following is a rough description of our disperser D proving 
Theorem 2.1. The high level structure of D will resemble the 
structure of SSE - we will recursively split the source X and 
look for entropy in the parts. However now we must output 
a single value (rather than a set) which can take both values 
0 and 1. This was problematic in SSE, even knowing where 
the ”good” part (containing a c-block-source) was! How can 
we do so now?
# Para 695 8
We now have at our disposal a much more powerful tool 
for generating challenges (and thus detecting entropy), namely 
the subsource somewhere disperser SSE. Note that in con-
structing SSE we only had essentially the somewhere c-block-
source extractor SB to (recursively) generate the challenges, 
but it depended on a structural property of the block it was 
applied on. Now SSE does not assume any structure on its 
input sources except sufficient entropy 5.
# Para 703 1
Let us now give a high level description of the disperser
# Para 704 6
D. It too will be a recursive procedure. If when processing 
some part Z of X it ”realizes” that a subpart Zi of Z has 
entropy, but not all the entropy of Z (namely Zi, Z is a 
2-block-source) then we will halt and produce the output 
of D. Intuitively, thinking about the Challenge-Response 
mechanism described above, the analysis implies that we
# Para 710 3
5There is a catch – it only works on subsources of them! 
This will cause us a lot of head ache; we will elaborate on it 
later.
# Para 713 3
can either pass or fail Zi (on appropriate subsources). But 
this means that the outcome of this ”entropy test” is a 1-bit 
disperser!
# Para 716 4
To capitalize on this idea, we want to use SSE to identify 
such a block-source in the recursion tree. As before, we scan 
the blocks from left to right, and want to distinguish three 
possibilities.
# Para 720 1
low Zi has low entropy. In this case we proceed to i + 1.
# Para 721 3
medium Zi has ”medium” entropy (Zi, Z is a block-source). 
In which case we halt and produce an output (zero or 
one).
# Para 724 2
high Zi has essentially all entropy of Z. In this case we 
recurse on the condensed block Zi.
# Para 726 6
As before, we use the Challenge-Response mechanism (with 
a twist). We will compute challenges C(Zi, Y) and responses 
R(Z, Y), all strings of length m. The responses are computed 
exactly as before, using the somewhere extractor pSE. The 
Challenges are computed using our subsource somewhere 
extractor SSE.
# Para 732 13
We really have 4 possibilities to distinguish, since when we 
halt we also need to decide which output bit we give. We will 
do so by deriving three tests from the above challenges and 
responses: (CH, RH), (CM, RM), (CL, RL) for high, medium 
and low respectively, as follows. Let m ≥ mH &gt;&gt; mM &gt;&gt; 
mL be appropriate integers: then in each of the tests above 
we restrict ourselves to prefixes of all strings of the appro-
priate lengths only. So every string in CM will be a prefix 
of length mM of some string in CH. Similarly, every string 
in RL is the length mL prefix of some string in RH. Now 
it is immediately clear that if CM is contained in RM, then 
CL is contained in RL. Thus these tests are monotone, if 
our sample fails the high test, it will definitely fail all tests.
# Para 745 1
Algorithm: D(z, y)
# Para 746 2
Let pSE(., .) be the somewhere extractor with a polyno-
mial number of outputs of Proposition 2.3.
# Para 748 2
Let SSE(.,.) be the subsource somewhere extractor of The-
orem 2.6.
# Para 750 2
Global Parameters: t, the branching factor of the tree. k 
the original entropy of the sources.
# Para 752 2
Local Parameters for recursive level: mL ≪ mM ≪ mH. 
Output will be an element of {0, 1}.
# Para 754 1
1. If z is shorter than √k, return 0.
# Para 755 1
2. Partition z into t equal parts z = z1, z2, ... , zt.
# Para 756 3
3. Compute three response sets RL, RM, RH using pSE(z, y). 
Rj will be the prefixes of length mj of the strings in 
pSE(z, y).
# Para 759 3
4. For each i ∈ [t], compute three challenge sets CiL, CiM, CiH 
using SSE(zi, y). Cij will be the prefixes of length mj 
of the strings in SSE(zi, y).
# Para 762 3
5. Let h be the smallest index for which the challenge set 
CL is not contained in the response set RL, if there is 
no such index, output 0 and halt.
# Para 765 3
6. If ChH is contained in RH and ChH is contained in RM, 
output 0 and halt. If ChH is contained in RH but ChH 
is not contained in RM, output 1 and halt.
# Para 768 1
677
# Para 769 1
t blocks
# Para 770 2
X 
low
# Para 772 3
fail 
fail 
fail
# Para 775 1
X_3
# Para 776 1
(X_3)_4
# Para 777 1
low
# Para 778 1
low
# Para 779 3
fail 
fail 
fail
# Para 782 3
fail 
fail 
fail
# Para 785 1
low
# Para 786 1
low
# Para 787 3
pass 
pass 
pass
# Para 790 1
high
# Para 791 1
low
# Para 792 1
low
# Para 793 1
t blocks
# Para 794 1
low
# Para 795 1
high
# Para 796 1
t blocks
# Para 797 1
med
# Para 798 1
n bits total
# Para 799 1
n/t bits total
# Para 800 1
n/t^2 bits total
# Para 801 3
fail 
fail 
fail
# Para 804 3
fail 
fail 
fail
# Para 807 3
pass 
pass 
fail
# Para 810 3
pass 
fail 
fail
# Para 813 1
Output 0	Output 1
# Para 814 1
Figure 2: Analysis of the disperser.
# Para 815 1
7. Output D(zh, y),
# Para 816 6
First note the obvious monotonicity of the tests. If Zi fails 
one of the tests it will certainly fail for shorter strings. Thus 
there are only four outcomes to the three tests, written in the 
order (low, medium, high): (pass, pass, pass), (pass, pass, fail), 
(pass, fail, fail) and (fail, fail, fail). Conceptually, the algo-
rithm is making the following decisions using the four tests:
# Para 822 2
1. (fail, fail, fail): Assume Zi has low entropy and proceed 
to block i + 1.
# Para 824 2
2. (pass, fail, fail): Assume Zi is medium, halt and output 
0.
# Para 826 2
3. (pass, pass, fail): Assume Zi is medium, halt and out-
put 1.
# Para 828 1
4. (pass, pass, pass): Assume Zi is high and recurse on Zi.
# Para 829 4
The analysis of this idea (depicted in Figure 2).turns out 
to be more complex than it seems. There are two reasons for 
that. Now we briefly explain them and the way to overcome 
them in the construction and analysis.
# Para 833 8
The first reason is the fact mentioned above, that SSE 
which generates the challenges, works only on a subsources 
of the original sources. Restricting to these subsources at 
some level of the recursion (as required by the analysis of of 
the test) causes entropy loss which affects both definitions 
(such as these entropy thresholds for decisions) and correct-
ness of SSE in higher levels of recursion. Controlling this en-
tropy loss is achieved by calling SSE recursively with smaller 
# Para 841 5
and smaller entropy requirements, which in turn limits the 
entropy which will be lost by these restrictions. In order not 
to lose all the entropy for this reason alone, we must work 
with special parameters of SSE, essentially requiring that at 
termination it has almost all the entropy it started with.
# Para 846 6
The second reason is the analysis of the test when we are 
in a medium block. In contrast with the above situation, we 
cannot consider the value of Zi fixed when we need it to fail 
on the Medium and Low tests. We need to show that for 
these two tests (given a pass for High), they come up both 
(pass, fail) and (fail, fail) each with positive probability.
# Para 852 6
Since the length of Medium challenges and responses is 
mM, the probability of failure is at least exp(−Q(mM)) (this 
follows relatively easily from the fact that the responses are 
somewhere random). If the Medium test fails so does the 
Low test, and thus (fail, fail) has a positive probability and 
our disperser D outputs 0 with positive probability.
# Para 858 10
To bound (pass, fail) we first observe (with a similar 
reasoning) that the low test fails with probability at least 
exp(−Q(mL)). But we want the medium test to pass at the 
same time. This probability is at least the probability that 
low fails minus the probability that medium fails. We already 
have a bound on the latter: it is at most poly(n)exp(−ℓmM). 
Here comes our control of the different length into play - we 
can make the mL sufficiently smaller than mM to yield this 
difference positive. We conclude that our disperser D out-
puts 1 with positive probability as well.
# Para 868 4
Finally, we need to take care of termination: we have to 
ensure that the recurrence always arrives at a medium sub-
part, but it is easy to chose entropy thresholds for low, medium 
and high to ensure that this happens.
# Para 872 1
678
# Para 873 1
6. RESILIENCY AND DEFICIENCY
# Para 874 3
In this section we will breifly discuss an issue which arises 
in our construction that we glossed over in the previous sec-
tions. Recall our definition of subsources:
# Para 877 4
DEfInItIOn 6.1 (SUBSOURCES). Given random variables 
Z and Zˆ on {0,1}n we say that Zˆ is a deficiency d subsource 
of Z and write Zˆ ⊆ Z if there exists a set A ⊆ {0,1}n such 
that (Z|A) = Zˆ and Pr[Z ∈ A] ≥ 2—d.
# Para 881 15
Recall that we were able to guarantee that our algorithms 
made the right decisions only on subsources of the original 
source. For example, in the construction of our final dis-
perser, to ensure that our algorithms correctly identify the 
right high block to recurse on, we were only able to guar-
antee that there are subsources of the original sources in 
which our algorithm makes the correct decision with high 
probability. Then, later in the analysis we had to further 
restrict the source to even smaller subsources. This leads to 
complications, since the original event of picking the correct 
high block, which occurred with high probability, may be-
come an event which does not occur with high probability 
in the current subsource. To handle these kinds of issues, 
we will need to be very careful in measuring how small our 
subsources are.
# Para 896 4
In the formal analysis we introduce the concept of re-
siliency to deal with this. To give an idea of how this works, 
here is the actual definition of somewhere subsource extrac-
tor that we use in the formal analysis.
# Para 900 6
DEfInItIOn 6.2 (SUBSOURCE SOmEWhERE ExtRaCtOR). 
A function SSE : {0, 1}n × {0, 1}n → ({0, 1}m)ℓ is a sub- 
source somewhere extractor with nrows output rows, entropy 
threshold k, deficiency def, resiliency res and error ǫ if for 
every (n, k)-sources X, Y there exist a deficiency def sub- 
source Xgood of X and a deficiency def subsource Ygood of
# Para 906 2
Y such that for every deficiency res subsource X&apos; of Xgood 
and deficiency res subsource Y&apos; of Ygood, the random vari-
# Para 908 1
able SSE(X&apos;,Y&apos;) is ǫ-close to a ℓ × m somewhere random
# Para 909 1
distribution.
# Para 910 7
It turns out that our subsource somewhere extractor does 
satisfy this stronger definition. The advantage of this defi-
nition is that it says that once we restrict our attention to 
the good subsources Xgood, Ygood, we have the freedom to fur-
ther restrict these subsources to smaller subsources, as long 
as our final subsources do not lose more entropy than the 
resiliency permits.
# Para 917 3
This issue of managing the resiliency for the various ob-
jects that we construct is one of the major technical chal-
lenges that we had to overcome in our construction.
# Para 920 1
7. OPEN PROBLEMS
# Para 921 9
Better Independent Source Extractors A bottleneck to 
improving our disperser is the block versus general 
source extractor of Theorem 2.7. A good next step 
would be to try to build an extractor for one block 
source (with only a constant number of blocks) and 
one other independent source which works for polylog-
arithmic entropy, or even an extractor for a constant 
number of sources that works for sub-polynomial en-
tropy.
# Para 930 9
Simple Dispersers While our disperser is polynomial time 
computable, it is not as explicit as one might have 
hoped. For instance the Ramsey Graph construction 
of Frankl-Wilson is extremely simple: For a prime p, 
let the vertices of the graph be all subsets of [p3] of 
size p2 − 1. Two vertices S, T are adjacent if and only 
if |S ∩ T | ≡ −1 mod p. It would be nice to find a good 
disperser that beats the Frankl-Wilson construction, 
yet is comparable in simplicity.
# Para 939 1
8. REFERENCES
# Para 940 2
[1] N. Alon. The shannon capacity of a union. 
Combinatorica, 18, 1998.
# Para 942 2
[2] B. Barak. A simple explicit construction of an 
n˜o(logn )-ramsey graph. Technical report, Arxiv, 2006.
# Para 944 1
http://arxiv.org/abs/math.CO/0601651.
# Para 945 5
[3] B. Barak, R. Impagliazzo, and A. Wigderson. 
Extracting randomness using few independent sources. 
In Proceedings of the 45th Annual IEEE Symposium 
on Foundations of Computer Science, pages 384–393, 
2004.
# Para 950 3
[4] B. Barak, G. Kindler, R. Shaltiel, B. Sudakov, and 
A. Wigderson. Simulating independence: New 
constructions of condensers, Ramsey graphs,
# Para 953 3
dispersers, and extractors. In Proceedings of the 37th 
Annual ACM Symposium on Theory of Computing, 
pages 1–10, 2005.
# Para 956 3
[5] J. Bourgain. More on the sum-product phenomenon in 
prime fields and its applications. International Journal 
of Number Theory, 1:1–32, 2005.
# Para 959 3
[6] J. Bourgain, N. Katz, and T. Tao. A sum-product 
estimate in finite fields, and applications. Geometric 
and Functional Analysis, 14:27–57, 2004.
# Para 962 1
[7] M. Capalbo, O. Reingold, S. Vadhan, and
# Para 963 4
A. Wigderson. Randomness conductors and 
constant-degree lossless expanders. In Proceedings of 
the 34th Annual ACM Symposium on Theory of 
Computing, pages 659–668, 2002.
# Para 967 3
[8] B. Chor and O. Goldreich. Unbiased bits from sources 
of weak randomness and probabilistic communication 
complexity. SIAM Journal on Computing,
# Para 970 1
17(2):230–261, 1988.
# Para 971 3
[9] P. Frankl and R. M. Wilson. Intersection theorems 
with geometric consequences. Combinatorica, 
1(4):357–368, 1981.
# Para 974 3
[10] P. Gopalan. Constructing ramsey graphs from boolean 
function representations. In Proceedings of the 21th 
Annual IEEE Conference on Computational
# Para 977 1
Complexity, 2006.
# Para 978 2
[11] V. Grolmusz. Low rank co-diagonal matrices and 
ramsey graphs. Electr. J. Comb, 7, 2000.
# Para 980 3
[12] V. Guruswami. Better extractors for better codes? 
Electronic Colloquium on Computational Complexity 
(ECCC), (080), 2003.
# Para 983 4
[13] C. J. Lu, O. Reingold, S. Vadhan, and A. Wigderson. 
Extractors: Optimal up to constant factors. In 
Proceedings of the 35th Annual ACM Symposium on 
Theory of Computing, pages 602–611, 2003.
# Para 987 3
[14] P. Miltersen, N. Nisan, S. Safra, and A. Wigderson. 
On data structures and asymmetric communication 
complexity. Journal of Computer and System
# Para 990 1
Sciences, 57:37–49, 1 1998.
# Para 991 1
679
# Para 992 4
[15] N. Nisan and D. Zuckerman. More deterministic 
simulation in logspace. In Proceedings of the 25th 
Annual ACM Symposium on Theory of Computing, 
pages 235–244, 1993.
# Para 996 3
[16] P. Pudlak and V. Rodl. Pseudorandom sets and 
explicit constructions of ramsey graphs. Submitted for 
publication, 2004.
# Para 999 4
[17] A. Rao. Extractors for a constant number of 
polynomially small min-entropy independent sources. 
In Proceedings of the 38th Annual ACM Symposium 
on Theory of Computing, 2006.
# Para 1003 3
[18] R. Raz. Extractors with weak random seeds. In 
Proceedings of the 37th Annual ACM Symposium on 
Theory of Computing, pages 11–20, 2005.
# Para 1006 4
[19] O. Reingold, R. Shaltiel, and A. Wigderson. 
Extracting randomness via repeated condensing. In 
Proceedings of the 41st Annual IEEE Symposium on 
Foundations of Computer Science, pages 22–31, 2000.
# Para 1010 4
[20] M. Santha and U. V. Vazirani. Generating 
quasi-random sequences from semi-random sources. 
Journal of Computer and System Sciences, 33:75–87, 
1986.
# Para 1014 4
[21] R. Shaltiel. Recent developments in explicit 
constructions of extractors. Bulletin of the European 
Association for Theoretical Computer Science, 
77:67–95, 2002.
# Para 1018 2
[22] A. Ta-Shma and D. Zuckerman. Extractor codes. 
IEEE Transactions on Information Theory, 50, 2004.
# Para 1020 6
[23] U. Vazirani. Towards a strong communication 
complexity theory or generating quasi-random 
sequences from two communicating slightly-random 
sources (extended abstract). In Proceedings of the 17th 
Annual ACM Symposium on Theory of Computing, 
pages 366–378, 1985.
# Para 1026 3
[24] A. Wigderson and D. Zuckerman. Expanders that 
beat the eigenvalue bound: Explicit construction and 
applications. Combinatorica, 19(1):125–138, 1999.
# Para 1029 1
680
