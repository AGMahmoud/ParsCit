<title> 2-Source Dispersers for Sub-Polynomial Entropy and +L+ Ramsey Graphs Beating the Frankl-Wilson Construction +L+ </title> <none> ∗ +L+ </none> <author> Boaz Barak +L+ </author> <affiliation> Department of Computer Science +L+ Princeton University +L+ </affiliation> <email> boaz@cs.princeton.edu +L+ </email> <author> Ronen Shaltiel ‡ +L+ </author> <affiliation> University of Haifa +L+ </affiliation> <address> Mount Carmel +L+ Haifa, Israel +L+ </address> <email> ronen@cs.haifa.ac.il +L+ </email> <sectionHeader> ABSTRACT +L+ </sectionHeader> <bodyText> The main result of this paper is an explicit disperser for two +L+ independent sources on n bits, each of entropy k = no(1). +L+ Put differently, setting N = 2n and K = 2k, we construct +L+ explicit N × N Boolean matrices for which no K × K sub- +L+ matrix is monochromatic. Viewed as adjacency matrices of +L+ bipartite graphs, this gives an explicit construction of K- +L+ Ramsey bipartite graphs of size N. +L+ This greatly improves the previous bound of k = o(n) of +L+ Barak, Kindler, Shaltiel, Sudakov and Wigderson [4]. It also +L+ significantly improves the 25-year record of k = ~O(√n) on +L+ the special case of Ramsey graphs, due to Frankl and Wilson +L+ [9]. +L+ The construction uses (besides ”classical” extractor ideas) +L+ almost all of the machinery developed in the last couple of +L+ years for extraction from independent sources, including: +L+ •	Bourgain’s extractor for 2 independent sources of some +L+ entropy rate < 1/2 [5] +L+ •	Raz’s extractor for 2 independent sources, one of which +L+ has any entropy rate > 1/2 [18] +L+ </bodyText> <footnote> ∗Supported by a Princeton University startup grant. +L+ †Most of this work was done while the author was visiting +L+ Princeton University and the Institute for Advanced Study. +L+ Supported in part by an MCD fellowship from UT Austin +L+ and NSF Grant CCR-0310960. +L+ ‡This research was supported by the United States-Israel +L+ Binational Science Foundation (BSF) grant 2004329. +L+ §This research was supported by NSF Grant CCR-0324906. +L+ </footnote> <copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee. +L+ </copyright> <note> STOC’06, May 21–23, 2006, Seattle, Washington, USA. +L+ </note> <copyright> Copyright 2006 ACM 1-59593-134-1/06/0005 ...$5.00. +L+ </copyright> <author> Anup Rao † +L+ </author> <affiliation> Department of Computer Science +L+ University of Texas at Austin +L+ </affiliation> <email> arao@cs.utexas.edu +L+ </email> <author> Avi Wigderson § +L+ </author> <affiliation> Institute for Advanced Study +L+ </affiliation> <address> Princeton +L+ New Jersey +L+ </address> <email> avi@math.ias.edu +L+ </email> <bodyText> •	Rao’s extractor for 2 independent block-sources of en- +L+ tropy no(1) [17] +L+ •	The “Challenge-Response” mechanism for detecting +L+ “entropy concentration” of [4]. +L+ The main novelty comes in a bootstrap procedure which +L+ allows the Challenge-Response mechanism of [4] to be used +L+ with sources of less and less entropy, using recursive calls +L+ to itself. Subtleties arise since the success of this mecha- +L+ nism depends on restricting the given sources, and so re- +L+ cursion constantly changes the original sources. These are +L+ resolved via a new construct, in between a disperser and +L+ an extractor, which behaves like an extractor on sufficiently +L+ large subsources of the given ones. +L+ </bodyText> <note> This version is only an extended abstract, please see the +L+ full version, available on the authors’ homepages, for more +L+ details. +L+ </note> <sectionHeader> Categories and Subject Descriptors +L+ </sectionHeader> <category> G.2.2 [Mathematics of Computing]: Discrete Mathe- +L+ matics—Graph algorithms +L+ </category> <sectionHeader> General Terms +L+ </sectionHeader> <keyword> Theory, Algorithms +L+ </keyword> <sectionHeader> Keywords +L+ </sectionHeader> <keyword> Dispersers, Ramsey Graphs, Independent Sources, Extrac- +L+ tors +L+ </keyword> <sectionHeader> 1. INTRODUCTION +L+ </sectionHeader> <bodyText> This paper deals with randomness extraction from weak +L+ random sources. Here a weak random source is a distribu- +L+ tion which contains some entropy. The extraction task is to +L+ design efficient algorithms (called extractors) to convert this +L+ entropy into useful form, namely a sequence of independent +L+ unbiased bits. Beyond the obvious motivations (potential +L+ use of physical sources in pseudorandom generators and in +L+ derandomization), extractors have found applications in a +L+ </bodyText> <page> 671 +L+ </page> <bodyText> variety of areas in theoretical computer science where ran- +L+ domness does not seem an issue, such as in efficient con- +L+ structions of communication networks [24, 7], error correct- +L+ ing codes [22, 12], data structures [14] and more. +L+ Most work in this subject over the last 20 years has fo- +L+ cused on what is now called seeded extraction, in which the +L+ extractor is given as input not only the (sample from the) +L+ defective random source, but also a few truly random bits +L+ (called the seed). A comprehensive survey of much of this +L+ body of work is [21]. +L+ Another direction, which has been mostly dormant till +L+ about two years ago, is (seedless, deterministic) extraction +L+ from a few independent weak sources. This kind of extrac- +L+ tion is important in several applications where it is unrealis- +L+ tic to have a short random seed or deterministically enumer- +L+ ate over its possible values. However, it is easily shown to be +L+ impossible when only one weak source is available. When at +L+ least 2 independent sources are available extraction becomes +L+ possible in principle. The 2-source case is the one we will +L+ focus on in this work. +L+ The rest of the introduction is structured as follows. We’ll +L+ start by describing our main result in the context of Ramsey +L+ graphs. We then move to the context of extractors and dis- +L+ perser, describing the relevant background and stating our +L+ result in this language. Then we give an overview of the +L+ construction of our dispersers, describing the main building +L+ blocks we construct along the way. As the construction is +L+ quite complex and its analysis quite subtle, in this proceed- +L+ ings version we try to abstract away many of the technical +L+ difficulties so that the main ideas, structure and tools used +L+ are highlighted. For that reason we also often state defini- +L+ tions and theorems somewhat informally. +L+ </bodyText> <subsectionHeader> 1.1 Ramsey Graphs +L+ </subsectionHeader> <construct> DefInItIOn 1.1. A graph on N vertices is called a K- +L+ Ramsey Graph if it contains no clique or independent set of +L+ size K. +L+ </construct> <bodyText> In 1947 Erd}os published his paper inaugurating the Prob- +L+ abilistic Method with a few examples, including a proof that +L+ most graphs on N = 2n vertices are 2n-Ramsey. The quest +L+ for constructing such graphs explicitly has existed ever since +L+ and lead to some beautiful mathematics. +L+ The best record to date was obtained in 1981 by Frankl +L+ and Wilson [9], who used intersection theorems for set sys- +L+ tems to construct N-vertex graphs which are 21�n log n-Ramsey. +L+ This bound was matched by Alon [1] using the Polynomial +L+ Method, by Grolmusz [11] using low rank matrices over rings, +L+ and also by Barak [2] boosting Abbot’s method with almost +L+ k-wise independent random variables (a construction that +L+ was independently discovered by others as well). Remark- +L+ ably all of these different approaches got stuck at essentially +L+ the same bound. In recent work, Gopalan [10] showed that +L+ other than the last construction, all of these can be viewed +L+ as coming from low-degree symmetric representations of the +L+ OR function. He also shows that any such symmetric rep- +L+ resentation cannot be used to give a better Ramsey graph, +L+ which gives a good indication of why these constructions +L+ had similar performance. Indeed, as we will discuss in a +L+ later section, the √n entropy bound initially looked like a +L+ natural obstacle even for our techniques, though eventually +L+ we were able to surpass it. +L+ The analogous question for bipartite graphs seemed much +L+ harder. +L+ </bodyText> <construct> DefInItIOn 1.2. A bipartite graph on two sets of N ver- +L+ tices is a K-Ramsey Bipartite Graph if it has no K × K +L+ complete or empty bipartite subgraph. +L+ </construct> <bodyText> While Erd}os’ result on the abundance of 2n-Ramsey graphs +L+ holds as is for bipartite graphs, until recently the best ex- +L+ plicit construction of bipartite Ramsey graphs was 2n/2- +L+ Ramsey, using the Hadamard matrix. This was improved +L+ last year, first to o(2n/2) by Pudlak and R}odl [16] and then +L+ to 2o(n) by Barak, Kindler, Shaltiel, Sudakov and Wigderson +L+ [4] . +L+ It is convenient to view such graphs as functions f : +L+ ({0, 1}n)2 → {0, 1}. This then gives exactly the definition +L+ of a disperser. +L+ </bodyText> <construct> DefInItIOn 1.3. A function f : ({0, 1}n)2 → {0, 1} is +L+ called a 2-source disperser for entropy k if for any two sets +L+ X, Y ⊂ {0, 1}n with | X | = |Y| = 2k, we have that the image +L+ f (X, Y) is {0, 1}. +L+ </construct> <bodyText> This allows for a more formal definition of explicitness: we +L+ simply demand that the function f is computable in polyno- +L+ mial time. Most of the constructions mentioned above are +L+ explicit in this sense.' +L+ Our main result (stated informally) significantly improves +L+ the bounds in both the bipartite and non-bipartite settings: +L+ </bodyText> <construct> TheOrem 1.4. For every N we construct polynomial time +L+ computable bipartite graphs which are 2n'(1)-Ramsey. A stan- +L+ dard transformation of these graphs also yields polynomial +L+ time computable ordinary Ramsey Graphs with the same pa- +L+ rameters. +L+ </construct> <subsectionHeader> 1.2 Extractors and Dispersers from indepen- +L+ dent sources +L+ </subsectionHeader> <bodyText> Now we give a brief review of past relevant work (with the +L+ goal of putting this paper in proper context) and describe +L+ some of the tools from these past works that we will use. +L+ We start with the basic definitions of k-sources by Nisan +L+ and Zuckerman [15] and of extractors and dispersers for in- +L+ dependent sources by Santha and Vazirani [20]. +L+ </bodyText> <construct> DefInItIOn 1.5 ([15], See alSO [8]). The min-entropy +L+ of a distribution X is the maximum k such that for every +L+ element x in its support, Pr[X = x] ≤ 2-k. If X is a dis- +L+ tribution on strings with min-entropy at least k, we will call +L+ X a k-source 2. +L+ </construct> <bodyText> To simplify the presentation, in this version of the paper +L+ we will assume that we are working with entropy as opposed +L+ to min-entropy. +L+ </bodyText> <construct> DefInItIOn 1.6 ([20]). A function f : ({0,1}n)c → +L+ {0, 1}m is a c-source (k, ǫ) extractor if for every family of c +L+ independent k-sources X', • • • , Xc, the output f (X', • • • , Xc) +L+ </construct> <footnote> 'The Abbot’s product based Ramsey-graph construction of +L+ [3] and the bipartite Ramsey construction of [16] only satisfy +L+ a weaker notion of explicitness. +L+ 2It is no loss of generality to imagine that X is uniformly +L+ distributed over some (unknown) set of size 2k. +L+ </footnote> <page> 672 +L+ </page> <bodyText> is a ǫ-close 3 to uniformly distributed on m bits. f is a dis- +L+ perser for the same parameters if the output is simply re- +L+ quired to have a support of relative size (1 − ǫ). +L+ To simplify the presentation, in this version of the paper, +L+ we will assume that ǫ = 0 for all of our constructions. +L+ In this language, Erd}os’ theorem says that most functions +L+ f : ({0, 1}n)2 → {0, 1} are dispersers for entropy 1 + logn +L+ (treating f as the characteristic function for the set of edges +L+ of the graph). The proof easily extends to show that indeed +L+ most such functions are in fact extractors. This naturally +L+ challenges us to find explicit functions f that are 2-source +L+ extractors. +L+ Until one year ago, essentially the only known explicit +L+ construction was the Hadamard extractor Had defined by +L+ Had(x,y) +L+ k > n/2 as observed by Chor and Goldreich [8] and can +L+ be extended to give m = Q(n) output bits as observed by +L+ Vazirani [23]. Over 20 years later, a recent breakthrough +L+ of Bourgain [5] broke this “1/2 barrier” and can handle 2 +L+ sources of entropy .4999n, again with linear output length +L+ m = 0(n). This seemingly minor improvement will be cru- +L+ cial for our work! +L+ </bodyText> <construct> TheOrem 1.7 ([5] ). There is a polynomial time com- +L+ putable 2-source extractor f : ({0, 1}n)2 → {0, 1}m for en- +L+ tropy .4999n and m = 0(n). +L+ </construct> <bodyText> No better bounds are known for 2-source extractors. Now +L+ we turn our attention to 2-source dispersers. It turned out +L+ that progress for building good 2-source dispersers came via +L+ progress on extractors for more than 2 sources, all happening +L+ in fast pace in the last 2 years. The seminal paper of Bour- +L+ gain, Katz and Tao [6] proved the so-called ”sum-product +L+ theorem” in prime fields, a result in arithmetic combina- +L+ torics. This result has already found applications in diverse +L+ areas of mathematics, including analysis, number theory, +L+ group theory and ... extractor theory. Their work implic- +L+ itly contained dispersers for c = O(log(n/k)) independent +L+ sources of entropy k (with output m = Q(k)). The use of +L+ the ”sum-product” theorem was then extended by Barak et +L+ al. [3] to give extractors with similar parameters. Note that +L+ for linear entropy k = 0(n), the number of sources needed +L+ for extraction c is a constant! +L+ Relaxing the independence assumptions via the idea of +L+ repeated condensing, allowed the reduction of the number +L+ of independent sources to c = 3, for extraction from sources +L+ of any linear entropy k = 0(n), by Barak et al. [4] and +L+ independently by Raz [18]. +L+ For 2 sources Barak et al. [4] were able to construct dis- +L+ persers for sources of entropy o(n). To do this, they first +L+ showed that if the sources have extra structure (block-source +L+ structure, defined below), even extraction is possible from 2 +L+ sources. The notion of block-sources, capturing ”semi inde- +L+ pendence” of parts of the source, was introduced by Chor +L+ and Goldreich [8]. It has been fundamental in the develop- +L+ ment of seeded extractors and as we shall see, is essential +L+ for us as well. +L+ </bodyText> <construct> DefInItIOn 1.8 ([8] ). A distribution X = X1, ... , Xc +L+ is a c-block-source of (block) entropy k if every block Xi +L+ has entropy k even conditioned on fixing the previous blocks +L+ X1, • • • , Xi_1 to arbitrary constants. +L+ </construct> <footnote> 3The error is usually measured in terms of ℓ1 distance or +L+ variation distance. +L+ </footnote> <bodyText> This definition allowed Barak et al. [4] to show that their +L+ extractor for 4 independent sources, actually performs as +L+ well with only 2 independent sources, as long as both are +L+ 2-block-sources. +L+ </bodyText> <construct> TheOrem 1.9 ([4] ). There exists a polynomial time com- +L+ putable extractor f : ({0, 1}n)2 → {0, 1} for 2 independent +L+ 2-block-sources with entropy o(n). +L+ </construct> <bodyText> There is no reason to assume that the given sources are +L+ block-sources, but it is natural to try and reduce to this +L+ case. This approach has been one of the most successful in +L+ the extractor literature. Namely try to partition a source +L+ X into two blocks X = X1, X2 such that X1, X2 form a +L+ 2-block-source. Barak et al. introduced a new technique to +L+ do this reduction called the Challenge-Response mechanism, +L+ which is crucial for this paper. This method gives a way to +L+ “find” how entropy is distributed in a source X, guiding the +L+ choice of such a partition. This method succeeds only with +L+ small probability, dashing the hope for an extractor, but still +L+ yielding a disperser. +L+ </bodyText> <construct> TheOrem 1.10 ([4] ). There exists a polynomial time +L+ computable 2-source disperser f : ({0, 1}n)2 → {0, 1} for +L+ entropy o(n). +L+ </construct> <bodyText> Reducing the entropy requirement of the above 2-source +L+ disperser, which is what we achieve in this paper, again +L+ needed progress on achieving a similar reduction for extrac- +L+ tors with more independent sources. A few months ago Rao +L+ [?] was able to significantly improve all the above results +L+ for c ≥ 3 sources. Interestingly, his techniques do not use +L+ arithmetic combinatorics, which seemed essential to all the +L+ papers above. He improves the results of Barak et al. [3] to +L+ give c = O((logn)/(logk))-source extractors for entropy k. +L+ Note that now the number c of sources needed for extraction +L+ is constant, even when the entropy is as low as nδ for any +L+ constant δ! +L+ Again, when the input sources are block-sources with suf- +L+ ficiently many blocks, Rao proves that 2 independent sources +L+ suffice (though this result does rely on arithmetic combina- +L+ torics, in particular, on Bourgain’s extractor). +L+ </bodyText> <construct> TheOrem 1.11 ([?] ). There is a polynomial time com- +L+ putable extractor f : ({0, 1}n)2 → {0, 1}m for 2 independent +L+ c-block-sources with block entropy k and m = 0(k), as long +L+ as c = O((log n)/(log k)). +L+ </construct> <bodyText> In this paper (see Theorem 2.7 below) we improve this +L+ result to hold even when only one of the 2 sources is a c- +L+ block-source. The other source can be an arbitrary source +L+ with sufficient entropy. This is a central building block in +L+ our construction. This extractor, like Rao’s above, critically +L+ uses Bourgain’s extractor mentioned above. In addition it +L+ uses a theorem of Raz [18] allowing seeded extractors to have +L+ ”weak” seeds, namely instead of being completely random +L+ they work as long as the seed has entropy rate > 1/2. +L+ </bodyText> <sectionHeader> 2. MAIN NOTIONS AND NEW RESULTS +L+ </sectionHeader> <bodyText> The main result of this paper is a polynomial time com- +L+ putable disperser for 2 sources of entropy no(1), significantly +L+ improving both the results of Barak et al. [4] (o(n) entropy). +L+ It also improves on Frankl and Wilson [9], who only built +L+ Ramsey Graphs and only for entropy ~O(√n). +L+ = (x, y)( mod 2). It is an extractor for entropy +L+ </bodyText> <page> 673 +L+ </page> <construct> ThEOREm 2.1 (MaIn thEOREm, REStatEd). There ex- +L+ ists a polynomial time computable 2-source disperser D : +L+ ({0, 1}n)2 → {0, 1} for entropy no(1). +L+ </construct> <bodyText> The construction of this disperser will involve the con- +L+ struction of an object which in some sense is stronger and +L+ in another weaker than a disperser: a subsource somewhere +L+ extractor. We first define a related object: a somewhere ex- +L+ tractor, which is a function producing several outputs, one of +L+ which must be uniform. Again we will ignore many technical +L+ issues such as error, min-entropy vs. entropy and more, in +L+ definitions and results, which are deferred to the full version +L+ of this paper. +L+ </bodyText> <construct> DEfInItIOn 2.2. A function f : ({0, 1}n)2 → ({0,1}m)ℓ +L+ is a 2-source somewhere extractor with ℓ outputs, for entropy +L+ k, if for every 2 independent k-sources X, Y there exists an +L+ i ∈ [ℓ] such the ith output f (X, Y)i is a uniformly distributed +L+ string of m bits. +L+ </construct> <bodyText> Here is a simple construction of such a somewhere extrac- +L+ tor with ℓ as large as poly(n) (and the p in its name will +L+ stress the fact that indeed the number of outputs is that +L+ large). It will nevertheless be useful to us (though its de- +L+ scription in the next sentence may be safely skipped). Define +L+ pSE(x, y)i = V(E(x, i), E(y, i)) where E is a ”strong” loga- +L+ rithmic seed extractor, and V is the Hadamard/Vazirani 2- +L+ source extractor. Using this construction, it is easy to see +L+ that: +L+ </bodyText> <construct> PROPOSItIOn 2.3. For every n, k there is a polynomial +L+ time computable somewhere extractor pSE : ({0, 1}n)2 → +L+ ({0, 1}m)ℓ with ℓ = poly(n) outputs, for entropy k, and m = +L+ Q(k). +L+ </construct> <bodyText> Before we define subsource somewhere extractor, we must +L+ first define a subsource. +L+ </bodyText> <construct> DEfInItIOn 2.4 (SUBSOURCES). Given random variables +L+ Z and Z^ on {0, 1}n we say that Z^ is a deficiency d subsource +L+ of Z and write Z^ ⊆ Z if there exists a set A ⊆ {0,1}n such +L+ that (Z|Z ∈ A) = Z^ and Pr[Z ∈ A] ≥ 2-d. +L+ </construct> <bodyText> A subsource somewhere extractor guarantees the ”some- +L+ where extractor” property only on subsources X', Y' of the +L+ original input distributions X, Y (respectively). It will be +L+ extremely important for us to make these subsources as large +L+ as possible (i.e. we have to lose as little entropy as possible). +L+ Controlling these entropy deficiencies is a major technical +L+ complication we have to deal with. However we will be in- +L+ formal with it here, mentioning it only qualitatively when +L+ needed. We discuss this issue a little more in Section 6. +L+ </bodyText> <construct> DEfInItIOn 2.5. A function f : ({0, 1}n)2 → ({0,1}m)ℓ +L+ is a 2-source subsource somewhere extractor with ℓ outputs +L+ for entropy k, if for every 2 independent k-sources X, Y there +L+ exists a subsource X^ of X, a subsource Y^ of Y and an i ∈ [ℓ] +L+ such the ith output f (^X, Y^)i is a uniformly distributed string +L+ of m bits. +L+ </construct> <bodyText> A central technical result for us is that with this ”sub- +L+ source” relaxation, we can have much fewer outputs – in- +L+ deed we’ll replace poly(n) outputs in our first construction +L+ above with no(1) outputs. +L+ </bodyText> <construct> ThEOREm 2.6 (SUBSOURCE SOmEWhERE ExtRaCtOR). +L+ For every δ > 0 there is a polynomial time computable sub- +L+ source somewhere extractor SSE : ({0, 1}n)2 → ({0, 1}m)ℓ +L+ with ℓ = no(1) outputs, for entropy k = nδ, with output +L+ m=√k. +L+ </construct> <bodyText> We will describe the ideas used for constructing this im- +L+ portant object and analyzing it in the next section, where +L+ we will also indicate how it is used in the construction of +L+ the final disperser. Here we state a central building block, +L+ mentioned in the previous section (as an improvement of the +L+ work of Rao [?]). We construct an extractor for 2 indepen- +L+ dent sources one of which is a block-sources with sufficient +L+ number of blocks. +L+ </bodyText> <construct> ThEOREm 2.7 (BlOCK SOURCE ExtRaCtOR). There is +L+ a polynomial time computable extractor B : ({0, 1}n)2 → +L+ {0,1}m for 2 independent sources, one of which is a c-block- +L+ sources with block entropy k and the other a source of en- +L+ tropy k, with m = 0(k), and c = O((log n)/(log k)). +L+ </construct> <bodyText> A simple corollary of this block-source extractor B, is the +L+ following weaker (though useful) somewhere block-source +L+ extractor SB. A source Z = Z1, Z2, • • • , Zt is a somewhere +L+ c-block-source of block entropy k if for some c indices i1 < +L+ i2 < • • • < ic the source Zi1, Zi2, • • • , Zic is a c-block-source. +L+ Collecting the outputs of B on every c-subset of blocks re- +L+ sults in that somewhere extractor. +L+ </bodyText> <construct> COROllaRY 2.8. There is a polynomial time computable +L+ somewhere extractorSB : ({0, 1}n)2 → ({0, 1}m)ℓ for2 inde- +L+ pendent sources, one of which is a somewhere c-block-sources +L+ with block entropy k and t blocks total and the other a source +L+ of entropy k, with m = 0(k), c = O((log n)/(log k)), and +L+ ℓ ≤ tc. +L+ </construct> <bodyText> In both the theorem and corollary above, the values of +L+ entropy k we will be interested in are k = no(1). It follows +L+ that a block-source with a constant c = O(1) suffices. +L+ </bodyText> <sectionHeader> 3. THE CHALLENGE-RESPONSE MECH- +L+ ANISM +L+ </sectionHeader> <bodyText> We now describe abstractly a mechanism which will be +L+ used in the construction of the disperser as well as the sub- +L+ source somewhere extractor. Intuitively, this mechanism al- +L+ lows us to identify parts of a source which contain large +L+ amounts of entropy. One can hope that using such a mech- +L+ anism one can partition a given source into blocks in a way +L+ which make it a block-source, or alternatively focus on a part +L+ of the source which is unusually condensed with entropy - +L+ two cases which may simplify the extraction problem. +L+ The reader may decide, now or in the middle of this +L+ section, to skip ahead to the next section which describes +L+ the construction of the subsource somewhere extractor SSE, +L+ which extensively uses this mechanism. Then this section +L+ may seem less abstract, as it will be clearer where this mech- +L+ anism is used. +L+ This mechanism was introduced by Barak et al. [4], and +L+ was essential in their 2-source disperser. Its use in this paper +L+ is far more involved (in particular it calls itself recursively, +L+ a fact which creates many subtleties). However, at a high +L+ level, the basic idea behind the mechanism is the same: +L+ Let Z be a source and Z' a part of Z (Z projected on a +L+ subset of the coordinates). We know that Z has entropy k, +L+ </bodyText> <page> 674 +L+ </page> <bodyText> and want to distinguish two possibilities: Z′ has no entropy +L+ (it is fixed) or it has at least k′ entropy. Z′ will get a pass +L+ or fail grade, hopefully corresponding to the cases of high or +L+ no entropy in Z′. +L+ Anticipating the use of this mechanism, it is a good idea +L+ to think of Z as a ”parent” of Z′, which wants to check if +L+ this ”child” has sufficient entropy. Moreover, in the context +L+ of the initial 2 sources X, Y we will operate on, think of Z +L+ as a part of X, and thus that Y is independent of Z and Z′. +L+ To execute this ”test” we will compute two sets of strings +L+ (all of length m, say): the Challenge C = C(Z′,Y) and +L+ the Response R = R(Z, Y). Z′ fails if C C R and passes +L+ otherwise. +L+ The key to the usefulness of this mechanism is the follow- +L+ ing lemma, which states that what ”should” happen, indeed +L+ happens after some restriction of the 2 sources Z and Y. +L+ We state it and then explain how the functions C and R are +L+ defined to accommodate its proof. +L+ </bodyText> <construct> Lemma 3.1. Assume Z, Y are sources of entropy k. +L+ </construct> <listItem> 1. If Z′ has entropy k′+O(m), then there are subsources +L+ Z^ of Z and Y^ of Y, such that +L+ Pr[^Z′ passes] = Pr[C(^Z′, Y^) C R(^Z, Y^)] > 1—nO(1)2−m +L+ 2. If Z′ is fixed (namely, has zero entropy), then for some +L+ subsources Z^ of Z and Y^ of Y, we have +L+ Pr[Z′ fails] = Pr[C(^Z′, Y^) C R(^Z, Y^)] = 1 +L+ </listItem> <bodyText> Once we have such a mechanism, we will design our dis- +L+ perser algorithm assuming that the challenge response mech- +L+ anism correctly identifies parts of the source with high or +L+ low levels of entropy. Then in the analysis, we will ensure +L+ that our algorithm succeeds in making the right decisions, +L+ at least on subsources of the original input sources. +L+ Now let us explain how to compute the sets C and R. We +L+ will use some of the constructs above with parameters which +L+ don’t quite fit. +L+ The response set R(Z, Y) = pSE(Z, Y) is chosen to be the +L+ output of the somewhere extractor of Proposition 2.3. The +L+ challenge set C(Z′, Y) = SSE(Z′, Y) is chosen to be the out- +L+ put of the subsource somewhere extractor of Theorem 2.6. +L+ Why does it work? We explain each of the two claims +L+ in the lemma in turn (and after each comment on the im- +L+ portant parameters and how they differ from Barak et al. +L+ [4]). +L+ </bodyText> <listItem> 1. Z′ has entropy. We need to show that Z′ passes the +L+ test with high probability. We will point to the out- +L+ put string in C(^Z′, Y^′) which avoids R(^Z, Y^) with high +L+ probability as follows. In the analysis we will use the +L+ union bound on several events, one associated with +L+ each (poly(n) many) string in pSE(^Z, Y^). We note +L+ that by the definition of the response function, if we +L+ want to fix a particular element in the response set to +L+ a particular value, we can do this by fixing E(Z, i) and +L+ E(Y, i). This fixing keeps the restricted sources inde- +L+ pendent and loses only O(m) entropy. In the subsource +L+ of Z′ guaranteed to exist by Theorem 2.6 we can afford +L+ to lose this entropy in Z′. Thus we conclude that one +L+ of its outputs is uniform. The probability that this +L+ output will equal any fixed value is thus 2−m, com- +L+ pleting the argument. We note that we can handle +L+ the polynomial output size of pSE, since the uniform +L+ string has length m = no(1) (something which could +L+ not be done with the technology available to Barak et +L+ al. [4]). +L+ 2. Z′ has no entropy. We now need to guarantee that +L+ in the chosen subsources (which we choose) ^Z, Y^, all +L+ strings in C = C(^Z′, Y^) are in R(^Z, Y^). First notice +L+ that as Z′ is fixed, C is only a function of Y. We +L+ set Y~ to be the subsource of Y that fixes all strings +L+ in C = C(Y) to their most popular values (losing +L+ only ℓm entropy from Y). We take care of includ- +L+ ing these fixed strings in R(Z, Y~) one at a time, by +L+ restricting to subsources assuring that. Let σ be any +L+ m-bit string we want to appear in R(Z, Y~). Recall that +L+ R(z, y) = V(E(z, i), E(y, i)). We pick a ”good” seed i, +L+ and restrict Z, Y~ to subsources with only O(m) less +L+ entropy by fixing E(Z, i) = a and E(Y~, i) = b to values +L+ (a, b) for which V(a, b) = σ. This is repeated suc- +L+ cessively ℓ times, and results in the final subsources +L+ ^Z, Y^ on which ^Z′ fails with probability 1. Note that +L+ we keep reducing the entropy of our sources ℓ times, +L+ which necessitates that this ℓ be tiny (here we could +L+ not tolerate poly(n), and indeed can guarantee no(1), +L+ at least on a subsource – this is one aspect of how cru- +L+ cial the subsource somewhere extractor SSE is to the +L+ construction. +L+ </listItem> <bodyText> We note that initially it seemed like the Challenge-Response +L+ mechanism as used in [4] could not be used to handle en- +L+ tropy that is significantly less than -,/n (which is approxi- +L+ mately the bound that many of the previous constructions +L+ got stuck at). The techniques of [4] involved partitioning +L+ the sources into t pieces of length n/t each, with the hope +L+ that one of those parts would have a significant amount of +L+ entropy, yet there’d be enough entropy left over in the rest +L+ of the source (so that the source can be partitioned into a +L+ block source). +L+ However it is not clear how to do this when the total +L+ entropy is less than -,/n. On the one hand we will have +L+ to partition our sources into blocks of length significantly +L+ more than -,/n (or the adversary could distribute a negligible +L+ fraction of entropy in all blocks). On the other hand, if +L+ our blocks are so large, a single block could contain all the +L+ entropy. Thus it was not clear how to use the challenge +L+ response mechanism to find a block source. +L+ </bodyText> <sectionHeader> 4. THE SUBSOURCE SOMEWHERE +L+ EXTRACTOR SSE +L+ </sectionHeader> <bodyText> We now explain some of the ideas behind the construction +L+ of the subsource somewhere extractor SSE of Theorem 2.6. +L+ Consider the source X. We are seeking to find in it a some- +L+ where c-block-source, so that we can use it (together with Y) +L+ in the block-source extractor of Theorem 2.8. Like in previ- +L+ ous works in the extractor literature (e.g. [19, 13]) we use a +L+ ”win-win” analysis which shows that either X is already a +L+ somewhere c-block-source, or it has a condensed part which +L+ contains a lot of the entropy of the source. In this case we +L+ proceed recursively on that part. Continuing this way we +L+ eventually reach a source so condensed that it must be a +L+ somewhere block source. Note that in [4], the challenge re- +L+ sponse mechanism was used to find a block source also, but +L+ there the entropy was so high that they could afford to use +L+ </bodyText> <page> 675 +L+ </page> <figure> Not Somewhere block source	n bits total +L+ 		t blocks			Outputs +L+ < k’ +L+ Challenge Challenge +L+ responded responded +L+ X +L+ low +L+ med +L+ high +L+ n/t bits total +L+ t blocks +L+ Challenge Unresponded +L+ SB +L+ Somewhere Block Source! +L+ med +L+ med +L+ low +L+ 0< low < k’/t +L+ k’/t < med < k’/c +L+ k’/c < high < k’ +L+ high +L+ med +L+ Random Row +L+ med +L+ SB +L+ </figure> <figureCaption> Figure 1: Analysis of the subsource somewhere extractor. +L+ a tree of depth 1. They did not need to recurse or condense +L+ the sources. +L+ </figureCaption> <bodyText> Consider the tree of parts of the source X evolved by +L+ such recursion. Each node in the tree corresponds to some +L+ interval of bit locations of the source, with the root node +L+ corresponding to the entire source. A node is a child of an- +L+ other if its interval is a subinterval of the parent. It can be +L+ shown that some node in the tree is ”good”; it corresponds +L+ to a somewhere c-source, but we don’t know which node is +L+ good. Since we only want a somewhere extractor, we can +L+ apply to each node the somewhere block-source extractor of +L+ Corollary 2.8 – this will give us a random output in every +L+ ”good” node of the tree. The usual idea is output all these +L+ values (and in seeded extractors, merge them using the ex- +L+ ternally given random seed). However, we cannot afford to +L+ do that here as there is no external seed and the number of +L+ these outputs (the size of the tree) is far too large. +L+ Our aim then will be to significantly prune this number +L+ of candidates and in fact output only the candidates on one +L+ path to a canonical”good” node. First we will give a very in- +L+ formal description of how to do this (Figure 1). Before call- +L+ ing SSE recursively on a subpart of a current part of X, we’ll +L+ use the ”Challenge-Response” mechanism described above +L+ to check if ”it has entropy”.4 We will recurse only with the +L+ first (in left-to-right order) part which passes the ”entropy +L+ test”. Thus note that we will follow a single path on this +L+ tree. The algorithm SSE will output only the sets of strings +L+ produced by applying the somewhere c-block-extractor SB +L+ on the parts visited along this path. +L+ Now let us describe the algorithm for SSE. SSE will be +L+ initially invoked as SSE(x, y), but will recursively call itself +L+ with different inputs z which will always be substrings of x. +L+ </bodyText> <footnote> 4We note that we ignore the additional complication that +L+ SSE will actually use recursion also to compute the challenge +L+ in the challenge-response mechanism. +L+ </footnote> <construct> Algorithm: SSE(z, y) +L+ </construct> <bodyText> Let pSE(., .) be the somewhere extractor with a polyno- +L+ mial number of outputs of Proposition 2.3. +L+ Let SB be the somewhere block source extractor of Corol- +L+ lary 2.8. +L+ Global Parameters: t, the branching factor of the tree. k +L+ the original entropy of the sources. +L+ Output will be a set of strings. +L+ </bodyText> <listItem> 1. If z is shorter than √k, return the empty set, else +L+ continue. +L+ 2. Partition z into t equal parts z = z1, z2, ... ,zt. +L+ 3. Compute the response set R(z, y) which is the set of +L+ strings output by pSE(z, y). +L+ 4. For i E [t], compute the challenge set C(zi, y), which +L+ is the set of outputs of SSE(zi, y). +L+ 5. Let h be the smallest index for which the challenge set +L+ C(zh, y) is not contained in the response set (set h = t +L+ if no such index exists). +L+ 6. Output SB(z, y) concatenated with SSE(zh, y). +L+ Proving that indeed there are subsources on which SSE +L+ will follow a path to a ”good” (for these subsources) node, +L+ is the heart of the analysis. It is especially complex due +L+ to the fact that the recursive call to SSE on subparts of +L+ the current part is used to generate the Challenges for the +L+ Challenge-Response mechanism. Since SSE works only on +L+ a subsources we have to guarantee that restriction to these +L+ does not hamper the behavior of SSE in past and future calls +L+ to it. +L+ Let us turn to the highlights of the analysis, for the proof +L+ of Theorem 2.6. Let k' be the entropy of the source Z at +L+ some place in this recursion. Either one of its blocks Zi has +L+ </listItem> <page> 676 +L+ </page> <bodyText> entropy k'/c, in which case it is very condensed, since its +L+ size is n/t for t ≫ c), or it must be that c of its blocks form +L+ a c-block source with block entropy k'/t (which is sufficient +L+ for the extractor B used by SB). In the 2nd case the fact +L+ that SB(z, y) is part of the output of of our SSE guarantees +L+ that we are somewhere random. If the 2nd case doesn’t hold, +L+ let Zi be the leftmost condensed block. We want to ensure +L+ that (on appropriate subsources) SSE calls itself on that ith +L+ subpart. To do so, we fix all Zj for j < i to constants zj. We +L+ are now in the position described in the Challenge-Response +L+ mechanism section, that (in each of the first i parts) there +L+ is either no entropy or lots of entropy. We further restrict +L+ to subsources as explained there which make all first i − 1 +L+ blocks fail the ”entropy test”, and the fact that Zi still has +L+ lots of entropy after these restrictions (which we need to +L+ prove) ensures that indeed SSE will be recursively applied +L+ to it. +L+ We note that while the procedure SSE can be described re- +L+ cursively, the formal analysis of fixing subsources is actually +L+ done globally, to ensure that indeed all entropy requirements +L+ are met along the various recursive calls. +L+ Let us remark on the choice of the branching parameter t. +L+ On the one hand, we’d like to keep it small, as it dominates +L+ the number of outputs tc of SB, and thus the total number of +L+ outputs (which is tc logt n). For this purpose, any t = no(1) +L+ will do. On the other hand, t should be large enough so that +L+ condensing is faster than losing entropy. Here note that if +L+ Z is of length n, its child has length n/t, while the entropy +L+ shrinks only from k' to k'/c. A simple calculation shows that +L+ if k(lo9t)/lo9c) > n2 then a c block-source must exist along +L+ such a path before the length shrinks to √k. Note that for +L+ k = nΩ(1) a (large enough) constant t suffices (resulting in +L+ only logarithmic number of outputs of SSE). This analysis +L+ is depicted pictorially in Figure 1. +L+ </bodyText> <sectionHeader> 5. THE FINAL DISPERSER D +L+ </sectionHeader> <bodyText> Following is a rough description of our disperser D proving +L+ Theorem 2.1. The high level structure of D will resemble the +L+ structure of SSE - we will recursively split the source X and +L+ look for entropy in the parts. However now we must output +L+ a single value (rather than a set) which can take both values +L+ 0 and 1. This was problematic in SSE, even knowing where +L+ the ”good” part (containing a c-block-source) was! How can +L+ we do so now? +L+ We now have at our disposal a much more powerful tool +L+ for generating challenges (and thus detecting entropy), namely +L+ the subsource somewhere disperser SSE. Note that in con- +L+ structing SSE we only had essentially the somewhere c-block- +L+ source extractor SB to (recursively) generate the challenges, +L+ but it depended on a structural property of the block it was +L+ applied on. Now SSE does not assume any structure on its +L+ input sources except sufficient entropy 5. +L+ Let us now give a high level description of the disperser +L+ D. It too will be a recursive procedure. If when processing +L+ some part Z of X it ”realizes” that a subpart Zi of Z has +L+ entropy, but not all the entropy of Z (namely Zi, Z is a +L+ 2-block-source) then we will halt and produce the output +L+ of D. Intuitively, thinking about the Challenge-Response +L+ mechanism described above, the analysis implies that we +L+ </bodyText> <footnote> 5There is a catch – it only works on subsources of them! +L+ This will cause us a lot of head ache; we will elaborate on it +L+ later. +L+ can either pass or fail Zi (on appropriate subsources). But +L+ this means that the outcome of this ”entropy test” is a 1-bit +L+ disperser! +L+ To capitalize on this idea, we want to use SSE to identify +L+ such a block-source in the recursion tree. As before, we scan +L+ the blocks from left to right, and want to distinguish three +L+ possibilities. +L+ low Zi has low entropy. In this case we proceed to i + 1. +L+ medium Zi has ”medium” entropy (Zi, Z is a block-source). +L+ In which case we halt and produce an output (zero or +L+ one). +L+ high Zi has essentially all entropy of Z. In this case we +L+ recurse on the condensed block Zi. +L+ As before, we use the Challenge-Response mechanism (with +L+ a twist). We will compute challenges C(Zi, Y) and responses +L+ R(Z, Y), all strings of length m. The responses are computed +L+ exactly as before, using the somewhere extractor pSE. The +L+ Challenges are computed using our subsource somewhere +L+ extractor SSE. +L+ We really have 4 possibilities to distinguish, since when we +L+ halt we also need to decide which output bit we give. We will +L+ do so by deriving three tests from the above challenges and +L+ responses: (CH, RH), (CM, RM), (CL, RL) for high, medium +L+ and low respectively, as follows. Let m ≥ mH >> mM >> +L+ mL be appropriate integers: then in each of the tests above +L+ we restrict ourselves to prefixes of all strings of the appro- +L+ priate lengths only. So every string in CM will be a prefix +L+ of length mM of some string in CH. Similarly, every string +L+ in RL is the length mL prefix of some string in RH. Now +L+ it is immediately clear that if CM is contained in RM, then +L+ CL is contained in RL. Thus these tests are monotone, if +L+ our sample fails the high test, it will definitely fail all tests. +L+ </footnote> <construct> Algorithm: D(z, y) +L+ </construct> <bodyText> Let pSE(., .) be the somewhere extractor with a polyno- +L+ mial number of outputs of Proposition 2.3. +L+ Let SSE(.,.) be the subsource somewhere extractor of The- +L+ orem 2.6. +L+ Global Parameters: t, the branching factor of the tree. k +L+ the original entropy of the sources. +L+ Local Parameters for recursive level: mL ≪ mM ≪ mH. +L+ Output will be an element of {0, 1}. +L+ </bodyText> <listItem> 1. If z is shorter than √k, return 0. +L+ 2. Partition z into t equal parts z = z1, z2, ... , zt. +L+ 3. Compute three response sets RL, RM, RH using pSE(z, y). +L+ Rj will be the prefixes of length mj of the strings in +L+ pSE(z, y). +L+ 4. For each i ∈ [t], compute three challenge sets CiL, CiM, CiH +L+ using SSE(zi, y). Cij will be the prefixes of length mj +L+ of the strings in SSE(zi, y). +L+ 5. Let h be the smallest index for which the challenge set +L+ CL is not contained in the response set RL, if there is +L+ no such index, output 0 and halt. +L+ 6. If ChH is contained in RH and ChH is contained in RM, +L+ output 0 and halt. If ChH is contained in RH but ChH +L+ is not contained in RM, output 1 and halt. +L+ </listItem> <page> 677 +L+ </page> <figure> t blocks +L+ X +L+ low +L+ fail +L+ fail +L+ fail +L+ X_3 +L+ (X_3)_4 +L+ low +L+ low +L+ fail +L+ fail +L+ fail +L+ fail +L+ fail +L+ fail +L+ low +L+ low +L+ pass +L+ pass +L+ pass +L+ high +L+ low +L+ low +L+ t blocks +L+ low +L+ high +L+ t blocks +L+ med +L+ n bits total +L+ n/t bits total +L+ n/t^2 bits total +L+ fail +L+ fail +L+ fail +L+ fail +L+ fail +L+ fail +L+ pass +L+ pass +L+ fail +L+ pass +L+ fail +L+ fail +L+ Output 0	Output 1 +L+ </figure> <figureCaption> Figure 2: Analysis of the disperser. +L+ </figureCaption> <listItem> 7. Output D(zh, y), +L+ </listItem> <bodyText> First note the obvious monotonicity of the tests. If Zi fails +L+ one of the tests it will certainly fail for shorter strings. Thus +L+ there are only four outcomes to the three tests, written in the +L+ order (low, medium, high): (pass, pass, pass), (pass, pass, fail), +L+ (pass, fail, fail) and (fail, fail, fail). Conceptually, the algo- +L+ rithm is making the following decisions using the four tests: +L+ </bodyText> <listItem> 1. (fail, fail, fail): Assume Zi has low entropy and proceed +L+ to block i + 1. +L+ 2. (pass, fail, fail): Assume Zi is medium, halt and output +L+ 0. +L+ 3. (pass, pass, fail): Assume Zi is medium, halt and out- +L+ put 1. +L+ 4. (pass, pass, pass): Assume Zi is high and recurse on Zi. +L+ </listItem> <bodyText> The analysis of this idea (depicted in Figure 2).turns out +L+ to be more complex than it seems. There are two reasons for +L+ that. Now we briefly explain them and the way to overcome +L+ them in the construction and analysis. +L+ The first reason is the fact mentioned above, that SSE +L+ which generates the challenges, works only on a subsources +L+ of the original sources. Restricting to these subsources at +L+ some level of the recursion (as required by the analysis of of +L+ the test) causes entropy loss which affects both definitions +L+ (such as these entropy thresholds for decisions) and correct- +L+ ness of SSE in higher levels of recursion. Controlling this en- +L+ tropy loss is achieved by calling SSE recursively with smaller +L+ and smaller entropy requirements, which in turn limits the +L+ entropy which will be lost by these restrictions. In order not +L+ to lose all the entropy for this reason alone, we must work +L+ with special parameters of SSE, essentially requiring that at +L+ termination it has almost all the entropy it started with. +L+ The second reason is the analysis of the test when we are +L+ in a medium block. In contrast with the above situation, we +L+ cannot consider the value of Zi fixed when we need it to fail +L+ on the Medium and Low tests. We need to show that for +L+ these two tests (given a pass for High), they come up both +L+ (pass, fail) and (fail, fail) each with positive probability. +L+ Since the length of Medium challenges and responses is +L+ mM, the probability of failure is at least exp(−Q(mM)) (this +L+ follows relatively easily from the fact that the responses are +L+ somewhere random). If the Medium test fails so does the +L+ Low test, and thus (fail, fail) has a positive probability and +L+ our disperser D outputs 0 with positive probability. +L+ To bound (pass, fail) we first observe (with a similar +L+ reasoning) that the low test fails with probability at least +L+ exp(−Q(mL)). But we want the medium test to pass at the +L+ same time. This probability is at least the probability that +L+ low fails minus the probability that medium fails. We already +L+ have a bound on the latter: it is at most poly(n)exp(−ℓmM). +L+ Here comes our control of the different length into play - we +L+ can make the mL sufficiently smaller than mM to yield this +L+ difference positive. We conclude that our disperser D out- +L+ puts 1 with positive probability as well. +L+ Finally, we need to take care of termination: we have to +L+ ensure that the recurrence always arrives at a medium sub- +L+ part, but it is easy to chose entropy thresholds for low, medium +L+ and high to ensure that this happens. +L+ </bodyText> <page> 678 +L+ </page> <sectionHeader> 6. RESILIENCY AND DEFICIENCY +L+ </sectionHeader> <bodyText> In this section we will breifly discuss an issue which arises +L+ in our construction that we glossed over in the previous sec- +L+ tions. Recall our definition of subsources: +L+ </bodyText> <construct> DEfInItIOn 6.1 (SUBSOURCES). Given random variables +L+ Z and Zˆ on {0,1}n we say that Zˆ is a deficiency d subsource +L+ of Z and write Zˆ ⊆ Z if there exists a set A ⊆ {0,1}n such +L+ that (Z|A) = Zˆ and Pr[Z ∈ A] ≥ 2—d. +L+ </construct> <bodyText> Recall that we were able to guarantee that our algorithms +L+ made the right decisions only on subsources of the original +L+ source. For example, in the construction of our final dis- +L+ perser, to ensure that our algorithms correctly identify the +L+ right high block to recurse on, we were only able to guar- +L+ antee that there are subsources of the original sources in +L+ which our algorithm makes the correct decision with high +L+ probability. Then, later in the analysis we had to further +L+ restrict the source to even smaller subsources. This leads to +L+ complications, since the original event of picking the correct +L+ high block, which occurred with high probability, may be- +L+ come an event which does not occur with high probability +L+ in the current subsource. To handle these kinds of issues, +L+ we will need to be very careful in measuring how small our +L+ subsources are. +L+ In the formal analysis we introduce the concept of re- +L+ siliency to deal with this. To give an idea of how this works, +L+ here is the actual definition of somewhere subsource extrac- +L+ tor that we use in the formal analysis. +L+ </bodyText> <construct> DEfInItIOn 6.2 (SUBSOURCE SOmEWhERE ExtRaCtOR). +L+ </construct> <bodyText> A function SSE : {0, 1}n × {0, 1}n → ({0, 1}m)ℓ is a sub- +L+ source somewhere extractor with nrows output rows, entropy +L+ threshold k, deficiency def, resiliency res and error ǫ if for +L+ every (n, k)-sources X, Y there exist a deficiency def sub- +L+ source Xgood of X and a deficiency def subsource Ygood of +L+ Y such that for every deficiency res subsource X' of Xgood +L+ and deficiency res subsource Y' of Ygood, the random vari- +L+ able SSE(X',Y') is ǫ-close to a ℓ × m somewhere random +L+ distribution. +L+ It turns out that our subsource somewhere extractor does +L+ satisfy this stronger definition. The advantage of this defi- +L+ nition is that it says that once we restrict our attention to +L+ the good subsources Xgood, Ygood, we have the freedom to fur- +L+ ther restrict these subsources to smaller subsources, as long +L+ as our final subsources do not lose more entropy than the +L+ resiliency permits. +L+ This issue of managing the resiliency for the various ob- +L+ jects that we construct is one of the major technical chal- +L+ lenges that we had to overcome in our construction. +L+ </bodyText> <sectionHeader> 7. OPEN PROBLEMS +L+ </sectionHeader> <bodyText> Better Independent Source Extractors A bottleneck to +L+ improving our disperser is the block versus general +L+ source extractor of Theorem 2.7. A good next step +L+ would be to try to build an extractor for one block +L+ source (with only a constant number of blocks) and +L+ one other independent source which works for polylog- +L+ arithmic entropy, or even an extractor for a constant +L+ number of sources that works for sub-polynomial en- +L+ tropy. +L+ Simple Dispersers While our disperser is polynomial time +L+ computable, it is not as explicit as one might have +L+ hoped. For instance the Ramsey Graph construction +L+ of Frankl-Wilson is extremely simple: For a prime p, +L+ let the vertices of the graph be all subsets of [p3] of +L+ size p2 − 1. Two vertices S, T are adjacent if and only +L+ if |S ∩ T | ≡ −1 mod p. It would be nice to find a good +L+ disperser that beats the Frankl-Wilson construction, +L+ yet is comparable in simplicity. +L+ </bodyText> <sectionHeader> 8. REFERENCES +L+ </sectionHeader> <reference> [1] N. Alon. The shannon capacity of a union. +L+ Combinatorica, 18, 1998. +L+ [2] B. Barak. A simple explicit construction of an +L+ n˜o(logn )-ramsey graph. Technical report, Arxiv, 2006. +L+ http://arxiv.org/abs/math.CO/0601651. +L+ [3] B. Barak, R. Impagliazzo, and A. Wigderson. +L+ Extracting randomness using few independent sources. +L+ In Proceedings of the 45th Annual IEEE Symposium +L+ on Foundations of Computer Science, pages 384–393, +L+ 2004. +L+ [4] B. Barak, G. Kindler, R. Shaltiel, B. Sudakov, and +L+ A. Wigderson. Simulating independence: New +L+ constructions of condensers, Ramsey graphs, +L+ dispersers, and extractors. In Proceedings of the 37th +L+ Annual ACM Symposium on Theory of Computing, +L+ pages 1–10, 2005. +L+ [5] J. Bourgain. More on the sum-product phenomenon in +L+ prime fields and its applications. International Journal +L+ of Number Theory, 1:1–32, 2005. +L+ [6] J. Bourgain, N. Katz, and T. Tao. A sum-product +L+ estimate in finite fields, and applications. Geometric +L+ and Functional Analysis, 14:27–57, 2004. +L+ [7] M. Capalbo, O. Reingold, S. Vadhan, and +L+ A. Wigderson. Randomness conductors and +L+ constant-degree lossless expanders. In Proceedings of +L+ the 34th Annual ACM Symposium on Theory of +L+ Computing, pages 659–668, 2002. +L+ [8] B. Chor and O. Goldreich. Unbiased bits from sources +L+ of weak randomness and probabilistic communication +L+ complexity. SIAM Journal on Computing, +L+ 17(2):230–261, 1988. +L+ [9] P. Frankl and R. M. Wilson. Intersection theorems +L+ with geometric consequences. Combinatorica, +L+ 1(4):357–368, 1981. +L+ [10] P. Gopalan. Constructing ramsey graphs from boolean +L+ function representations. In Proceedings of the 21th +L+ Annual IEEE Conference on Computational +L+ Complexity, 2006. +L+ [11] V. Grolmusz. Low rank co-diagonal matrices and +L+ ramsey graphs. Electr. J. Comb, 7, 2000. +L+ [12] V. Guruswami. Better extractors for better codes? +L+ Electronic Colloquium on Computational Complexity +L+ (ECCC), (080), 2003. +L+ [13] C. J. Lu, O. Reingold, S. Vadhan, and A. Wigderson. +L+ Extractors: Optimal up to constant factors. In +L+ Proceedings of the 35th Annual ACM Symposium on +L+ Theory of Computing, pages 602–611, 2003. +L+ [14] P. Miltersen, N. Nisan, S. Safra, and A. Wigderson. +L+ On data structures and asymmetric communication +L+ complexity. Journal of Computer and System +L+ Sciences, 57:37–49, 1 1998. +L+ </reference> <page> 679 +L+ </page> <reference> [15] N. Nisan and D. Zuckerman. More deterministic +L+ simulation in logspace. In Proceedings of the 25th +L+ Annual ACM Symposium on Theory of Computing, +L+ pages 235–244, 1993. +L+ [16] P. Pudlak and V. Rodl. Pseudorandom sets and +L+ explicit constructions of ramsey graphs. Submitted for +L+ publication, 2004. +L+ [17] A. Rao. Extractors for a constant number of +L+ polynomially small min-entropy independent sources. +L+ In Proceedings of the 38th Annual ACM Symposium +L+ on Theory of Computing, 2006. +L+ [18] R. Raz. Extractors with weak random seeds. In +L+ Proceedings of the 37th Annual ACM Symposium on +L+ Theory of Computing, pages 11–20, 2005. +L+ [19] O. Reingold, R. Shaltiel, and A. Wigderson. +L+ Extracting randomness via repeated condensing. In +L+ Proceedings of the 41st Annual IEEE Symposium on +L+ Foundations of Computer Science, pages 22–31, 2000. +L+ [20] M. Santha and U. V. Vazirani. Generating +L+ quasi-random sequences from semi-random sources. +L+ Journal of Computer and System Sciences, 33:75–87, +L+ 1986. +L+ [21] R. Shaltiel. Recent developments in explicit +L+ constructions of extractors. Bulletin of the European +L+ Association for Theoretical Computer Science, +L+ 77:67–95, 2002. +L+ [22] A. Ta-Shma and D. Zuckerman. Extractor codes. +L+ IEEE Transactions on Information Theory, 50, 2004. +L+ [23] U. Vazirani. Towards a strong communication +L+ complexity theory or generating quasi-random +L+ sequences from two communicating slightly-random +L+ sources (extended abstract). In Proceedings of the 17th +L+ Annual ACM Symposium on Theory of Computing, +L+ pages 366–378, 1985. +L+ [24] A. Wigderson and D. Zuckerman. Expanders that +L+ beat the eigenvalue bound: Explicit construction and +L+ applications. Combinatorica, 19(1):125–138, 1999. +L+ </reference> <page> 680 +L+ </page> 
