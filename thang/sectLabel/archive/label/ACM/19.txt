title ||| A Similarity Measure for Motion Stream
title ||| Segmentation and Recognition*
author ||| Chuanjun Li	B. Prabhakaran
affiliation ||| Department of Computer Science
affiliation ||| The University of Texas at Dallas, Richardson, TX 75083
email ||| {chuanjun, praba}@utdallas.edu
sectionHeader ||| ABSTRACT
bodyText ||| Recognition of motion streams such as data streams gener-
bodyText ||| ated by different sign languages or various captured human
bodyText ||| body motions requires a high performance similarity mea-
bodyText ||| sure. The motion streams have multiple attributes, and mo-
bodyText ||| tion patterns in the streams can have different lengths from
bodyText ||| those of isolated motion patterns and different attributes
bodyText ||| can have different temporal shifts and variations. To ad-
bodyText ||| dress these issues, this paper proposes a similarity measure
bodyText ||| based on singular value decomposition (SVD) of motion ma-
bodyText ||| trices. Eigenvector differences weighed by the corresponding
bodyText ||| eigenvalues are considered for the proposed similarity mea-
bodyText ||| sure. Experiments with general hand gestures and human
bodyText ||| motion streams show that the proposed similarity measure
bodyText ||| gives good performance for recognizing motion patterns in
bodyText ||| the motion streams in real time.
sectionHeader ||| Categories and Subject Descriptors: H.2.8 [Database
sectionHeader ||| Management]: Database Applications – Data Mining
sectionHeader ||| General Terms: Algorithm
sectionHeader ||| Keywords: Pattern recognition, gesture, data streams, seg-
sectionHeader ||| mentation, singular value decomposition.
sectionHeader ||| 1. INTRODUCTION
bodyText ||| Motion streams can be generated by continuously per-
bodyText ||| formed sign language words [14] or captured human body
bodyText ||| motions such as various dances. Captured human motions
bodyText ||| can be applied to the movie and computer game industries
bodyText ||| by reconstructing various motions from video sequences [10]
bodyText ||| or images [15] or from motions captured by motion capture
bodyText ||| systems [4]. Recognizing motion patterns in the streams
bodyText ||| with unsupervised methods requires no training process, and
bodyText ||| is very convenient when new motions are expected to be
bodyText ||| added to the known pattern pools. A similarity measure
bodyText ||| with good performance is thus necessary for segmenting and
bodyText ||| recognizing the motion streams. Such a similarity measure
bodyText ||| needs to address some new challenges posed by real world
footnote ||| *Work supported partially by the National Science Founda-
footnote ||| tion under Grant No. 0237954 for the project CAREER:
footnote ||| Animation Databases.
copyright ||| Permission to make digital or hard copies of all or part of this work for
copyright ||| personal or classroom use is granted without fee provided that copies are
copyright ||| not made or distributed for profit or commercial advantage and that copies
copyright ||| bear this notice and the full citation on the first page. To copy otherwise, to
copyright ||| republish, to post on servers or to redistribute to lists, requires prior specific
copyright ||| permission and/or a fee.
copyright ||| Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$5.00.
bodyText ||| motion streams: first, the motion patterns have dozens of at-
bodyText ||| tributes, and similar patterns can have different lengths due
bodyText ||| to different motion durations; second, different attributes of
bodyText ||| similar motions have different variations and different tem-
bodyText ||| poral shifts due to motion variations; and finally, motion
bodyText ||| streams are continuous, and there are no obvious ”pauses”
bodyText ||| between neighboring motions in a stream. A good similarity
bodyText ||| measure not only needs to capture the similarity of complete
bodyText ||| motion patterns, but also needs to capture the differences
bodyText ||| between complete motion patterns and incomplete motion
bodyText ||| patterns or sub-patterns in order to segment a stream for
bodyText ||| motion recognition.
bodyText ||| As the main contribution of this paper, we propose a sim-
bodyText ||| ilarity measure to address the above issues. The proposed
bodyText ||| similarity measure is defined based on singular value decom-
bodyText ||| position of the motion matrices. The first few eigenvectors
bodyText ||| are compared for capturing the similarity of two matrices,
bodyText ||| and the inner products of the eigenvectors are given differ-
bodyText ||| ent weights for their different contributions. We propose to
bodyText ||| use only the eigenvalues corresponding to the involved eigen-
bodyText ||| vectors of the two motion matrices as weights. This simple
bodyText ||| and intuitive weighing strategy gives the same importance to
bodyText ||| eigenvalues of the two matrices. We also show that the 95%
bodyText ||| variance rule for choosing the number of eigenvectors [13] is
bodyText ||| not sufficient for recognizing both isolated patterns and mo-
bodyText ||| tion streams. Our experiments demonstrate that at least the
bodyText ||| first 6 eigenvectors need to be considered for motion streams
bodyText ||| of either 22 attribute or 54 attributes, and the first 6 eigen-
bodyText ||| values accounts for more than 99.5% of the total variance in
bodyText ||| the motion matrices.
sectionHeader ||| 2. RELATED WORK
bodyText ||| Multi-attribute pattern similarity search, especially in con-
bodyText ||| tinuous motion streams, has been widely studied for sign
bodyText ||| language recognition and for motion synthesis in computer
bodyText ||| animation. The recognition methods usually include tem-
bodyText ||| plate matching by distance measures and hidden Markov
bodyText ||| models (HMM).
bodyText ||| Template matching by using similarity/distance measures
bodyText ||| has been employed for multi-attribute pattern recognition.
bodyText ||| Joint angles are extracted in [11] as features to represent dif-
bodyText ||| ferent human body static poses for the Mahalanobis distance
bodyText ||| measure of two joint angle features. Similarly, momentum,
bodyText ||| kinetic energy and force are constructed in [2,5] as activ-
bodyText ||| ity measure and prediction of gesture boundaries for various
bodyText ||| segments of the human body, and the Mahalanobis distance
bodyText ||| function of two composite features are solved by dynamic
bodyText ||| programming.
page ||| 89
bodyText ||| Similarity measures are defined for multi-attribute data
bodyText ||| in [6,12,16] based on principal component analysis (PCA).
bodyText ||| Inner products or angular differences of principal compo-
bodyText ||| nents (PCs) are considered for similarity measure defini-
bodyText ||| tions, with different weighted strategies for different PCs.
bodyText ||| Equal weights are considered for different combinations of
bodyText ||| PCs in [6], giving different PCs equal contributions to the
bodyText ||| similarity measure. The similarity measure in [12] takes the
bodyText ||| minimum of two weighted sums of PC inner products, and
bodyText ||| the two sums are respectively weighted by different weights.
bodyText ||| A global weight vector is obtained by taking into account all
bodyText ||| available isolated motion patterns in [16], and this weight
bodyText ||| vector is used for specifying different contributions from dif-
bodyText ||| ferent PC inner products to the similarity measure Eros.
bodyText ||| The dominating first PC and a normalized eigenvalue vector
bodyText ||| are considered in [7,8] for pattern recognition. In contrast,
bodyText ||| this paper propose to consider the first few PCs, and the
bodyText ||| angular differences or inner products of different PCs are
bodyText ||| weighted by different weights which depends on the data
bodyText ||| variances along the corresponding PCs.
bodyText ||| The HMM technique has been widely used for sign lan-
bodyText ||| guage recognition, and different recognition rates have been
bodyText ||| reported for different sign languages and different feature se-
bodyText ||| lection approaches. Starner et al. [14] achieved 92% and 98%
bodyText ||| word accuracy respectively for two systems, the first of the
bodyText ||| systems used a camera mounted on a desk and the second
bodyText ||| one used a camera in a user’s cap for extracting features
bodyText ||| as the input of HMM. Similarly Liang and Ouhyoung [9]
bodyText ||| used HMM for postures, orientations and motion primitives
bodyText ||| as features extracted from continuous Taiwan sign language
bodyText ||| streams and an average 80.4% recognition rate was achieved.
bodyText ||| In contrast, the approach proposed in this paper is an un-
bodyText ||| supervised approach, and no training as required for HMM
bodyText ||| recognizers is needed.
sectionHeader ||| 3. SIMILARITY MEASURE FOR MOTION
sectionHeader ||| STREAM RECOGNITION
bodyText ||| The joint positional coordinates or joint angular values of
bodyText ||| a subject in motion can be represented by a matrix: the
bodyText ||| columns or attributes of the matrix are for different joints,
bodyText ||| and the rows or frames of the matrix are for different time
bodyText ||| instants. Similarity of two motions is the similarity of the
bodyText ||| resulting motion matrices, which have the same number of
bodyText ||| attributes or columns, and yet can have different number
bodyText ||| of rows due to different motion durations. To capture the
bodyText ||| similarity of two matrices of different lengths, we propose
bodyText ||| to apply singular value decomposition (SVD) to the motion
bodyText ||| matrices in order to capture the similarity of the matrix
bodyText ||| geometric structures. Hence we briefly present SVD and its
bodyText ||| associated properties below before proposing the similarity
bodyText ||| measure based on SVD in this section.
subsectionHeader ||| 3.1 Singular Value Decomposition
bodyText ||| The geometric structure of a matrix can be revealed by
bodyText ||| the SVD of the matrix. As shown in [3], any real m x n
bodyText ||| matrix A can be decomposed into A = UEVT , where U =
bodyText ||| [u1, u2, ... , um] E Rm×m and V = [v1, v2, ... , vn] E RnXn
bodyText ||| are two orthogonal matrices, and E is a diagonal matrix with
bodyText ||| diagonal entries being the singular values of A: v1 &gt; v2 &gt;
bodyText ||| . . . &gt; Qmin(m,n) &gt; 0. Column vectors ui and vi are the ith
bodyText ||| left and right singular vectors of A, respectively.
bodyText ||| It can be shown that the right singular vectors of the sym-
bodyText ||| metric n x n matrix M = AT A are identical to the corre-
bodyText ||| sponding right singular vectors of A, referred to as eigenvec-
bodyText ||| tors of M. The singular values of M, or eigenvalues of M,
bodyText ||| are squares of the corresponding singular values of A. The
bodyText ||| eigenvector with the largest eigenvalue gives the first prin-
bodyText ||| cipal component. The eigenvector with the second largest
bodyText ||| eigenvalue is the second principal component and so on.
subsectionHeader ||| 3.2 Similarity Measure
bodyText ||| Since SVD exposes the geometric structure of a matrix, it
bodyText ||| can be used for capturing the similarity of two matrices. We
bodyText ||| can compute the SVD of M = AT A instead of computing
bodyText ||| the SVD of A to save computational time. The reasons are
bodyText ||| that the eigenvectors of M are identical to the corresponding
bodyText ||| right singular vectors of A, the eigenvalues of M are the
bodyText ||| squares of the corresponding singular values of A, and SVD
bodyText ||| takes O(n 3) time for the n x n M and takes O(mn2) time
bodyText ||| with a large constant for the m x n A, and usually m &gt; n.
bodyText ||| Ideally, if two motions are similar, their corresponding
bodyText ||| eigenvectors should be parallel to each other, and their cor-
bodyText ||| responding eigenvalues should also be proportional to each
bodyText ||| other. This is because the eigenvectors are the correspond-
bodyText ||| ing principal components, and the eigenvalues reflect the
bodyText ||| variances of the matrix data along the corresponding prin-
bodyText ||| cipal components. But due to motion variations, all corre-
bodyText ||| sponding eigenvectors cannot be parallel as shown in Fig-
bodyText ||| ure 1. The parallelness or angular differences of two eigen-
bodyText ||| vectors u and v can be described by the absolute value of
bodyText ||| their inner products: l cosOl = lu • vl/(lullvl) = lu • vl, where
bodyText ||| lul = lvl = 1. We consider the absolute value of the in-
bodyText ||| ner products because eigenvectors can have different signs
bodyText ||| as shown in [8].
bodyText ||| Since eigenvalues are numerically related to the variances
bodyText ||| of the matrix data along the associated eigenvectors, the im-
bodyText ||| portance of the eigenvector parallelness can be described by
bodyText ||| the corresponding eigenvalues. Hence, eigenvalues are to be
bodyText ||| used to give different weights to different eigenvector pairs.
bodyText ||| Figure 2 shows that the first eigenvalues are the dominat-
bodyText ||| ing components of all the eigenvalues, and other eigenval-
bodyText ||| ues become smaller and smaller and approach zero. As the
bodyText ||| eigenvalues are close to zero, their corresponding eigenvec-
bodyText ||| tors can be very different even if two matrices are similar.
bodyText ||| Hence not all the eigenvectors need to be incorporated into
bodyText ||| the similarity measure.
bodyText ||| Since two matrices have two eigenvalues for the corre-
bodyText ||| sponding eigenvector pair, these two eigenvalues should have
bodyText ||| equal contributions or weights to the eigenvector parallel-
bodyText ||| ness. In addition, the similarity measure of two matrices
bodyText ||| should be independent to other matrices, hence only eigen-
bodyText ||| vectors and eigenvalues of the two matrices should be con-
bodyText ||| sidered.
bodyText ||| Based on the above discussions, we propose the following
bodyText ||| similarity measure for two matrices Q and P:
equation ||| k
equation ||| 1
equation |||   (Q, P) =
equation ||| 2 i=1
bodyText ||| where vi and Ai are the ith eigenvalues corresponding to the
bodyText ||| ith eigenvectors ui and vi of square matrices of Q and P,
bodyText ||| respectively, and 1 &lt; k &lt; n. Integer k determines how many
bodyText ||| eigenvectors are considered and it depends on the number
bodyText ||| of attributes n of motion matrices. Experiments with hand
bodyText ||| gesture motions (n = 22) and human body motions (n =
equation ||| n
equation ||| ((�i/
equation ||| i=1
equation ||| n
equation ||| Qi+Ai/
equation ||| i=1
equation ||| Ai)lui •vil)
page ||| 90
figureCaption ||| Figure 1: Eigenvectors of similar patterns. The first
figureCaption ||| eigenvectors are similar to each other, while other
figureCaption ||| eigenvectors, such as the second vectors shown in
figureCaption ||| the bottom, can be quite different.
bodyText ||| 54) in Section 4 show that k = 6 is large enough without
bodyText ||| loss of pattern recognition accuracy in streams. We refer to
bodyText ||| this non-metric similarity measure as k Weighted Angular
bodyText ||| Similarity (kWAS) , which captures the angular similarities
bodyText ||| of the first k corresponding eigenvector pairs weighted by
bodyText ||| the corresponding eigenvalues.
bodyText ||| It can be easily verified that the value of kWAS ranges over
bodyText ||| [0,1]. When all corresponding eigenvectors are normal to
bodyText ||| each other, the similarity measure will be zero, and when two
bodyText ||| matrices are identical, the similarity measure approaches the
bodyText ||| maximum value one if k approaches n.
subsectionHeader ||| 3.3 Stream Segmentation Algorithm
bodyText ||| In order to recognize motion streams, we assume one mo-
bodyText ||| tion in a stream has a minimum length l and a maximum
bodyText ||| length L. The following steps can be applied to incremen-
bodyText ||| tally segment a stream for motion recognition:
listItem ||| 1. SVD is applied to all isolated motion patterns P to
listItem ||| obtain their eigenvectors and eigenvalues. Let S be
listItem ||| the incremented stream length for segmentation, and
listItem ||| let L be the location for segmentation. Initially L = l.
listItem ||| 2. Starting from the beginning of the stream or the end of
listItem ||| the previously recognized motion, segment the stream
listItem ||| at location L. Compute the eigenvectors and eigenval-
listItem ||| ues of the motion segment Q.
listItem ||| 3. Compute kWAS between Q and all motion patterns
bodyText ||| P. Update T... to be the highest similarity after the
bodyText ||| previous motion’s recognition.
listItem ||| 4. If L+S &lt; L, update L = L+S and go to step 2. Other-
bodyText ||| wise, the segment corresponding to T,r,.. is recognized
bodyText ||| to be the motion pattern which gives the highest simi-
bodyText ||| larity T..., update L = l starting from the end of the
bodyText ||| last recognized motion pattern and go to step 2.
figure ||| 100
figure ||| 99
figure ||| 95
figure ||| 93
figure ||| 91
figure ||| 89
figure ||| 87
figure ||| 1	2	3	4	5	6	7	8
figure ||| Number of Eigenvalues
figureCaption ||| Figure 2: Accumulated eigenvalue percentages in
figureCaption ||| total eigenvalues for CyberGlove data and captured
figureCaption ||| human body motion data. There are 22 eigenvalues
figureCaption ||| for the CyberGlove data and 54 eigenvalues for the
figureCaption ||| captured motion data. The sum of the first 2 eigen-
figureCaption ||| values is more than 95% of the corresponding total
figureCaption ||| eigenvalues, and the sum of the first 6 eigenvalues is
figureCaption ||| almost 100% of the total eigenvalues.
sectionHeader ||| 4. PERFORMANCE EVALUATION
bodyText ||| This section evaluates experimentally the performances
bodyText ||| of the similarity measure kWAS proposed in this paper. It
bodyText ||| has been shown in [16] that Eros [16] outperforms other
bodyText ||| similarity measures mentioned in Section 2 except MAS [8].
bodyText ||| Hence in this section, we compare the performances of the
bodyText ||| proposed kWAS with Eros and MAS for recognizing similar
bodyText ||| isolated motion patterns and for segmenting and recognizing
bodyText ||| motion streams from hand gesture capturing CyberGlove
bodyText ||| and human body motion capture system.
subsectionHeader ||| 4.1 Data Generation
bodyText ||| A similarity measure should be able to be used not only
bodyText ||| for recognizing isolated patterns with high accuracy, but also
bodyText ||| for recognizing patterns in continuous motions or motion
bodyText ||| streams. Recognizing motion streams is more challenging
bodyText ||| than recognizing isolated patterns. This is because many
bodyText ||| very similar motion segments or sub-patterns needs to be
bodyText ||| compared in order to find appropriate segmentation loca-
bodyText ||| tions, and a similarity measure should capture the difference
bodyText ||| between a complete motion or pattern and its sub-patterns.
bodyText ||| Hence, both isolated motion patterns and motion streams
bodyText ||| were generated for evaluating the performance of kWAS.
bodyText ||| Two data sources are considered for data generation: a Cy-
bodyText ||| berGlove for capturing hand gestures and a Vicon motion
bodyText ||| capture system for capturing human body motions.
subsubsectionHeader ||| 4.1.1 CyberGlove Data
bodyText ||| A CyberGlove is a fully instrumented data glove that pro-
bodyText ||| vides 22 sensors for measuring hand joint angular values to
bodyText ||| capture motions of a hand, such as American Sign Language
bodyText ||| (ASL) words for hearing impaired. The data for a hand ges-
bodyText ||| ture contain 22 angular values for each time instant/frame,
bodyText ||| one value for a joint of one degree of freedom. The mo-
bodyText ||| tion data are extracted at around 120 frames per second.
bodyText ||| Data matrices thus have 22 attributes for the CyberGlove
bodyText ||| motions.
bodyText ||| One hundred and ten different isolated motions were gen-
bodyText ||| erated as motion patterns, and each motion was repeated
bodyText ||| for three times, resulting in 330 isolated hand gesture mo-
bodyText ||| tions. Some motions have semantic meanings. For example,
figure ||| 2	4	6	8	10	12	14	16	18	20	22
figure ||| Motion341
figure ||| Motion342
figure ||| Motion341
figure ||| Motion342
figure ||| 2	4	6	8	10	12	14	16	18	20	22
figure ||| Component of Second Eigenvector
figure ||| 	0.6 0.4 0.2 0 −0.2 −0.4
figure ||| 0.2
figure ||| 0.1
figure ||| 0
figure ||| −0.1
figure ||| −0.2
figure ||| −0.3
figure ||| −0.4
figure ||| −0.5
figure ||| −0.6
figure ||| −0.7
figure ||| Component of First Eigenvector
figure ||| 97
figure ||| 85
figure ||| CyberGlove Data
figure ||| MoCap Data
page ||| 91
bodyText ||| the motion for BUS as shown in Table 1 is for the ASL sign
bodyText ||| ”bus”. Yet for segmentation and recognition, we only re-
bodyText ||| quire that each individual motion be different from others,
bodyText ||| and thus some motions are general motions, and do not have
bodyText ||| any particular semantic meanings, such as the THUMBUP
bodyText ||| motion in Table 1.
bodyText ||| The following 18 motions shown in Table 1 were used to
bodyText ||| generate continuous motions or streams. Twenty four dif-
bodyText ||| ferent motion streams were generated for segmentation and
bodyText ||| recognition purpose. There are 5 to 10 motions in a stream
bodyText ||| and 150 motions in total in 24 streams, with 6.25 motions in
bodyText ||| a stream on average. It should be noted that variable-length
bodyText ||| transitional noises occur between successive motions in the
bodyText ||| generated streams.
tableCaption ||| Table 1: Individual motions used for streams
table ||| 35 60 70 80 90 BUS GOODBYE
table ||| HALF IDIOM JAR JUICE KENNEL KNEE
table ||| MILK TV SCISSOR SPREAD THUMBUP
subsubsectionHeader ||| 4.1.2 Motion Capture Data
bodyText ||| The motion capture data come from various motions cap-
bodyText ||| tured collectively by using 16 Vicon cameras and the Vicon
bodyText ||| iQ Workstation software. A dancer wears a suit of non-
bodyText ||| reflective material and 44 markers are attached to the body
bodyText ||| suit. After system calibration and subject calibration, global
bodyText ||| coordinates and rotation angles of 19 joints/segments can
bodyText ||| be obtained at about 120 frames per second for any mo-
bodyText ||| tion. Similarity of patterns with global 3D positional data
bodyText ||| can be disguised by different locations, orientations or differ-
bodyText ||| ent paths of motion execution as illustrated in Figure 3(a).
bodyText ||| Since two patterns are similar to each other because of sim-
bodyText ||| ilar relative positions of corresponding body segments at
bodyText ||| corresponding time, and the relative positions of different
bodyText ||| segments are independent of locations or orientations of the
bodyText ||| body, we can transform the global position data into local
bodyText ||| position data as follows.
bodyText ||| Let Xp, Yp, Zp be the global coordinates of one point on
bodyText ||| pelvis, the selected origin of the ”moving” local coordinate
bodyText ||| system, and a,,3, -y be the rotation angles of the pelvis seg-
bodyText ||| ment relative to the global coordinate system axes, respec-
bodyText ||| tively. The translation matrix is T as follows:
equation ||| 1	0	0	0
equation ||| 0	1	0	0
equation ||| 0	0	1	0
equation ||| —Xp	—Yp	—Zp	1
equation ||| The rotation matrix R = R. x Ry x Rz, where
equation ||| 1	0	0	0
equation ||| 0 cos a — sin a 0
equation ||| 0 sin a cos a 0
equation ||| 0 0 0 1
equation ||| cos,3 0 sin,3 0
equation ||| 0	1	0	0
equation ||| —sin,3 0 cos,3 0
equation ||| 0	0	0	1
figure ||| Motion Capture Frames	Motion Capture Frames
figure ||| (a)	(b)
figureCaption ||| Figure 3: 3D motion capture data for similar motions
figureCaption ||| executed at different locations and in different orien-
figureCaption ||| tations: (a) before transformation; (b) after transfor-
figureCaption ||| mation.
equation ||| cos-y —sin -y 0 0
equation ||| sin -y cos -y 0 0
equation ||| 0 0 1 0
equation ||| 0 0 0 1
bodyText ||| Let X, Y, Z be the global coordinates of one point on any
bodyText ||| segments, and x, y, z be the corresponding transformed local
bodyText ||| coordinates. x, y and z can be computed as follows:
equation ||| [x y z 1]=[X Y Z 1] x T x R
bodyText ||| The transformed data are positions of different segments
bodyText ||| relative to a moving coordinate system with the origin at
bodyText ||| some fixed point of the body, for example the pelvis. The
bodyText ||| moving coordinate system is not necessarily aligned with
bodyText ||| the global system, and it can rotate with the body. So data
bodyText ||| transformation includes both translation and rotation, and
bodyText ||| the transformed data would be translation and rotation in-
bodyText ||| variant as shown in Figure 3(b). The coordinates of the
bodyText ||| origin pelvis are not included, thus the transformed matri-
bodyText ||| ces have 54 columns.
bodyText ||| Sixty two isolated motions including Taiqi, Indian dances,
bodyText ||| and western dances were performed for generating motion
bodyText ||| capture data, and each motion was repeated 5 times, yield-
bodyText ||| ing 310 isolated human motions. Every repeated motion has
bodyText ||| a different location and different durations, and can face
bodyText ||| different orientations. Twenty three motion streams were
bodyText ||| generated for segmentation. There are 3 to 5 motions in
bodyText ||| a stream, and 93 motions in total in 23 streams, with 4.0
bodyText ||| motions in a stream on average.
subsectionHeader ||| 4.2 Performance of kWAS for Capturing Sim-
subsectionHeader ||| ilarities and Segmenting Streams
bodyText ||| We first apply kWAS to isolated motion patterns to show
bodyText ||| that the proposed similarity measure kWAS can capture the
bodyText ||| similarities of isolated motion patterns. Then kWAS is ap-
bodyText ||| plied to motion streams for segmenting streams and recog-
bodyText ||| nizing motion patterns in the streams. We experimented
bodyText ||| with different k values in order to find out the smallest k
bodyText ||| without loss of good performance.
bodyText ||| Figure 2 shows the accumulated eigenvalue percentages
bodyText ||| averaged on 330 hand gestures and 310 human motions, re-
bodyText ||| spectively. Although the first two eigenvalues account for
figure ||| 1500
figure ||| 1000
figure ||| 500
figure ||| 0
figure ||| −500
figure ||| −1000
figure ||| −1500
figure ||| 0	50	100	150	200	250	300	350	400	450
figure ||| −1000
figure ||| 0	50	100	150	200	250	300	350	400	450
figure ||| 1000
figure ||| 500
figure ||| 0
figure ||| −500
figure ||| 2000
figure ||| 1500
figure ||| 1000
figure ||| 500
figure ||| 0
figure ||| 0	50	100	150	200	250	300	350	400	450
figure ||| −1000
figure ||| 0	50	100	150	200	250	300	350	400	450
figure ||| 1000
figure ||| 500
figure ||| 0
figure ||| −500
figure ||| T=	
figure ||| R, =	
figure ||| Ry =	
figure ||| Rz =	
figure ||| 92
figure ||| Number of Nearest Neghbors (Most Simlar Patterns)
figureCaption ||| Figure 4: Recognition rate of similar CyberGlove
figureCaption ||| motion patterns. When k is 3, kWAS can find the
figureCaption ||| most similar motions for about 99.7% of 330 mo-
figureCaption ||| tions, and can find the second most similar motions
figureCaption ||| for 97.5% of the them.
figure ||| Number of Nearest Neighbors (Most aimilar Patterns1
figureCaption ||| Figure 5: Recognition rate of similar captured mo-
figureCaption ||| tion patterns. When k is 5, by using kWAS, the most
figureCaption ||| similar motions of all 310 motions can be found, and
figureCaption ||| the second most similar motions of 99.8% of the 310
figureCaption ||| motions can also be found.
bodyText ||| more than 95% of the respective sums of all eigenvalues,
bodyText ||| considering only the first two eigenvectors for kWAS is not
bodyText ||| sufficient as shown in Figure 4 and Figure 5. For Cyber-
bodyText ||| Glove data with 22 attributes, kWAS with k = 3 gives the
bodyText ||| same performance as kWAS with k = 22, and for motion
bodyText ||| capture data with 54 attributes, kWAS with k = 5 gives the
bodyText ||| same performance as kWAS with k = 54. Figure 4 and Fig-
bodyText ||| ure 5 illustrate that kWAS can be used for finding similar
bodyText ||| motion patterns and outperforms MAS and Eros for both
bodyText ||| hand gesture and human body motion data.
bodyText ||| The steps in Section 3.3 are used for segmenting streams
bodyText ||| and recognizing motions in streams. The recognition accu-
bodyText ||| racy as defined in [14] is used for motion stream recognition.
bodyText ||| The motion recognition accuracies are shown in Table 2. For
bodyText ||| both CyberGlove motion and captured motion data, k = 6
bodyText ||| is used for kWAS, which gives the same accuracy as k = 22
bodyText ||| for CyberGlove data and k = 54 for motion capture data,
bodyText ||| respectively.
bodyText ||| Figure 6 shows the time taken for updating the candi-
bodyText ||| date segment, including updating the matrix, computing the
bodyText ||| SVD of the updated matrix, and computing the similarities
bodyText ||| of the segment and all motion patterns. The code imple-
bodyText ||| mented in C++ was run on one 2.70 GHz Intel processor
bodyText ||| of a GenuineIntel Linux box. There are 22 attributes for
bodyText ||| the CyberGlove streams, and 54 attributes for the captured
figureCaption ||| Figure 6: Computation time for stream segment up-
figureCaption ||| date and similarity computation.
tableCaption ||| Table 2: Stream Pattern Recognition Accuracy (%)
table ||| Similarity Measures	CyberGlove	Motion Capture
table ||| 	Streams	Streams
table ||| Eros	68.7	78.5
table ||| MAS	93.3	78.5
table ||| kWAS (k=6)	94.0	94.6
bodyText ||| motion streams. Hence updating captured motion segments
bodyText ||| takes longer than updating CyberGlove motion segments as
bodyText ||| shown in Figure 6. The time required by kWAS is close to
bodyText ||| the time required by MAS, and is less than half of the time
bodyText ||| taken by using Eros.
subsectionHeader ||| 4.3 Discussions
bodyText ||| kWAS captures the similarity of square matrices of two
bodyText ||| matrices P and Q, yet the temporal order of pattern execu-
bodyText ||| tion is not revealed in the square matrices. As shown in [7],
bodyText ||| two matrices with the identical row vectors in different or-
bodyText ||| ders have identical eigenvectors and identical eigenvalues. If
bodyText ||| different temporal orders of pattern execution yield patterns
bodyText ||| with different semantic meanings, we need to further con-
bodyText ||| sider the temporal execution order, which is not reflected in
bodyText ||| the eigenvectors and eigenvalues and has not been consid-
bodyText ||| ered previously in [6,12,16].
bodyText ||| Since the first eigenvectors are close or parallel for similar
bodyText ||| patterns, we can project pattern A onto its first eigenvector
bodyText ||| ul by Aul. Then similar patterns would have similar projec-
bodyText ||| tions (called projection vectors hereafter), showing similar
bodyText ||| temporal execution orders while the projection variations
bodyText ||| for each pattern can be maximized. The pattern projection
bodyText ||| vectors can be compared by computing their dynamic time
bodyText ||| warping (DTW) distances, for DTW can align sequences
bodyText ||| of different lengths and can be solved easily by dynamic
bodyText ||| programming [1]. Incorporating temporal order information
bodyText ||| into the similarity measure can be done as for MAS in [7]
bodyText ||| if motion temporal execution orders cause motion pattern
bodyText ||| ambiguity to kWAS.
sectionHeader ||| 5. CONCLUSIONS
bodyText ||| This paper has proposed a similarity measure kWAS for
bodyText ||| motion stream segmentation and motion pattern recogni-
bodyText ||| tion. kWAS considers the first few k eigenvectors and com-
bodyText ||| putes their angular similarities/differences, and weighs con-
bodyText ||| tributions of different eigenvector pairs by their correspond-
figure ||| 100
figure ||| 99
figure ||| 98
figure ||| 97
figure ||| 96
figure ||| 95
figure ||| 94
figure ||| 93
figure ||| 92
figure ||| 91
figure ||| 90
figure ||| 1	2
figure ||| kWAS (k = 22)
figure ||| kWAS (k = 5)
figure ||| kWAS (k = 3)
figure ||| kWAS (k = 2)
figure ||| MAS
figure ||| EROS
figure ||| 99.5
figure ||| 98.5
figure ||| 97.5
figure ||| 96.5
figure ||| 95.5
figure ||| 100
figure ||| 99
figure ||| 98
figure ||| 97
figure ||| 96
figure ||| 95
figure ||| 123	4
figure ||| kWna (k = 541
figure ||| kWna (k = 51
figure ||| kWna (k = 41
figure ||| kWna (k = 31
figure ||| Mna
figure ||| EROa
figure ||| 20
figure ||| 18
figure ||| 16
figure ||| 14
figure ||| 12
figure ||| 10
figure ||| 8
figure ||| 6
figure ||| 4
figure ||| 2
figure ||| 0
figure ||| CyberGlove Streams	Motion Capture Streams
figure ||| MAS
figure ||| kWAS (k = 6)
figure ||| EROS
page ||| 93
bodyText ||| ing eigenvalues. Eigenvalues from two motion matrices are
bodyText ||| given equal importance to the weights. Experiments with
bodyText ||| CyberGlove hand gesture streams and captured human body
bodyText ||| motions such as Taiqi and dances show that kWAS can rec-
bodyText ||| ognize 100% most similar isolated patterns and can recog-
bodyText ||| nize 94% motion patterns in continuous motion streams.
sectionHeader ||| 6. REFERENCES
reference ||| [1] D. Berndt and J. Clifford. Using dynamic time
reference ||| warping to find patterns in time series. In AAAI-94
reference ||| Workshop on Knowledge Discovery in Databases,
reference ||| pages 229–248, 1994.
reference ||| [2] V. M. Dyaberi, H. Sundaram, J. James, and G. Qian.
reference ||| Phrase structure detection in dance. In Proceedings of
reference ||| the ACM Multimedia Conference 2004, pages 332–335,
reference ||| Oct. 2004.
reference ||| [3] G. H. Golub and C. F. V. Loan. Matrix Computations.
reference ||| The Johns Hopkins University Press,
reference ||| Baltimore,Maryland, 1996.
reference ||| [4] L. Ikemoto and D. A. Forsyth. Enriching a motion
reference ||| collection by transplanting limbs. In Proceedings of the
reference ||| 2004 ACM SIGGRAPH/Eurographics symposium on
reference ||| Computer animation, pages 99 – 108, 2004.
reference ||| [5] K. Kahol, P. Tripathi, S. Panchanathan, and
reference ||| T. Rikakis. Gesture segmentation in complex motion
reference ||| sequences. In Proceedings of IEEE International
reference ||| Conference on Image Processing, pages II – 105–108,
reference ||| Sept. 2003.
reference ||| [6] W. Krzanowski. Between-groups comparison of
reference ||| principal components. J. Amer. Stat. Assoc.,
reference ||| 74(367):703–707, 1979.
reference ||| [7] C. Li, B. Prabhakaran, and S. Zheng. Similarity
reference ||| measure for multi-attribute data. In Proceedings of the
reference ||| 2005 IEEE International Conference on Acoustics,
reference ||| Speach, and Signal Processing (ICASSP), Mar. 2005.
reference ||| [8] C. Li, P. Zhai, S.-Q. Zheng, and B. Prabhakaran.
reference ||| Segmentation and recognition of multi-attribute
reference ||| motion sequences. In Proceedings of the ACM
reference ||| Multimedia Conference 2004, pages 836–843, Oct.
reference ||| 2004.
reference ||| [9] R. H. Liang and M. Ouhyoung. A real-time continuous
reference ||| gesture recognition system for sign language. In
reference ||| Proceedings of the 3rd. International Conference on
reference ||| Face and Gesture Recognition, pages 558–565, 1998.
reference ||| [10] K. Pullen and C. Bregler. Motion capture assisted
reference ||| animation: texturing and synthesis. In SIGGRAPH,
reference ||| pages 501–508, 2002.
reference ||| [11] G. Qian, F. Guo, T. Ingalls, L. Olson, J. James, and
reference ||| T. Rikakis. A gesture-driven multimodal interactive
reference ||| dance system. In Proceedings of IEEE International
reference ||| Conference on Multimedia and Expo, June 2004.
reference ||| [12] C. Shahabi and D. Yan. Real-time pattern isolation
reference ||| and recognition over immersive sensor data streams.
reference ||| In Proceedings of the 9th International Conference on
reference ||| Multi-Media Modeling, pages 93–113, Jan 2003.
reference ||| [13] A. Singhal and D. E. Seborg. Clustering of
reference ||| multivariate time-series data. In Proceedings of the
reference ||| American Control Conference, pages 3931–3936, 2002.
reference ||| [14] T. Starner, J. Weaver, and A. Pentland. Real-time
reference ||| american sign language recognition using desk and
reference ||| wearable computer based video. IEEE Transactions
reference ||| on Pattern Analysis and Machine Intelligence,
reference ||| 20(12):1371–1375, 1998.
reference ||| [15] C. J. Taylor. Reconstruction of articulated objects
reference ||| from point correspondences in a single image.
reference ||| Computer Vision and Image Understanding,
reference ||| 80(3):349–363, 2000.
reference ||| [16] K. Yang and C. Shahabi. A PCA-based similarity
reference ||| measure for multivariate time series. In Proceedings of
reference ||| the Second ACM International Workshop on
reference ||| Multimedia Databases, pages 65–74, Nov. 2004.
page ||| 94
