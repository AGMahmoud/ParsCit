title ||| A New Approach to Intranet Search
title ||| Based on Information Extraction
author ||| Hang Li, Yunbo Cao
affiliation ||| Microsoft Research Asia
address ||| 5F Sigma Center
address ||| No.49 Zhichun Road,
address ||| Haidian, Beijing, China, 100080
email ||| {hangli, yucao}@microsoft.com
author ||| Jun Xu*
affiliation ||| College of Software
affiliation ||| Nankai University
address ||| No.94 Weijin Road,
address ||| Tianjin, China, 300071
email ||| nkxj@yahoo.com.cn
author ||| Yunhua Hu*
affiliation ||| Dept. of Computer Science
affiliation ||| Xi’an Jiaotong University
address ||| No 28, West Xianning Road,
address ||| Xi&apos;an, China, 710049
email ||| yunhuahu@mail.xjtu.edu.cn
author ||| Shenjie Li*
affiliation ||| Dept. of Computer Science
affiliation ||| Hong Kong University of Science and Technology
affiliation ||| Kowloon, Hong Kong, China
email ||| lisj@cs.ust.hk
sectionHeader ||| ABSTRACT
bodyText ||| This paper is concerned with ‘intranet search’. By intranet search, we
bodyText ||| mean searching for information on an intranet within an organization.
bodyText ||| We have found that search needs on an intranet can be categorized into
bodyText ||| types, through an analysis of survey results and an analysis of search
bodyText ||| log data. The types include searching for definitions, persons, experts,
bodyText ||| and homepages. Traditional information retrieval only focuses on
bodyText ||| search of relevant documents, but not on search of special types of
bodyText ||| information. We propose a new approach to intranet search in which
bodyText ||| we search for information in each of the special types, in addition to
bodyText ||| the traditional relevance search. Information extraction technologies
bodyText ||| can play key roles in such kind of ‘search by type’ approach, because
bodyText ||| we must first extract from the documents the necessary information in
bodyText ||| each type. We have developed an intranet search system called
bodyText ||| ‘Information Desk’. In the system, we try to address the most
bodyText ||| important types of search first - finding term definitions, homepages of
bodyText ||| groups or topics, employees’ personal information and experts on
bodyText ||| topics. For each type of search, we use information extraction
bodyText ||| technologies to extract, fuse, and summarize information in advance.
bodyText ||| The system is in operation on the intranet of Microsoft and receives
bodyText ||| accesses from about 500 employees per month. Feedbacks from users
bodyText ||| and system logs show that users consider the approach useful and the
bodyText ||| system can really help people to find information. This paper describes
bodyText ||| the architecture, features, component technologies, and evaluation
bodyText ||| results of the system.
sectionHeader ||| Categories and Subject Descriptors
category ||| H.3.3 [Information Storage and Retrieval]: Information Search
category ||| and Retrieval – search process; I.7.m [Document and Text
category ||| Processing]: Miscellaneous
copyright ||| Permission to make digital or hard copies of all or part of this work for
copyright ||| personal or classroom use is granted without fee provided that copies are
copyright ||| not made or distributed for profit or commercial advantage and that
copyright ||| copies bear this notice and the full citation on the first page. To copy
copyright ||| otherwise, or republish, to post on servers or to redistribute to lists,
copyright ||| requires prior specific permission and/or a fee.
note ||| CIKM’05, October 31-November 5, 2005, Bremen, Germany.
note ||| Copyright 2005 ACM 1-59593-140-6/05/0010...$5.00.
author ||| Dmitriy Meyerzon
affiliation ||| Microsoft Corporation
affiliation ||| One Microsoft Way,
address ||| Redmond, WA, USA, 98052
email ||| dmitriym@microsoft.com
sectionHeader ||| General Terms: Algorithms, Experimentation, Human
sectionHeader ||| Factors
sectionHeader ||| Keywords: Intranet search, information extraction, metadata
sectionHeader ||| extraction, expert finding, definition search
sectionHeader ||| 1. INTRODUCTION
bodyText ||| Internet search has made significant progress in recent years. In
bodyText ||| contrast, intranet search does not seem to be so successful. The
bodyText ||| IDC white paper entitled “The high cost of not finding
bodyText ||| information” [13] reports that information workers spend from
bodyText ||| 15% to 35% of their work time on searching for information and
bodyText ||| 40% of information workers complain that they cannot find the
bodyText ||| information they need to do their jobs on their company intranets.
bodyText ||| Many commercial systems [35, 36, 37, 38, 39] have been
bodyText ||| developed for intranet search. However, most of them view
bodyText ||| intranet search as a problem of conventional relevance search. In
bodyText ||| relevance search, when a user types a query, the system returns a
bodyText ||| list of ranked documents with the most relevant documents on the
bodyText ||| top.
bodyText ||| Relevance search can only serve average needs well. It cannot,
bodyText ||| however, help users to find information in a specific type, e.g.,
bodyText ||| definitions of a term and experts on a topic. The characteristic of
bodyText ||| intranet search does not seem to be sufficiently leveraged in the
bodyText ||| commercial systems.
bodyText ||| In this paper, we try to address intranet search in a novel approach.
bodyText ||| We assume that the needs of information access on intranets can
bodyText ||| be categorized into searches for information in different types. An
bodyText ||| analysis on search log data on the intranet of Microsoft and an
bodyText ||| analysis on the results of a survey conducted at Microsoft have
bodyText ||| verified the correctness of the assumption.
bodyText ||| Our proposal then is to take a strategy of ‘divide-and-conquer’.
bodyText ||| We first figure out the most important types of search, e.g.,
bodyText ||| definition search, expert search. For each type, we employ
bodyText ||| information extraction technologies to extract, fuse, and
bodyText ||| summarize search results in advance. Finally, we combine all the
bodyText ||| types of searches together, including the traditional relevance
page ||| 460
bodyText ||| search, in a unified system. In this paper, we refer to the approach
bodyText ||| as ‘search by type’. Search by type can also be viewed as a
bodyText ||| simplified version of Question Answering, adapted to intranet.
bodyText ||| The advantage of the new search approach lies in that it can help
bodyText ||| people find the types of information which relevance search
bodyText ||| cannot easily find. The approach is particularly reasonable on
bodyText ||| intranets, because in such space users are information workers and
bodyText ||| search needs are business oriented.
bodyText ||| We have developed a system based on the approach, which is
bodyText ||| called ‘Information Desk’. Information Desk can help users to
bodyText ||| find term definitions, homepages of groups or topics, employees’
bodyText ||| personal information and experts on topics, on their company
bodyText ||| intranets.
bodyText ||| The system has been put into practical use since November 24th,
bodyText ||| 2004. Each month, about 500 Microsoft employees make access
bodyText ||| to the system. Both the results of an analysis on a survey and the
bodyText ||| results of an analysis on system log show that the features of
bodyText ||| definition search and homepage search are really helpful. The
bodyText ||| results also show that search by type is necessary at enterprise.
sectionHeader ||| 2. RELATED WORK
sectionHeader ||| 2.1 Intranet Search
bodyText ||| The needs on search on intranets are huge. It is estimated that
bodyText ||| intranets at enterprises have tens or even hundreds of times larger
bodyText ||| data collections (both structured and unstructured) than internet.
bodyText ||| As explained above, however, many users are not satisfied with
bodyText ||| the current intranet search systems. How to help people access
bodyText ||| information on intranet is a big challenge in information retrieval.
bodyText ||| Much effort has been made recently on solutions both in industry
bodyText ||| and in academia.
bodyText ||| Many commercial systems [35, 36, 37, 38, 39] dedicated to
bodyText ||| intranet search have been developed. Most of the systems view
bodyText ||| intranet search as a problem of conventional relevance search.
bodyText ||| In the research community, ground designs, fundamental
bodyText ||| approaches, and evaluation methodologies on intranet search have
bodyText ||| been proposed.
bodyText ||| Hawking et al [17] made ten suggestions on how to conduct high
bodyText ||| quality intranet search. Fagin et al [12] made a comparison
bodyText ||| between internet search and intranet search. Recently, Hawking
bodyText ||| [16] conducted a survey on previous work and made an analysis
bodyText ||| on the intranet search problem. Seven open problems on intranet
bodyText ||| search were raised in their paper.
bodyText ||| Chen et al [3] developed a system named ‘Cha-Cha’, which can
bodyText ||| organize intranet search results in a novel way such that the
bodyText ||| underlying structure of the intranet is reflected. Fagin et al [12]
bodyText ||| proposed a new ranking method for intranet search, which
bodyText ||| combine various ranking heuristics. Mattox et al [25] and
bodyText ||| Craswell et al [7] addressed the issue of expert finding on a
bodyText ||| company intranet. They developed methods that can automatically
bodyText ||| identify experts in an area using documents on the intranet.
bodyText ||| Stenmark [30] proposed a method for analyzing and evaluating
bodyText ||| intranet search tools.
subsectionHeader ||| 2.2 Question Answering
bodyText ||| Question Answering (QA) particularly that in TREC
bodyText ||| (http://trec.nist.gov/) is an application in which users type
bodyText ||| questions in natural language and the system returns short and
bodyText ||| usually single answers to the questions.
bodyText ||| When the answer is a personal name, a time expression, or a place
bodyText ||| name, the QA task is called ‘Factoid QA’. Many QA systems have
bodyText ||| been developed, [2, 4, 18, 20, 22, 27]. Factoid QA usually
bodyText ||| consists of the following steps: question type identification,
bodyText ||| question expansion, passage retrieval, answer ranking, and answer
bodyText ||| creation.
bodyText ||| TREC also has a task of ‘Definitional QA’. In the task, “what is
bodyText ||| &lt;term&gt;” and “who is &lt;person&gt;” questions are answered in a
bodyText ||| single combined text [1, 11, 15, 33, 34]. A typical system consists
bodyText ||| of question type identification, document retrieval, key sentence
bodyText ||| matching, kernel fact finding, kernel fact ranking, and answer
bodyText ||| generation.
sectionHeader ||| 3. OUR APPROACH TO INTRANET
sectionHeader ||| SEARCH
bodyText ||| Search is nothing but collecting information based on users’
bodyText ||| information access requests. If we can correctly gather
bodyText ||| information on the basis of users’ requests, then the problem is
bodyText ||| solved. Current intranet search is not designed along this
bodyText ||| direction. Relevance search can help create a list of ranked
bodyText ||| documents that serve only average needs well. The limitation of
bodyText ||| this approach is clear. That is, it cannot help users to find
bodyText ||| information of a specific type, e.g., definitions of a term. On the
bodyText ||| other hand, Question Answering (QA) is an ideal form for
bodyText ||| information access. When a user inputs a natural language
bodyText ||| question or a query (a combination of keywords) as a description
bodyText ||| of his search need, it is ideal to have the machine ‘understand’ the
bodyText ||| input and return only the necessary information based on the
bodyText ||| request. However, there are still lots of research work to do before
bodyText ||| putting QA into practical uses. In short term, we need consider
bodyText ||| adopting a different approach.
bodyText ||| One question arises here: can we take a hybrid approach?
bodyText ||| Specifically, on one hand, we adopt the traditional approach for
bodyText ||| search, and on the other hand, we realize some of the most
bodyText ||| frequently asked types of search with QA. Finally, we integrate
bodyText ||| them in a single system. For the QA part, we can employ
bodyText ||| information extraction technologies to extract, fuse, and
bodyText ||| summarize the results in advance. This is exactly the proposal we
bodyText ||| make to intranet search.
bodyText ||| Can we categorize users’ search needs easily? We have found that
bodyText ||| we can create a hierarchy of search needs for intranet search, as
bodyText ||| will be explained in section 4.
bodyText ||| On intranets, users are information workers and their motivations
bodyText ||| for conducting search are business oriented. We think, therefore,
bodyText ||| that our approach may be relatively easily realized on intranets
bodyText ||| first. (There is no reason why we cannot apply the same approach
bodyText ||| to the internet, however.)
bodyText ||| To verify the correctness of the proposal, we have developed a
bodyText ||| system and made it available internally at Microsoft. The system
bodyText ||| called Information Desk is in operation on the intranet of
bodyText ||| Microsoft and receives accesses from about 500 employees per
bodyText ||| month.
bodyText ||| At Information Desk, we try to solve the most important types of
bodyText ||| search first - find term definitions, homepages of groups or topics,
bodyText ||| experts on topics, and employees’ personal information. We are
page ||| 461
bodyText ||| also trying to increase the number of search types, and integrate
bodyText ||| them with the conventional relevance search. We will explain the
bodyText ||| working of Information Desk in section 5.
sectionHeader ||| 4. ANALYSIS OF SEARCH NEEDS
bodyText ||| In this section, we describe our analyses on intranet search needs
bodyText ||| using search query logs and survey results.
subsectionHeader ||| 4.1 Categorization of Search Needs
bodyText ||| In order to understand the underlying needs of search queries, we
bodyText ||| would need to ask the users about their search intentions.
bodyText ||| Obviously, this is not feasible. We conducted an analysis by using
bodyText ||| query log data. Here query log data means the records on queries
bodyText ||| typed by users, and documents clicked by the users after sending
bodyText ||| the queries.
bodyText ||| Our work was inspirited by those of Rose and Levinson [28]. In
bodyText ||| their work, they categorized the search needs of users on internet
bodyText ||| by analyzing search query logs.
bodyText ||| We tried to understand users’ search needs on intranet by
bodyText ||| identifying and organizing a manageable number of categories of
bodyText ||| the needs. The categories encompass the majority of actual
bodyText ||| requests users may have when conducting search on an intranet.
bodyText ||| We used a sample of queries from the search engine of the
bodyText ||| intranet of Microsoft. First, we brainstormed a number of
bodyText ||| categories, based on our own experiences and previous work.
bodyText ||| Then, we modified the categories, including adding, deleting, and
bodyText ||| merging categories, by assigning queries to the categories.
bodyText ||| Given a query, we used the following information to deduce the
bodyText ||| underlying search need:
listItem ||| •	the query itself
listItem ||| •	the documents returned by the search engine
listItem ||| •	the documents clicked on by the user
bodyText ||| For example, if a user typed a keyword of ‘.net’ and clicked a
bodyText ||| homepage of .net, then we judged that the user was looking for a
bodyText ||| homepage of .net.
bodyText ||| As we repeated the process, we gradually reached the conclusion
bodyText ||| that search needs on intranet can be categorized as a hierarchical
bodyText ||| structure shown in Figure 1. In fact, the top level of the hierarchy
bodyText ||| resembles that in the taxonomy proposed by Rose and Levinson
bodyText ||| for internet [28]. However, the second level differs. On intranet,
bodyText ||| users’ search needs are less diverse than those on internet, because
bodyText ||| the users are information workers and their motivations of
bodyText ||| conducting search are business oriented.
bodyText ||| There is a special need called ‘tell me about’ here. It is similar to
bodyText ||| the traditional relevance search. Many search needs are by nature
bodyText ||| difficult to be categorized, for example, “I want to find documents
bodyText ||| related to both .net and SQL Server”. We can put them into the
bodyText ||| category.
bodyText ||| We think that the search needs are not Microsoft specific; one can
bodyText ||| image that similar needs exist in other companies as well.
figure ||| 	When (time)
figure ||| 	Where
figure ||| 	(place)
figure ||| 	Why (reason)
figure ||| Informational	What is
figure ||| 	(definition) knows
figure ||| 	Who	about (expert)
figure ||| 	Who is
figure ||| 	(person)
figure ||| 	How
figure ||| 	to (manual)
figure ||| 	Tell
figure ||| 	me about (relevance)
figure ||| 	Group
figure ||| 	Person
figure ||| 	Product
figure ||| Navigational	
figure ||| 	Technology
figure ||| 	Services
figure ||| Transactional	
figureCaption ||| Figure 1. Categories of search needs
subsectionHeader ||| 4.2 Analysis on Search Needs – by Query Log
bodyText ||| We have randomly selected 200 unique queries and tried to assign
bodyText ||| the queries to the categories of search needs described above.
bodyText ||| Table 1 shows the distribution. We have also picked up the top
bodyText ||| 350 frequently submitted queries and assigned them to the
bodyText ||| categories. Table 2 shows the distribution. (There is no result for
bodyText ||| ‘why’, ‘what is’, and ‘who knows about’, because it is nearly
bodyText ||| impossible to guess users’ search intensions by only looking at
bodyText ||| query logs.)
bodyText ||| For random queries, informational needs are dominating. For high
bodyText ||| frequency queries, navigational needs are dominating. The most
bodyText ||| important types for random queries are relevance search, personal
bodyText ||| information search, and manual search. The most important types
bodyText ||| for high frequency queries are home page search and relevance
bodyText ||| search.
subsectionHeader ||| 4.3 Analysis on Search Needs – by Survey
bodyText ||| We can use query log data to analyze users’ search needs, as
bodyText ||| described above. However, there are two shortcomings in the
bodyText ||| approach. First, sometimes it is difficult to guess the search
bodyText ||| intensions of users by only looking at query logs. This is
bodyText ||| especially true for the categories of ‘why’ and ‘what’. Usually it is
bodyText ||| hard to distinguish them from ‘relevance search’. Second, query
bodyText ||| log data cannot reveal users’ potential search needs. For example,
bodyText ||| many employees report that they have needs of searching for
bodyText ||| experts on specific topics. However, it is difficult to find expert
bodyText ||| searches from query log at a conventional search engine, because
bodyText ||| users understand that such search is not supported and they do not
bodyText ||| conduct the search.
bodyText ||| To alleviate the negative effect, we have conducted another
bodyText ||| analysis through a survey. Although a survey also has limitation
bodyText ||| (i.e., it only asks people to answer pre-defined questions and thus
bodyText ||| can be biased), it can help to understand the problem from a
bodyText ||| different perspective.
page ||| 462
tableCaption ||| Table 1. Distribution of search needs for random queries
table ||| Category of Search Needs	Percentage
table ||| When	0.02
table ||| Where	0.02
table ||| Why	NA
table ||| What is	NA
table ||| Who knows about	NA
table ||| Who is	0.23
table ||| How to	0.105
table ||| Tell me about	0.46
table ||| Informational total	0.835
table ||| Groups	0.03
table ||| Persons	0.005
table ||| Products	0.02
table ||| Technologies	0.02
table ||| Services	0.06
table ||| Navigational total	0.135
table ||| Transactional	0.025
table ||| Other	0.005
tableCaption ||| Table 2. Distribution of search needs for high frequency queries
table ||| Category of Search Needs	Relative Prevalence
table ||| When	0.0057
table ||| Where	0.0143
table ||| Why	NA
table ||| What is	NA
table ||| Who knows about	NA
table ||| Who is	0.0314
table ||| How to	0.0429
table ||| Tell me about	0.2143
table ||| Informational total	0.3086
table ||| Groups	0.0571
table ||| Persons	0.0057
table ||| Products	0.26
table ||| Technologies	0.0829
table ||| Services	0.2371
table ||| Navigational total	0.6428
table ||| Transactional	0.0086
table ||| Other	0.04
figureCaption ||| Figure 2. Survey results on search needs
bodyText ||| In the survey, we have asked questions regarding to search needs
bodyText ||| at enterprise. 35 Microsoft employees have taken part in the
bodyText ||| survey. Figure 2 shows the questions and the corresponding
bodyText ||| results.
bodyText ||| We see from the answers that definition search, manual search,
bodyText ||| expert finding, personal information search, and time schedule
bodyText ||| search are requested by the users. Homepage finding on
bodyText ||| technologies and products are important as well. Search for a
bodyText ||| download site is also a common request.
bodyText ||| I have experiences of conducting search at Microsoft intranet in
bodyText ||| order to (multiple choice)
figure ||| •	download a software, a document, or a picture. E.g., &quot;getting
figure ||| MSN logo&quot;
figure ||| 71 %
figure ||| •	make use of a service. E.g., &quot;getting a serial number of
figure ||| Windows&quot;
figure ||| 	53 %
figure ||| •	none of the above
figure ||| 18 %
figure ||| I have experiences of conducting search at Microsoft intranet to
figure ||| look for the web sites (or homepages) of (multiple choice)
figure ||| •	technologies
figure ||| 74 %
figure ||| •	products
figure ||| 	74 %
figure ||| •	services
figure ||| 	68 %
figure ||| •	projects
figure ||| 	68 %
figure ||| •	groups
figure ||| 	60 %
figure ||| •	persons
figure ||| 42 %
figure ||| •	none of the above
figure ||| 11 %
figure ||| I have experiences of conducting search at Microsoft intranet in
figure ||| which the needs can be translated into questions like? (multiple
figure ||| choice)
figure ||| •	‘what is’ - e.g., &quot;what is blaster&quot;
figure ||| 77 %
figure ||| •	‘how to’ - &quot;how to submit expense report&quot;
figure ||| 54 %
figure ||| •	‘where’ - e.g., &quot;where is the company store&quot;
figure ||| 51 %
figure ||| •	‘who knows about’ - e.g., &quot;who knows about data mining&quot;
figure ||| 51 %
figure ||| •	‘who is’ - e.g., &quot;who is Rick Rashid&quot;
figure ||| 45 %
figure ||| •	‘when’ - e.g., &quot;when is TechFest&apos;05 &quot;
figure ||| 42 %
figure ||| •	‘why’ - e.g., &quot;why do Windows NT device drivers contain
figure ||| trusted code&quot;
figure ||| 28 %
figure ||| •	none of the above
figure ||| 14 %
page ||| 463
figure ||| Longhorn	Go			
figure ||| What is	Who is	Where is homepage of	Who knows about			
figure ||| 	What is	Who isWhere is homepage of	Who knows about		
figure ||| 	Definition of Longhorn	
figure ||| 	Longhorn is the codename for the next release of the Windows operating system, planned for release in FY 2005. Longhorn will further Microsoft’s long term vision for ...	
figure ||| 	http://url1	
figure ||| 	Longhorn is a platform that enables incredible user experiences that are unlike anything possible with OS releases to date. This session describes our approach and philosophy that...	
figure ||| 	http://url2	
figure ||| 	Longhorn is the platform in which significant improvements in the overall manageability of the system by providing the necessary infrastructure to enable standardized configuration/change management, structured eventing and monitoring, and a unified software distribution mechanism will be made. In order to achieve this management with each Longhorn...	
figure ||| 	http://url3	
figure ||| 	Longhorn is the evolution of the .NET Framework on the client and the biggest investment that Microsoft has made in the Windows client development platform in years. Longhorn is the platform for smart , connected...	
figure ||| 	http://url4	
figure ||| 	Longhorn is the platform for smart, connected applications, combining the best features of the Web, such as ease of deployment and rich content with the power of the Win32 development platform, enabling developers to build a new breed of applications that take real advantage of the connectivity, storage, and graphical capabilities of the modern personal	
figure ||| 	computer .	
figure ||| 	http//url5	
figure ||| 	Office	Go		
figure ||| 	What is	Who is	Where is homepage of	Who knows about		
figure ||| 	What is	Who is	Where is homepage of	Who knows about		
figure ||| 	Homepages of Office	
figure ||| 	Office Portal Site	
figure ||| 	This is the internal site for Office	
figure ||| 	http://url1	
figure ||| 	Office Site (external)	
figure ||| 	Microsoft.com site offering information on the various Office products. Links include FAQs, downloads, support, and more.	
figure ||| 	http:/url2	
figure ||| 	Office	
figure ||| 	New Office Site	
figure ||| 	http://url3	
figure ||| 	Office Office	
figure ||| 	http://url4	
figure ||| 	Data Mining	Go		
figure ||| 	What is	Who is	Where is homepage of	Who knows about		
figure ||| 	What is	Who is	Where is homepage of	Who knows about		
figure ||| 	People Associated with Data mining	
figure ||| 	Jamie MacLennan	DEVELOPMENT LEAD	
figure ||| 	US-SQL Data Warehouse +1 (425) XXXXXXX XXXXXX Associated documents(4):		
figure ||| 	•	is author of document entitled Data Mining Tutorial		
figure ||| 	http://url1		
figure ||| 	•	is author of document entitled Solving Business Problems Using Data Mining		
figure ||| 	http://url2		
figure ||| 	Jim Gray	DISTINGUISHED ENGINEER		
figure ||| 	US-WAT MSR San Francisco +XXXXXXXXXXX	
figure ||| 	Associated documents(2):	
figure ||| 	•	is author of document entitled Mainlining Data Mining	
figure ||| 	http://url3	
figure ||| 	•	is author of document entitled Data Mining the SDSS SkyServer Database	
figure ||| 	http://url4	
figure ||| Bill Gates	Go			
figure ||| What is	Who is	Where is homepage of	Who knows about			
figure ||| What is	Who is	Where is homepage of	Who knows about			
figure ||| 	Bill Gates	CHRMN &amp; CHIEF SFTWR ARCHITECT		
figure ||| 	US-Executive-Chairman	
figure ||| 	+1 (425) XXXXXXX XXXXXX	
figure ||| 	Documents of Bill Gates(118)	
figure ||| 	•	My advice to students: Education counts	
figure ||| 	http://url1	
figure ||| 	•	Evento NET Reviewers – Seattle –7/8 Novembro	
figure ||| 	http://url2	
figure ||| 	•	A Vision for Life Long Learning – Year 2020	
figure ||| 	http://url3	
figure ||| 	•	Bill Gates answers most frequently asked questions.	
figure ||| 	http://url4	
figure ||| 	&gt;&gt;more 	
figure ||| 	Top 10 terms appearing in documents of Bill Gates	
figure ||| 	Term 1 (984.4443) Term 2 (816.4247) Term 3 (595.0771) Term 4 (578.5604) Term 5 (565.7299) Term 6 (435.5366) Term 7 (412.4467) Term 8 (385.446) Term 9 (346.5993) Term 10 (345.3285)	
figureCaption ||| Figure 3: Information Desk system
sectionHeader ||| 5. INFORMATION DESK
sectionHeader ||| 5.1 Features
bodyText ||| Currently Information Desk provides four types of search. The
bodyText ||| four types are:
listItem ||| 1. ‘what is’ – search of definitions and acronyms. Given a term,
listItem ||| it returns a list of definitions of the term. Given an acronym, it
listItem ||| returns a list of possible expansions of the acronym.
listItem ||| 2. ‘who is’ – search of employees’ personal information. Given
listItem ||| the name of a person, it returns his/her profile information,
listItem ||| authored documents and associated key terms.
listItem ||| 3. ‘where is homepage of’ – search of homepages. Given the
listItem ||| name of a group, a product, or a technology, it returns a list of
listItem ||| its related home pages.
listItem ||| 4. ‘who knows about’ – search of experts. Given a term on a
listItem ||| technology or a product, it returns a list of persons who might
listItem ||| be experts on the technology or the product.
figureCaption ||| Figure 4. Workflow of Information Desk
bodyText ||| There are check boxes on the UI, and each represents one search
bodyText ||| type. In search, users can designate search types by checking the
bodyText ||| corresponding boxes and then submit queries. By default, all the
bodyText ||| boxes are checked.
bodyText ||| For example, when users type ‘longhorn’ with the ‘what is’ box
bodyText ||| checked, they get a list of definitions of ‘Longhorn’ (the first
bodyText ||| snapshot in figure 3). Users can also search for homepages (team
bodyText ||| web sites) related to ‘Office’, using the ‘where is homepage’
bodyText ||| feature (the second snapshot in figure 3). Users can search for
bodyText ||| experts on, for example, ‘data mining’ by asking ‘who knows
bodyText ||| about data mining’ (the third snapshot in figure 3). Users can also
bodyText ||| get a list of documents that are automatically identified as being
bodyText ||| authored by ‘Bill Gates’, for example, with the ‘who is’ feature
bodyText ||| (the last snapshot in figure 3). The top ten key terms found in his
bodyText ||| documents are also given.
bodyText ||| Links to the original documents, from which the information has
bodyText ||| been extracted, are also available on the search result UIs.
subsectionHeader ||| 5.2 Technologies
subsectionHeader ||| 5.2.1 Architecture
bodyText ||| Information Desk makes use of information extraction
bodyText ||| technologies to support the search by type feaatures. The
bodyText ||| technologies include automatically extracting document metadata
bodyText ||| and domain specific knowledge from a web site using information
bodyText ||| extraction technologies. The domain specific knowledge includes
bodyText ||| definition, acronym, and expert. The document metadata includes
bodyText ||| title, author, key term, homepage. Documents are in the form of
bodyText ||| Word, PowerPoint, or HTML. Information Desk stores all the
bodyText ||| data in Microsoft SQL Server and provides search using web
figure ||| homepage
figure ||| term
figure ||| Where is homepage of
figure ||| Crawler &amp;
figure ||| Extractor
figure ||| definition
figure ||| acronym
figure ||| document
figure ||| key term
figure ||| person
figure ||| document
figure ||| what is
figure ||| who is
figure ||| who knows about
figure ||| MS Web
figure ||| Information Desk
figure ||| Web Server
figure ||| term
figure ||| person
figure ||| term
page ||| 464
bodyText ||| services. Figure 4 shows the workflow of Information Desk.
bodyText ||| Currently, there are 4 million documents crawled from the
bodyText ||| Microsoft intranet.
bodyText ||| Below we explain each feature in details. Table 3 shows which
bodyText ||| feature employs what kind of mining technology.
tableCaption ||| Table 3. Information extraction technologies employed
table ||| 	`What is&apos;	`Who is&apos;	`Who knows	`Where is homepage&apos;
table ||| 			about&apos;	
table ||| Definition extraction	Yes			
table ||| Acronym extraction	Yes			
table ||| Homepage				Yes
table ||| finding				
table ||| Title		Yes	Yes	
table ||| extraction				
table ||| Author extraction		Yes	Yes	
table ||| Key term extraction		Yes	Yes	
table ||| Expert mining			Yes	
subsubsectionHeader ||| 5.2.2 `What is&apos;
bodyText ||| There are two parts in the feature: definition finding and acronym
bodyText ||| recognition.
bodyText ||| In definition finding, we extract from the entire collection of
bodyText ||| documents &lt;term, definition, score&gt; triples. They are respectively
bodyText ||| a term, a definitional excerpt of the term, and its score
bodyText ||| representing its likelihood of being a good definition. We assign
bodyText ||| the scores using a statistical model. Both paragraphs and
bodyText ||| sentences can be considered as definition excerpts in our approach.
bodyText ||| Currently, we only consider the use of paragraphs.
bodyText ||| As model, we employ SVM (Support Vector Machines) [31],
bodyText ||| which identifies whether a given paragraph is a definition of the
bodyText ||| first noun phrase (term) in the paragraph. There are positive
bodyText ||| features in the SVM model. For example, if the term appears at
bodyText ||| the beginning of the paragraph or repeatedly occurs in the
bodyText ||| paragraph, then it is likely the paragraph is a (good) definition on
bodyText ||| the term. There are also negative features. If words like `she&apos;, `he&apos;,
bodyText ||| or `said&apos; occurs in the paragraph, or many adjectives occur in the
bodyText ||| paragraph, then it is likely the paragraph is not a (good) definition.
bodyText ||| In search, given a query term, we retrieve all the triples matched
bodyText ||| against the query term and present the corresponding definitions
bodyText ||| in descending order of the scores.
bodyText ||| The top 1 and top 3 precision of our approach in definition
bodyText ||| ranking are 0.550 and 0.887 respectively. They are much better
bodyText ||| than the baseline method of employing relevance search.
bodyText ||| Methods for extracting definitions from documents have been
bodyText ||| proposed [1, 10, 11, 15, 21, 24, 33]. All of the methods resorted to
bodyText ||| human-defined rules for the extraction and did not consider
bodyText ||| ranking of definitions. In Information Desk, we rank definitions
bodyText ||| according to their likelihoods of being good definitions,
bodyText ||| represented by SVM scores. See [32] for details.
bodyText ||| In acronym recognition, we find candidate acronym and candidate
bodyText ||| expansion pairs from text using pattern matching. There are ten
bodyText ||| types of patterns. For example, one of them is `&lt;expansion&gt;
bodyText ||| (&lt;acronym&gt;)&apos; in which &lt;expansion&gt; denotes a phrase with the
bodyText ||| first letters in the words capitalized and &lt;acronym&gt; denotes a
bodyText ||| sequence of the capitalized letters in the same order. The pattern
bodyText ||| matches sentences such as &quot;Active Directory is implemented
bodyText ||| using the Lightweight Directory Access Protocol (LDAP)&quot;. We
bodyText ||| then store all the acronyms, their expansions, and the numbers of
bodyText ||| occurrences of the expansions.
bodyText ||| In search, given an acronym, we retrieve all the expansions
bodyText ||| against the acronym and present the corresponding expansions in
bodyText ||| descending order of their numbers of occurrences.
subsubsectionHeader ||| 5.2.3 `Who is&apos;
bodyText ||| We first harvest all the employees&apos; personal information from a
bodyText ||| database. It includes name, alias, title, and contact information.
bodyText ||| We next automatically extract titles and authors from all the Word
bodyText ||| and PowerPoint documents on the intranet. With the extracted
bodyText ||| titles and authors we bring together all the documents to each
bodyText ||| person, which are thought authored by him/her. Finally, we
bodyText ||| extract key terms from the documents for each person and pick up
bodyText ||| the top ten key terms in terms of TF-IDF. This feature lies mainly
bodyText ||| on document metadata extraction.
bodyText ||| Metadata of documents such as title and author is useful for
bodyText ||| document processing. However, people seldom define document
bodyText ||| metadata by themselves. We collected 6,000 Word and 6,000
bodyText ||| PowerPoint documents and examined how many titles and authors
bodyText ||| in the file properties are correct. We found that the accuracies
bodyText ||| were only 0.265 and 0.126 respectively.
bodyText ||| We take a machine learning approach to automatically extract
bodyText ||| titles and authors from the bodies of Office documents, as shown
bodyText ||| in Figure 5. We annotate titles in sample documents (for Word
bodyText ||| and PowerPoint respectively) and take them as training data, train
bodyText ||| statistical models, and perform title extraction using the trained
bodyText ||| models. In the models, we mainly utilize format information such
bodyText ||| as font size as features. As models, we employ Perceptron with
bodyText ||| Uneven Margins [23].
bodyText ||| Experimental results indicate that our approach works well for
bodyText ||| title extraction from general documents. Our method can
bodyText ||| significantly outperform the baselines: one that always uses the
bodyText ||| first lines as titles and the other that always uses the lines in the
bodyText ||| largest font sizes as titles. Precision and recall for title extraction
bodyText ||| from Word are 0.875 and 0.899 respectively, and precision and
figureCaption ||| Figure 5. Title and author extraction from four example
figure ||| PowerPoint documents
figure ||| Microsoft Project 2002 Project Guide
figure ||| Architecture and Extensibi(ity
figure ||| White Paper
figure ||| DRAFT
page ||| 465
bodyText ||| recall for title extraction from PowerPoint are 0.907 and 0.951
bodyText ||| respectively.
bodyText ||| Metadata extraction has been intensively studied. For instance,
bodyText ||| Han et al [14] proposed a method for metadata extraction from
bodyText ||| research papers. They considered the problem as that of
bodyText ||| classification based on SVM. They mainly used linguistic
bodyText ||| information as features. To the best of our knowledge, no
bodyText ||| previous work has been done on metadata extraction from general
bodyText ||| documents. We report our title extraction work in details in [19].
bodyText ||| The feature of ‘who is’ can help find documents authored by a
bodyText ||| person, but existing in different team web sites. Information
bodyText ||| extraction (specifically metadata extraction) makes the aggregation
bodyText ||| of information possible.
subsubsectionHeader ||| 5.2.4 ‘Who knows about’
bodyText ||| The basic idea for the feature is that if a person has authored many
bodyText ||| documents on an issue (term), then it is very likely that he/she is an
bodyText ||| expert on the issue, or if the person’s name co-occurs in many times
bodyText ||| with the issue, then it is likely that he/she is an expert on the issue.
bodyText ||| As described above, we can extract titles, authors, and key terms
bodyText ||| from all the documents. In this way, we know how many times each
bodyText ||| person is associated with each topic in the extracted titles and in the
bodyText ||| extracted key terms. We also go through all the documents and see
bodyText ||| how many times each person’s name co-occurs with each topic in
bodyText ||| text segments within a pre-determined window size.
bodyText ||| In search, we use the three types of information: topic in title, topic
bodyText ||| in key term, and topic in text segment to rank persons, five persons
bodyText ||| for each type. We rank persons with a heuristic method and return
bodyText ||| the list of ranked persons. A person who has several documents with
bodyText ||| titles containing the topic will be ranked higher than a person whose
bodyText ||| name co-occurs with the topic in many documents.
bodyText ||| It appears that the results of the feature largely depend on the size of
bodyText ||| document collection we crawl. Users’ feedbacks on the results show
bodyText ||| that sometimes the results are very accurate, however, sometimes
bodyText ||| they are not (due to the lack of information).
bodyText ||| Craswell et al. developed a system called ‘P@NOPTIC’, which can
bodyText ||| automatically find experts using documents on an intranet [7]. The
bodyText ||| system took documents as plain texts and did not utilize metadata of
bodyText ||| documents as we do at Information Desk.
subsubsectionHeader ||| 5.2.5 ‘Where is homepage of’
bodyText ||| We identify homepages (team web sites) using several rules. Most of
bodyText ||| the homepages at the intranet of Microsoft are created by
bodyText ||| SharePoint, a product of Microsoft. From SharePoint, we can obtain
bodyText ||| a property of each page called ‘ContentClass’. It tells exactly
bodyText ||| whether a web page corresponds to a homepage or a team site. So
bodyText ||| we know it is a homepage (obviously, this does not apply in
bodyText ||| general). Next we use several patterns to pull out titles from the
bodyText ||| homepages. The precision of home page identification is nearly
bodyText ||| 100%.
bodyText ||| In search, we rank the discovered home pages related to a query
bodyText ||| term using the URL lengths of the home pages. A home page with a
bodyText ||| shorter URL will be ranked higher.
bodyText ||| TREC has a task called ‘home/named page finding’ [8, 9], which is
bodyText ||| to find home pages talking about a topic. Many methods have been
bodyText ||| developed for pursuing the task [5, 6, 26, 29]. Since we can identify
bodyText ||| homepages by using special properties on our domain, we do not
bodyText ||| consider employing a similar method.
sectionHeader ||| 6. EVALUATION
bodyText ||| Usually it is hard to conduct evaluation on a practical system. We
bodyText ||| evaluated the usefulness of Information Desk by conducting a
bodyText ||| survey and by recording system logs.
bodyText ||| We have found from analysis results that the ‘what is’ and ‘where is
bodyText ||| homepage of’ features are very useful. The ‘who is’ feature works
bodyText ||| well, but the ‘who knows about’ feature still needs improvements.
subsectionHeader ||| 6.1 Survey Result Analysis
bodyText ||| The survey described in section 4.3 also includes feedbacks on
bodyText ||| Information Desk.
bodyText ||| Figure 6 shows a question on the usefulness of the features and a
bodyText ||| summary on the answers. We see that the features ‘where is
bodyText ||| homepage of’ and ‘what is’ are regarded useful by the responders in
bodyText ||| the survey.
bodyText ||| Figure 7 shows a question on new features and a summary on the
bodyText ||| answers. We see that the users want to use the features of ‘how to’,
bodyText ||| ‘when’, ‘where’ and ‘why’ in the future. This also justifies the
bodyText ||| correctness of our claim on intranet search made in section 4.
bodyText ||| Figure 8 shows a question on purposes of use and a digest on the
bodyText ||| results. About 50% of the responders really want to use Information
bodyText ||| Desk to search for information.
bodyText ||| There is also an open-ended question asking people to make
bodyText ||| comments freely. Figure 9 gives some typical answers from the
bodyText ||| responders. The first and second answers are very positive, while the
bodyText ||| third and fourth point out the necessity of increasing the coverage of
bodyText ||| the system.
bodyText ||| Figure 6. Users’ evaluation on Information Desk
bodyText ||| Figure 7. New features expected by users
figure ||| Which feature of Information Desk has helped you in finding
figure ||| information?
figure ||| •	‘where is homepage of’ - finding homepages
figure ||| 54 %
figure ||| •	‘what is’ - finding definitions/acronyms
figure ||| 25 %
figure ||| •	‘who is’ - finding information about people
figure ||| 18 %
figure ||| •	‘who knows about’ - finding experts
figure ||| 3 %
figure ||| What kind of new feature do you want to use at Information
figure ||| Desk? (multiple choice)
figure ||| •	‘how to’ - e.g., &quot;how to activate Windows&quot;
figure ||| 57 %
figure ||| •	‘when’ - e.g., &quot;when is Yukon RTM&quot;
figure ||| 57 %
figure ||| •	‘where’ - e.g., &quot;where can I find an ATM&quot;
figure ||| 39 %
figure ||| •	‘why’ - e.g., &quot;why doesn&apos;t my printer work&quot;
figure ||| 28 %
figure ||| •	others
figure ||| 9 %
page ||| 466
figure ||| I visited Information Desk today to
figure ||| •	conduct testing on Information Desk
figure ||| 	54 %
figure ||| •	search for information related to my work
figure ||| 	46 %
figureCaption ||| Figure 8. Motivation of using Information Desk
figure ||| Please provide any additional comments, thanks!
figure ||| •	This is a terrific tool! Including ‘how to’ and ‘when’
figure ||| capabilities will put this in the ‘can’t live without it’
figure ||| category.
figure ||| •	Extremely successful searching so far! Very nice product
figure ||| with great potential.
figure ||| •	I would like to see more ‘Microsoftese’ definitions. There is
figure ||| a lot of cultural/tribal knowledge here that is not explained
figure ||| anywhere.
figure ||| •	Typing in my team our website doesn’t come up in the
figure ||| results, is there any way we can provide content for the
figure ||| search tool e.g., out group sharepoint URL?
figure ||| •	...
figureCaption ||| Figure 9. Typical user comments to Information Desk
subsectionHeader ||| 6.2 System Log Analysis
bodyText ||| We have made log during the running of Information Desk. The
bodyText ||| log includes user IP addresses, queries and clicked documents
bodyText ||| (recall that links to the original documents, from which
bodyText ||| information has been extraction, are given in search). The log data
bodyText ||| was collected from 1,303 unique users during the period from
bodyText ||| November 26th, 2004 to February 22nd, 2005. The users were
bodyText ||| Microsoft employees.
bodyText ||| In the log, there are 9,076 query submission records. The records
bodyText ||| include 4,384 unique query terms. About 40% of the queries are
bodyText ||| related to the ‘what is’ feature, 29% related to ‘where is homepage
bodyText ||| of’, 30% related to ‘who knows about’ and 22% related to ‘who
bodyText ||| is’. A query can be related to more than one feature.
bodyText ||| In the log, there are 2,316 clicks on documents after query
bodyText ||| submissions. The numbers of clicks for the ‘what is’, ‘where is
bodyText ||| homepage of’, ‘who knows about’, and ‘who is’ features are 694,
bodyText ||| 1041, 200 and 372, respectively. Note that for ‘what is’, ‘where is
bodyText ||| home page of’, and ‘who knows about’ we conduct ranking on
bodyText ||| retrieved information. The top ranked results are considered to be
bodyText ||| the best. If a user has clicked a top ranked document, then it
bodyText ||| means that he is interested in the document, and thus it is very
bodyText ||| likely he has found the information he looks for. Thus a system
bodyText ||| which has higher average rank of clicks is better than the other
bodyText ||| that does not. We used average rank of clicked documents to
bodyText ||| evaluate the performances of the features. The average ranks of
bodyText ||| clicks for ‘what is’, ‘where is homepage of’ and ‘who knows
bodyText ||| about’ are 2.4, 1.4 and 4.7 respectively. The results indicate that
bodyText ||| for the first two features, users usually can find information they
bodyText ||| look for on the top three answers. Thus it seems safe to say that
bodyText ||| the system have achieved practically acceptable performances for
bodyText ||| the two features. As for ‘who is’, ranking of a person’s documents
bodyText ||| does not seem to be necessary and the performance should be
bodyText ||| evaluated in a different way. (For example, precision and recall of
bodyText ||| metadata extraction as we have already reported in section 5).
sectionHeader ||| 7. CONCLUSION
bodyText ||| In this paper, we have investigated the problem of intranet search
bodyText ||| using information extraction.
bodyText ||| •	Through an analysis of survey results and an analysis of
bodyText ||| search log data, we have found that search needs on intranet
bodyText ||| can be categorized into a hierarchy.
bodyText ||| •	Based on the finding, we propose a new approach to intranet
bodyText ||| search in which we conduct search for each special type of
bodyText ||| information.
bodyText ||| •	We have developed a system called ‘Information Desk’,
bodyText ||| based on the idea. In Information Desk, we provide search on
bodyText ||| four types of information - finding term definitions,
bodyText ||| homepages of groups or topics, employees’ personal
bodyText ||| information and experts on topics. Information Desk has
bodyText ||| been deployed to the intranet of Microsoft and has received
bodyText ||| accesses from about 500 employees per month. Feedbacks
bodyText ||| from users show that the proposed approach is effective and
bodyText ||| the system can really help employees to find information.
bodyText ||| •	For each type of search, information extraction technologies
bodyText ||| have been used to extract, fuse, and summarize information
bodyText ||| in advance. High performance component technologies for
bodyText ||| the mining have been developed.
bodyText ||| As future work, we plan to increase the number of search types
bodyText ||| and combine them with conventional relevance search.
sectionHeader ||| 8. ACKNOWLEDGMENTS
bodyText ||| We thank Jin Jiang, Ming Zhou, Avi Shmueli, Kyle Peltonen,
bodyText ||| Drew DeBruyne, Lauri Ellis, Mark Swenson, and Mark Davies for
bodyText ||| their supports to the project.
sectionHeader ||| 9. REFERENCES
reference ||| [1] S. Blair-Goldensohn, K.R. McKeown, A.H. Schlaikjer. A
reference ||| Hybrid Approach for QA Track Definitional Questions. In
reference ||| Proc. of Twelfth Annual Text Retrieval Conference (TREC-
reference ||| 12), NIST, Nov., 2003.
reference ||| [2] E. Brill, S. Dumais, and M. Banko, An Analysis of the
reference ||| AskMSR Question-Answering System, EMNLP 2002
reference ||| [3] M. Chen, A. Hearst, A. Marti, J. Hong, and J. Lin, Cha-Cha:
reference ||| A System for Organizing Intranet Results. Proceedings of the
reference ||| 2nd USENIX Symposium on Internet Technologies and
reference ||| Systems. Boulder, CO. Oct. 1999.
reference ||| [4] C. L. A. Clarke, G. V. Cormack, T. R. Lynam, C. M. Li, and
reference ||| G. L. McLearn, Web Reinforced Question Answering
reference ||| (MultiText Experiments for TREC 2001). TREC 2001
reference ||| [5] N. Craswell, D. Hawking, and S.E. Robertson. Effective site
reference ||| finding using link anchor information. In Proc. of the 24th
reference ||| annual international ACM SIGIR conference on research
reference ||| and development in information retrieval, pages 250--257,
reference ||| 2001.
reference ||| [6] N. Craswell, D. Hawking, and T. Upstill. TREC12 Web and
reference ||| Interactive Tracks at CSIRO. In TREC12 Proceedings, 2004.
reference ||| [7] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins.
reference ||| P@noptic expert: Searching for experts not just for
reference ||| documents. Poster Proceedings of AusWeb&apos;01,
page ||| 467
reference ||| 2001b./urlausweb.scu.edu.au/aw01/papers/edited/vercoustre/
reference ||| paper.htm.
reference ||| [8] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.
reference ||| Overview of the TREC-2003 Web Track. In NIST Special
reference ||| Publication: 500-255, The Twelfth Text REtrieval
reference ||| Conference (TREC 2003), Gaithersburg, MD, 2003.
reference ||| [9] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu. Task
reference ||| Descriptions: Web Track 2003. In TREC12 Proceedings,
reference ||| 2004.
reference ||| [10] H. Cui, M-Y. Kan, and T-S. Chua. Unsupervised Learning of
reference ||| Soft Patterns for Definitional Question Answering,
reference ||| Proceedings of the Thirteenth World Wide Web conference
reference ||| (WWW 2004), New York, May 17-22, 2004.
reference ||| [11] A. Echihabi, U.Hermjakob, E. Hovy, D. Marcu, E. Melz, D.
reference ||| Ravichandran. Multiple-Engine Question Answering in
reference ||| TextMap. In Proc. of Twelfth Annual Text Retrieval
reference ||| Conference (TREC-12), NIST, Nov., 2003.
reference ||| [12] R. Fagin, R. Kumar, K. S. McCurley, J. Novak, D.
reference ||| Sivakumar, J. A. Tomlin, and D. P. Williamson. Searching
reference ||| the workplace web. Proc. 12th World Wide Web Conference,
reference ||| Budapest, 2003.
reference ||| [13] S. Feldman and C. Sherman. The high cost of not finding
reference ||| information. Technical Report #29127, IDC, April 2003.
reference ||| [14] H. Han, C. L. Giles, E. Manavoglu, H. Zha, Z. Zhang, and E.
reference ||| A. Fox. Automatic Document Metadata Extraction using
reference ||| Support Vector Machines. In Proceedings of the third
reference ||| ACM/IEEE-CS joint conference on Digital libraries, 2003
reference ||| [15] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, J.
reference ||| Williams, J. Bensley. Answer Mining by Combining
reference ||| Extraction Techniques with Abductive Reasoning. In Proc.
reference ||| of Twelfth Annual Text Retrieval Conference (TREC-12),
reference ||| NIST, Nov., 2003.
reference ||| [16] D. Hawking. Challenges in Intranet search. Proceedings of
reference ||| the fifteenth conference on Australasian database. Dunedin,
reference ||| New Zealand, 2004.
reference ||| [17] D. Hawking, N. Craswell, F. Crimmins, and T. Upstill.
reference ||| Intranet search: What works and what doesn&apos;t. Proceedings
reference ||| of the Infonortics Search Engines Meeting, San Francisco,
reference ||| April 2002.
reference ||| [18] E. Hovy, L. Gerber, U. Hermjakob, M. Junk, and C. Y. Lin.
reference ||| Question Answering in Webclopedia. TREC 2000
reference ||| [19] Y. Hu, H. Li, Y. Cao, D. Meyerzon, and Q. Zheng.
reference ||| Automatic Extraction of Titles from General Documents
reference ||| using Machine Learning. To appear at Proc. of Joint
reference ||| Conference on Digital Libraries (JCDL), 2005. Denver,
reference ||| Colorado, USA. 2005.
reference ||| [20] A. Ittycheriah and S. Roukos, IBM&apos;s Statistical Question
reference ||| Answering System-TREC 11. TREC 2002
reference ||| [21] J. Klavans and S. Muresan. DEFINDER: Rule-Based
reference ||| Methods for the Extraction of Medical Terminology and
reference ||| their Associated Definitions from On-line Text. In
reference ||| Proceedings of AMIA Symposium 2000.
reference ||| [22] C. C. T. Kwok, O. Etzioni, and D. S. Weld, Scaling question
reference ||| answering to the Web. WWW-2001: 150-161
reference ||| [23] Y. Li, H Zaragoza, R Herbrich, J Shawe-Taylor, and J. S.
reference ||| Kandola. The Perceptron Algorithm with Uneven Margins.
reference ||| in Proceedings of ICML&apos;02.
reference ||| [24] B. Liu, C. W. Chin, and H. T. Ng. Mining Topic-Specific
reference ||| Concepts and Definitions on the Web. In Proceedings of the
reference ||| twelfth international World Wide Web conference (WWW-
reference ||| 2003), 20-24 May 2003, Budapest, HUNGARY.
reference ||| [25] D. Mattox, M. Maybury and D. Morey. Enterprise Expert
reference ||| and Knowledge Discovery. Proceedings of the HCI
reference ||| International &apos;99 (the 8th International Conference on
reference ||| Human-Computer Interaction) on Human-Computer
reference ||| Interaction: Communication, Cooperation, and Application
reference ||| Design-Volume 2 - Volume 2. 1999.
reference ||| [26] P. Ogilvie and J. Callan. Combining Structural Information
reference ||| and the Use of Priors in Mixed Named-Page and Homepage
reference ||| Finding. In TREC12 Proceedings, 2004.
reference ||| [27] D. R. Radev, W. Fan, H. Qi, H. Wu, and A. Grewal.
reference ||| Probabilistic question answering on the web. WWW 2002:
reference ||| 408-419
reference ||| [28] D. E. Rose and D. Levinson. Understanding user goals in
reference ||| web search. Proceedings of the 13th international World
reference ||| Wide Web conference on Alternate track papers &amp; posters,
reference ||| 2004 New York, USA.
reference ||| [29] J. Savoy, Y. Rasolofo, and L. Perret, L. Report on the TREC-
reference ||| 2003 Experiment: Genomic and Web Searches. In TREC12
reference ||| Proceedings, 2004.
reference ||| [30] D. Stenmark. A Methodology for Intranet Search Engine
reference ||| Evaluations. Proceedings of IRIS22, Department of CS/IS,
reference ||| University of Jyväskylä, Finland, August 1999.
reference ||| [31 ] V. N. Vapnik. The Nature of Statistical Learning Theory.
reference ||| Springer, 1995.
reference ||| [32] J. Xu, Y. Cao, H. Li, and M. Zhao. Ranking Definitions with
reference ||| Supervised Learning Methods. In Proc. of 14th International
reference ||| World Wide Web Conference (WWW05), Industrial and
reference ||| Practical Experience Track, Chiba, Japan, pp.811-819, 2005.
reference ||| [33] J. Xu, A. Licuanan, R. Weischedel. TREC 2003 QA at BBN:
reference ||| Answering Definitional Questions. In Proc. of 12th Annual
reference ||| Text Retrieval Conference (TREC-12), NIST, Nov., 2003.
reference ||| [34] H. Yang, H. Cui, M. Maslennikov, L. Qiu, M-Y. Kan, and T-
reference ||| S. Chua, QUALIFIER in TREC-12 QA Main Task. TREC
reference ||| 2003: 480-488
reference ||| [35] Intellectual capital management products. Verity,
reference ||| http://www.verity.com/
reference ||| [36] IDOL server. Autonomy,
reference ||| http://www.autonomy.com/content/home/
reference ||| [37] Fast data search. Fast Search &amp; Transfer,
reference ||| http://www.fastsearch.com/
reference ||| [38] Atomz intranet search. Atomz, http://www.atomz.com/
reference ||| [39] Google Search Appliance. Google,
reference ||| http://www.google.com/enterprise/
page ||| 468
