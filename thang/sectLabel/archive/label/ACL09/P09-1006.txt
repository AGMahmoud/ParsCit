title ||| Exploiting Heterogeneous Treebanks for Parsing
author ||| Zheng-Yu Niu, Haifeng Wang, Hua Wu
affiliation ||| Toshiba (China) Research and Development Center
address ||| 5/F., Tower W2, Oriental Plaza, Beijing, 100738, China
email ||| {niuzhengyu,wanghaifeng,wuhua}@rdc.toshiba.com.cn
sectionHeader ||| Abstract
bodyText ||| We address the issue of using heteroge-
bodyText ||| neous treebanks for parsing by breaking
bodyText ||| it down into two sub-problems, convert-
bodyText ||| ing grammar formalisms of the treebanks
bodyText ||| to the same one, and parsing on these
bodyText ||| homogeneous treebanks. First we pro-
bodyText ||| pose to employ an iteratively trained tar-
bodyText ||| get grammar parser to perform grammar
bodyText ||| formalism conversion, eliminating prede-
bodyText ||| fined heuristic rules as required in previ-
bodyText ||| ous methods. Then we provide two strate-
bodyText ||| gies to refine conversion results, and adopt
bodyText ||| a corpus weighting technique for parsing
bodyText ||| on homogeneous treebanks. Results on the
bodyText ||| Penn Treebank show that our conversion
bodyText ||| method achieves 42% error reduction over
bodyText ||| the previous best result. Evaluation on
bodyText ||| the Penn Chinese Treebank indicates that a
bodyText ||| converted dependency treebank helps con-
bodyText ||| stituency parsing and the use of unlabeled
bodyText ||| data by self-training further increases pars-
bodyText ||| ing f-score to 85.2%, resulting in 6% error
bodyText ||| reduction over the previous best result.
sectionHeader ||| 1 Introduction
bodyText ||| The last few decades have seen the emergence of
bodyText ||| multiple treebanks annotated with different gram-
bodyText ||| mar formalisms, motivated by the diversity of lan-
bodyText ||| guages and linguistic theories, which is crucial to
bodyText ||| the success of statistical parsing (Abeille et al.,
bodyText ||| 2000; Brants et al., 1999; Bohmova et al., 2003;
bodyText ||| Han et al., 2002; Kurohashi and Nagao, 1998;
bodyText ||| Marcus et al., 1993; Moreno et al., 2003; Xue et
bodyText ||| al., 2005). Availability of multiple treebanks cre-
bodyText ||| ates a scenario where we have a treebank anno-
bodyText ||| tated with one grammar formalism, and another
bodyText ||| treebank annotated with another grammar formal-
bodyText ||| ism that we are interested in. We call the first
bodyText ||| a source treebank, and the second a target tree-
bodyText ||| bank. We thus encounter a problem of how to
bodyText ||| use these heterogeneous treebanks for target gram-
bodyText ||| mar parsing. Here heterogeneous treebanks refer
bodyText ||| to two or more treebanks with different grammar
bodyText ||| formalisms, e.g., one treebank annotated with de-
bodyText ||| pendency structure (DS) and the other annotated
bodyText ||| with phrase structure (PS).
bodyText ||| It is important to acquire additional labeled data
bodyText ||| for the target grammar parsing through exploita-
bodyText ||| tion of existing source treebanks since there is of-
bodyText ||| ten a shortage of labeled data. However, to our
bodyText ||| knowledge, there is no previous study on this is-
bodyText ||| sue.
bodyText ||| Recently there have been some works on us-
bodyText ||| ing multiple treebanks for domain adaptation of
bodyText ||| parsers, where these treebanks have the same
bodyText ||| grammar formalism (McClosky et al., 2006b;
bodyText ||| Roark and Bacchiani, 2003). Other related works
bodyText ||| focus on converting one grammar formalism of a
bodyText ||| treebank to another and then conducting studies on
bodyText ||| the converted treebank (Collins et al., 1999; Forst,
bodyText ||| 2003; Wang et al., 1994; Watkinson and Manand-
bodyText ||| har, 2001). These works were done either on mul-
bodyText ||| tiple treebanks with the same grammar formalism
bodyText ||| or on only one converted treebank. We see that
bodyText ||| their scenarios are different from ours as we work
bodyText ||| with multiple heterogeneous treebanks.
bodyText ||| For the use of heterogeneous treebanks1, we
bodyText ||| propose a two-step solution: (1) converting the
bodyText ||| grammar formalism of the source treebank to the
bodyText ||| target one, (2) refining converted trees and using
bodyText ||| them as additional training data to build a target
bodyText ||| grammar parser.
bodyText ||| For grammar formalism conversion, we choose
bodyText ||| the DS to PS direction for the convenience of the
bodyText ||| comparison with existing works (Xia and Palmer,
bodyText ||| 2001; Xia et al., 2008). Specifically, we assume
bodyText ||| that the source grammar formalism is dependency
footnote ||| 1Here we assume the existence of two treebanks.
page ||| 46
note ||| Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 46â54,
note ||| Suntec, Singapore, 2-7 August 2009. cï¿½2009 ACL and AFNLP
bodyText ||| grammar, and the target grammar formalism is
bodyText ||| phrase structure grammar.
bodyText ||| Previous methods for DS to PS conversion
bodyText ||| (Collins et al., 1999; Covington, 1994; Xia and
bodyText ||| Palmer, 2001; Xia et al., 2008) often rely on pre-
bodyText ||| defined heuristic rules to eliminate converison am-
bodyText ||| biguity, e.g., minimal projection for dependents,
bodyText ||| lowest attachment position for dependents, and the
bodyText ||| selection of conversion rules that add fewer num-
bodyText ||| ber of nodes to the converted tree. In addition, the
bodyText ||| validity of these heuristic rules often depends on
bodyText ||| their target grammars. To eliminate the heuristic
bodyText ||| rules as required in previous methods, we propose
bodyText ||| to use an existing target grammar parser (trained
bodyText ||| on the target treebank) to generate N-best parses
bodyText ||| for each sentence in the source treebank as conver-
bodyText ||| sion candidates, and then select the parse consis-
bodyText ||| tent with the structure of the source tree as the con-
bodyText ||| verted tree. Furthermore, we attempt to use con-
bodyText ||| verted trees as additional training data to retrain
bodyText ||| the parser for better conversion candidates. The
bodyText ||| procedure of tree conversion and parser retraining
bodyText ||| will be run iteratively until a stopping condition is
bodyText ||| satisfied.
bodyText ||| Since some converted trees might be imper-
bodyText ||| fect from the perspective of the target grammar,
bodyText ||| we provide two strategies to refine conversion re-
bodyText ||| sults: (1) pruning low-quality trees from the con-
bodyText ||| verted treebank, (2) interpolating the scores from
bodyText ||| the source grammar and the target grammar to se-
bodyText ||| lect better converted trees. Finally we adopt a cor-
bodyText ||| pus weighting technique to get an optimal combi-
bodyText ||| nation of the converted treebank and the existing
bodyText ||| target treebank for parser training.
bodyText ||| We have evaluated our conversion algorithm on
bodyText ||| a dependency structure treebank (produced from
bodyText ||| the Penn Treebank) for comparison with previous
bodyText ||| work (Xia et al., 2008). We also have investi-
bodyText ||| gated our two-step solution on two existing tree-
bodyText ||| banks, the Penn Chinese Treebank (CTB) (Xue et
bodyText ||| al., 2005) and the Chinese Dependency Treebank
bodyText ||| (CDT)2 (Liu et al., 2006). Evaluation on WSJ data
bodyText ||| demonstrates that it is feasible to use a parser for
bodyText ||| grammar formalism conversion and the conversion
bodyText ||| benefits from converted trees used for parser re-
bodyText ||| training. Our conversion method achieves 93.8%
bodyText ||| f-score on dependency trees produced from WSJ
bodyText ||| section 22, resulting in 42% error reduction over
bodyText ||| the previous best result for DS to PS conversion.
bodyText ||| Results on CTB show that score interpolation is
footnote ||| 2Available at http://ir.hit.edu.cn/.
bodyText ||| more effective than instance pruning for the use
bodyText ||| of converted treebanks for parsing and converted
bodyText ||| CDT helps parsing on CTB. When coupled with
bodyText ||| self-training technique, a reranking parser with
bodyText ||| CTB and converted CDT as labeled data achieves
bodyText ||| 85.2% f-score on CTB test set, an absolute 1.0%
bodyText ||| improvement (6% error reduction) over the previ-
bodyText ||| ous best result for Chinese parsing.
bodyText ||| The rest of this paper is organized as follows. In
bodyText ||| Section 2, we first describe a parser based method
bodyText ||| for DS to PS conversion, and then we discuss pos-
bodyText ||| sible strategies to refine conversion results, and
bodyText ||| finally we adopt the corpus weighting technique
bodyText ||| for parsing on homogeneous treebanks. Section
bodyText ||| 3 provides experimental results of grammar for-
bodyText ||| malism conversion on a dependency treebank pro-
bodyText ||| duced from the Penn Treebank. In Section 4, we
bodyText ||| evaluate our two-step solution on two existing het-
bodyText ||| erogeneous Chinese treebanks. Section 5 reviews
bodyText ||| related work and Section 6 concludes this work.
sectionHeader ||| 2 Our Two-Step Solution
subsectionHeader ||| 2.1 Grammar Formalism Conversion
bodyText ||| Previous DS to PS conversion methods built a
bodyText ||| converted tree by iteratively attaching nodes and
bodyText ||| edges to the tree with the help of conversion
bodyText ||| rules and heuristic rules, based on current head-
bodyText ||| dependent pair from a source dependency tree and
bodyText ||| the structure of the built tree (Collins et al., 1999;
bodyText ||| Covington, 1994; Xia and Palmer, 2001; Xia et
bodyText ||| al., 2008). Some observations can be made on
bodyText ||| these methods: (1) for each head-dependent pair,
bodyText ||| only one locally optimal conversion was kept dur-
bodyText ||| ing tree-building process, at the risk of pruning
bodyText ||| globally optimal conversions, (2) heuristic rules
bodyText ||| are required to deal with the problem that one
bodyText ||| head-dependent pair might have multiple conver-
bodyText ||| sion candidates, and these heuristic rules are usu-
bodyText ||| ally hand-crafted to reflect the structural prefer-
bodyText ||| ence in their target grammars. To overcome these
bodyText ||| limitations, we propose to employ a parser to gen-
bodyText ||| erate N-best parses as conversion candidates and
bodyText ||| then use the structural information of source trees
bodyText ||| to select the best parse as a converted tree.
bodyText ||| We formulate our conversion method as fol-
bodyText ||| lows.
bodyText ||| Let CDS be a source treebank annotated with
bodyText ||| DS and CPS be a target treebank annotated with
bodyText ||| PS. Our goal is to convert the grammar formalism
bodyText ||| of CDS to that of CPS.
bodyText ||| We first train a constituency parser on CPS
page ||| 47
bodyText ||| Input: CPS, CDS, Q, and a constituency parser	Output: Converted trees Cps
listItem ||| 1. Initialize:
listItem ||| â Set Cps,0 as null, DevScore=0, q=0;
listItem ||| â Split CPS into training set CPS,train and development set CPS,dev;
listItem ||| â Train the parser on CPS,train and denote it by Pqâl;
listItem ||| 2. Repeat:
listItem ||| â Use Pq_i to generate N-best PS parses for each sentence in CDS, and convert PS to DS for each parse;
listItem ||| â For each sentence in CDS Do
listItem ||| o ï¿½t=argmaxt Score (xi,t), and select the ï¿½t-th parse as a converted tree for this sentence;
listItem ||| â Let CD S,q S represent these converted trees, and let Ctrain=CPS,train U CDSS,q ;
listItem ||| â Train the parser on Ctrain, and denote the updated parser by Pq;
listItem ||| â Let DevScoreq be the f-score of Pq on CPS,dev;
listItem ||| â If DevScoreq &gt; DevScore Then DevScore=DevScoreq, and Cps=Cps,q;
listItem ||| â Else break;
listItem ||| â q++;
listItem ||| Until q &gt; Q
tableCaption ||| Table 1: Our algorithm for DS to PS conversion.
bodyText ||| (90% trees in CPS as training set CPS,train, and
bodyText ||| other trees as development set CPS,dev) and then
bodyText ||| let the parser generate N-best parses for each sen-
bodyText ||| tence in CDS.
bodyText ||| Let n be the number of sentences (or trees) in
bodyText ||| CDS and ni be the number of N-best parses gen-
bodyText ||| erated by the parser for the i-th (1 &lt; i &lt; n) sen-
bodyText ||| tence in CDS. Let xi,t be the t-th (1 &lt; t &lt; ni)
bodyText ||| parse for the i-th sentence. Let yi be the tree of the
bodyText ||| i-th (1 &lt; i &lt; n) sentence in CDS.
bodyText ||| To evaluate the quality of xi,t as a conversion
bodyText ||| candidate for yi, we convert xi,t to a dependency
bodyText ||| tree (denoted as xDS) and then use unlabeled de-
bodyText ||| pendency f-score to measure the similarity be-
bodyText ||| tween xDS and yi. Let Score(xi,t) denote the
bodyText ||| unlabeled dependency f-score of xDS against yi.
bodyText ||| Then we determine the converted tree for yi by
bodyText ||| maximizing Score(xi,t) over the N-best parses.
bodyText ||| The conversion from PS to DS works as fol-
bodyText ||| lows:
bodyText ||| Step 1. Use a head percolation table to find the
bodyText ||| head of each constituent in xi,t.
bodyText ||| Step 2. Make the head of each non-head child
bodyText ||| depend on the head of the head child for each con-
bodyText ||| stituent.
bodyText ||| Unlabeled dependency f-score is a harmonic
bodyText ||| mean of unlabeled dependency precision and unla-
bodyText ||| beled dependency recall. Precision measures how
bodyText ||| many head-dependent word pairs found in xDS
bodyText ||| i,t
bodyText ||| are correct and recall is the percentage of head-
bodyText ||| dependent word pairs defined in the gold-standard
bodyText ||| tree that are found in xDS. Here we do not take
bodyText ||| dependency tags into consideration for evaluation
bodyText ||| since they cannot be obtained without more so-
bodyText ||| phisticated rules.
bodyText ||| To improve the quality of N-best parses, we at-
bodyText ||| tempt to use the converted trees as additional train-
bodyText ||| ing data to retrain the parser. The procedure of
bodyText ||| tree conversion and parser retraining can be run it-
bodyText ||| eratively until a termination condition is satisfied.
bodyText ||| Here we use the parserâs f-score on CPS,dev as a
bodyText ||| termination criterion. If the update of training data
bodyText ||| hurts the performance on CPS,dev, then we stop
bodyText ||| the iteration.
bodyText ||| Table 1 shows this DS to PS conversion algo-
bodyText ||| rithm. Q is an upper limit of the number of loops,
bodyText ||| and Q&gt;0.
subsectionHeader ||| 2.2 Target Grammar Parsing
bodyText ||| Through grammar formalism conversion, we have
bodyText ||| successfully turned the problem of using hetero-
bodyText ||| geneous treebanks for parsing into the problem of
bodyText ||| parsing on homogeneous treebanks. Before using
bodyText ||| converted source treebank for parsing, we present
bodyText ||| two strategies to refine conversion results.
bodyText ||| Instance Pruning For some sentences in
bodyText ||| CDS, the parser might fail to generate high qual-
bodyText ||| ity N-best parses, resulting in inferior converted
bodyText ||| trees. To clean the converted treebank, we can re-
bodyText ||| move the converted trees with low unlabeled de-
bodyText ||| pendency f-scores (defined in Section 2.1) before
bodyText ||| using the converted treebank for parser training
page ||| 48
figureCaption ||| Figure 1: A parse tree in CTB for a sentence of
figureCaption ||| &quot; t1t A&lt;world&gt; -X &lt;every&gt; ï¿½ &lt;country&gt; A
figureCaption ||| Vï¿½&lt;people&gt; N&lt;all&gt; 4E&lt;with&gt; H A&lt;eyes&gt;
figureCaption ||| R hJ &lt;cast&gt; # A&lt;Hong Kong&gt; &quot; with
figureCaption ||| &quot;People from all over the world are cast-
figureCaption ||| ing their eyes on Hong Kong&quot; as its English
figureCaption ||| translation.
bodyText ||| because these trees are &quot;misleading&quot; training in-
bodyText ||| stances. The number of removed trees will be de-
bodyText ||| termined by cross validation on development set.
bodyText ||| Score Interpolation Unlabeled dependency
bodyText ||| f-scores used in Section 2.1 measure the quality of
bodyText ||| converted trees from the perspective of the source
bodyText ||| grammar only. In extreme cases, the top best
bodyText ||| parses in the N-best list are good conversion can-
bodyText ||| didates but we might select a parse ranked quite
bodyText ||| low in the N-best list since there might be con-
bodyText ||| flicts of syntactic structure definition between the
bodyText ||| source grammar and the target grammar.
bodyText ||| Figure 1 shows an example for illustration of
bodyText ||| a conflict between the grammar of CDT and
bodyText ||| that of CTB. According to Chinese head percola-
bodyText ||| tion tables used in the PS to DS conversion tool
bodyText ||| &quot;Penn2Malt&quot; 3 and Charniakâs parser4, the head
bodyText ||| of VP-2 is the word &quot; 4E &quot; (a preposition, with
bodyText ||| &quot;BA&quot; as its POS tag in CTB), and the head of
bodyText ||| IP-OBJ is R hJ &quot; . Therefore the word &quot; R
bodyText ||| hJ&quot; depends on the word &quot;4E&quot; . But according
bodyText ||| to the annotation scheme in CDT (Liu et al., 2006),
bodyText ||| the word &quot;4E&quot; is a dependent of the word &quot;R
bodyText ||| hJ &quot; . The conflicts between the two grammars
bodyText ||| may lead to the problem that the selected parses
bodyText ||| based on the information of the source grammar
bodyText ||| might not be preferred from the perspective of the
footnote ||| 3Available at http://w3.msi.vxu.se/ânivre/.
footnote ||| 4Available at http://www.cs.brown.edu/âec/.
bodyText ||| target grammar.
bodyText ||| Therefore we modified the selection metric in
bodyText ||| Section 2.1 by interpolating two scores, the prob-
bodyText ||| ability of a conversion candidate from the parser
bodyText ||| and its unlabeled dependency f-score, shown as
bodyText ||| follows:
equation ||| Score(xi,t) = AxProb(xi,t)+(1âA)xScore(xi,t). (1)
bodyText ||| The intuition behind this equation is that converted
bodyText ||| trees should be preferred from the perspective of
bodyText ||| both the source grammar and the target grammar.
bodyText ||| Here 0 &lt; A &lt; 1. Prob(xi,t) is a probability pro-
bodyText ||| duced by the parser for xi,t (0 &lt; Prob(xi,t) &lt; 1).
bodyText ||| The value of A will be tuned by cross validation on
bodyText ||| development set.
bodyText ||| After grammar formalism conversion, the prob-
bodyText ||| lem now we face has been limited to how to build
bodyText ||| parsing models on multiple homogeneous tree-
bodyText ||| bank. A possible solution is to simply concate-
bodyText ||| nate the two treebanks as training data. However
bodyText ||| this method may lead to a problem that if the size
bodyText ||| of CPS is significantly less than that of converted
bodyText ||| CDS, converted CDS may weaken the effect CPS
bodyText ||| might have. One possible solution is to reduce the
bodyText ||| weight of examples from converted CDS in parser
bodyText ||| training. Corpus weighting is exactly such an ap-
bodyText ||| proach, with the weight tuned on development set,
bodyText ||| that will be used for parsing on homogeneous tree-
bodyText ||| banks in this paper.
sectionHeader ||| 3 Experiments of Grammar Formalism
sectionHeader ||| Conversion
subsectionHeader ||| 3.1 Evaluation on WSJ section 22
bodyText ||| Xia et al. (2008) used WSJ section 19 from the
bodyText ||| Penn Treebank to extract DS to PS conversion
bodyText ||| rules and then produced dependency trees from
bodyText ||| WSJ section 22 for evaluation of their DS to PS
bodyText ||| conversion algorithm. They showed that their
bodyText ||| conversion algorithm outperformed existing meth-
bodyText ||| ods on the WSJ data. For comparison with their
bodyText ||| work, we conducted experiments in the same set-
bodyText ||| ting as theirs: using WSJ section 19 (1844 sen-
bodyText ||| tences) as CPS, producing dependency trees from
bodyText ||| WSJ section 22 (1700 sentences) as CDS5, and
bodyText ||| using labeled bracketing f-scores from the tool
bodyText ||| &quot;EVALB&quot; on WSJ section 22 for performance
bodyText ||| evaluation.
footnote ||| 5We used the tool &quot;Penn2Malt&quot; to produce dependency
footnote ||| structures from the Penn Treebank, which was also used for
footnote ||| PS to DS conversion in our conversion algorithm.
page ||| 49
table ||| 	DevScore	All the sentences
table ||| 		LR	LP	F
table ||| Models	(%)	(%)	(%)	(%)
table ||| The best result of
table ||| Xia et al. (2008)	-	90.7	88.1	89.4
table ||| Q-0-method	86.8	92.2	92.8	92.5
table ||| Q-10-method	88.0	93.4	94.1	93.8
tableCaption ||| Table 2: Comparison with the work of Xia et al.
table ||| (2008) on WSJ section 22.
table ||| 		All the sentences
table ||| 	DevScore	LR	LP	F
table ||| Models	(%)	(%)	(%)	(%)
table ||| Q-0-method	91.0	91.6	92.5	92.1
table ||| Q-10-method	91.6	93.1	94.1	93.6
tableCaption ||| Table 3: Results of our algorithm on WSJ section
tableCaption ||| 2-18 and 20-22.
bodyText ||| We employed Charniakâs maximum entropy in-
bodyText ||| spired parser (Charniak, 2000) to generate N-best
bodyText ||| (N=200) parses. Xia et al. (2008) used POS
bodyText ||| tag information, dependency structures and depen-
bodyText ||| dency tags in test set for conversion. Similarly, we
bodyText ||| used POS tag information in the test set to restrict
bodyText ||| search space of the parser for generation of better
bodyText ||| N-best parses.
bodyText ||| We evaluated two variants of our DS to PS con-
bodyText ||| version algorithm:
bodyText ||| Q-0-method: We set the value of Q as 0 for a
bodyText ||| baseline method.
bodyText ||| Q-10-method: We set the value of Q as 10 to
bodyText ||| see whether it is helpful for conversion to retrain
bodyText ||| the parser on converted trees.
bodyText ||| Table 2 shows the results of our conversion al-
bodyText ||| gorithm on WSJ section 22. In the experiment
bodyText ||| of Q-10-method, DevScore reached the highest
bodyText ||| value of 88.0% when q was 1. Then we used
bodyText ||| Cps,1 as the conversion result. Finally Q-10-
bodyText ||| method achieved an f-score of 93.8% on WSJ sec-
bodyText ||| tion 22, an absolute 4.4% improvement (42% er-
bodyText ||| ror reduction) over the best result of Xia et al.
bodyText ||| (2008). Moreover, Q-10-method outperformed Q-
bodyText ||| 0-method on the same test set. These results indi-
bodyText ||| cate that it is feasible to use a parser for DS to PS
bodyText ||| conversion and the conversion benefits from the
bodyText ||| use of converted trees for parser retraining.
subsectionHeader ||| 3.2 Evaluation on WSJ section 2-18 and
subsectionHeader ||| 20-22
bodyText ||| In this experiment we evaluated our conversion al-
bodyText ||| gorithm on a larger test set, WSJ section 2-18 and
bodyText ||| 20-22 (totally 39688 sentences). Here we also
bodyText ||| used WSJ section 19 as CPS. Other settings for
table ||| Training data	All the sentences
table ||| 	LR	LP	F
table ||| 	(%)	(%)	(%)
table ||| 1 x CTB + CDTPS	84.7	85.1	84.9
table ||| 2 x CTB + CDTPS	85.1	85.6	85.3
table ||| 5 x CTB + CDTPS	85.0	85.5	85.3
table ||| 10 x CTB +CDTPS	85.3	85.8	85.6
table ||| 20 x CTB +CDTPS	85.1	85.3	85.2
table ||| 50 x CTB +CDTPS	84.9	85.3	85.1
tableCaption ||| Table 4: Results of the generative parser on the de-
tableCaption ||| velopment set, when trained with various weight-
tableCaption ||| ing of CTB training set and CDTPS.
bodyText ||| this experiment are as same as that in Section 3. 1,
bodyText ||| except that here we used a larger test set.
bodyText ||| Table 3 provides the f-scores of our method with
bodyText ||| Q equal to 0 or 10 on WSJ section 2-18 and
bodyText ||| 20-22.
bodyText ||| With Q-10-method, DevScore reached the high-
bodyText ||| est value of 91.6% when q was 1. Finally Q-
bodyText ||| 10-method achieved an f-score of 93.6% on WSJ
bodyText ||| section 2-18 and 20-22, better than that of Q-0-
bodyText ||| method and comparable with that of Q-10-method
bodyText ||| in Section 3.1. It confirms our previous finding
bodyText ||| that the conversion benefits from the use of con-
bodyText ||| verted trees for parser retraining.
sectionHeader ||| 4 Experiments of Parsing
bodyText ||| We investigated our two-step solution on two ex-
bodyText ||| isting treebanks, CDT and CTB, and we used CDT
bodyText ||| as the source treebank and CTB as the target tree-
bodyText ||| bank.
bodyText ||| CDT consists of 60k Chinese sentences, anno-
bodyText ||| tated with POS tag information and dependency
bodyText ||| structure information (including 28 POS tags, and
bodyText ||| 24 dependency tags) (Liu et al., 2006). We did not
bodyText ||| use POS tag information as inputs to the parser in
bodyText ||| our conversion method due to the difficulty of con-
bodyText ||| version from CDT POS tags to CTB POS tags.
bodyText ||| We used a standard split of CTB for perfor-
bodyText ||| mance evaluation, articles 1-270 and 400-1151 as
bodyText ||| training set, articles 301-325 as development set,
bodyText ||| and articles 271-300 as test set.
bodyText ||| We used Charniakâs maximum entropy inspired
bodyText ||| parser and their reranker (Charniak and Johnson,
bodyText ||| 2005) for target grammar parsing, called a gener-
bodyText ||| ative parser (GP) and a reranking parser (RP) re-
bodyText ||| spectively. We reported ParseVal measures from
bodyText ||| the EVALB tool.
page ||| 50
table ||| 		All the sentences
table ||| 		LR	LP	F
table ||| Models	Training data	(%)	(%)	(%)
table ||| GP	CTB	79.9	82.2	81.0
table ||| RP	CTB	82.0	84.6	83.3
table ||| GP	10 x CTB + CDTPS	80.4	82.7	81.5
table ||| RP	10 x CTB + CDTPS	82.8	84.7	83.8
tableCaption ||| Table 5: Results of the generative parser (GP) and
tableCaption ||| the reranking parser (RP) on the test set, when
tableCaption ||| trained on only CTB training set or an optimal
tableCaption ||| combination of CTB training set and CDTPS.
subsectionHeader ||| 4.1 Results of a Baseline Method to Use CDT
bodyText ||| We used our conversion algorithm6 to convert the
bodyText ||| grammar formalism of CDT to that of CTB. Let
bodyText ||| CDTPS denote the converted CDT by our method.
bodyText ||| The average unlabeled dependency f-score of trees
bodyText ||| in CDTPS was 74.4%, and their average index in
bodyText ||| 200-best list was 48.
bodyText ||| We tried the corpus weighting method when
bodyText ||| combining CDTPS with CTB training set (abbre-
bodyText ||| viated as CTB for simplicity) as training data, by
bodyText ||| gradually increasing the weight (including 1, 2, 5,
bodyText ||| 10, 20, 50) of CTB to optimize parsing perfor-
bodyText ||| mance on the development set. Table 4 presents
bodyText ||| the results of the generative parser with various
bodyText ||| weights of CTB on the development set. Consid-
bodyText ||| ering the performance on the development set, we
bodyText ||| decided to give CTB a relative weight of 10.
bodyText ||| Finally we evaluated two parsing models, the
bodyText ||| generative parser and the reranking parser, on the
bodyText ||| test set, with results shown in Table 5. When
bodyText ||| trained on CTB only, the generative parser and the
bodyText ||| reranking parser achieved f-scores of 81.0% and
bodyText ||| 83.3%. The use of CDTPS as additional training
bodyText ||| data increased f-scores of the two models to 81.5%
bodyText ||| and 83.8%.
subsectionHeader ||| 4.2 Results of Two Strategies for a Better Use
subsectionHeader ||| of CDT
subsubsectionHeader ||| 4.2.1 Instance Pruning
bodyText ||| We used unlabeled dependency f-score of each
bodyText ||| converted tree as the criterion to rank trees in
bodyText ||| CDTPS and then kept only the top M trees
bodyText ||| with high f-scores as training data for pars-
bodyText ||| ing, resulting in a corpus CDTPMS. M var-
bodyText ||| ied from 100% xICDTPSI to 10% xICDTPSI
bodyText ||| with 10%xICDTPSI as the interval. ICDTPSI
footnote ||| 6The setting for our conversion algorithm in this experi-
footnote ||| ment was as same as that in Section 3.1. In addition, we used
footnote ||| CTB training set as CPS,trï¿½iï¿½, and CTB development set as
footnote ||| CPS,dev.
table ||| 		All the sentences
table ||| 		LR	LP	F
table ||| Models	Training data	(%)	(%)	(%)
table ||| GP	CTB + CDTa S	81.4	82.8	82.1
table ||| RP	CTB + CDTa S	83.0	85.4	84.2
tableCaption ||| Table 6: Results of the generative parser and the
tableCaption ||| reranking parser on the test set, when trained on
tableCaption ||| an optimal combination of CTB training set and
tableCaption ||| converted CDT.
bodyText ||| is the number of trees in CDTPS. Then
bodyText ||| we tuned the value of M by optimizing the
bodyText ||| parserâs performance on the development set with
bodyText ||| 10 x CTB+CDTPMS as training data. Finally the op-
bodyText ||| timal value of M was 100%x I CDT I. It indicates
bodyText ||| that even removing very few converted trees hurts
bodyText ||| the parsing performance. A possible reason is that
bodyText ||| most of non-perfect parses can provide useful syn-
bodyText ||| tactic structure information for building parsing
bodyText ||| models.
subsubsectionHeader ||| 4.2.2 Score Interpolation
bodyText ||| We used ï¿½Score(xj,t)7 to replace Score(xj,t) in
bodyText ||| our conversion algorithm and then ran the updated
bodyText ||| algorithm on CDT. Let CDTP S denote the con-
bodyText ||| verted CDT by this updated conversion algorithm.
bodyText ||| The values of A (varying from 0.0 to 1.0 with 0.1
bodyText ||| as the interval) and the CTB weight (including 1,
bodyText ||| 2, 5, 10, 20, 50) were simultaneously tuned on the
bodyText ||| development set8. Finally we decided that the op-
bodyText ||| timal value of A was 0.4 and the optimal weight of
bodyText ||| CTB was 1, which brought the best performance
bodyText ||| on the development set (an f-score of 86.1%). In
bodyText ||| comparison with the results in Section 4.1, the
bodyText ||| average index of converted trees in 200-best list
bodyText ||| increased to 2, and their average unlabeled depen-
bodyText ||| dency f-score dropped to 65.4%. It indicates that
bodyText ||| structures of converted trees become more consis-
bodyText ||| tent with the target grammar, as indicated by the
bodyText ||| increase of average index of converted trees, fur-
bodyText ||| ther away from the source grammar.
bodyText ||| Table 6 provides f-scores of the generative
bodyText ||| parser and the reranker on the test set, when
bodyText ||| trained on CTB and CDTP S. We see that the
bodyText ||| performance of the reranking parser increased to
footnote ||| 7Before calculating	ï¿½Score(xi,t),	we normal-
footnote ||| ized the values of Prob(xi,t) for each N-best list
footnote ||| by	(1)	Prob(xi,t)=Prob(xi,t)-Min(Prob(xi,*)),
footnote ||| (2)Prob(xi,t)=Prob(xi,t)/Max(Prob(xi,*)),	resulting
footnote ||| in that their maximum value was 1 and their minimum value
footnote ||| was 0.
footnote ||| 8Due to space constraint, we do not show f-scores of the
footnote ||| parser with different values of A and the CTB weight.
page ||| 51
table ||| 		All the sentences
table ||| 		LR	LP	F
table ||| Models	Training data	(%)	(%)	(%)
table ||| Self-trained GP	10 ï¿½ T+10 ï¿½ D+P	83.0	84.5	83.7
table ||| Updated RP	CTB+CDT. S	84.3	86.1	85.2
tableCaption ||| Table 7: Results of the self-trained gen-
tableCaption ||| erative parser and updated reranking parser
tableCaption ||| on the test set. 10 x T+10 x D+P stands for
tableCaption ||| 10 x CTB+10 x CDTP s+PDC.
bodyText ||| 84.2% f-score, better than the result of the rerank-
bodyText ||| ing parser with CTB and CDTPS as training data
bodyText ||| (shown in Table 5). It indicates that the use of
bodyText ||| probability information from the parser for tree
bodyText ||| conversion helps target grammar parsing.
subsectionHeader ||| 4.3 Using Unlabeled Data for Parsing
bodyText ||| Recent studies on parsing indicate that the use of
bodyText ||| unlabeled data by self-training can help parsing
bodyText ||| on the WSJ data, even when labeled data is rel-
bodyText ||| atively large (McClosky et al., 2006a; Reichart
bodyText ||| and Rappoport, 2007). It motivates us to em-
bodyText ||| ploy self-training technique for Chinese parsing.
bodyText ||| We used the POS tagged People Daily corpus9
bodyText ||| (Jan. 1998âJun. 1998, and Jan. 2000âDec.
bodyText ||| 2000) (PDC) as unlabeled data for parsing. First
bodyText ||| we removed the sentences with less than 3 words
bodyText ||| or more than 40 words from PDC to ease pars-
bodyText ||| ing, resulting in 820k sentences. Then we ran the
bodyText ||| reranking parser in Section 4.2.2 on PDC and used
bodyText ||| the parses on PDC as additional training data for
bodyText ||| the generative parser. Here we tried the corpus
bodyText ||| weighting technique for an optimal combination
bodyText ||| of CTB, CDTP s and parsed PDC, and chose the
bodyText ||| relative weight of both CTB and CDTP s as 10
bodyText ||| by cross validation on the development set. Fi-
bodyText ||| nally we retrained the generative parser on CTB,
bodyText ||| CDTP s and parsed PDC. Furthermore, we used
bodyText ||| this self-trained generative parser as a base parser
bodyText ||| to retrain the reranker on CTB and CDTP s.
bodyText ||| Table 7 shows the performance of self-trained
bodyText ||| generative parser and updated reranker on the test
bodyText ||| set, with CTB and CDTP s as labeled data. We see
bodyText ||| that the use of unlabeled data by self-training fur-
bodyText ||| ther increased the reranking parserâs performance
bodyText ||| from 84.2% to 85.2%. Our results on Chinese data
bodyText ||| confirm previous findings on English data shown
bodyText ||| in (McClosky et al., 2006a; Reichart and Rap-
bodyText ||| poport, 2007).
footnote ||| 9Available at http://icl.pku.edu.cn/.
subsectionHeader ||| 4.4 Comparison with Previous Studies for
subsectionHeader ||| Chinese Parsing
bodyText ||| Table 8 and 9 present the results of previous stud-
bodyText ||| ies on CTB. All the works in Table 8 used CTB
bodyText ||| articles 1-270 as labeled data. In Table 9, Petrov
bodyText ||| and Klein (2007) trained their model on CTB ar-
bodyText ||| ticles 1-270 and 400-1151, and Burkett and Klein
bodyText ||| (2008) used the same CTB articles and parse trees
bodyText ||| of their English translation (from the English Chi-
bodyText ||| nese Translation Treebank) as training data. Com-
bodyText ||| paring our result in Table 6 with that of Petrov
bodyText ||| and Klein (2007), we see that CDTP s helps pars-
bodyText ||| ing on CTB, which brought 0.9% f-score improve-
bodyText ||| ment. Moreover, the use of unlabeled data further
bodyText ||| boosted the parsing performance to 85.2%, an ab-
bodyText ||| solute 1.0% improvement over the previous best
bodyText ||| result presented in Burkett and Klein (2008).
sectionHeader ||| 5 Related Work
bodyText ||| Recently there have been some studies address-
bodyText ||| ing how to use treebanks with same grammar for-
bodyText ||| malism for domain adaptation of parsers. Roark
bodyText ||| and Bachiani (2003) presented count merging and
bodyText ||| model interpolation techniques for domain adap-
bodyText ||| tation of parsers. They showed that their sys-
bodyText ||| tem with count merging achieved a higher perfor-
bodyText ||| mance when in-domain data was weighted more
bodyText ||| heavily than out-of-domain data. McClosky et al.
bodyText ||| (2006b) used self-training and corpus weighting to
bodyText ||| adapt their parser trained on WSJ corpus to Brown
bodyText ||| corpus. Their results indicated that both unla-
bodyText ||| beled in-domain data and labeled out-of-domain
bodyText ||| data can help domain adaptation. In comparison
bodyText ||| with these works, we conduct our study in a dif-
bodyText ||| ferent setting where we work with multiple het-
bodyText ||| erogeneous treebanks.
bodyText ||| Grammar formalism conversion makes it possi-
bodyText ||| ble to reuse existing source treebanks for the study
bodyText ||| of target grammar parsing. Wang et al. (1994)
bodyText ||| employed a parser to help conversion of a tree-
bodyText ||| bank from a simple phrase structure to a more in-
bodyText ||| formative phrase structure and then used this con-
bodyText ||| verted treebank to train their parser. Collins et al.
bodyText ||| (1999) performed statistical constituency parsing
bodyText ||| of Czech on a treebank that was converted from
bodyText ||| the Prague Dependency Treebank under the guid-
bodyText ||| ance of conversion rules and heuristic rules, e.g.,
bodyText ||| one level of projection for any category, minimal
bodyText ||| projection for any dependents, and fixed position
bodyText ||| of attachment. Xia and Palmer (2001) adopted bet-
bodyText ||| ter heuristic rules to build converted trees, which
page ||| 52
table ||| Models	&lt; 40 words			All the sentences
table ||| 	LR	LP	F	LR	LP	F
table ||| 	(%)	(%)	(%)	(%)	(%)	(%)
table ||| Bikel &amp; Chiang (2000)	76.8	77.8	77.3	-	-	-
table ||| Chiang &amp; Bikel (2002)	78.8	81.1	79.9	-	-	-
table ||| Levy &amp; Manning (2003)	79.2	78.4	78.8	-	-	-
table ||| Bikelâs thesis (2004)	78.0	81.2	79.6	-	-	-
table ||| Xiong et. al. (2005)	78.7	80.1	79.4	-	-	-
table ||| Chen et. al. (2005)	81.0	81.7	81.2	76.3	79.2	77.7
table ||| Wang et. al. (2006)	79.2	81.1	80.1	76.2	78.0	77.1
tableCaption ||| Table 8: Results of previous studies on CTB with CTB articles 1-270 as labeled data.
table ||| 	&lt; 40 words			All the sentences
table ||| 	LR	LP	F	LR	LP	F
table ||| Models	(%)	(%)	(%)	(%)	(%)	(%)
table ||| Petrov &amp; Klein (2007)	85.7	86.9	86.3	81.9	84.8	83.3
table ||| Burkett &amp; Klein (2008)	-	-	-	-	-	84.2
tableCaption ||| Table 9: Results of previous studies on CTB with more labeled data.
bodyText ||| reflected the structural preference in their target
bodyText ||| grammar. For acquisition of better conversion
bodyText ||| rules, Xia et al. (2008) proposed to automati-
bodyText ||| cally extract conversion rules from a target tree-
bodyText ||| bank. Moreover, they presented two strategies to
bodyText ||| solve the problem that there might be multiple
bodyText ||| conversion rules matching the same input depen-
bodyText ||| dency tree pattern: (1) choosing the most frequent
bodyText ||| rules, (2) preferring rules that add fewer number
bodyText ||| of nodes and attach the subtree lower.
bodyText ||| In comparison with the works of Wang et al.
bodyText ||| (1994) and Collins et al. (1999), we went fur-
bodyText ||| ther by combining the converted treebank with the
bodyText ||| existing target treebank for parsing. In compar-
bodyText ||| ison with previous conversion methods (Collins
bodyText ||| et al., 1999; Covington, 1994; Xia and Palmer,
bodyText ||| 2001; Xia et al., 2008) in which for each head-
bodyText ||| dependent pair, only one locally optimal conver-
bodyText ||| sion was kept during tree-building process, we
bodyText ||| employed a parser to generate globally optimal
bodyText ||| syntactic structures, eliminating heuristic rules for
bodyText ||| conversion. In addition, we used converted trees to
bodyText ||| retrain the parser for better conversion candidates,
bodyText ||| while Wang et al. (1994) did not exploit the use of
bodyText ||| converted trees for parser retraining.
sectionHeader ||| 6 Conclusion
bodyText ||| We have proposed a two-step solution to deal with
bodyText ||| the issue of using heterogeneous treebanks for
bodyText ||| parsing. First we present a parser based method
bodyText ||| to convert grammar formalisms of the treebanks to
bodyText ||| the same one, without applying predefined heuris-
bodyText ||| tic rules, thus turning the original problem into the
bodyText ||| problem of parsing on homogeneous treebanks.
bodyText ||| Then we present two strategies, instance pruning
bodyText ||| and score interpolation, to refine conversion re-
bodyText ||| sults. Finally we adopt the corpus weighting tech-
bodyText ||| nique to combine the converted source treebank
bodyText ||| with the existing target treebank for parser train-
bodyText ||| ing.
bodyText ||| The study on the WSJ data shows the benefits of
bodyText ||| our parser based approach for grammar formalism
bodyText ||| conversion. Moreover, experimental results on the
bodyText ||| Penn Chinese Treebank indicate that a converted
bodyText ||| dependency treebank helps constituency parsing,
bodyText ||| and it is better to exploit probability information
bodyText ||| produced by the parser through score interpolation
bodyText ||| than to prune low quality trees for the use of the
bodyText ||| converted treebank.
bodyText ||| Future work includes further investigation of
bodyText ||| our conversion method for other pairs of grammar
bodyText ||| formalisms, e.g., from the grammar formalism of
bodyText ||| the Penn Treebank to more deep linguistic formal-
bodyText ||| ism like CCG, HPSG, or LFG.
sectionHeader ||| References
reference ||| Anne Abeille, Lionel Clement and Francois Toussenel. 2000.
reference ||| Building a Treebank for French. In Proceedings of LREC
reference ||| 2000, pages 87-94.
reference ||| Daniel Bikel and David Chiang. 2000. Two Statistical Pars-
reference ||| ing Models Applied to the Chinese Treebank. In Proceed-
reference ||| ings of the Second SIGHAN workshop, pages 1-6.
reference ||| Daniel Bikel. 2004. On the Parameter Space of Generative
reference ||| Lexicalized Statistical Parsing Models. Ph.D. thesis, Uni-
reference ||| versity of Pennsylvania.
reference ||| Alena Bohmova, Jan Hajic, Eva Hajicova and Barbora
reference ||| Vidova-Hladka. 2003. The Prague Dependency Tree-
reference ||| bank: A Three-Level Annotation Scenario. Treebanks:
page ||| 53
reference ||| Building and Using Annotated Corpora. Kluwer Aca-
reference ||| demic Publishers, pages 103-127.
reference ||| Thorsten Brants, Wojciech Skut and Hans Uszkoreit. 1999.
reference ||| Syntactic Annotation of a German Newspaper Corpus. In
reference ||| Proceedings of the ATALA Treebank Workshop, pages 69-
reference ||| 76.
reference ||| David Burkett and Dan Klein. 2008. Two Languages are
reference ||| Better than One (for Syntactic Parsing). In Proceedings of
reference ||| EMNLP 2008, pages 877-886.
reference ||| Eugene Charniak. 2000. A Maximum Entropy Inspired
reference ||| Parser. In Proceedings of NAACL 2000, pages 132-139.
reference ||| Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine
reference ||| N-Best Parsing and MaxEnt Discriminative Reranking. In
reference ||| Proceedings ofACL 2005, pages 173-180.
reference ||| Ying Chen, Hongling Sun and Dan Jurafsky. 2005. A Cor-
reference ||| rigendum to Sun and Jurafsky (2004) Shallow Semantic
reference ||| Parsing of Chinese. University of Colorado at Boulder
reference ||| CSLR Tech Report TR-CSLR-2005-01.
reference ||| David Chiang and Daniel M. Bikel. 2002. Recovering La-
reference ||| tent Information in Treebanks. In Proceedings of COL-
reference ||| ING 2002, pages 1-7.
reference ||| Micheal Collins, Lance Ramshaw, Jan Hajic and Christoph
reference ||| Tillmann. 1999. A Statistical Parser for Czech. In Pro-
reference ||| ceedings ofACL 1999, pages 505-512.
reference ||| Micheal Covington. 1994. GB Theory as Dependency
reference ||| Grammar. Research Report AI-1992-03.
reference ||| Martin Forst. 2003. Treebank Conversion - Establishing
reference ||| a Testsuite for a Broad-Coverage LFG from the TIGER
reference ||| Treebank. In Proceedings of LINC at EACL 2003, pages
reference ||| 25-32.
reference ||| Chunghye Han, Narae Han, Eonsuk Ko and Martha Palmer.
reference ||| 2002. Development and Evaluation of a Korean Treebank
reference ||| and its Application to NLP. In Proceedings ofLREC 2002,
reference ||| pages 1635-1642.
reference ||| Sadao Kurohashi and Makato Nagao. 1998. Building a
reference ||| Japanese Parsed Corpus While Improving the Parsing Sys-
reference ||| tem. In Proceedings of LREC 1998, pages 719-724.
reference ||| Roger Levy and Christopher Manning. 2003. Is It Harder to
reference ||| Parse Chinese, or the Chinese Treebank? In Proceedings
reference ||| ofACL 2003, pages 439-446.
reference ||| Ting Liu, Jinshan Ma and Sheng Li. 2006. Building a Depen-
reference ||| dency Treebank for Improving Chinese Parser. Journal of
reference ||| Chinese Language and Computing, 16(4):207-224.
reference ||| Mitchell P. Marcus, Beatrice Santorini and Mary Ann
reference ||| Marcinkiewicz. 1993. Building a Large Annotated Cor-
reference ||| pus of English: The Penn Treebank. Computational Lin-
reference ||| guistics, 19(2):313-330.
reference ||| David McClosky, Eugene Charniak and Mark Johnson.
reference ||| 2006a. Effective Self-Training for Parsing. In Proceed-
reference ||| ings of NAACL 2006, pages 152-159.
reference ||| David McClosky, Eugene Charniak and Mark Johnson.
reference ||| 2006b. Reranking and Self-Training for Parser Adapta-
reference ||| tion. In Proceedings of COLING/ACL 2006, pages 337-
reference ||| 344.
reference ||| Antonio Moreno, Susana Lopez, Fernando Sanchez and
reference ||| Ralph Grishman. 2003. Developing a Syntactic Anno-
reference ||| tation Scheme and Tools for a Spanish Treebank. Tree-
reference ||| banks: Building and Using Annotated Corpora. Kluwer
reference ||| Academic Publishers, pages 149-163.
reference ||| Slav Petrov and Dan Klein. 2007. Improved Inference for
reference ||| Unlexicalized Parsing. In Proceedings of HLT/NAACL
reference ||| 2007, pages 404-411.
reference ||| Roi Reichart and Ari Rappoport. 2007. Self-Training for En-
reference ||| hancement and Domain Adaptation of Statistical Parsers
reference ||| Trained on Small Datasets. In Proceedings of ACL 2007,
reference ||| pages 616-623.
reference ||| Brian Roark and Michiel Bacchiani. 2003. Supervised and
reference ||| Unsupervised PCFG Adaptation to Novel Domains. In
reference ||| Proceedings of HLT/NAACL 2003, pages 126-133.
reference ||| Jong-Nae Wang, Jing-Shin Chang and Keh-Yih Su. 1994.
reference ||| An Automatic Treebank Conversion Algorithm for Corpus
reference ||| Sharing. In Proceedings ofACL 1994, pages 248-254.
reference ||| Mengqiu Wang, Kenji Sagae and Teruko Mitamura. 2006. A
reference ||| Fast, Accurate Deterministic Parser for Chinese. In Pro-
reference ||| ceedings of COLING/ACL 2006, pages 425-432.
reference ||| Stephen Watkinson and Suresh Manandhar. 2001. Translat-
reference ||| ing Treebank Annotation for Evaluation. In Proceedings
reference ||| of ACL Workshop on Evaluation Methodologies for Lan-
reference ||| guage and Dialogue Systems, pages 1-8.
reference ||| Fei Xia and Martha Palmer. 2001. Converting Dependency
reference ||| Structures to Phrase Structures. In Proceedings of HLT
reference ||| 2001, pages 1-5.
reference ||| Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmer
reference ||| and Dipti Misra. Sharma. 2008. Towards a Multi-
reference ||| Representational Treebank. In Proceedings of the 7th In-
reference ||| ternational Workshop on Treebanks and Linguistic Theo-
reference ||| ries, pages 159-170.
reference ||| Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin and
reference ||| Yueliang Qian. 2005. Parsing the Penn Chinese Tree-
reference ||| bank with Semantic Knowledge. In Proceedings of IJC-
reference ||| NLP 2005, pages 70-81.
reference ||| Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha Palmer.
reference ||| 2005. The Penn Chinese TreeBank: Phrase Structure An-
reference ||| notation of a Large Corpus. Natural Language Engineer-
reference ||| ing, 11(2):207-238.
page ||| 54
