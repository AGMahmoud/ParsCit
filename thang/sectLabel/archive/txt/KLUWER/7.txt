	Mobile Networks and Applications 9, 9–20, 2004 © 2004 Kluwer Academic Publishers. Manufactured in The Netherlands.
A Fair and Traffic Dependent Scheduling Algorithm for Bluetooth
Scatternets
ROHIT KAPOOR
University of California, Los Angeles (UCLA), USA
ANDREA ZANELLA
University of Padova, Italy
MARIO GERLA
University of California, Los Angeles (UCLA), USA
Abstract. The Bluetooth specification defines the notion of interconnected piconets, called scatternets, but does not define the actual +L+ mechanisms and algorithms necessary to set up and maintain them. The operation of a scatternet requires some Bluetooth units to be inter- +L+ piconet units (gateways), which need to time-division multiplex their presence among their piconets. This requires a scatternet-scheduling +L+ algorithm that can schedule the presence of these units in an efficient manner. In this paper, we propose a distributed scatternet-scheduling +L+ scheme that is implemented using the HOLD mode of Bluetooth and adapts to non-uniform and changing traffic. Another attribute of the +L+ scheme is that it results in fair allocation of bandwidth to each Bluetooth unit. This scheme provides an integrated solution for both intra- +L+ and inter-piconet scheduling, i.e., for polling of slaves and scheduling of gateways.
Keywords: Bluetooth, scatternet, scheduling, fairness
1. Introduction
The Bluetooth [10] technology was developed as a replace- +L+ ment of cables between electronic devices and this is perhaps +L+ its most obvious use. But, it is the ability of Bluetooth de- +L+ vices to form small networks called piconets that opens up a +L+ whole new arena for applications where information may be +L+ exchanged seamlessly among the devices in the piconet. Typ- +L+ ically, such a network, referred to as a PAN (Personal Area +L+ Network), consists of a mobile phone, laptop, palmtop, head- +L+ set, and other electronic devices that a person carries around +L+ in his every day life. The PAN may, from time to time, also +L+ include devices that are not carried along with the user, e.g., +L+ an access point for Internet access or sensors located in a +L+ room. Moreover, devices from other PANs can also be in- +L+ terconnected to enable sharing of information.
The networking capabilities of Bluetooth can be further +L+ enhanced by interconnecting piconets to form scatternets. +L+ This requires that some units be present in more than one pi- +L+ conet. These units, called gateways, need to time-division +L+ their presence among the piconets. An important issue with +L+ the gateways is that their presence in different piconets needs +L+ to be scheduled in an efficient manner. Moreover, since the +L+ gateway cannot receive information from more than one pi- +L+ conet at a time, there is a need to co-ordinate the presence of +L+ masters and gateways.
Some previous work has looked at scheduling in a piconet +L+ [2,5] and also in a scatternet. In [4], the authors define a +L+ Rendezvous-Point based architecture for scheduling in a scat- +L+ ternet, which results in the gateway spending a fixed fraction +L+ of its time in each piconet. Such a fixed time-division of the +L+ gateway may clearly be inefficient since traffic is dynamic. +L+ In [9], the authors propose the Pseudo-Random Coordinated +L+ Scatternet Scheduling (PCSS) scheme in which Bluetooth +L+ nodes assign meeting points with their peers. The sequence +L+ of meeting points follows a pseudo-random process that leads +L+ to unique meeting points for different peers of a node. The +L+ intensity of these meeting points may be increased or de- +L+ creased according to the traffic intensity. This work presents +L+ performance results for various cases. In [11], a scatternet- +L+ scheduling algorithm based on the concept of a switch table, +L+ which can be dynamically adjusted based on traffic load, is +L+ presented. In [1], the authors present a credit-based schedul- +L+ ing scheme based on the SNIFF mode of Bluetooth, where +L+ credits may be reallocated to cater to changing traffic.
Our scheduling scheme addresses the issues of fairness and +L+ utilization of bandwidth. Since Bluetooth is a low-bandwidth +L+ environment, it is important that bandwidth should be effi- +L+ ciently utilized. Also, since a low bandwidth can easily lead +L+ to starvation of flows, another metric we focus on is fairness. +L+ We propose a distributed scatternet-scheduling algorithm that +L+ is implemented using the HOLD mode [10] of Bluetooth +L+ and adapts to non-uniform and changing traffic. This algo- +L+ rithm provides an integrated solution for both intra- and inter- +L+ piconet scheduling, i.e., for polling of slaves and scheduling +L+ of gateways. The algorithm leads to a high bandwidth utiliza- +L+ tion and results in a fair division of (a) the piconet bandwidth +L+ between the slaves of a piconet and (b) the gateway presence +L+ among different piconets.
In section 2, we discuss the Bluetooth technology. In +L+ section 3, we present a definition of fairness in the context
10	R. KAPOOR ET AL.
Figure 1. Gateway may be viewed as a virtual master and masters as virtual
slaves.
of Bluetooth scatternets, which takes into account intra- and +L+ inter-piconet max-min fairness. Section 4 describes the al- +L+ gorithm and proves its fairness property. Section 5 presents +L+ simulation results and section 6 presents the conclusions.
2. Bluetooth technology
The Bluetooth system [3] operates in the worldwide un- +L+ licensed 2.4 GHz Industrial–Scientific–Medical (ISM) fre- +L+ quency band. To make the link robust to interference, it uses +L+ a Frequency Hopping (FH) technique with 79 radio carriers. +L+ It allows a raw data transmission rate of 1 Mbit/s.
Two or more Bluetooth units sharing the same channel +L+ form a piconet. Each piconet consists of a master unit and +L+ up to seven active slave units. The master unit polls the slave +L+ units according to a polling algorithm and a slave is only al- +L+ lowed to transmit after the master has polled it. The piconet +L+ capacity is thus, shared among the slave units according to the +L+ polling algorithm.
Furthermore, two or more piconets can be interconnected, +L+ forming a scatternet. This requires a unit, called an inter- +L+ piconet unit (gateway), to be a part of more than one piconet. +L+ Such a unit can simultaneously be a slave member of multi- +L+ ple piconets, but a master in only one, and can transmit and +L+ receive data in only one piconet at a time; so participation in +L+ multiple piconets has to be on a time-division multiplex ba- +L+ sis. The time of the gateway is, thus, also shared among the +L+ piconets it belongs to. In this work, we assume that the gate- +L+ way can only be a slave in its piconets. If a gateway were +L+ to be a master in a piconet, it would lead to the stoppage of +L+ all transmission in the piconet when the gateway visits some +L+ other piconet. Thus, we believe that the use of the gateway as +L+ a slave is the most efficient method of scatternetting.
3. Fair allocation of bandwidth
As introduced in the previous section, units belonging to a +L+ piconet share the piconet capacity according to the polling +L+ algorithm used by the master. In an analogous manner, gate- +L+ ways in a scatternet divide their time among their different +L+ piconets, according to the “master-listening” algorithm they +L+ use. It can be noted that there is a duality in this architec- +L+ ture. On the one hand, a master divides its capacity among +L+ the units of its piconet by using a polling algorithm. On the
other hand, a gateway shares its capacity among the piconets +L+ it belongs to, on the basis of a scheduling algorithm it uses +L+ for listening to the masters. The gateway, can, then be viewed +L+ as a “virtual master” and its masters can be viewed as “virtual +L+ slaves” forming a “virtual piconet”, in which the polling cycle +L+ is, actually, the “listening cycle” of the gateway. A graphical +L+ interpretation of this duality is given in figure 1, in which the +L+ solid line shows the actual piconets, and the dotted line shows +L+ the virtual piconet.
Due to this duality, we design our scheduling scheme such +L+ that the same scheduling algorithm is used for fair sharing of +L+ both (a) the piconet capacity among slaves and (b) the gate- +L+ way time among piconets.
We now give a definition of max-min fairness [7]. We then +L+ go on to define max-min fairness in the context of Bluetooth +L+ scatternets, by considering (a) intra-piconet fairness, i.e., fair- +L+ ness in division of piconet bandwidth among slaves (both +L+ gateway and non-gateway) of a piconet and (b) inter-piconet +L+ fairness, i.e., fairness in division of the gateway’s presence +L+ among its piconets. We first define a ‘feasible’ rate distribu- +L+ tion since this is used in the definition of max-min fairness.
Definition 1 (Feasible). A rate distribution is feasible if rates +L+ are non-negative, the aggregate rate is not greater than one, +L+ and no unit receives a higher rate than required.
Definition 2 (Max-min fairness). An allocation of rates η1, +L+ η2, ... , ηs among s units is max-min fair if it is feasible, and +L+ for each unit i, ηi cannot be increased (while maintaining fea- +L+ sibility) without decreasing η j for some other unit j for which +L+ ηj &lt;, ηi.
The distribution of max-min fair rates depends upon the +L+ set of rate demands (traffic generated) of the units. In the +L+ following subsections, we discuss factors that determine the +L+ max-min “fair share” of a slave (gateway or non-gateway). +L+ We call these factors the Piconet Presence Fraction and the +L+ Scatternet Presence Fraction and show how they may be used +L+ to calculate the “fair share” for a slave in a scatternet.
3.1. Piconet presence fraction
Consider a piconet consisting of gateway and non-gateway +L+ slaves in which the master has complete knowledge of the rate +L+ demands of all slaves (an ideal master). Using this knowl- +L+ edge, the master polls the slaves in a max-min fair man- +L+ ner such that each slave gets its “fair share” of the master’s +L+ polling. We refer to the “fair share” received by a slave as the +L+ “piconet presence fraction” (PPF) of the slave. The gateway +L+ has a PPF for each piconet it belongs to.
Consider the piconets shown in figures 2(a) and 2(b), each +L+ consisting of one gateway and two slaves, with the traffic rates +L+ of each slave as shown. In figure 2(a) (Piconet I), the PPF of +L+ each non-gateway slave is 0.2, while the PPF of the gateway +L+ is 0.6. In figure 2(b) (Piconet II), the PPFs of the slaves are +L+ 0.2 and 0.4, while the PPF of the gateway is 0.4.
A FAIR AND TRAFFIC DEPENDENT SCHEDULING	11
Figure 2. Piconets with traffic rates between master and each slave shownn.
3.2. Scatternet presence fraction
A gateway will, in general, be a slave in multiple piconets and +L+ may have different amounts of traffic to exchange with each +L+ piconet. Consider an ideal gateway that has complete knowl- +L+ edge of the rate demands of all its masters. The gateway can +L+ then divide its presence among its piconets in a max-min fair +L+ manner, giving each piconet a “fair share” of its presence. We +L+ call this fair share the “scatternet presence fraction” (SPF) of +L+ the gateway for the piconet. The importance of the SPF is that +L+ a fair division of the gateway’s presence among its piconets +L+ can be achieved based on the SPF.
Consider the piconets of figure 2 again, but the gateway of +L+ each of the piconets now connects them to form a scatternet, +L+ as shown in figure 3. The traffic requirements are the same as +L+ shown in figure 2. The SPF of the gateway is 0.5 in Piconet I +L+ and 0.5 in Piconet II.
3.3. Fair share
We see that for a gateway to be fair, there are two kinds of +L+ fairness it has to achieve: that dictated by the PPFs, which +L+ achieves fairness between the gateway and the other slaves of +L+ a piconet, and that of the SPFs, which distributes the presence +L+ of the gateway between its piconets in a fair manner. Both +L+ these kinds of fairness may not always be completely achiev- +L+ able and this can lead to a change in the values of PPF and +L+ SPF, as we now discuss.
We observe that an ideal master (as in section 3.1) does +L+ not give a gateway more than the PPF of its polling. Thus, +L+ if the SPF of a gateway is greater than its PPF for a piconet, +L+ the gateway spends a fraction of its time equal to the PPF +L+ in the piconet. The gateway cannot stay for a fraction equal +L+ to its SPF in the piconet since it is limited by its PPF. Thus, +L+ the extra scatternet presence fraction (the difference of the +L+ SPF and the PPF) is redistributed in a fair manner among +L+ the gateway’s other piconets for which the SPF is less than +L+ the PPF. This may increase the SPF of the gateway in the +L+ other piconets. In other words, the gateway behaves as if +L+ its SPF in a particular piconet is reduced to the PPF and +L+ thus, its SPF in the other piconets increases. We refer to this +L+ changed SPF as the “updated SPF” of the gateway in a pi- +L+ conet.
Similarly, an ideal gateway does not stay a fraction of time +L+ more than the SPF in a piconet. Thus, if the PPF of the gate-
Table 1
Calculation of fair share of the gateway in the two piconets of figure 3.
	Piconet I	Piconet II
Actual traffic rate	0.7	0.6
PPF	0.6	0.4
SPF	0.5	0.5
Updated PPF	0.6	0.4
Updated SPF	0.6	0.4
Fair share	0.6	0.4
Figure 3. Gateway shared between two piconets; traffic rates between slaves
and the master are shown.
way in the piconet is greater than the SPF, the gateway spends +L+ a fraction of time equal to the SPF in the piconet. The remain- +L+ ing PPF of the gateway (the difference of the PPF and the +L+ SPF) is redistributed in a fair manner among the other slaves +L+ of the piconet (if this other slave is a gateway, it is redistrib- +L+ uted to it if its SPF is greater than its PPF in the piconet). This +L+ may increase the PPF of these slaves. We refer to this changed +L+ PPF as the “updated PPF” of the slave in the piconet. In case +L+ there is no such redistribution, the updated PPF is equal to the +L+ PPF and the updated SPF is equal to the SPF.
The fair share can now be calculated from the “updated +L+ PPF” and the “updated SPF” as the minimum of these two +L+ quantities. Note that all these quantities – PPF, SPF, updated +L+ PPF, updated SPF and fair share–are dependent on the traffic. +L+ Any change in traffic demand of a unit may lead to a change +L+ in some of these quantities. We explain the calculation of the +L+ fair share using some examples.
An example is given in table 1, which shows the actual traf- +L+ fic rate, PPF, SPF, Updated PPF, Updated SPF and fair share +L+ of the gateway in the two piconets of figure 3. In Piconet II, +L+ the gateway has a PPF of 0.4, which is less than the SPF. In +L+ Piconet I, the gateway has a PPF of 0.6 and an SPF of 0.5. +L+ Thus, the extra scatternet presence fraction of the gateway in +L+ Piconet II (the difference between the SPF and the PPF) is +L+ given to Piconet I, which has a higher traffic rate than may +L+ be allowed by the SPF. This is reflected in the “updated SPF” +L+ values. Thus, the “fair share” of the gateway in Piconet I is +L+ 0.6 and in Piconet II is 0.4. The fair shares of the non-gateway +L+ slaves are equal to their PPF.
As another example, consider the scatternet consisting of +L+ 5 piconets with the traffic rates shown as in figure 4. As shown +L+ in table 2, gateway G2 has a PPF of 0.5 and an SPF of 0.4 in +L+ Piconet B. Thus, the “updated PPF” of G2 in Piconet B is 0.4. +L+ The extra PPF (= PPF − SPF) is added to the PPF of gateway
12	R. KAPOOR ET AL.
Figure 4. Scatternet with two gateways.
Table 2
Calculation of fair share of the gateways G1 and G2 in the scatternet of
figure 4.
Gateway G1	Piconet A	Piconet B	Piconet C
Actual traffic rate	0.4	0.6	0.1
PPF	0.25	0.5	0.1
SPF	0.4	0.5	0.1
Updated PPF	0.25	0.6	0.1
Updated SPF	0.25	0.65	0.1
Fair share	0.25	0.6	0.1
Gateway G2	Piconet B	Piconet D	Piconet E
Actual traffic rate	0.7	0.2	0.4
PPF	0.5	0.2	0.4
SPF	0.4	0.2	0.4
Updated PPF	0.4	0.2	0.4
Updated SPF	0.4	0.2	0.4
Fair share	0.4	0.2	0.4
G1 in Piconet B. The “updated PPF” of G1 in Piconet B is, +L+ thus, 0.6.
Also, gateway G1 has a PPF of 0.25 and an SPF of 0.4 +L+ in Piconet A. Thus, the “updated SPF” of G1 in Piconet A is +L+ 0.25. The extra SPF (= SPF − PPF) is added to the SPF of G1 +L+ in Piconet B. The “updated SPF” of G1 in Piconet B, is thus, +L+ equal to 0.65. The fair shares can now be easily calculated.
A division of the master’s polling and the gateway’s pres- +L+ ence based on PPF and SPF as described in this section takes +L+ into account the traffic demands of the slaves and the gate- +L+ ways and leads to fairness in the scatternet. In the next sec- +L+ tion, we introduce and describe an algorithm that aims to +L+ achieve such a fair distribution of bandwidth.
4. Description of algorithm
We first explain how the algorithm works in the case of a sin- +L+ gle piconet with no gateway. We then extend the algorithm +L+ to the case of a scatternet and explain how the coordination +L+ between the master and the gateways is achieved. We then +L+ prove the fairness of the algorithm.
4.1. Single piconet with no gateways
The polling algorithm is based on the master estimating the +L+ traffic rate between each slave and itself. This traffic rate is +L+ the sum of the traffic rates from the master to a slave and in
the reverse direction. We assume, in order to simplify the ex- +L+ planation of the algorithm, that traffic flows only from slaves +L+ to master; masters generate no traffic to slaves. The same al- +L+ gorithm also applies with little change when traffic flows in +L+ both directions (explained later).
The master uses a Round Robin polling scheme, with the +L+ modification that a slave is skipped if it does not belong to the +L+ “active list” of the master. The slaves are moved in and out +L+ of the active list on the basis of two variables that the master +L+ maintains for each slave. These two variables are:
r – estimate of the rate of traffic generated by the slave; +L+ N – estimate of the queue length of the slave.
When a slave is polled, the master–slave pair gets a chance +L+ to exchange a maximum amount of data in each direction, +L+ denoted by M. After each such polling phase, the master up- +L+ dates the values of N and r in the following manner:
For the slave just polled:
N=N+rτ −x, (1)
αr+(1−α)xT ,	x &lt;M,
αr+(1−α)xT + δ, x=M.
For other slaves:
N = N + rτ, (3)
where τ is the time elapsed since the last update, x is the +L+ amount of data exchanged during the poll phase, T is the total +L+ time elapsed since the last poll of the same slave, α is a para- +L+ meter used to smooth the rate estimation and δ is a parameter +L+ used to probe for more bandwidth. Note that x is the actual +L+ amount of data exchanged, which may be less than or equal +L+ to M, depending upon the number of packets in the slave’s +L+ queue. Since N is an estimate of the slave’s queue length and +L+ r is an estimate of the rate at which traffic is generated, N is +L+ increased at the rate of r (as in equations (1) and (3)). Also, +L+ when a slave is polled, N is decreased by the amount of data +L+ exchanged ((equation 1)).
After updating these values, the master determines the +L+ changes to be made to the active list. A slave is added or +L+ deleted from the active list depending upon whether its value +L+ of N is greater or smaller than a “threshold”. The value of +L+ this threshold is the minimum amount of data that the master +L+ would like the slave to have in order to poll it. We choose +L+ a value equal to a multiple of a DH5 packet for the thresh- +L+ old since this packet incurs least overhead (the selection of +L+ the value of the threshold is discussed further in the next sub- +L+ section). Thus, a slave is present in the active list if the mas- +L+ ter’s estimate of the value of N for the slave is greater than the +L+ threshold. This makes the simple Round Robin polling strat- +L+ egy adaptive to traffic and enables it to utilize bandwidth ef- +L+ ficiently, even when slaves have different rates of traffic. The +L+ maximum amount of data that can be exchanged at each poll, +L+ M, is also set equal to the threshold. Note that if the amount +L+ of data, x, in the slave’s queue is less than the threshold, the +L+ polling of the slave ends after this data has been exchanged.
r=	
(2)
A FAIR AND TRAFFIC DEPENDENT SCHEDULING	13
If the value of N is less than the threshold for all the slaves, +L+ then the slave whose value of N is estimated to take the small- +L+ est time to reach the threshold is polled, i.e., the slave for +L+ which the value of (Threshold − N)/r is the smallest.
The master now goes to the next slave according to the +L+ Round Robin ordering of slaves. If the slave is present in the +L+ active list, it is polled. Else, the procedure is repeated for the +L+ next slave in the Round Robin ordering.
Also, note that if the amount of data sent by the slave x +L+ is equal to M, r is increased by a small amount, δ. This is +L+ basically an attempt by the slave to probe for more bandwidth +L+ if it is able to send data at the present rate. The usefulness +L+ of this increase is evident in the proof of fairness in the next +L+ section. The value of δ chosen is 0.15 and that of α is 0.65. +L+ We also discuss the rationale behind choosing these values in +L+ the proof of fairness.
If traffic flows in both directions, i.e., from the slaves to +L+ the master and in the reverse direction, x is the average of +L+ the amount of data exchanged in the two directions, r refers +L+ to the average of the rate-estimations of the two directions +L+ and N refers to the average of the queue length estimates of +L+ the two directions. Also, if the number of packets in either +L+ direction is less than the threshold, the polling of the slave +L+ continues till in both directions, (a) there is no more data to +L+ send or (b) amount of data equal to the threshold has been +L+ exchanged.
The initial value of N is set to the threshold (to ensure that +L+ slaves get polled at the beginning) and that of r is set to 0.25 +L+ (as a reasonable value). Note that the algorithm converges to +L+ the fair share, but a careful selection of initial values makes +L+ the initial convergence faster.
Another advantage of such a scheme is that it may allow +L+ the master to go into a power-saving mode if it realizes that no +L+ slave has sufficient packets to send, i.e., if N is smaller than +L+ the threshold for all slaves. Though we do not explore this +L+ option in this paper, it may be useful since Bluetooth devices +L+ are expected to work in power-constrained environments.
To improve the algorithm, we add a heuristic to it. The +L+ maximum number of polling cycles that a slave is not polled +L+ is bounded. If a slave generates a large burst of data occa- +L+ sionally and then does not generate any data for a long time, +L+ the value of r for the slave may be very low. This may cause +L+ the value of N for the slave to be lower than the threshold +L+ for a long time. By limiting the maximum number of cycles +L+ missed by the slave, we make sure that such a behavior of the +L+ slave does not lead to its starvation. In the experiments, this +L+ value is taken to be equal to 5 cycles. We now explain how +L+ the above algorithm works in a scatternet.
4.2. Scatternet
Scheduling of gateways using Rendezvous Points. Before +L+ describing how the algorithm works in a scatternet, we briefly +L+ discuss the notion of Rendezvous Points (RPs) described +L+ in [4]. A RP is a slot at which a master and a gateway have +L+ agreed to meet, i.e., at this slot, the master will poll the gate- +L+ way and the gateway will listen to the master. In [4], RPs are
implemented using the SNIFF mode of Bluetooth, but we im- +L+ plement RPs using the HOLD mode [ 10]. In the HOLD mode, +L+ the slave does not have to listen to the master for a certain time +L+ period and may use this time to visit other piconets. Prior to +L+ entering the HOLD mode, the master and the slave agree on +L+ the time duration the slave remains in the HOLD mode. We +L+ implement our algorithm using RPs as described below.
The working of the algorithm in a scatternet is very similar +L+ to its operation in a piconet. The master continues to poll the +L+ non-gateway slaves in the same manner as described in the +L+ previous section with the modification that a gateway is polled +L+ at a Rendezvous Point. Each RP is a slot at which a particular +L+ gateway is polled and a master has different RPs for each of its +L+ gateways. These RPs are always unique (i.e., a master cannot +L+ have the same RP with more than one gateway). Since the +L+ gateway must be polled at the RP, this has implications in the +L+ polling of the other slaves (discussed later). Once a gateway +L+ has been polled, the master continues with the polling of the +L+ other slaves in the same manner as described in the previous +L+ section, i.e., it checks its active list to see if the next slave in +L+ the polling cycle is to be polled and so on.
In order to divide its time among different piconets in a +L+ fair manner, the gateway performs similar calculations as de- +L+ scribed in the earlier section for the master. The gateway +L+ maintains values of N and r for each piconet it belongs to and +L+ these values are updated each time a gateway is polled (i.e., +L+ at each RP). Thus, the calculations performed by a gateway at +L+ each RP are:
For the piconet in which the gateway was just polled:
N=N+rτ −x, (4)
x
αr+(1−α)T, x &lt;M,
αr+(1−α) xT + δ, x=M. +L+ For other piconets:
N = N + rτ, (6)
where τ is the time elapsed since the last update, x is the +L+ amount of data exchanged during the poll phase, T is the to- +L+ tal time elapsed since the gateway was polled in the same pi- +L+ conet, and α and δ are as defined earlier.
Moreover, at each RP, the gateway and the master negoti- +L+ ate the next RP between them. The assignment of this next +L+ RP takes into account the fairness between (a) the gateway +L+ and other slaves in a piconet and (b) the presence of the gate- +L+ way in different piconets. Also, we again employ a heuristic +L+ that improves the algorithm. When the next RP is being nego- +L+ tiated, we keep a bound on the maximum value this can take. +L+ This prevents a piconet from not being visited by a gateway +L+ for a long time. The maximum value of this next RP used in +L+ our experiments is 400 slots.
We now see how the master and the gateway use the in- +L+ formation that they have to achieve fairness in the scatternet. +L+ When a gateway is polled at a RP, the gateway and the master +L+ do the following.
r=	
(5)
14	R. KAPOOR ET AL.
(i) Gateway. The gateway calculates the number of slots, +L+ Nthresh after which N for the piconet will become greater +L+ than the threshold; Nthresh = (threshold − N)/r, where +L+ threshold is as explained in the previous section, N and +L+ r are values maintained by the gateway for the piconet. +L+ The gateway makes use of this value and does not visit +L+ a piconet till its estimate of N for the piconet becomes +L+ greater than the threshold. This is similar to the algo- +L+ rithm used by the master in which a slave is not polled till +L+ the master’s estimate of N for the slave becomes greater +L+ than the threshold. Thus, the gateway tries to divide its +L+ time between the piconets in a fair manner, i.e., accord- +L+ ing to the SPFs. Note that Nthresh may be negative if N +L+ is greater than the threshold. Also, Nthresh is allowed to +L+ have a maximum value of 400.
Moreover, each time a gateway visits a piconet, it knows +L+ the RPs for the other piconets it belongs to (except right +L+ at the beginning or when the gateway is added to another +L+ piconet).
(ii) Master. The master calculates the number of slots after +L+ which the gateway can be polled such that the fairness +L+ with other slaves is maintained. It adopts the following +L+ procedure to achieve this:
It maintains a counter, num_slots (which is initialized +L+ to 0) and checks the value of N for each slave, in a cyclic +L+ order, starting from the slave after the current gateway in +L+ the cyclic order to the slave before the current gateway. +L+ The master checks if the value of N for the slave will be +L+ greater than the threshold after num_slots slots. If this +L+ condition is true, num_slots is incremented by twice the +L+ value of the threshold. After incrementing num_slots, the +L+ master also checks to see if it has a RP with any gate- +L+ way whose value is equal to num_slots and increments +L+ num_slots by twice the value of the threshold if this is +L+ true. This ensures that the master has a unique RP for +L+ each of its gateways. Note that num_slots is incremented +L+ by twice the value of the threshold since the master ex- +L+ pects to exchange threshold slots of data with a slave in +L+ each direction.
The master uses the above procedure to estimate the num- +L+ ber of slaves who will have their value of N greater than +L+ the threshold when the master polls the slaves in their +L+ cyclic order starting from the gateway just polled. The +L+ value of num_slots determines the number of slots which +L+ the master expects to use in polling the other slaves in +L+ one cycle before polling the gateway again and is thus, +L+ used by the master to maintain fairness between the gate- +L+ way and the other slaves in the piconet. Again, note that +L+ num_slots is allowed to have a maximum value of 400.
The master and the gateway now exchange the informa- +L+ tion they have to calculate their next RP. This exchange takes +L+ place using the LMP_hold_req PDU of the LMP (Link Man- +L+ ager Protocol) layer. This PDU carries a hold instant and a +L+ hold time, which are used to specify the instant at which the +L+ hold will become effective and the hold time, respectively. +L+ When the master is sending a packet to a gateway, the value +L+ of num_slots can be sent after hold instant and hold time in +L+ the packet. The master also sends the values of its RPs with +L+ its other gateways in the packet. Similarly, the gateway sends +L+ the master the values of its RPs with other piconets and the +L+ value of Nthresh also in an LMP_hold_req PDU. The master +L+ now knows all the RPs of the gateway; similarly, the gateway +L+ knows all the RPs of the master.
Note that the above information exchange requires a min- +L+ imal change in the Bluetooth specifications that the contents +L+ of the LMP_hold_req PDU need to be enhanced. This PDU is +L+ 1-slot in length; thus, some bandwidth of the master is wasted +L+ in sending these PDUs. This wasted bandwidth can be re- +L+ duced by increasing the value of threshold, i.e., the maximum +L+ data that a slave and a master may exchange in each direc- +L+ tion during one poll of the slave. On the other hand, a large +L+ value of the threshold will lead to larger delays for packets. +L+ Thus, we have a tradeoff here. We choose a threshold value +L+ equal to three times a DH5 packet. The effect of this wasted +L+ bandwidth can be seen in the experiments section where the +L+ piconet capacity used is slightly less than 1. Note that we +L+ pay a small price here to get perfect coordination between the +L+ master and the gateway and also to get a high degree of fair- +L+ ness in the system, as the experiments later demonstrate.
Now, the master and the gateway both have complete in- +L+ formation. So, each of them calculates the next RP in the +L+ following manner:
They take the maximum value out of num_slots and Nthresh +L+ and as long as this value is the same as one of the RPs (note +L+ that all relevant RPs are known to both the master and the +L+ gateway), the value is incremented by 2 · threshold. The value +L+ at the end of this small procedure is the next RP between the +L+ gateway and the master. Since this value takes into account +L+ both Nthresh and num_slots, it incorporates both the fairness +L+ of the master’s polling and the gateway’s presence.
Note that the value of num_slots calculated by the master is +L+ just an estimate (the master assumes that each slave included +L+ in the calculation of num_slots will exchange threshold slots +L+ of data with the master in each direction, but this may not be +L+ true). Thus, the master may have polled all the slaves that had +L+ to be polled before the RP of the gateway (according to the +L+ estimate in the calculation of num_slots) and still be left with +L+ some slots before the RP. In this case, the master just contin- +L+ ues polling the slaves in their cyclic order and polls the gate- +L+ way when the time for the RP arrives. Note that this means +L+ that the master may have to force a slave to send a packet +L+ smaller than a certain length. For example, if two slots are +L+ left for the RP, then the master will send a 1-slot packet and +L+ ask the slave being polled to do the same. Note that the Blue- +L+ tooth header has 4 bits to represent the packet type and these +L+ can represent 16 packet types. For ACL links, 10 (7 data, +L+ 3 control packets) of the packet types are defined. We use 2 +L+ of the remaining bit sequences to send packets that force the +L+ slave to send packets smaller than or equal to a certain length. +L+ This is shown in table 3.
From table 3, we see that this procedure is adopted if the +L+ number of slots left for the RP is less than 10 (if the number +L+ of slots left for the RP is greater than or equal to 10, then the
A FAIR AND TRAFFIC DEPENDENT SCHEDULING	15
Table 3
Procedure adopted by the master if slots left for the RP is less than 10.
Slots left for RP	Maximum	Maximum
length of packet	length of packet
sent by master	sent by slave
2	1	1
4	1	1
6	3	3
8	3	3
slave’s packet length does not have to be restricted). Thus, +L+ if the slots left for the RP is 2, the master can send a packet +L+ of maximum length = 1 and the gateway can send a packet +L+ of maximum length = 1 and so on. Note that for reasons of +L+ fairness, the maximum packet length for the master and the +L+ gateway is the same. Since the master needs to restrict the +L+ maximum length of the gateway’s packet to either 1 or 3 (as +L+ shown in table 3), we need 2 packet types to achieve this. This +L+ procedure effectively suspends the polling of a slave to honor +L+ a RP with a gateway. The polling of the slave continues after +L+ the gateway has been polled.
In addition, a gateway may lose a slot in switching from +L+ one piconet to another. This loss is unavoidable since piconets +L+ are in general, not synchronized in time. In the experiments in +L+ the paper, we set the value of the threshold to three times the +L+ payload of a DH5 packet, which can give a switching loss of +L+ about 3 % at heavy loads (every 2 · threshold slots, the gateway +L+ loses about one slot in switching). At light loads, this switch- +L+ ing loss does not lead to inefficiency since the sum of the fair +L+ shares of the gateway in all its piconets is less than 1 and even +L+ after the switching loss, the gateway is able to obtain its fair +L+ share. The simulations in the next section do not take this +L+ switching loss into account and thus, the bandwidth received +L+ by the gateway under heavy loads will be a little smaller than +L+ the one shown in the results.
4.3. Proof offairness
We now prove that the above algorithm leads to a max-min +L+ fair distribution of the bandwidth of a scatternet among units. +L+ We start by proving this in the case of a piconet. In the next +L+ step, we will extend the proof to the general case of a scatter- +L+ net.
4.3.1. Fairness in a piconet
Let us introduce the following notation:
S: number of slave units in the piconet; +L+ gi : rate-demand of the ith unit;
ηi: rate achieved by the ith unit;
ri: rate-estimation of the ith unit (as defined in equa- +L+ tion (2)),
where ηi and ri are average values.
Slave unit i is referred to as “satisfied”, if it achieves it rate +L+ demand, i.e., ηi = gi; else, the slave unit is referred to as +L+ “unsatisfied”. Also, in the proof that follows, “slot” refers to +L+ “Bluetooth slot”; “unit” and “slave unit” may be used inter- +L+ changeably.
If there is one slave unit in a piconet, then it will always get +L+ polled and hence, the algorithm is fair. We prove the fairness +L+ when there are two or more slave units.
We first make the following observations:
(a) If a unit has a rate-estimation, r &gt; 0.25, it will never +L+ achieve a lesser rate than any other unit.
r is an estimation of the average number of slots of traf- +L+ fic that a master–slave pair will generate per slot in each di- +L+ rection. Thus, a rate of 0.25 means that a master–slave pair +L+ generates, on the average, “threshold” slots of traffic in each +L+ direction in every 4 · threshold slots. Suppose a piconet has +L+ two slaves, and the first has a rate-estimation, r &gt; 0.25, then +L+ the first slave will be polled at least once in every 4 · threshold +L+ slots, i.e., will get on the average at least threshold polling +L+ slots out of every 2 · threshold, regardless of the r of the other +L+ slave (since N increases at the rate of r, N will increase by +L+ at least 0.25 · 4 · threshold = threshold; thus, the slave will +L+ enter into the “active list” in 4 · threshold slots). Thus, it will +L+ never achieve a lesser rate than another unit. It is easy to see +L+ that this property would be true if there were more than two +L+ slaves (two slaves is the worst case).
(b) For δ &gt; 0.1 and α &gt; 0.6, an unsatisfied slave will tend to +L+ a rate-estimation of at least 0.25.
For an unsatisfied slave, the second part of equation (2) +L+ (when x = M) is always used for updating the rate. Thus, if +L+ ri is the ith rate-estimation:
rn+1 = αrn + (1 − α) T + δ.
This leads to (as n becomes very large):
r=(1−α)M
Thus, for δ &gt; 0.1 and α &gt; 0. 6, for any value of T, the +L+ value of r tends to at least 0.25.
(c) As long as there is an unsatisfied unit, the utilization of
the system capacity is 1 (for δ &gt; 0.15 and α &gt; 0.65).
Consider a piconet consisting of seven slave units, in +L+ which the first unit, unit1 is unsatisfied. From (a) and (b), +L+ unit1 will never achieve a lesser rate than any other unit; this +L+ means that it will be polled at least once for each time the +L+ other slaves are polled. The value of T (as in equation (2)) for +L+ unit1 is thus, at most, 14 · threshold. For this value of T and +L+ for δ = 0.15 and α = 0. 65, r for unit1 tends to at least 0.5. +L+ A value of r = 0.5 for a slave unit means that it can be polled +L+ all the time (since N increases at the rate of r, N will increase +L+ by at least 0.5 · 2 · threshold = threshold; thus, the slave will +L+ enter into the “active list” in 2 · threshold slots, which is also +L+ the time of its polling). Thus, the system capacity is totally +L+ utilized. If there were less than 7 slave units, the value of T +L+ would be smaller (than 14 · threshold), and r would tend to a +L+ higher value (than 0.5).
αn−k
Tk	+
δ	 &gt;
1−α
∞
E
k=0
δ
	.
1−α
16	R. KAPOOR ET AL.
We choose values of δ and α to satisfy the above proper- +L+ ties, i.e., δ = 0.15 and α = 0.65.
The following statements hold.
(i) Units with the same rate-demand achieve the same aver- +L+ age rate:
gi = gj ⇒ ηi = ηj.
We prove this by contradiction. Suppose there are two units, +L+ unit1 and unit2 with rate demands g1 and g2, respectively, +L+ such that g1 = g2. Also, suppose one unit achieves a higher +L+ average rate than the other, η1 &gt; η2.
Now, unit2 does not achieve its rate-demand (since η1 &gt; +L+ η2). Unit1 may or may not achieve its rate demand. From +L+ property (b), unit2 will always tend to a value at least equal to +L+ 0.25, since it is an unsatisfied slave. Using property (a), this +L+ implies that η2 cannot be less than η1. This is a contradiction.
(ii) Units with a higher rate-demand achieve an average rate +L+ at least equal to that achieved by units with a lower rate- +L+ demand:
gi &gt; gj ⇒ ηi i ηj.
This can be proved by contradiction in the same manner as in +L+ part (i).
Now, without loss of generality, let us partition the slave +L+ units into two sets, S1 and S2, in such a way that units in S1 +L+ are satisfied, while units in S2 are not.
•	If the set S2 is empty, than all the units achieve their rate- +L+ demand and the system is fair.
•	If the set S2 is not empty, then using statements (i) and (ii), +L+ all units share the bandwidth in a fair manner. Moreover, +L+ since S2 contains at least one unit, the total system capac- +L+ ity is utilized. Hence, it is not possible to increase the rate +L+ of a unit in S2 without decreasing the rate of some other +L+ unit.
4.3.2. Fairness in a scatternet
The proof of fairness for a scatternet follows trivially from +L+ that for a piconet. We make the following two observations:
(1) The gateway visits a piconet only after the estimation +L+ of N for the piconet becomes greater than the threshold (it +L+ calculates Nthresh while determining the next RP). In other +L+ words, the “virtual master” (gateway) does not poll (visit) its +L+ “virtual slave” (master) till the estimate of N becomes greater +L+ than the threshold. This is similar to the algorithm used by +L+ the master to poll the slaves in which a slave is not polled till +L+ its estimate of N becomes greater than the threshold. Thus, +L+ the gateway divides its presence among its piconets in a fair +L+ manner, i.e., according to the SPF. Note that if the PPF for +L+ a gateway in a piconet is less than its SPF, the master does +L+ not poll the gateway for more than the PPF. Thus, the appar- +L+ ent rate demand and SPF for the gateway in the piconet are +L+ reduced. This may increase the SPF of the gateway in other +L+ piconets. In this case, the gateway divides its presence ac- +L+ cording to the updated SPFs.
(2) While calculating the next RP for a gateway, the mas- +L+ ter calculates the num_slots value which estimates the num- +L+ ber of slaves in one polling cycle (starting from the slave after +L+ the gateway in the polling cycle) who will have their values +L+ of N greater than the threshold at the estimated time of their +L+ poll. This achieves fairness between the gateway and the non- +L+ gateway slaves. Also, the master continues to use the same +L+ algorithm for polling non-gateway slaves in a scatternet as +L+ described for a piconet in section 4.1. This maintains fairness +L+ between non-gateway slaves, i.e., the division is done accord- +L+ ing to the PPFs (or the updated PPFs).
4.4. Overhead/limitations of the algorithm
The rate calculations will lead to a higher load on the system. +L+ Also, the algorithm does not take into account SCO links. We +L+ believe (and as has been shown in [6]) that ACL links are +L+ capable of carrying voice with small delays. The controlled +L+ channel access in Bluetooth can ensure good support of voice +L+ using ACL links. Also, scheduling in a scatternet where SCO +L+ links are allowed may not be feasible. Since SCO links re- +L+ quire a periodic reservation of two slots every two, four or +L+ six slots, meeting the demands of such a link with a gateway +L+ may be impossible when the gateway is visiting some other +L+ piconet.
5. Experiments and results
In this section, we present simulation results, which show that +L+ the algorithm satisfies the fairness criteria described earlier. +L+ We start with simple topologies that illustrate the behavior of +L+ the algorithm and then show that it also works well in more +L+ complex topologies. There are three topologies that the exper- +L+ iments focus on and these demonstrate the behavior of the al- +L+ gorithm – a topology with (a) a gateway belonging to two pi- +L+ conets, (b) a gateway belonging to three piconets and (c) a pi- +L+ conet having two gateways. The experiments also show the +L+ adaptivity of the algorithm, i.e., how quickly the algorithm +L+ adapts to changing traffic demands of slaves.
In the experiments, we specify the “rate of a slave”, which +L+ refers to the sum of the rates at which a slave generates data +L+ for a master (i.e., the rate demand of a slave) and the mas- +L+ ter generates data for the slave. Moreover, unless mentioned +L+ otherwise, we assume that the traffic rate from a slave to a +L+ master is equal to that from the master to the slave. Thus, a +L+ slave having a rate of 0.4 means that the slave generates data +L+ at the rate of 0.2 Bluetooth slots per slot and the master also +L+ has a rate demand of 0.2 towards the slave. As we show in the +L+ section on asymmetric traffic, the algorithm works well even +L+ if these two rates are not the same.
The simulation environment used in our experiments is +L+ NS-2 [8]. We have augmented NS-2 with the Bluetooth +L+ model. The simulator models the Bluetooth baseband, LMP +L+ and L2CAP layers and enables the creation of piconets and +L+ scatternets. The model contains most of the standard features +L+ of Bluetooth like Frequency Hopping, Multi-Slot Packets, +L+ Fast ARQ (Automatic Retransmission Query). Note that as +L+ mentioned earlier, in our simulator, the switching loss asso- +L+ ciated with the gateway moving from one piconet to another
A FAIR AND TRAFFIC DEPENDENT SCHEDULING	17
Figure 5. Example scatternet.
is not taken into account. This effect can lead to the gate- +L+ way losing up to 3% of slots at heavy loads. The experiment +L+ results are thus, a slight overestimate.
In the experiments, all traffic generated is CBR. Each ex- +L+ periment is run for a system time of 32 sec. In the experi- +L+ ments, the term “slave” refers to a non-gateway slave; a gate- +L+ way slave is referred to as “gateway”. Also, in experiments +L+ where the PPF and the SPF values (and not the updated PPF +L+ and the updated SPF) are shown, the PPF and the updated PPF +L+ are equal and the SPF and the updated SPF are also equal. In +L+ the graphs, “BW” in the index stands for bandwidth, “GW” +L+ stands for gateway.
5.1. Single gateway in two piconets
We first consider the simple topology shown in figure 5, +L+ which consists of two piconets, numbered I and II, connected +L+ by a single gateway. We consider various cases by changing +L+ the traffic and the number of slaves in the piconets.
Experiment 1. Adaptation between gateway and
non-gateway slave traffic
Each piconet has one non-gateway slave that generates very +L+ high traffic, with rate equal to 1, to the master. The gateway +L+ has equal traffic to both masters. We vary the gateway traffic +L+ to show the fair sharing of the piconet bandwidth between the +L+ gateway and the slave. We show the results for one piconet +L+ since the two piconets are exactly symmetric.
Figure 6(a) shows the sharing of bandwidth between the +L+ gateway and slave for different values of gateway traffic. It +L+ also shows the fair share of the slave and the total fraction +L+ of the bandwidth obtained by the gateway and the slave in +L+ the piconet. It can be seen that the slave obtains a bandwidth +L+ equal to its fair share for different values of gateway traffic. +L+ Moreover, the sum of the bandwidths obtained by the slave +L+ and the gateway is nearly equal to 1. The reason for this to +L+ be slightly less than 1 is that some of the piconet capacity is +L+ used in sending LMP _ hold _req PDUs of the LMP layer.
In figure 6(b), the comparison of the fraction of the band- +L+ width obtained by the gateway to its SPF (PPF and SPF are +L+ equal) is shown. Figure 6(b) shows that the gateway gets al- +L+ most equal to its fair share of the bandwidth for all values +L+ of traffic. Again, the reason that the gateway obtains slightly +L+ less than its fair share is because some of the slots are used +L+ for LMP PDUs. This also explains why the gateway obtains +L+ slightly less than the slave in figure 6(a).
Figure 6. (a) Sharing of bandwidth between gateway and slave. (b) Compar- +L+ ison of fraction of bandwidth obtained to SPF for the gateway.
Experiment 2. Different traffic to piconets
The same topology as in the previous case, but each slave has +L+ a traffic rate of 0.3 to the master. The gateway has a fixed traf- +L+ fic rate of 0.2 to the master of Piconet I and variable traffic to +L+ the other master. The PPF and SPF of the gateway in the first +L+ piconet are, thus, both equal to 0.2. The traffic in Piconet I +L+ does not change and the gateway and the slave get a constant +L+ fraction of 0.2 and 0.3 of the piconet bandwidth, respectively.
Figure 7(a) shows the sharing of bandwidth between the +L+ gateway and slave for different values of gateway traffic, +L+ while figure 7(b) shows the comparison of the fraction of the +L+ bandwidth obtained by the gateway in Piconet II to its SPF +L+ and PPF. From the graphs, we can see that when the gate- +L+ way has different traffic to piconets, it divides its presence +L+ among the piconets according to the traffic offered and in a +L+ fair manner (again, the gateway obtains slightly less than its +L+ fair share due to the LMP PDUs). Also, the gateway makes +L+ use of the lower traffic offered by the slave in Piconet II to +L+ obtain a higher share of the bandwidth in Piconet II.
Experiment 3. Different number of slaves
Piconet I has 3 slaves, while the number of slaves in Piconet II +L+ is variable. Each slave generates traffic to the master at the +L+ rate of 0.2. The gateway has a traffic rate of 0.3 to Piconet I +L+ and 0.8 to Piconet II. The PPF and SPF of the gateway in +L+ Piconet I are, thus, 0.2 and 0.3, respectively. In Piconet II, the +L+ value of PPF changes depending upon the number of slaves.
In Piconet I, the slaves get a bandwidth fraction of 0.2 +L+ and the gateway gets 0.3. Figure 8(a) shows the sharing
18	R. KAPOOR ET AL.
Figure 7. (a) Sharing of bandwidth between gateway and slave in Piconet II.
(b) Comparison of fraction of bandwidth obtained by the gateway to SPF and
PPF in Piconet II.
of bandwidth between the gateway and each slave in Pi- +L+ conet II. Figure 8(b) shows the comparison of the fraction +L+ of the bandwidth obtained by the gateway in Piconet II to the +L+ SPF and PPF. The gateway receives a fraction of the band- +L+ width almost equal to its fair share. Also, as the number of +L+ slaves increases, the fraction of the bandwidth received by +L+ the gateway (and each slave) reduces in a fair manner.
Experiment 4. Asymmetric traffic
We now consider a case where the traffic rates from Master +L+ to Slave and Slave to Master are different (asymmetric traf- +L+ fic). We consider the same topology as in experiment 2 of the +L+ current section, with the non-gateway slaves having the same +L+ rate as in experiment 2. The gateway has a fixed traffic rate of +L+ 0.2 to the master of Piconet I and variable traffic to the other +L+ master. The variable traffic is such that traffic from Master +L+ to Slave has a rate of 0.1 and traffic from Slave to Master +L+ varies.
Figure 9 shows the comparison of bandwidth fraction ob- +L+ tained by the gateway in this experiment versus that obtained +L+ by the gateway in experiment 2 in Piconet II for different val- +L+ ues of gateway traffic (which is the sum of master to gateway +L+ and gateway to master traffic rates). We see that the fraction +L+ is slightly lower than the fraction obtained in experiment 2. +L+ Asymmetric traffic leads to wastage of slots, since an empty +L+ slot is returned in one direction where there is no data to send. +L+ It can be seen though, that the gateway still behaves in an ap-
Figure 8. (a) Sharing of bandwidth between gateway and slave in Piconet II.
(b) Comparison of fraction of bandwidth obtained by the gateway to SPF and
PPF in Piconet II.
Figure 9. Comparison of fraction of bandwidth obtained by gateway in this
experiment with that in experiment 2 in Piconet II.
proximately fair manner. All other bandwidth fractions for +L+ slaves and the gateway are the same as in experiment 2.
5.2. Single gateway shared between three piconets
We now consider a topology, where a gateway is shared be- +L+ tween 3 piconets, numbered I, II and III. Piconet I has 5, Pi- +L+ conet II has 1 and Piconet III has 4 slaves. Each slave has +L+ a traffic rate of 0.2. The gateway has a traffic rate of 0.2 to +L+ Piconet I, 0.3 to Piconet III and a variable rate to Piconet II. +L+ All traffic is symmetric (same from master to slave and from +L+ slave to master).
A FAIR AND TRAFFIC DEPENDENT SCHEDULING	19
Figure 10. Bandwidth fraction received by gateway in the three piconets.
Figure 12. Fraction of bandwidth and fair share of GW1 and GW2 in Pi- +L+ conet II.
Figure 11. Example scatternet topology.
Figure 10 shows the fraction of bandwidth obtained by the +L+ gateway in each piconet with increasing gateway traffic rate to +L+ Piconet II. It also shows the PPF and the Updated SPF of the +L+ gateway in Piconet II. We do not show the fair shares of the +L+ gateway in Piconet I and III since they are constant (0.16 and +L+ 0.2, respectively). It can be seen that the gateway manages +L+ to get close to its fair share in the 3 piconets. The slaves in +L+ Piconet I get a bandwidth fraction of 0.16 and the slaves in +L+ Piconet II and III get a bandwidth fraction of 0.2 (all these are +L+ equal to their fair shares).
5.3. Piconet with two gateways
We now show the working of the algorithm in a piconet hav- +L+ ing 2 gateways, as shown in figure 11. Piconets I, II and III +L+ have 6, 2 and 4 non-gateway slaves, respectively. There are +L+ two gateways, GW 1 between Piconets I and II; and GW 2 be- +L+ tween Piconets II and III. All slaves have a traffic rate of 0.2.
GW 1 has a traffic rate of 0.2 in Piconet I and 0.5 in Piconet II.
GW 2 has a traffic rate of 0.2 in Piconet III. We vary the traf- +L+ fic rate of GW 2 in Piconet II and show the fair sharing of +L+ bandwidth.
Figure 12 shows the fraction of bandwidth obtained by +L+ GW 1 and GW 2 in Piconet II compared to their fair shares. +L+ The x-axis denotes GW 2 traffic in Piconet II. It can be seen +L+ that the bandwidth fractions obtained are very close to the +L+ fair value. The non-gateway slaves of Piconet II receive a +L+ bandwidth fraction of 0.2, which is equal to their fair share +L+ (not shown in the figure). The bandwidth fraction received by +L+ slaves in Piconets I and III does not change for different val- +L+ ues of GW2 traffic in Piconet II. The fair share of each slave +L+ (including the gateway) in Piconet I is 0.14 and in Piconet III
Figure 13. Actual rate estimation of the gateway and its ideal value.
is 0.2; the bandwidth fraction received by each slave is very +L+ close to these fair shares.
5.4. Adaptivity to changing traffic demands
We now show how quickly the algorithm is able to adapt to +L+ changing traffic. We again consider the scenario of experi- +L+ ment 1 of section 5.1, consisting of two piconets, each hav- +L+ ing a non-gateway slave, connected by a single gateway. The +L+ non-gateway slaves have a traffic rate of 1; the gateway has +L+ equal traffic to both the masters. We vary the traffic rate of +L+ the gateway as time progresses: for the first 2.5 seconds, the +L+ gateway’s rate is 0. 1, for the next 2.5 seconds, it is 0.5 and for +L+ the remaining time, it is 0.3.
Figure 13 shows the actual rate estimation of the gateway +L+ (and its ideal value) versus time. It can be seen that the rate +L+ estimation adapts very quickly to the new rate. For example, +L+ when the rate changes from 0.1 to 0.5, the rate estimation +L+ reaches a value of 0.45 in about half a second after 2.5 sec. +L+ Thus, the algorithm adapts to quickly changing traffic.
6. Conclusions
This paper proposed a distributed scatternet-scheduling algo- +L+ rithm that adapts to non-uniform and changing traffic. This
20	R. KAPOOR ET AL.
algorithm provides an integrated solution for both intra- and +L+ inter-piconet scheduling and can be implemented using the +L+ HOLD mode of Bluetooth. Through analysis and simulations, +L+ we showed that the algorithm is traffic-adaptive and results in +L+ a fair allocation of bandwidth to units. We explained earlier +L+ that the algorithm may allow a unit to go into a power-saving +L+ mode.
In future, we would like to explore this option, which also +L+ assumes importance since Bluetooth devices will most likely +L+ operate in a power-constrained environment. As future work, +L+ we would also like to evaluate the performance of TCP and +L+ other kinds of traffic on our algorithm. We are also working +L+ towards interfacing the algorithm with requirements of higher +L+ layers. In this respect, we are working towards providing QoS +L+ support using the algorithm.
References
[1] S. Baatz, M. Frank, C. Kehl, P. Martini and C. Scholz, Adaptive scat- +L+ ternet support for Bluetooth using sniff mode, in: Proc. of IEEE LCN +L+ (2001).
[2] A. Das, A. Ghose, A. Razdan, H. Saran and R. Shorey, Enhancing per- +L+ formance of asynchronous data traffic over the Bluetooth wireless ad- +L+ hoc network, in: Proc. ofIEEE INFOCOM’2001 (2001).
[3] J. Haartsen, BLUETOOTH – the universal radio interface for ad hoc +L+ wireless connectivity, Ericsson Review 3 (1998) 110–117.
[4] P. Johansson, M. Kazantzidis, R. Kapoor and M. Gerla, Bluetooth – an +L+ enabler for personal area networking, IEEE Network Magazine, Wire- +L+ less Personal Area Network (September 2001).
[5] M. Kalia, D. Bansal and R. Shorey, MAC scheduling and SAR policies +L+ for Bluetooth: A master driven TDD pico-cellular wireless system, in: +L+ Proc. of 6th IEEE International Workshop on Mobile Multimedia Com- +L+ munications (MOMUC) (1999).
[6] R. Kapoor, L. Chen, Y. Lee and M. Gerla, Bluetooth: carrying voice +L+ over ACL links, in: Proc. of MWCN (2002).
[7] A. Mayer, Y. Ofek and M. Yung, Approximating max-min fair rates via +L+ distributed local scheduling with partial information, in: Proc. of IEEE +L+ INFOCOM (1996).
[8] NS-2 simulator, http://www.isi.edu/nsnam/ns/
[9] A. Racz, G. Miklos, F. Kubinszky and A. Valko, A pseudo-random +L+ coordinated scheduling algorithm for Bluetooth scatternets, in: Proc. +L+ ofMobiHoc (2001).
[10] Specifications of the Bluetooth System – core, Vol. 1, v. 1.1, www. +L+ Bluetooth.com
[11] W. Zhang and G. Cao, A flexible scatternet-wide scheduling algorithm +L+ for Bluetooth networks, in: Proc. ofIEEE IPCCC (2002).
Rohit Kapoor received his Bachelor degree in com- +L+ puter science in 1999 from the University of Roor- +L+ kee, India. He is currently a Ph.D. candidate at the +L+ University of California, Los Angeles (UCLA). His +L+ research focuses on Bluetooth-based personal area +L+ networks. He is a member of the Network Research +L+ Lab at UCLA.
E-mail: rohitk@cs.ucla.edu
Andrea Zanella received the Ph.D. degree in tele- +L+ communication engineering from the University of +L+ Padova, Italy, in 2002. Prior to that he received +L+ the Dr. Ing. degree (comparable to Master degree) +L+ in computer engineering in 1998, still from the Uni- +L+ versity of Padova. He spent nine months, in 2001, as +L+ post-doc researcher at the Department of Computer +L+ Science of the University of California, Los Ange- +L+ les (UCLA), where he was engaged in research on +L+ Wireless Networks and Wireless Access to Internet +L+ under the supervision of Prof. Mario Gerla. Currently, he is a research fellow +L+ in the Department of Information Engineering of the University of Padova, +L+ Italy. His research interests are mainly focused on topics related to wireless +L+ and mobile networking. In particular, in the last period, he has been working +L+ on the performance aspects of wireless personal area networks based on the +L+ Bluetooth standard.
E-mail: zanella@dei.unipd.it
Mario Gerla is a professor in the Computer Science +L+ Department at UCLA. He received his graduate de- +L+ gree in engineering from the Politecnico di Milano +L+ in 1966, and his M.S. and Ph.D. degrees in engi- +L+ neering from UCLA in 1970 and 1973, respectively. +L+ He joined the faculty of the UCLA Computer Sci- +L+ ence Department in 1977. His current research is +L+ in the area of analysis, design and control of com- +L+ munication networks. Ongoing projects include the +L+ design and evaluation of QoS routing and multicast +L+ algorithms for IP domains, the design and evaluation of all-optical network +L+ topologies and access protocols, the design of wireless mobile, multimedia +L+ networks for mobile computing applications, and the development of mea- +L+ surement methods and tools for evaluating the performance of high-speed +L+ networks and applications.
E-mail: gerla@cs.ucla.edu
