A Dependability Perspective on Emerging Technologies
Lucian Prodan	Mihai Udrescu	Mircea Vladutiu
Advanced Computing Systems and Architectures (ACSA) Laboratory,
Computer Science and Engineering Department, “Politehnica” University of Timisoara,
2 V.Parvan Blvd, 300223 Timisoara, Romania
www.acsa.upt.ro
+40-722-664779	+40-723-154989	+40-256-403258
lprodan@cs.upt.ro	mudrescu@cs.upt.ro	mvlad@cs.upt.ro
ABSTRACT
Emerging technologies are set to provide further provisions for +L+ computing in times when the limits of current technology of +L+ microelectronics become an ever closer presence. A technology +L+ roadmap document lists biologically-inspired computing and +L+ quantum computing as two emerging technology vectors for novel +L+ computing architectures [43]. But the potential benefits that will +L+ come from entering the nanoelectronics era and from exploring +L+ novel nanotechnologies are foreseen to come at the cost of +L+ increased sensitivity to influences from the surrounding +L+ environment. This paper elaborates on a dependability perspective +L+ over these two emerging technology vectors from a designer’s +L+ standpoint. Maintaining or increasing the dependability of +L+ unconventional computational processes is discussed in two +L+ different contexts: one of a bio-inspired computing architecture +L+ (the Embryonics project) and another of a quantum computational +L+ architecture (the QUERIST project).
Categories and Subject Descriptors
B.8.1 [Performance and Reliability]: Reliability, Testing, and +L+ Fault-Tolerance.
C.4 [Performance of Systems]: Fault-Tolerance, Reliability, +L+ Availability, and Serviceability.
General Terms
Design, Reliability, Theory.
Keywords
Dependability, emerging technologies, evolvable hardware, bio- +L+ inspired computing, bio-inspired digital design, Embryonics, +L+ reliability, quantum computing, fault-tolerance assessment.
1. INTRODUCTION
High-end computing has reached nearly every corner of our +L+ present day life, in a variety of forms taylored to accommodate +L+ either general purpose or specialized applications. Computers
Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that +L+ copies bear this notice and the full citation on the first page. To copy +L+ otherwise, or republish, to post on servers or to redistribute to lists, +L+ requires prior specific permission and/or a fee.
CF’06, May 3–5, 2006, Ischia, Italy.
Copyright 2006 ACM 1-59593-302-6/06/0005...$5.00. +L+ may be considerred as fine exponents of the present days’ +L+ technological wave – if not their finest, they certainly do count as +L+ solid, indispensable support for the finest.
From the very beginning of the computing advent, the main target +L+ was squeezing out any additional performance. The inception +L+ period was not always trouble-free, accurate computation results +L+ being required at an ever faster pace on a road that has become +L+ manifold: some applications do require computational speed as a +L+ top priority; others are set for the highest possible dependability, +L+ while still delivering sufficient performance levels.
Several definitions for dependability have been proposed: “the +L+ ability of a system to avoid service failures that are more frequent +L+ or more severe than is acceptable” [2], or “the property of a +L+ computer system such that reliance can justifiably be placed on +L+ the service it delivers” [9][45]. Dependability is therefore a +L+ synthetic term specifying a qualitative system descriptor that can +L+ generally be quantified through a list of attributes including +L+ reliability, fault tolerance, availability, and others.
In real world, a dependable system would have to operate +L+ normally over extended periods of time before experiencing any +L+ fail (reliability, availability) and to recover quickly from errors +L+ (fault tolerance, self-test and self-repair). The term “acceptable” +L+ has an essential meaning within the dependability’s definition, +L+ setting the upper limits of the damages that can be supported by +L+ the system while still remaining functional or computationally +L+ accurate. A dependability analysis should take into consideration +L+ if not quantitative figures for the acceptable damage limit, at least +L+ a qualitative parameter representation for its attributes.
Dependable systems are therefore crucial for applications that +L+ prohibit or limit human interventions, such as long-term exposure +L+ to aggressive (or even hostile) environments. The best examples +L+ are long term operating machines as required by managing deep- +L+ underwater/nuclear activities and outer space exploration.
There are three main concerns that should be posed through a +L+ system’s design in order to achieve high dependability [42]:
1. Specifying the dependability requirements: selecting the +L+ dependability requirements that have to be pursued in +L+ building the computing system, based on known or assumed +L+ goals for the part of the world that is directly affected by the +L+ computing system;
2. Designing and implementing the computing system so as to +L+ achieve the dependability required. However, this step is hard +L+ to implement since the system reliability cannot be satisfied
187
simply from careful design. Some techniques can be used to +L+ help to achieve this goal, such as using fault injection to +L+ evaluate the design process.
3. Validating a system: gaining confidence that a certain +L+ dependability requirement/goal has been attained.
This paper will address these main concerns through an attempt to +L+ provide an in-depth view over modern computing directions and +L+ paradigms, which we consider to be representative for the efforts +L+ involved in improving overall dependability.
1.1 Motivations
We have listed some of the applications of dependable computing +L+ systems as linked to activities that take place in special +L+ environments, such as deep underwater or outer space. At a very +L+ first sight, these applications would appear specific enough to not +L+ encourage a specific design for dependability approach in +L+ computing. However, evidence suggest this is hardly the case; on +L+ the contrary, it is difficult to imagine a domain left unconquered +L+ by computer systems during times when industrial, transport, +L+ financial services and others do rely heavily on accurate computer +L+ operation at any given moment. If computer innacuracies could be +L+ more easily overlooked at home, professional environments +L+ cannot accept such missbehaviors.
Yet the recent history of computing provides evidence that +L+ dependability is not a sine qua non feature. During their life +L+ cycle, electronic devices constantly suffer a number of influences +L+ that manifest predominantly over transient regimes, which in turn +L+ introduce a variety of errors unified in the literature under the +L+ name of transient faults, soft errors or single event upsets (SEUs). +L+ The rate electronic devices are affected with is known under the +L+ term of soft error rate or simply SER and is measured in fails per +L+ unit time. Because it relies on transient phenomena due to +L+ changing states and logical values, digital electronics makes up +L+ for a special category that is also affected by soft errors. No +L+ matter the name they are referred under, these errors affect the +L+ computing processes and are due to electromagnetic noise and/or +L+ external radiations rather than design or manufacturing flaws [28].
One cause at the origin of soft fails affecting digital devices is +L+ known to be due to radioactive decay processes. Radioactive +L+ isotopes, widely used for a range of purposes, might contaminate +L+ semiconductor materials leading to soft errors; evidence is +L+ available throughout the literature, both by empirical observations +L+ and experimental results [20]. Consequently, cosmic rays, +L+ containing a broad range of energized atomic/subatomic particles +L+ may lead to the appearance of soft fails.
Computers therefore are susceptive to soft errors, an issue that +L+ will potentially become essential with the advent of emerging +L+ technologies. As acknowledged by the International Technology +L+ Roadmap for Semiconductors (ITRS), issued at the end of 2004 +L+ [43], the microelectronics industry faces a challenging task in +L+ going to and beyond 45nm scale in order to address “beyond +L+ CMOS” applications. Scaling down the technology will enable an +L+ extremely large number of devices to be integrated onto the same +L+ chip. However, the great challenge will be to ensure the new +L+ devices will be operational at this scale [6], since they will exhibit +L+ a sensitive behavior to soft fails. In order to address the negative +L+ effects brought by technology scaling, it is to be expected that +L+ significant control resources will need to be implemented [3].
Another challenging aspect concerning emerging technologies is +L+ to match the newly developed device technologies with new +L+ system architectures, a synergistic/collaborative development of +L+ the two being seen as likely to be very rewarding. The potential of +L+ biologically-inspired and quantum computing architectures is +L+ acknowledged by the ITRS report on emerging technologies [43] +L+ (see Figure 1). This paper will investigate the relevance of soft +L+ fails and attempt to provide means of harnessing their negative +L+ effects on modern computing in the context of biologically- +L+ inspired and quantum computing architectures.
Figure 1: Bio-inspired and quantum computing are
acknowledged as architectural technology vectors in emerging
technologies [43]
1.2 Paper Outline
This paper is structured as follows. Section 2 will address the first +L+ main concern, that is, specifying and selecting dependability +L+ requirements that will have to be pursued when building a +L+ computational platform. Parameters that describe and quantify +L+ dependability attributes, such as reliability, will be introduced, +L+ with a highlight on their accepted models and their issues. A +L+ particular consideration will be given to the failure rate parameter, +L+ which is the basis of all reliability analyses.
Section 3 will approach some of the means for design for +L+ dependability; it will therefore elaborate upon two emerging +L+ technology vectors, as seen by the ITRS report [43], which define +L+ two novel architectures, namely biologically-inspired (or bio- +L+ inspired) and quantum computing. We will introduce two projects +L+ and their corresponding architectures, called Embryonics (as a +L+ biologically-inspired computing platform) and QUERIST (as a +L+ quantum computing platform designed to allow and study error +L+ injection). These two architectures are representative for the +L+ coming age of nano-computing, where computational processes +L+ take place as encoded at the very inner core level of matter, be it +L+ semiconductor material (for nanoelectronics, targetted here by the +L+ Embryonics project) or atomic scale dynamics (for quantum +L+ computing, targetted here by the QUERIST project). This section +L+ will then introduce dependability aspects within bio-inspired +L+ computing (the Embryonics project being investigated in +L+ SubSection 3.1) and within quantum computing (the QUERIST +L+ project being investigated in SubSection 3.2).
Finally, Section 4 will present the conclusions and prospects for +L+ designing emerging technology dependable computing systems, +L+ as we see them.
188
2. DEPENDABILITY ATTRIBUTES
An important dependability attribute for any given system lies in +L+ its capacity to operate reliably for a given time interval, knowing +L+ that normal operation was delivered at initial time [8]. Reliability +L+ functions are modelled as exponential functions of parameter A, +L+ which is the failure rate. The reliability of a system is the +L+ consequence of the reliability of all of its subsystems. The +L+ heterogeneity of the system leads to a difficult quantitative +L+ assessment of its overall reliability; moreover, estimating the +L+ reliability functions is further made difficult because formal +L+ rigour is not commercially available, this being kept under +L+ military mandate [44].
The failure rate for a given system can be modelled as a function +L+ of the failure rates of its individual subsystems, suggestions being +L+ present in the MIL-HDBC-217 document, which is publicly +L+ available [44]. However, this document has been strongly +L+ criticized for its failure rate estimations based on the Arrhenius +L+ model, which relates the failure rate to the operating temperature:
where K is a constant, KB is Boltzmann’s constant, T is the +L+ absolute temperature and E is the “activation energy” for the +L+ process [18]. Quantitative values for failure rates show significant +L+ differences between those predicted using MIL-HDBC-217 and +L+ those from testing real devices (see Figure 2). There are two +L+ conclusions that can be drawn from this:
1. quantitative estimations for failure rate values are strongly +L+ dependant on the quality of information used; unfortunately, +L+ current reliable information about electronic devices is known +L+ to be lacking [44];
2. despite differences between predicted and real values, the +L+ MIL-HDBC-217 methodology can be useful for qualitative +L+ analyses in order to take decisions regarding sub-system parts +L+ that should benefit from improved designs.
Figure 2. Predicted vs real failure rates plotted against
temperature [18]
So far the failure rate of digital devices has been considerred as +L+ due to internal causes. However, this is not always the case, soft +L+ fails being equally present due to the aggressive influences of the +L+ external environment, which also have to be modelled [22]. The +L+ external envirnment features highly dynamic changes in its +L+ parameters, which will eventually affect the normal operation of +L+ digital devices that lack sufficient protection or ability to adapt.
Ideally, computing devices would behave in a consistent and +L+ accurate manner regardless of fluctuations in environmental +L+ parameters. This is either a consequence of soft error mitigation +L+ techniques or due to flexible hardware/software functionality that +L+ allow the system as a whole to adapt to environamental changes +L+ and tolerate induced faults.
While certain soft error mitigation techniques are available, the +L+ technology scaling towards nanoelectronics affects their +L+ efficiency by integrating a larger number of devices per chip +L+ (which requires a larger amount of redundant/control logic or +L+ other measures), which feature, at the same time, smaller +L+ dimensions (which renders an electronic device much more +L+ senzitive to the influence of stray energetic particles that reach it +L+ as part of cosmic rays). Both aspects are involved in the +L+ development of the two emerging technology vectors mentioned +L+ in SubSection 1.1, although having slightly different motivations: +L+ while the nature of the quantum environment prohibits precise +L+ computation in the absence of fault tolerance techniques, such +L+ techniques are targetted by bio-inspired computing as means of +L+ improving the dependability of a computing platform.
2.1 Bio-Inspired Computing
If living beings may be considered to fulfill computational tasks, +L+ then Nature is the ultimate engineer: each of the living beings +L+ exhibit solutions that were successfully tested and refined in such +L+ ways human engineers will never afford. One reason is time: the +L+ testing period coinciding with the very existence of life itself. +L+ Another reason is variety and complexity: Nature has found and +L+ adapted a variety of solutions to address complex survivability +L+ issues in a dynamically changing environment. No matter how +L+ Nature approached the process of evolution, engineering could +L+ perhaps benefit most from drawing inspiration from its +L+ mechanisms rather from trying to develop particular techniques.
Bio-inspired computing is not a new idea. John von Neumann was +L+ preoccupied to design a machine that could replicate itself and +L+ was quite interested in the study of how the behavior of the +L+ human brain could be implemented by a computer [13][14]. He +L+ also pioneered the field of dependable computing by studying the +L+ possibility of building reliable machines out of unreliable +L+ components [15]. Unfortunately, the dream of implementing his +L+ self-reproducing automata could not become true until the 1990s, +L+ when massively programmable logic opened the new era of +L+ reconfigurable computing.
But when trying to adapt nature’s mechanisms in digital devices, +L+ it becomes most evident that biological organisms are rightfully +L+ the most intricate structures known to man. They continuously +L+ demonstrate a highly complex behavior due to massive, parallel +L+ cooperation between huge numbers of relatively simple elements, +L+ the cells. And considering uncountable variety of living beings, +L+ with a life span up to several hundreds (for the animal regnum) or +L+ even thousands (for the vegetal regnum) of years, it seems nature +L+ is the closest spring of inspiration for designing dependable, fault +L+ tolerant systems.
Investigating the particularities of natural systems, a taxonomy of +L+ three categories of processes can be identified [32]:
1. Phylogenetic processes constitute the first level of +L+ organization of the living matter. They are concerned with the +L+ temporal evolution of the genetic heritage of all individuals,
E
λ= Ke KBT	(1)
−
189
therefore mastering the evolution of all species. The +L+ phylogenetic processes rely on mechanisms such as +L+ recombination and mutation, which are essentially +L+ nondeterministic; the error rate ensures here nature’s +L+ diversity.
2. Ontogenetic processes represent the second level of +L+ organization of the living matter. They are also concerned +L+ with the temporal evolution of the genetic heritage of, in this +L+ case, a single, multicellular individual, therefore mastering an +L+ individual’s development from the stage of a single cell, the +L+ zygote, through succesive cellular division and specialization, +L+ to the adult stage. These processes rely on deterministic +L+ mechanisms; any error at this level results in malformations.
3. Epigenetic processes represent the third level of organization +L+ of the living matter. They are concerned with the integration +L+ of interactions with the surrounding environment therefore +L+ resulting in what we call learning systems.
This taxonomy is important in that it provides a model called POE +L+ (from Phylogeny, Ontogeny and Epigenesis) that inspires the +L+ combination of processes in order to create novel bio-inspired +L+ hardware (see Figure 3). We believe this is also important from a +L+ dependability engineering perspective, for the following reasons:
1. Phylogenetic processes were assimilated by modern +L+ computing as evolutionary computation, including genetic +L+ algorithms and genetic programming. The essence of any +L+ genetic algorithm is the derivation of a solution space based +L+ on recombination, crossover and mutation processes that +L+ spawn a population of individuals, each encoding a possible +L+ solution. One may consider that each such step, with the +L+ exception of discovering the solution, is equivalent to a +L+ process of error injection, which in turn leads to wandering +L+ from the optimal solution (or class of solutions). However, +L+ genetic algorithms prove to be successful despite this error +L+ injection, the fitness function being responsible for the +L+ successful quantification of the significance of the “error”. +L+ Therefore genetic computation is intrinsicaly resilient to +L+ faults and errors, largely due to the fact that they are part of +L+ the very process that generates the solutions.
2. Ontogenetic processes have been implemented in digital +L+ hardware with modular and uniform architectures. Such an +L+ architecture enables the implementation of mechanisms +L+ similar to the cellular division and cellular differentiation that +L+ take place in living beings [31]. These mechanisms bring the +L+ advantage of distributed and hierarchical fault tolerance +L+ strategies: the uniformity of the architecture also makes any +L+ module to be universal, that is, to be able to take over the role +L+ of any other damaged module.
3. Epigenetic processes were assimilated by modern computing +L+ mainly as artificial neural networks (or ANNs) as inspired by +L+ the nervous system, and much less as inspired by the immune +L+ or endocrine systems from superior multicellular living +L+ beings. ANNs are known to have a generalization capacity, +L+ that is, to respond well even if the input patterns are not part +L+ of the patterns used during the learning phase. This means that +L+ ANNs possess a certain ability to tolerante faults, whether +L+ they manifest at the inputs or inside their intenal architecture.
With the advent of field programmable logic (of which the most +L+ salient representative are the FPGAs) it is now possible to change +L+ hardware functionality through software, thus allowing +L+ information to govern matter in digital electronics. This is not +L+ dissimilar to what happens in nature: information coded in DNA +L+ affects the development of an organism. A special kind of such +L+ digital devices that change dynamically their behavior are known +L+ as evolvable or adaptive hardware; they are bio-inspired +L+ computing systems whose behaviors may change according to +L+ computational targets, or, if harsh or unknown environments are +L+ to be explored, for the purpose of maximizing dependability.
Figure 3. The POE model of bio-inspired systems [32]
2.2 Quantum Computing
Error detection and correction techniques are vital in quantum +L+ computing due to the destructive effect of the environment, which +L+ therefore acts as an omnipresent error generator. Error detection +L+ and correction must provide a safe recovery process within +L+ quantum computing processes through keeping error propagation +L+ under control. Without such dependability techniques there could +L+ be no realistic prospect of an operational quantum computational +L+ device [19].
There are two main sources of errors: the first is due to the +L+ erroneous behavior of the quantum gate, producing the so-called +L+ processing errors; the second is due to the macroscopic +L+ environment that interacts with the quantum state, producing the +L+ storing and transmitting errors.
The consistency of any quantum computation process can be +L+ destroyed by innacuracies and errors if the error probability in the +L+ basic components (qubits, quantum gates) excedes an accuracy +L+ threshold. This is a critical aspect since the microscopic quantum +L+ states are prone to frequent errors.
The main error source is the decoherence effect [16]. The +L+ environment is constantly attempting to measure the sensitive +L+ quantum superposition state, a phenomenon that cannot be +L+ avoided technologically since it is not (yet) possible to isolate +L+ them perfectly. The superposition state will decay through +L+ measuring and will therefore become a projection of the state +L+ vector onto a basis vector (or eigenstate). The most insidious +L+ error, however, appears when decoherence affects the quantum +L+ amplitudes without destroying them; this is similar to small +L+ analog errors. Issues stated above are solved, on one hand, +L+ through intrinsic fault tolerance by technological implementation +L+ (topological interactions [1]) and, on the other hand, by error +L+ correcting techniques at the unitary (gate network) level. We will +L+ focus on the error detecting and correcting techniques, which are +L+ difficult to approach due to quantum constraints: the useful state
190
can neither be observed (otherwise it will decohere), nor can it be +L+ cloned.
2.2.1 Background
As expressed in bra-ket notation [16], the qubit is a normalized
vector in some Hilbert space H2 , { 0 , 1 } being the orthonormal
basis: ψ = a0 0 + a1 1 ( a0, a1 ∈ C are the so-called quantum
amplitudes, representing the square root of the associated +L+ measurement probabilities for the eigenstates
respectively, with a0 2 + a1 2 =1). Therefore, the qubit can be
affected by 3 types of errors:
Bit flip errors are somewhat similar to classical bit flip errors. For +L+ a single qubit things are exactly the same as in classical +L+ computation: 0 H 1 , 1 H 0 . For 2 or more qubits, flip errors
affecting the state may modify it or leave it unchanged. For +L+ instance,	if we consider the so-called cat state
ψ Cat = 2 ( 00 + 11 ) [19], and the first qubit is affected by a
bit flip error, the resulting state will be yr Cat H 2 ( 10 + 01) .
But, if both qubits are affected by bit flips, there will be no
change in the state: V Cat H 2 ( 11 + 00 ) = ψ Cat
Phase errors affect the phase of one of the qubit&apos;s amplitudes and
is expressed as 0 H 0 , 1 H − 1 . This type of error is very
dangerous, due to its propagation behavior but it only makes +L+ sense when dealing with superposition states. If we consider an +L+ equally weighted qubit superposition state and inject a phase
error, this results in 2 ( 0 + 1 )H 2 ( 0 − 1 ) .
There is a strict correspondence between bit flip and phase error +L+ types due to the way they map onto Hilbert spaces with the same +L+ dimension but different basis. The bit flip is an error from the
{ 0 , 1 } , whereas the phase error appears in the
same space with basis r	0 + 1, �(0 − 1 )⎫⎬⎭ or{ +
The space basis conversion, in this case, is made by applying the +L+ Hadamard transform; Figure 4 shows an example of transforming +L+ a bit flip error into a phase error (A, and vice versa (B.
Small amplitude errors: amplitudes a0 and a1 of the quantum bit
can be affected by small errors, similar to analog errors. Even if +L+ such an error does not destroy the superposition and conserves the +L+ value of the superposed states, small amplitude errors could +L+ accumulate over time, eventually ruining the computation. In +L+ order to avoid this situation, specific methodologies for digitizing +L+ small errors are used to reduce them to a non-fault or a bit-flip +L+ [19].
Due to the quantum physics laws, fault tolerance techniques have +L+ to comply with the following computational constraints:
– The observation destroys the state. Since observation is +L+ equivalent to measurement, this leads to destroying the +L+ useful state superposition.
– Information copying is impossible. Quantum physics renders +L+ the cloning of a quantum state impossible, meaning that a +L+ quantum state cannot be copied correctly. Therefore +L+ quantum error correction must address the following +L+ problems:
Non-destructive measurement. Despite the first constraint it is +L+ necessary to find a way to measure the encoded information +L+ without destroying it. Because the encoded state cannot be +L+ measured directly, one needs to properly prepare some scratch +L+ (ancilla) qubits, which can then be measured.
Fault-tolerant recovery. Due to the high error rate in quantum +L+ computational devices, it is likely that the error recovery itself +L+ will be affected by errors. If the recovery process is not fault- +L+ tolerant, then any error coding becomes useless.
Phase error backward propagation. If we consider the XOR gate +L+ from Figure 5(A, a flip error affecting the target qubit (b) will +L+ propagate backwards and also affect the source qubit. This is due +L+ to the gate network equivalence from Figure 5(B and the basis +L+ transformation described by Figure 4.
Figure 4. Correspondence between bit flip and phase errors
Figure 5. (A The backward propagation of a phase error for
the XOR gate; (B Gate network equivalence
In order to deal with the problems described the next strategies +L+ have to be followed:
Digitizing small errors. The presence of small errors is not a +L+ major concern, as they can be digitized using a special technique +L+ based on measuring auxiliary (ancilla) qubits [19].
Ancilla usage. Since qubit cloning is impossible, a majority +L+ voting strategy is difficult to implement. However, by using +L+ ancilla qubits, the eigenstate information can be duplicated inside +L+ the existing superposition, resulting in the entanglement of the +L+ ancilla with the useful data. Because any measurement performed +L+ on the ancilla could have repercussions on the useful qubits, the +L+ appropriate strategy will employ special coding for both data +L+ qubits and ancilla (data errors only will be copied onto the +L+ ancilla), followed by the computation of an error syndrome, +L+ which has to be obtained through measuring the ancilla (see +L+ Figure 6).
Avoiding massive spreading of phase errors. As shown +L+ previously, a phase error on the target qubit will propagate on all +L+ source qubits. The solution is to use more ancilla qubits as targets, +L+ so that no ancilla qubit is used more than once.
0 and 1
	2
space with basis
191
1
Figure 6. Fault-tolerant procedure with ancilla qubits
Ancilla and syndrome accuracy. Setting the ancilla code to some +L+ known quantum state could be an erroneous process. Computing +L+ the syndrome is also prone to errors. Hence, on one hand, one has +L+ to make sure that the ancilla qubits are in the right state by +L+ verifying and recovering them if needed; on the other hand, in +L+ order to have a reliable syndrome, it must be computed +L+ repeatedly.
Error recovery. As the small errors can be digitized (therefore, +L+ they are either corrected or transformed into bit flip errors), the +L+ recovery must deal only with bit flip and phase errors. A state that +L+ needs to be recovered is described by:
Correcting a bit flip error 1means applying the negation unitarytransformation UN = ux = ° Oj to the affected qubit. To
correct phase and combined errors, the following unitary +L+ operators	will	have	to	be	applied	respectively:
⎡1	0 ⎤	⎡0	−i ⎤
UZ=⎢0	−1], UY = UN⋅UZ	⎣i	.
			0 ⎥⎦
2.2.2 Quantum Error Correcting Codes
Quantum error coding and correcting (QECC) is performed with +L+ special coding techniques inspired from the classic Hamming +L+ codes. The classical error coding is adapted so that it becomes +L+ suitable for the quantum strategy, allowing only the ancilla qubits +L+ to be measured.
The state-of-the-art in QECC is represented by the stabilizer +L+ encoding, a particular case being the Steane codes (the Shor codes +L+ may also be used [29]). Steane&apos;s 7-qubit code is a single error +L+ correcting code inspired from classical Hamming coding and can +L+ be adapted for ancilla coding as well. Therefore it cannot recover +L+ from two identical qubit faults, but it can recover from a bit flip a +L+ phase flip. The Steane 7-qubit coding of 0 and 1 consists of
an equally weighted superposition of all the valid Hamming 7-bit +L+ words with an even and odd number of 1s, respectively:
= 1
S	3	odd⎞u0u1u2u3c0c1c2
⎜	⎟
⎝	⎠
=1 1111111 + 1101000 + 1010001 + 1000110
232
+ 0110100 + 0100011 + 0011010 + 0001101
Applying the Steane coding on an arbitrary given quantum state
ψ = a0 0 +a1 1 transforms it into V S = a0 0 S + a1 1 S. This
code was designed to correct bit-flip errors, but by changing the +L+ basis (through a Hadamard transform) the phase error transforms +L+ into a bit flip error, which can then be corrected:
2.2.3 Fault Tolerance Methodologies
Quantum error-correcting codes exist for r errors, r ∈ ICY, r ≥ 1. +L+ Therefore a non-correctable error occurs if a number of r +1
errors occur simultaneously before the recovery process.
If the probability of a quantum gate error or storage error in the
time unit is of orderξ , then the probability of an error affecting
the processed data block becomes of order �r+1 , which is
negligible if r is sufficiently large. However, by increasing r the +L+ safe recovery also becomes more complex and hence prone to
errors: it is possible that r +1 errors accumulate in the block
before the recovery is performed.
Considering the relationship between r and the number of +L+ computational steps required for computing the syndrome is
polynomial of the order rp . It was proven that in order to reduce +L+ as much as possible the error probability r must be chosen so that
1
−
r — e 1K p [7][19]. By consequence, if attempting to execute N
cycles Sof error correction without any r+1 errors accumulating
before the recovery ends, then N — exp⎜ξp
⎛ − 1
. Therefore the
accuracy degree will be of the form � —(logN)−p , which is
better than the accuracy degree corresponding to the no-coding
case, � — N-1. However, there exists a Nmax so that if N &gt; Nmax
then non-correctable error becomes likely, which limits the length +L+ of the recovery process. Given the extremely large number of +L+ gates employed by a quantum algorithm implementation, Nmax
also has to be very large; for Shor&apos;s algorithm Nmax must be
higher than 3⋅109 [30].
As shown in Figure 7, the required accuracy degree approaches +L+ today&apos;s technological limits (tipically 10-3 for p=4) after N=105. +L+ For a fault tolerant encoding solution for Shor algorithm +L+ implementation this should have happened after N=109 [19][34].
+	(2)	Additional fault tolerance must be employed in order to preserve
⎯⎯⎯→⎨error ⎧ ⎪ ⎪ ⎪ ⎪ ⎩
a
a1 0 + a0 1 for a flip error
.
a0 0 −a1 1 for a phase error
a1 0 − a0 1 for both flip and phase errors
0 0 +a1 1 if no error occurs
a00+a11
= 1 0000000 + 0010111 + 0101110 + 0111001
+ 1001011 + 1011100 + 1100101 + 1110010
232
(
)
1
=
S	232	evelu0`2u3c01c2)
0
u0u1u2u3c0c1c2
+	(3)
)
0 S =H⋅ 0S= 1
1 S =H⋅ 1 S = 1


2
2
(0S+1S)
(0S−1S)
(4)
reliable quantum computation over an arbitrary number of +L+ computational steps. Concatenated coding represents one such +L+ technique, which improves the reliability by shaping the size of
192
the code blocks and the number of code levels. It is also resource +L+ demanding and vulnerable to correlated errors [19][37].
Another approach, replacing the concatenated codes, is based on +L+ Reconfigurable Quantum Gate Arrays (RQGAs) [34][37], which +L+ are used for configuring ECC circuits based on stabilizer codes +L+ [7][33]. By using a quantum configuration register for the RQGA +L+ (i.e. a superposition of classical configurations), the +L+ reconfigurable circuit is brought to a state where it represents a +L+ simultaneous superposition of distinct ECC circuits. After +L+ measuring the configuration register, only one ECC circuit is +L+ selected and used; if k distinct ECC circuits were superposed and
the gate error rate is � , then the overall gate error probability
becomes �k (see Figure 8). As a result, the accuracy threshold +L+ value for the RQGA solution clearly dominates the technological +L+ accuracy limit, as shown in Figure 9 [37].
Figure 7. Accuracy plots: p=3 for xi1, p=4 for xi2, p=5 for xi3;
xi4 for no-coding, ref is the reference accuracy (i.e. the
accuracy allowed by today&apos;s state of the art technology)
Figure 8. A quantum configuration register acts as a
superposition of k distinct circuits sharing the same input
state and the same output qubits
3. DEPENDABLE SYSTEM DESIGN
In order to model the erroneous behavior of a device of system it +L+ is necessary to understand the causality of phenomena concerned. +L+ A defect affecting a device from a physical point of view is called +L+ a fault, or a fail. Faults may be put in evidence through logical +L+ misbehavior, in which case they transform into errors. Finally, +L+ errors accumulating can lead to system failure [8]. The fault- +L+ error-failure causal chain is essential to developping techniques +L+ that reduce the risk of error occurrence, even in the presence of +L+ faults, in order to minimize the probability of a system failure, +L+ and can be architecture specific. We will elaborate next on
techniques used by a bio-inspired and by a quantum computing +L+ platform.
Figure 9. Evolution of accuracy threshold value for RQHW
stabilizer codes (xir); the technological accuracy limit (dim) is
also provided for a relevant comparison
3.1 The Embryonics Approach
Several years before his untimely death John von Neumann began +L+ developping a theory of automata, which was to contain a +L+ systematic theory of mixed mathematical and logical forms, +L+ aimed to a better understanding of both natural systems and +L+ computers [14]. The essence of von Neumann’s message appears +L+ to entail the formula “genotype + ribotype = phenotype”. He +L+ provided the foundations of a self-replicating machine (the +L+ phenotype), consisting of its complete description (the genotype), +L+ which is interpreted by a ribosome (the ribotype).
Embryonics (a contraction for embryonic electronics) is a long +L+ term research project launched by the Logic Systems Laboratory +L+ at the Swiss Federal Institute of Technology, Lausanne, +L+ Switzerland. Its aim is to explore the potential of biologically- +L+ inspired mechanisms by borrowing and adapting them from +L+ nature into digital devices for the purpose of endowing them with +L+ the remarkable robustness present in biological entities [39]. +L+ Though perhaps fuzzy at a first glance, analogies between biology +L+ and electronics are presented in Table 1 [12][31].
But if we consider that the function of a living cell is determined +L+ by the genome, and that a computer’s functionality is determined +L+ by the operating program, then the two worlds may be regarded as +L+ sharing a certain degree of similarity. Three fundamental features +L+ shared by living entities are required to be targetted by +L+ Embryonics in order to embody the formula “genotype + ribotype +L+ = phenotype” into digital hardware:
–multicellular organisms are made of a finite number of cells, +L+ which in turn are made of a finite number of chemically +L+ bonded molecules;
–each cell (beginning with the original cell, the zygote) may +L+ generate one or several daughter cell(s) through a process +L+ called cellular division; both the parent and the daughter +L+ cell(s) share the same genetic information, called the genome;
–different types of cells may exist due to cellular +L+ differentiation, a process through which only a part of the +L+ genome is executed.
These fundamental features led the Embryonics project to settle +L+ for an architectural hierarchy of four levels (see Figure 10). We +L+ will not delve very deep inside the Embryonics’phylosophy, as +L+ such details were broadly covered by literature [12][20][23][24] +L+ [25][40]; we will, however, introduce each of the four levels in
193
order to be able to see how this bio-inspired platform fits modern +L+ design for dependability efforts.
Table 1. Analogies present in Embryonics [12]
Biology	Electronics
Multicellular organism	Parallel computer systems
Cell	Processor
Molecule	FPGA Element
Figure 10. Structural hierarchy in Embryonics [12]
The upmost level in Embryonics, bearing a certain similarity to +L+ what is found in nature, is the population level, composed of a +L+ number of organisms. One level down the hierarchy constitutes +L+ the organismic level, and corresponds to individual entities in a +L+ variety of functionalities and sizes. Each of the organisms may be +L+ further decomposed into smaller, simpler parts, called cells, which +L+ in turn may be decomposed in molecules. According to +L+ Embryonics, a biological organism corresponds in the world of +L+ digital systems to a complete computer, a biological cell is +L+ equivalent to a processor, and the smallest part in biology, the +L+ molecule, may be seen as the smallest, programmable element in +L+ digital electronics (see Table 1).
An extremely valuable consequence of the Embryonics +L+ architecture is that each cell is &quot;universal&quot;, containing a copy of +L+ the whole of the organism’s genetic material, the genome. This +L+ enables very flexible redundancy strategies, the living organisms +L+ being capable of self-repair (healing) or self-replication (cloning) +L+ [12]. Self-replication may be of great interest in the +L+ nanoelectronics era, where extremely large areas of +L+ programmable logic will probably render any centralized control +L+ very inefficient. Instead, the self-replication mechanism +L+ implemented in Embryonics will allow the initial colonization of +L+ the entire programmable array in a decentralized and distributed +L+ manner. Figure 11 presents an example of such colonization. At +L+ initial time the configuration bitstream (containing the genome) +L+ enters the bottom left corner of a programmable array and, at each +L+ clock cycle, the genome is pushed through and partitions the +L+ programmable space accordingly.
From a dependability standpoint, the Embryonics hierarchical +L+ architecture offers incentives for an also hierarchical self-repair
strategy. Because the target applications are those in which the +L+ failure frequency must be very low to be “acceptable”, two levels +L+ of self-repair are offered: at the molecular level (programmable +L+ logic is susceptible to soft fail occurrences) and at the cellular +L+ level (soft fails manifest at this level as soft errors).
Let us consider an example of a simple cell made of 3 lines and 3 +L+ columns of molecules, of which one column contains spare +L+ molecules. If a fault occurs inside an active cell, it can be repaired +L+ through transferring its functionality toward the appropriate spare +L+ molecule, which will become active (see Figure 12).
Figure 11. Space colonization in Embryonics [11]
Figure 12. Self-repair at the molecular level: faulty molecule +L+ E is replaced by spare molecule H, which becomes active [39]
The self-repair process at molecular level ensures the fault +L+ recovery as long as there are spare molecules left for repair. +L+ However, it is possible for a cell to experience a multiple error, in +L+ which case the self-repair mechanism at the molecular level can +L+ no longer reconfigure the inside of the cell successfully. If such a +L+ situation arises, then a second self-repair strategy is trigerred at a +L+ higher level. The cell will “die”, therefore trigerring the self- +L+ repair at the cellular level, the entire column containing the faulty +L+ cell (cell C in this example) being deactivated, its role being taken +L+ by the nearest spare column to the right (see Figure 13).
A critique that could be addressed to the current Embryonics +L+ design would be its strategy of self-repair at the higher, cellular +L+ level: in case of a faulty cell, an entire column containing that cell +L+ will be deactivated, its role being transferred to the first available +L+ column of spares to the right (see Figure 13). There are two points +L+ in which this strategy could benefit:
194
1. Instead of deactivating a whole column of cells, it would be +L+ more efficient to only deactivate the faulty cell only (see +L+ Figure 14). The resources affected by the role transfer would +L+ be greatly reduced (one cell versus an entire column), +L+ coupled with the fact that particle flux generating soft fails is +L+ unlikely to be homogeneous and isotrope. This means +L+ regions assimilable more likely to cells rather than entire +L+ column of cells would be more affected by soft fails, not to +L+ mention that during genetic data transfer (required by taking +L+ over the role of the faulty cell) there is a greater risk of +L+ enduring a new soft fail (moving data is much more sensitive +L+ to soft fails than static data) [5][10].
2. Such a strategy would be consistent with that used for the +L+ self-repair at the molecular level, which would simplify a +L+ thorough reliability analysis. Concatenated coding would +L+ also seem easier to be implemented and the strategy +L+ consistency would mean that concatenated coding would not +L+ be limited to a two-level hierarchy [20][21].
Figure 13. Molecular self-repair failure: the cell “dies” +L+ (bottom), triggering the cellular self-repair (top) [39]
We consider a cell of M lines and N columns, being composed of +L+ modules of M lines and n+s columns (for instance, the cell +L+ presented in Figure 12 consists of a single such module of two +L+ active columns and one spare column), of which s are spares. In +L+ order to meet certain reliability criteria, it is necessary to know +L+ what is the number s of spare columns of molecules that +L+ correspond to n columns of active molecules, that is, the +L+ horizontal dimensions for such a module. We will not provide a +L+ thorough reliability analysis, as this has been done previously +L+ [4][17][20][21]; instead, we will analyze the influences of the +L+ proposed consistent self-repair strategy at both molecular and +L+ cellular levels through the use of logic molecules. Therefore +L+ Equation (5) holds:
k
RModRow (t)=Prob{ no fails} (t) + ∑ Prob{ i fails} (t)
i=1	(5)
N=k(n+s)
where RModRow(t) represents the reliability function for a row +L+ within a module. Then, considering the failure rate for one
molecule λ, the probability of all molecules (both active and
spare) to operate normally in a module’s row becomes:
Prob{ no fails} (t) = e−&quot;n+s)t	(6)
The probability of a row enduring i fails in the active molecules +L+ part is the conditional probability of having n-i active molecules +L+ operating normally, while a number of s-i spare molecules are +L+ ready to activate (that is, they are not affected by errors +L+ themselves):
Prob{ i fails} (t) = Prob{ i fails active} (t)
⋅Prob{i spares ok}(t)
Prob{ i fails active} (t) = (n Je λ(n-i)t (1 − eλ(n-i)t)	(8)
i	l
Prob{ i spares ok} (t) = (k )e−λit (1 − a λ(k−i)t
i
Then the reliability function for an entire cell is the cummulated +L+ reliability functions for the total number of modules:
RCell(t) = [RModRow(t)]MN`	(10)
Figure 14. Proposed reconfiguration strategy at the cellular
level
A self-repair strategy that conserves the consistency between the +L+ molecular and the cellular level would allow for a more +L+ straightforward reliability analysis. Basically, it would be +L+ sufficient to substitute dimension parameters in Equations (5)- +L+ (10) with those approapriate to the analysis of an organism +L+ instead of a cell. To illustrate this, we will consider an organism +L+ of M* lines and N* columns, being composed of modules of M* +L+ lines and n*+s* columns, of which s* are spares; we will also use +L+ the organism partitioning into modules, similar to the partitioning +L+ of cells used before. Therefore Equation (5) transforms into +L+ Equation (11):
k
RCellMR (t)=Prob*{nofails}(t)+∑Prob* { ifails} (t) i = 1	(11)
N*=k*(n +s )
where RCellMR (t) represents the reliability function for a row of
cells within an organism module. In this case, the significance of +L+ the terms will be as follows:
Prob {no fails} (t) = [RCell ( tj +s	(12)
⋅
(7)
(9)
195
While Equation (7) continues to hold under the form of Equation +L+ (13), the significance of its terms will change according to the +L+ dimensions at the cellular level:
⎛ ⎞ −
Prob*{ifailsactive}(t)=⎜
 i
(t)(1−R
L
ll(t
⎛ ⎞	−
Prob*{isparesok}(t)=⎜k*
J
RCiell(t)(1−RCkelli(t))	(15) i
t0, t1,..., tm−1 will be given by +L+ The outputs ofthe firstcycle, whichare also inputs forthe +L+ The used FTAMs are only valid if the relationship between the
experimental ξsim and the assumed singular error rateξ is of the
order gsim _ � 2 [19].
) )(14)
Finally, similar to Equation(10), the reliabilityfunctionforan
entire organism is the cummulated re
liability functions for the
total number of its modules:
ROrg(t)=
 [
RCellMR(t)
]
MN
 �
+
s	(16)
Equations (5) to (16) provide the basics forathorough reliability
analysis for the proposed, uniformstrategy ofhierarchical
reconfiguration, as opposedto the analysis providedby [21],
whichspecifically targetted the currentEmbryonics architecture.
Despite having settled the reliabilitymodel, bothanalyses are
incomplete, inthatthe failure rate parameteris missing, which
makes aprecise, quantitative dependability targetdifficultto
meet. However, areliability analysis is still valuable from a
qualitative pointofview, allowingadirectcomparison of
differentsystems.
3.2 The QUERISTApproach
Inorderto deal with errors inducedby the constantinfluence of
the external environmentuponcomputational processes, the
following assumptions were made: errors appear randomly, are
uncorrelated (neitherinspace, norintime), there are no storage
errors, andthere are no leakage phenomenainvolved[19].
Classical HDL-based faultinjectionmethodologies can be
mappedto simulatingquantumcircuits withoutintervention
providedthatthe new errorand faultmodels are takeninto
account[35]. Ofcourse, efficiencycriteriarequire thatthey be
adaptedto one ofthe available efficientsimulationframeworks
[36][38][41]. QUERIST(from QUantum ERrorInjection
Simulation Tool) is the name ofsucha project, fostering
simulated faultinjectiontechniques inquantum circuits [34].
Similarto classical computation, simulatedfaultinjection is used
in orderto evaluate the employed FTAMS (FaultTolerance
Algorithms andMethodologies) [26][27].
Anoverview ofthe QUERISTprojectis presented in Figure 15.
The three cycles ofinitialization, simulation, anddata
computationare commonto bothclassical andquantum
approaches. The firstcycle takes the quantumcircuitHDL
description as aninput. Two abstractinputs are considered, the
HDL model andthe assumederrormodel; the firstinfluences how
the HDLdescription is presented, while the secondone dictates
the testscenario by definingthe start/stop simulationstates (since
qubits are equallyprone to error, all the signals mustbe
observed). HDLmodelingofquantumcircuits inorderto attain
effi
cient simulation is discussed in [34][35][36][38].
196
secondcycle consists oftime diagrams forall qubits, from the +L+ startto the stop states. Useful information, extracted fromthe raw, +L+ bubble-bit-represented, qubittraces are comparedto correctqubit +L+ values, the resultbeingthe probabilistic accuracy thresholdvalue, +L+ inthe thirdcycle. The initialization andsimulation cycles depend +L+ on specific aspects ofquantum circuitsimulation [35]. The data +L+ processing cycle is independentfromthe specific simulation +L+ framework andis aimedatdeterminingthe accuracythresholdas +L+ the mainreliability measure thatalso defines the feasibility ofthe +L+ quantum circuitimplementations.
Suppose that, atsimulationtime twe observe signals
{s0,s1,...,sn}
 . In ouranalysis,
si
 is the state observedduringnon- +L+ faulty simulation, so forthe same state ina faulty environmentwe +L+ will have the state
si* .
Forvalidationofthe quantum FTAMs, we needto compare
si
with
si*.
 This can be done by using operator
 d
if
(si,s
;
)
 . This
mean
s that the total number of overall state errors at simulation
n − 1
time tis
. et=∑d
if
(si,s
;
)The error rate on the overall observed
4. CONCLUSIONS
This paper presented arguments in favor of two novel computing +L+ architectures for the purpose of addressing the challenges raised +L+ by the forthcoming nanoelectronics era. Distributed self-testing +L+ and self-repairing will probably become a must in the next years +L+ as centralized control logic is expected to become unable to +L+ harness the extremely large number of devices, all equally prone +L+ to errors, that will be integrated onto the same chip. Bio-inspired +L+ computing brings valuable techniques that explore the potential of +L+ massively parallel, distributed computation and fault-tolerance +L+ that will likely provide an essential help to jumpstart new +L+ nanoelectronic architectures. As one of the representatives of bio- +L+ inspired computing, the Embryonics project presents a +L+ hierarchical architecture that achieves fault tolerance through +L+ implementing an also hierarchical reconfiguration. A similar +L+ approach for maximizing fault tolerance is present in quantum +L+ computing, the QUERIST project; even if bio-inspired and +L+ quantum computing may seem dissimilar at a first glance, they +L+ both achieve fault tolerance by adapting the same techniques from +L+ classical computing and using essentially the same error model.
Nanoelectronics will potentially change the way computing +L+ systems are designed, not only because of the sheer number of +L+ devices that will coexist onto the same chip, but also because of +L+ the sensitivity of these devices.
Prob*
{ i fails} (t) = Prob* { i fails active} (t)
⋅
⋅ Prob* { i spares ok} (t)
(13)
i=0
simulation cycle, consistofatestscenario and an executable HDL +L+ model withthe correspondingentanglementanalysis, dictatedby +L+ the bubble-bitencoded quantum states [36][38]. The outputofthe
states at moments
1 m−1
ξsim	∑ et •
m
j=0 �
Figure 15. An overview of the QUERIST project
Therefore, if nanoelectronics is to be employed to build +L+ dependable computing machines (a certain contradiction +L+ notwithstanding), valuable expertise in design can be drawn from +L+ natural sciences. While biology provides countless examples of +L+ successfully implemented fault tolerance strategies, physics offers +L+ theoretical foundations, both of which were found to share +L+ common ground. It is perhaps a coincidence worth exploring in +L+ digital computing.
5. REFERENCES
[1] Aharonov, D., Ben-Or, M. Fault Tolerant Quantum +L+ Computation with Constant Error. Proc. ACM 29th Ann. +L+ Symposium on Theory of Computing, El Paso, Texas, May +L+ 1997, pp. 176-188.
[2] Avižienis, A., Laprie, J.C., Randell, B., Landwehr, C. Basic +L+ Concepts and Taxonomy of Dependable and Secure +L+ Computing. IEEE Transactions on Dependable and Secure +L+ Computing, 1, 1 (Jan-Mar 2004), 11-33.
[3] Butts, M., DeHon, A., Golstein, S.C. Molecular Electronics: +L+ Devices, Systems and Tools for Gigagate, Gigabit Chips. +L+ Proc. Intl. Conference on CAD (ICCAD’02), 2002, pp. 433- +L+ 440.
[4] Canham, R., Tyrrell, A. An Embryonic Array with Improved +L+ Efficiency and Fault Tolerance. Proc. IEEE NASA/DoD +L+ Conference on Evolvable Hardware, Chicago Il, 2003, 275- +L+ 282.
[5] Gaisler, J. Evaluation of a 32-Bit Microprocessor with Built- +L+ In Concurrent Error Detection. Proc. 27th Annual Intl. +L+ Symposium on Fault-Tolerant Computing (FTCS-27), 1997, +L+ pp. 42-46.
[6] Goldstein, S.C. The Challenges and Opportunities of +L+ Nanoelectronics. Proc. Government Microcircuit Applica- +L+ tions and Critical Technology Conference (GOMAC Tech - +L+ 04), Monterey, CA, March 2004.
[7] Gottesman, D. Class of quantum error-correcting codes +L+ saturating the quantum Hamming bound. Phys. Rev. A 54, +L+ 1996, pp. 1862-1868.
[8] Johnson, B.W. Design and Analysis of Fault-Tolerant +L+ Digital Systems. Addison-Wesley, 1989.
[9] Laprie, J.-C. (Ed.). Dependability: Basic Concepts and +L+ Terminology. Dependable Computing and Fault-Tolerant +L+ Systems Series, Vol. 5, Springer-Verlag, Vienna, 1992.
[10] Liden, P., Dahlgren, P., Johansson, R., Karlsson, J. On +L+ Latching Probability of Particle Induced Transients in +L+ Combinational Networks. Proc. Intl. Symposium on Fault- +L+ Tolerant Computing (FTCS-24), 1994, pp.340-349.
[11] Mange, D., Sipper, M., Stauffer, A., Tempesti, G. Toward +L+ Robust Integrated Circuits: The Embryonics Approach. Proc. +L+ of the IEEE, vol. 88, No. 4, April 2000, pp. 516-541.
[12] Mange, D. and Tomassini, M. eds. Bio -Inspired Computing +L+ Machines: Towards Novel Computational Architectures. +L+ Presses Polytechniques et Universitaires Romandes, +L+ Lausanne, Switzerland, 1998.
[13] Von Neumann, J. The Computer and the Brain (2nd edition). +L+ Physical Science, 2000.
[14] Von Neumann, J. The Theory of Self-Reproducing +L+ Automata. A. W. Burks, ed. University of Illinois Press, +L+ Urbana, IL, 1966.
[15] Von Neumann, J. Probabilistic Logic and the Synthesis of +L+ Reliable Organisms from Unreliable Components. In C.E. +L+ Shannon, J. McCarthy (eds.) Automata Studies, Annals of +L+ Mathematical Studies 34, Princeton University Press, 1956, +L+ 43-98.
[16] Nielsen, M.A., Chuang, I.L. Quantum Computation and +L+ Quantum Information. Cambridge University Press, 2000.
[17] Ortega, C., Tyrrell, A. Reliability Analysis in Self-Repairing +L+ Embryonic Systems. Proc. 1st NASA/DoD Workshop on +L+ Evolvable Hardware, Pasadena CA, 1999, 120-128.
[18] O’Connor, P.D.T. Practical Reliability Engineering. John +L+ Wiley &amp; Sons, 4th edition, 2002.
[19] Preskill, J. Fault Tolerant Quantum Computation. In H.K. +L+ Lo, S. Popescu and T.P. Spiller, eds. Introduction to +L+ Quantum Computation, World Scientific Publishing Co., +L+ 1998.
197
[20] Prodan, L. Self-Repairing Memory Arrays Inspired by +L+ Biological Processes. Ph.D. Thesis, “Politehnica” University +L+ of Timisoara, Romania, October 14, 2005.
[21] Prodan, L., Udrescu, M., Vladutiu, M. Survivability Analysis +L+ in Embryonics: A New Perspective. Proc. IEEE NASA/DoD +L+ Conference on Evolvable Hardware, Washington DC, 2005, +L+ 280-289.
[22] Prodan, L., Udrescu, M., Vladutiu, M. Self-Repairing +L+ Embryonic Memory Arrays. Proc. IEEE NASA/DoD +L+ Conference on Evolvable Hardware, Seattle WA, 2004, 130- +L+ 137.
[23] Prodan, L., Tempesti, G., Mange, D., and Stauffer, A. +L+ Embryonics: Electronic Stem Cells. Proc. Artificial Life VIII, +L+ The MIT Press, Cambridge MA, 2003, 101-105.
[24] Prodan, L., Tempesti, G., Mange, D., and Stauffer, A. +L+ Embryonics: Artificial Cells Driven by Artificial DNA. +L+ Proc. 4th International Conference on Evolvable Systems +L+ (ICES2001), Tokyo, Japan, LNCS vol. 2210, Springer, +L+ Berlin, 2001, 100-111.
[25] Prodan, L., Tempesti, G., Mange, D., and Stauffer, A. +L+ Biology Meets Electronics: The Path to a Bio-Inspired +L+ FPGA. In Proc. 3rd International Conference on Evolvable +L+ Systems (ICES2000), Edinburgh, Scotland, LNCS 1801, +L+ Springer, Berlin, 2000, 187-196.
[26] Rimen, M., Ohlsson, J., Karlsson, J., Jenn, E., Arlat, J. +L+ Validation of fault tolerance by fault injection in VHDL +L+ simulation models. Rapport LAAS No. 92469, December +L+ 1992.
[27] Rimen, M., Ohlsson, J., Karlsson, J., Jenn, E., Arlat, J. +L+ Design guidelines of a VHDL-based simulation tool for the +L+ validation of fault tolerance. Rapport LAAS No931 70, Esprit +L+ Basic Research Action No. 6362, May 1993.
[28] Shivakumar, P., Kistler, M., Keckler, S.W., Burger, D., +L+ Alvisi, L. Modelling the Effect of Technology Trends on the +L+ Soft Error Rate of Combinational Logic. Proc. Intl. +L+ Conference on Dependable Systems and Networks (DSN), +L+ June 2002, pp. 389-398.
[29] Shor, P. Fault-tolerant quantum computation.
arXiv.org:quant-ph/9605011, 1996.
[30] Shor, P. Algorithms for Quantum Computation: Discrete +L+ Logarithms and Factoring. Proc. 35th Symp. on Foundations +L+ of Computer Science, 1994, pp. 124-134.
[31] Sipper, M., Mange, D., Stauffer, A. Ontogenetic Hardware. +L+ BioSystems, 44, 3, 1997, 193-207.
[32] Sipper, M., Sanchez, E., Mange, D., Tomassini, M., Perez- +L+ Uribe, A., Stauffer, A. A Phylogenetic, Ontogenetic and +L+ Epigenetic View of Bio-Inspired Hardware Systems. IEEE +L+ Transactions on Evolutionary Computation, 1, 1, April 1997, +L+ 83-97.
[33] Steane, A. Multiple Particle Interference and Quantum Error +L+ Correction. Proc. Roy. Soc. Lond. A 452, 1996, pp. 2551.
[34] Udrescu, M. Quantum Circuits Engineering: Efficient +L+ Simulation and Reconfigurable Quantum Hardware. Ph.D. +L+ Thesis, “Politehnica” University of Timisoara, Romania, +L+ November 25, 2005.
[35] Udrescu, M., Prodan, L., Vladutiu, M. Simulated Fault +L+ Injection in Quantum Circuits with the Bubble Bit +L+ Technique. Proc. International Conference &quot;Adaptive and +L+ Natural Computing Algorithms&quot;, pp. 276-279.
[36] Udrescu, M., Prodan, L., Vladutiu, M. The Bubble Bit +L+ Technique as Improvement of HDL-Based Quantum Circuits +L+ Simulation. IEEE 38th Annual Simulation Symposium, San +L+ Diego CA, USA, 2005, pp. 217-224.
[37] Udrescu, M., Prodan, L., Vladutiu, M. Improving Quantum +L+ Circuit Dependability with Reconfigurable Quantum Gate +L+ Arrays. 2nd ACM International Conference on Computing +L+ Frontiers, Ischia, Italy, 2005, pp. 133-144.
[38] Udrescu, M., Prodan, L., Vladutiu, M. Using HDLs for +L+ describing quantum circuits: a framework for efficient +L+ quantum algorithm simulation. Proc. 1st ACM Conference +L+ on Computing Frontiers, Ischia, Italy, 2004, 96-110.
[39] Tempesti, G. A Self-Repairing Multiplexer-Based FPGA +L+ Inspired by Biological Processes. Ph.D. Thesis No. 1827, +L+ Logic Systems Laboratory, The Swiss Federal Institute of +L+ Technology, Lausanne, 1998.
[40] Tempesti, G., Mange, D., Petraglio, E., Stauffer, A., Thoma +L+ Y. Developmental Processes in Silicon: An Engineering +L+ Perspective. Proc. IEEE NASA/DoD Conference on +L+ Evolvable Hardware, Chicago Il, 2003, 265-274.
[41] Viamontes, G., Markov, I., Hayes, J.P. High-performance +L+ QuIDD-based Simulation of Quantum Circuits. Proc. Design +L+ Autom. and Test in Europe (DATE), Paris, France, 2004, pp. +L+ 1354-1359.
[42] Yu, Y., Johnson, B.W. A Perspective on the State of +L+ Research on Fault Injection Techniques. Technical Report +L+ UVA-CSCS-FIT-001, University of Virginia, May 20, 2002.
[43] ***. ITRS – International Technology Roadmap for Semic- +L+ onductors, Emerging Research Devices, 2004, http://www. +L+ itrs.net/Common/2004Update/2004_05_ERD.pdf
[44] ***. Society of Reliability Engineers, http://www.sre.org/ +L+ pubs/
[45] ***. http://www.dependability.org/wg10.4/
198
