Accelerated Focused Crawling through
Online Relevance Feedback*
Soumen Chakrabartit	Kunal Punera	Mallela Subramanyam
IIT Bombay	IIT Bombay	University of Texas, Austin
Abstract
The organization of HTML into a tag tree structure, which +L+ is rendered by browsers as roughly rectangular regions with +L+ embedded text and HREF links, greatly helps surfers locate +L+ and click on links that best satisfy their information need. +L+ Can an automatic program emulate this human behavior +L+ and thereby learn to predict the relevance of an unseen +L+ HREF target page w.r.t. an information need, based on +L+ information limited to the HREF source page? Such a +L+ capability would be of great interest in focused crawling and +L+ resource discovery, because it can fine-tune the priority of +L+ unvisited URLs in the crawl frontier, and reduce the number +L+ of irrelevant pages which are fetched and discarded.
We show that there is indeed a great deal of usable +L+ information on a HREF source page about the relevance +L+ of the target page. This information, encoded suitably, can +L+ be exploited by a supervised apprentice which takes online +L+ lessons from a traditional focused crawler by observing +L+ a carefully designed set of features and events associated +L+ with the crawler. Once the apprentice gets a sufficient +L+ number of examples, the crawler starts consulting it to +L+ better prioritize URLs in the crawl frontier. Experiments on +L+ a dozen topics using a 482-topic taxonomy from the Open +L+ Directory (Dmoz) show that online relevance feedback can +L+ reduce false positives by 30% to 90%.
Categories and subject descriptors:
H.5.4 [Information interfaces and presentation]: +L+ Hypertext/hypermedia; I.5.4 [Pattern recognition]: +L+ Applications, Text processing; I.2.6 [Artificial +L+ intelligence]: Learning; I.2.8 [Artificial intelligence]: +L+ Problem Solving, Control Methods, and Search.
General terms: Algorithms, performance, +L+ measurements, experimentation.
Keywords: Focused crawling, Document object model, +L+ Reinforcement learning.
1 Introduction
Keyword search and clicking on links are the dominant +L+ modes of accessing hypertext on the Web. Support for +L+ keyword search through crawlers and search engines is very +L+ mature, but the surfing paradigm is not modeled or assisted
*(Note: The HTML version of this paper is best viewed using +L+ Microsoft Internet Explorer. To view the HTML version using +L+ Netscape, add the following line to your ~/.Xdefaults or +L+ ~/.Xresources file:
Netscape*documentFonts.charset*adobe-fontspecific: iso-8859-1
For printing use the PDF version, as browsers may not print the +L+ mathematics properly.)
tContact author, email soumen@cse.iitb.ac.in
Copyright is held by the author/owner(s).
WWW2002, May 7–11, 2002, Honolulu, Hawaii, USA. +L+ ACM 1-58113-449-5/02/0005
Figure 1: A basic focused crawler controlled by one topic +L+ classifier/learner.
as well. Support for surfing is limited to the basic interface +L+ provided by Web browsers, except for a few notable research +L+ prototypes.
While surfing, the user typically has a topic-specific +L+ information need, and explores out from a few known +L+ relevant starting points in the Web graph (which may be +L+ query responses) to seek new pages relevant to the chosen +L+ topic/s. While deciding for or against clicking on a specific +L+ link (u, v), humans use a variety of clues on the source +L+ page u to estimate the worth of the (unseen) target page +L+ v, including the tag tree structure of u, text embedded in +L+ various regions of that tag tree, and whether the link is +L+ relative or remote. “Every click on a link is a leap of faith” +L+ [19], but humans are very good at discriminating between +L+ links based on these clues.
Making an educated guess about the worth of clicking +L+ on a link (u, v) without knowledge of the target v is +L+ central to the surfing activity. Automatic programs which +L+ can learn this capability would be valuable for a number +L+ of applications which can be broadly characterized as +L+ personalized, topic-specific information foragers.
Large-scale, topic-specific information gatherers are +L+ called focused crawlers [1, 9, 14, 28, 30]. In contrast to giant, +L+ all-purpose crawlers which must process large portions of +L+ the Web in a centralized manner, a distributed federation of +L+ focused crawlers can cover specialized topics in more depth +L+ and keep the crawl more fresh, because there is less to cover +L+ for each crawler.
In its simplest form, a focused crawler consists of a +L+ supervised topic classifier (also called a ‘learner’) controlling +L+ the priority of the unvisited frontier of a crawler (see +L+ Figure 1). The classifier is trained a priori on document +L+ samples embedded in a topic taxonomy such as Yahoo! +L+ or Dmoz. It thereby learns to label new documents as +L+ belonging to topics in the given taxonomy [2, 5, 21]. The +L+ goal of the focused crawler is to start from nodes relevant +L+ to a focus topic c* in the Web graph and explore links to +L+ selectively collect pages about c*, while avoiding fetching +L+ pages not about c*.
Suppose the crawler has collected a page u and
If Pr(c*|u) is large enough
then enqueue all outlinks v of u +L+ with priority Pr(c*|u)
Dmoz +L+ topic +L+ taxonomy
Class models +L+ consisting of +L+ term stats
Frontier URLS +L+ priority queue
Pick +L+ best
Crawler
Seed +L+ URLs
Baseline learner
Submit page for classification
Newly fetched
page u
Crawl
database
148
encountered in u an unvisited link to v. A simple crawler +L+ (which we call the baseline) will use the relevance of u +L+ to topic c* (which, in a Bayesian setting, we can denote +L+ Pr(c*lu)) as the estimated relevance of the unvisited page +L+ v. This reflects our belief that pages across a hyperlink +L+ are more similar than two randomly chosen pages on the +L+ Web, or, in other words, topics appear clustered in the +L+ Web graph [11, 23]. Node v will be added to the crawler’s +L+ priority queue with priority Pr(c*lu). This is essentially a +L+ “best-first” crawling strategy. When v comes to the head +L+ of the queue and is actually fetched, we can verify if the +L+ gamble paid off, by evaluating Pr(c* lv). The fraction of +L+ relevant pages collected is called the harvest rate. If V +L+ is the set of nodes collected, the harvest rate is defined +L+ as (1/lVl) E vEVPr(c*lv). Alternatively, we can measure +L+ the loss rate, which is one minus the harvest rate, i.e., the +L+ (expected) fraction of fetched pages that must be thrown +L+ away. Since the effort on relevant pages is well-spent, +L+ reduction in loss rate is the primary goal and the most +L+ appropriate figure of merit.
For focused crawling applications to succeed, the “leap +L+ of faith” from u to v must pay off frequently. In other words, +L+ if Pr(c*lv) is often much less than the preliminary estimate +L+ Pr(c*lu), a great deal of network traffic and CPU cycles +L+ are being wasted eliminating bad pages. Experience with +L+ random walks on the Web show that as one walks away +L+ from a fixed page u0 relevant to topic c0, the relevance of +L+ successive nodes u1, u2,... to c0 drops dramatically within +L+ a few hops [9, 23]. This means that only a fraction of out- +L+ links from a page is typically worth following. The average +L+ out-degree of the Web graph is about 7 [29]. Therefore, a +L+ large number of page fetches may result in disappointment, +L+ especially if we wish to push the utility of focused crawling +L+ to topic communities which are not very densely linked.
Even w.r.t. topics that are not very narrow, the +L+ number of distracting outlinks emerging from even fairly +L+ relevant pages has grown substantially since the early +L+ days of Web authoring [4]. Template-based authoring, +L+ dynamic page generation from semi-structured databases, +L+ ad links, navigation panels, and Web rings contribute many +L+ irrelevant links which reduce the harvest rate of focused +L+ crawlers. Topic-based link discrimination will also reduce +L+ these problems.
1.1 Our contribution: Leaping with more faith +L+ In this paper we address the following questions:
How much information about the topic of the HREF +L+ target is available and/or latent in the HREF source page, +L+ its tag-tree structure, and its text? Can these sources be +L+ exploited for accelerating a focused crawler? 
Our basic idea is to use two classifiers. Earlier, the regular +L+ baseline classifier was used to assign priorities to unvisited +L+ frontier nodes. This no longer remains its function. The role +L+ of assigning priorities to unvisited URLs in the crawl frontier +L+ is now assigned to a new learner called the apprentice, and +L+ the priority of v is specific to the features associated with +L+ the (u, v) link which leads to it1. The features used by the +L+ apprentice are derived from the Document Object Model or
&apos;If many u’s link to a single v, it is easiest to freeze the priority of
v when the first-visited u linking to v is assessed, but combinations +L+ of scores are also possible.
Figure 2: The apprentice is continually presented with +L+ training cases (u, v) with suitable features. The apprentice +L+ is interposed where new outlinks (u, v) are registered with +L+ the priority queue, and helps assign the unvisited node v a +L+ better estimate of its relevance.
DOM (http://www.w3.org/DOM/) of u. Meanwhile, the role +L+ of the baseline classifier becomes one of generating training +L+ instances for the apprentice, as shown in Figure 2. We may +L+ therefore regard the baseline learner as a critic or a trainer, +L+ which provides feedback to the apprentice so that it can +L+ improve “on the job.”
The critic-apprentice paradigm is related to reinforce- +L+ ment learning and AI programs that learn to play games +L+ [26, §1.2]. We argue that this division of labor is natural +L+ and effective. The baseline learner can be regarded as +L+ a user specification for what kind of content is desired. +L+ Although we limit ourselves to a generative statistical model +L+ for this specification, this can be an arbitrary black-box +L+ predicate. For rich and meaningful distinction between +L+ Web communities and topics, the baseline learner needs +L+ to be fairly sophisticated, perhaps leveraging off human +L+ annotations on the Web (such as topic directories). In +L+ contrast, the apprentice specializes in how to locate pages +L+ to satisfy the baseline learner. Its feature space is more +L+ limited, so that it can train fast and adapt nimbly to +L+ changing fortunes at following links during a crawl. In +L+ Mitchell’s words [27], the baseline learner recognizes “global +L+ regularity” while the apprentice helps the crawler adapt +L+ to “local regularity.” This marked asymmetry between +L+ the classifiers distinguishes our approach from Blum and +L+ Mitchell’s co-training technique [3], in which two learners +L+ train each other by selecting unlabeled instances.
Using a dozen topics from a topic taxonomy derived +L+ from the Open Directory, we compare our enhanced crawler +L+ with the baseline crawler. The number of pages that are +L+ thrown away (because they are irrelevant), called the loss +L+ rate, is cut down by 30–90%. We also demonstrate that +L+ the fine-grained tag-tree model, together with our synthesis +L+ and encoding of features for the apprentice, are superior to +L+ simpler alternatives.
1.2 Related work
Optimizing the priority of unvisited URLs on the crawl +L+ frontier for specific crawling goals is not new. FISHSEARCH +L+ by De Bra et al. [12, 13] and SHARKSEARCH by Hersovici +L+ et al. [16] were some of the earliest systems for localized +L+ searches in the Web graph for pages with specified keywords.
... submit (u,v) +L+ to the apprentice
If Pr(c*|u)is +L+ large enough...
Dmoz +L+ topic +L+ taxonomy
Submit page for classification
Baseline learner (Critic)
+ -
Apprentice
assigns more
accurate priority
to node v	Frontier URLS
priority queue
Class models +L+ consisting of +L+ term stats
Apprentice learner
Class +L+ models
Newly fetched
page u
Crawler
Pick +L+ best
Online +L+ training
An instance (u,v) +L+ for the apprentice
Pr(c*|v)
u
Crawl +L+ database
Pr(c|u) for +L+ all classes c
v
149
In another early paper, Cho et al. [10] experimented with a +L+ variety of strategies for prioritizing how to fetch unvisited +L+ URLs. They used the anchor text as a bag of words to +L+ guide link expansion to crawl for pages matching a specified +L+ keyword query, which led to some extent of differentiation +L+ among out-links, but no trainer-apprentice combination was +L+ involved. No notion of supervised topics had emerged at +L+ that point, and simple properties like the in-degree or the +L+ presence of specified keywords in pages were used to guide +L+ the crawler.
Topical locality on the Web has been studied for a few +L+ years. Davison made early measurements on a 100000- +L+ node Web subgraph [11] collected by the DISCOWEB system. +L+ Using the standard notion of vector space TFIDF similarity +L+ [31], he found that the endpoints of a hyperlink are much +L+ more similar to each other than two random pages, and that +L+ HREFs close together on a page link to documents which are +L+ more similar than targets which are far apart. Menczer has +L+ made similar observations [23]. The HYPERCLASS hypertext +L+ classifier also uses such locality patterns for better semi- +L+ supervised learning of topics [7], as does IBM’s Automatic +L+ Resource Compilation (ARC) and Clever topic distillation +L+ systems [6, 8].
Two important advances have been made beyond the +L+ baseline best-first focused crawler: the use of context graphs +L+ by Diligenti et al. [14] and the use of reinforcement learning +L+ by Rennie and McCallum [30]. Both techniques trained +L+ a learner with features collected from paths leading up to +L+ relevant nodes rather than relevant nodes alone. Such paths +L+ may be collected by following backlinks.
Diligenti et al. used a classifier (learner) that regressed +L+ from the text of u to the estimated link distance from u to +L+ some relevant page w, rather than the relevance of u or an +L+ outlink (u, v), as was the case with the baseline crawler. +L+ This lets their system continue expanding u even if the +L+ reward for following a link is not immediate, but several +L+ links away. However, they do favor links whose payoffs +L+ are closest. Our work is specifically useful in conjunction +L+ with the use of context graphs: when the context graph +L+ learner predicts that a goal is several links away, it is crucial +L+ to offer additional guidance to the crawler based on local +L+ structure in pages, because the fan-out at that radius could +L+ be enormous.
Rennie and McCallum [30] also collected paths leading +L+ to relevant nodes, but they trained a slightly different +L+ classifier, for which:
9 An instance was a single HREF link like (u, v).
9 The features were terms from the title and headers +L+ (&lt;h1&gt; ... &lt;/h1&gt; etc.) of u, together with the text +L+ in and ‘near’ the anchor (u, v). Directories and +L+ pathnames were also used. (We do not know the +L+ precise definition of ‘near’, or how these features were +L+ encoded and combined.)
9 The prediction was a discretized estimate of the +L+ number of relevant nodes reachable by following (u, v), +L+ where the reward from goals distant from v was +L+ geometrically discounted by some factor γ &lt; 1/2 per +L+ hop.
Rennie and McCallum obtained impressive harvests of +L+ research papers from four Computer Science department +L+ sites, and of pages about officers and directors from 26 +L+ company Websites.
Lexical proximity and contextual features have been +L+ used extensively in natural language processing for disam- +L+ biguating word sense [15]. Compared to plain text, DOM +L+ trees and hyperlinks give us a richer set of potential features.
Aggarwal et al. have proposed an “intelligent crawling” +L+ framework [1] in which only one classifier is used, but similar +L+ to our system, that classifier trains as the crawl progresses. +L+ They do not use our apprentice-critic approach, and do not +L+ exploit features derived from tag-trees to guide the crawler.
The “intelligent agents” literature has brought forth +L+ several systems for resource discovery and assistance to +L+ browsing [19]. They range between client- and site-level +L+ tools. Letizia [18], Powerscout, and WebWatcher [17] are +L+ such systems. Menczer and Belew proposed InfoSpiders +L+ [24], a collection of autonomous goal-driven crawlers without +L+ global control or state, in the style of genetic algorithms. A +L+ recent extensive study [25] comparing several topic-driven +L+ crawlers including the best-first crawler and InfoSpiders +L+ found the best-first approach to show the highest harvest +L+ rate (which our new system outperforms).
In all the systems mentioned above, improving the +L+ chances of a successful “leap of faith” will clearly reduce +L+ the overheads of fetching, filtering, and analyzing pages. +L+ Furthermore, whereas we use an automatic first-generation +L+ focused crawler to generate the input to train the apprentice, +L+ one can envisage specially instrumented browsers being used +L+ to monitor users as they seek out information.
We distinguish our work from prior art in the following +L+ important ways:
Two classifiers: We use two classifiers. The first one is +L+ used to obtain ‘enriched’ training data for the second one. +L+ (A breadth-first or random crawl would have a negligible +L+ fraction of positive instances.) The apprentice is a simplified +L+ reinforcement learner. It improves the harvest rate, thereby +L+ ‘enriching’ the data collected and labeled by the first learner +L+ in turn.
No manual path collection: Our two-classifier frame- +L+ work essentially eliminates the manual effort needed to +L+ create reinforcement paths or context graphs. The input +L+ needed to start off a focused crawl is just a pre-trained topic +L+ taxonomy (easily available from the Web) and a few focus +L+ topics.
Online training: Our apprentice trains continually, ac- +L+ quiring ever-larger vocabularies and improving its accuracy +L+ as the crawl progresses. This property holds also for the +L+ “intelligent crawler” proposed by Aggarwal et al., but they +L+ have a single learner, whose drift is controlled by precise +L+ relevance predicates provided by the user.
No manual feature tuning: Rather than tune ad-hoc +L+ notions of proximity between text and hyperlinks, we encode +L+ the features of link (u, v) using the DOM-tree of u, and +L+ automatically learn a robust definition of ‘nearness’ of a +L+ textual feature to (u, v). In contrast, Aggarwal et al +L+ use many tuned constants combining the strength of text- +L+ and link-based predictors, and Rennie et al. use domain +L+ knowledge to select the paths to goal nodes and the word +L+ bags that are submitted to their learner.
150
2 Methodology and algorithms
We first review the baseline focused crawler and then +L+ describe how the enhanced crawler is set up using the +L+ apprentice-critic mechanism.
2.1 The baseline focused crawler
The baseline focused crawler has been described in detail +L+ elsewhere [9, 14], and has been sketched in Figure 1. Here +L+ we review its design and operation briefly.
There are two inputs to the baseline crawler.
•	A topic taxonomy or hierarchy with example URLs +L+ for each topic.
•	One or a few topics in the taxonomy marked as the +L+ topic(s) of focus.
Although we will generally use the terms ‘taxonomy’ and +L+ ‘hierarchy’, a topic tree is not essential; all we really need is +L+ a two-way classifier where the classes have the connotations +L+ of being ‘relevant’ or ‘irrelevant’ to the topic(s) of focus. +L+ A topic hierarchy is proposed purely to reduce the tedium +L+ of defining new focused crawls. With a two-class classifier, +L+ the crawl administrator has to seed positive and negative +L+ examples for each crawl. Using a taxonomy, she composes +L+ the ‘irrelevant’ class as the union of all classes that are not +L+ relevant. Thanks to extensive hierarchies like Dmoz in the +L+ public domain, it should be quite easy to seed topic-based +L+ crawls in this way.
The baseline crawler maintains a priority queue on the +L+ estimated relevance of nodes v which have not been visited, +L+ and keeps removing the highest priority node and visiting it, +L+ expanding its outlinks and checking them into the priority +L+ queue with the relevance score of v in turn. Despite its +L+ extreme simplicity, the best-first crawler has been found to +L+ have very high harvest rates in extensive evaluations [25].
Why do we need negative examples and negative classes +L+ at all? Instead of using class probabilities, we could maintain +L+ a priority queue on, say, the TFIDF cosine similarity +L+ between u and the centroid of the seed pages (acting as an +L+ estimate for the corresponding similarity between v and the +L+ centroid, until v has been fetched). Experience has shown +L+ [32] that characterizing a negative class is quite important to +L+ prevent the centroid of the crawled documents from drifting +L+ away indefinitely from the desired topic profile.
In this paper, the baseline crawler also has the implicit +L+ job of gathering instances of successful and unsuccessful +L+ “leaps of faith” to submit to the apprentice, discussed next.
2.2 The basic structure of the apprentice +L+ learner
In estimating the worth of traversing the HREF (u, v), we +L+ will limit our attention to u alone. The page u is modeled +L+ as a tag tree (also called the Document Object Model or +L+ DOM). In principle, any feature from u, even font color and +L+ site membership may be perfect predictors of the relevance +L+ of v. The total number of potentially predictive features will +L+ be quite staggering, so we need to simplify the feature space +L+ and massage it into a form suited to conventional learning +L+ algorithms. Also note that we specifically study properties +L+ of u and not larger contexts such as paths leading to u, +L+ meaning that our method may become even more robust and
useful in conjunction with context graphs or reinforcement +L+ along paths.
Initially, the apprentice has no training data, and passes +L+ judgment on (u, v) links according to some fixed prior +L+ obtained from a baseline crawl run ahead of time (e.g., see +L+ the statistics in §3.3). Ideally, we would like to train the +L+ apprentice continuously, but to reduce overheads, we declare +L+ a batch size between a few hundred and a few thousand +L+ pages. After every batch of pages is collected, we check if any +L+ page u fetched before the current batch links to some page +L+ v in the batch. If such a (u, v) is found, we extract suitable +L+ features for (u, v) as described later in this section, and add +L+ ((u, v), Pr(c* |v)� as another instance of the training data for +L+ the apprentice. Many apprentices, certainly the simple naive +L+ Bayes and linear perceptrons that we have studied, need not +L+ start learning from scratch; they can accept the additional +L+ training data with a small additional computational cost.
2.2.1 Preprocessing the DOM tree
First, we parse u and form the DOM tree for u. Sadly, +L+ much of the HTML available on the Web violates any +L+ HTML standards that permit context-free parsing, but +L+ a variety of repair heuristics (see, e.g., HTML Tidy, +L+ available at http://www.w3.org/People/Raggett/tidy/) +L+ let us generate reasonable DOM trees from bad HTML.
Figure 3: Numbering of DOM leaves used to derive offset +L+ attributes for textual tokens. ‘@’ means “is at offset”.
Second, we number all leaf nodes consecutively from left +L+ to right. For uniformity, we assign numbers even to those +L+ DOM leaves which have no text associated with them. The +L+ specific &lt;a href ... &gt; which links to v is actually an internal
node a,, which is the root of the subtree containing the
anchor text of the link (u, v). There may be other element
tags such as &lt;em&gt; or &lt;b&gt; in the subtree rooted at a,. Let +L+ the leaf or leaves in this subtree be numbered f(a,) through
r(a,) ≥ f(a,). We regard the textual tokens available from
any of these leaves as being at DOM offset zero w.r.t. the +L+ (u, v) link. Text tokens from a leaf numbered p, to the left of
f(a,), are at negative DOM offset p — f(a,). Likewise, text
from a leaf numbered p to the right of r(a,) are at positive +L+ DOM offset p — r(a,). See Figure 3 for an example.
2.2.2 Features derived from the DOM and text +L+ tokens
Many related projects mentioned in §1.2 use a linear notion +L+ of proximity between a HREF and textual tokens. In the +L+ ARC system, there is a crude cut-off distance measured
@-2		@-1		@0		@0		@1		@2		@3
TEXT
tt
li
TEXT
TEXT
ul +L+ li +L+ a
HREF
font
TEXT
TEXT
li
TEXT em
TEXT
li
151
in bytes to the left and right of the anchor. In the +L+ Clever system, distance is measured in tokens, and the +L+ importance attached to a token decays with the distance. +L+ In reinforcement learning and intelligent predicate-based +L+ crawling, the exact specification of neighborhood text is not +L+ known to us. In all cases, some ad-hoc tuning appears to be +L+ involved.
We claim (and show in §3.4) that the relation between +L+ the relevance of the target v of a HREF (u, v) and the +L+ proximity of terms to (u, v) can be learnt automatically. The +L+ results are better than ad-hoc tuning of cut-off distances, +L+ provided the DOM offset information is encoded as features +L+ suitable for the apprentice.
One obvious idea is to extend the Clever model: a page +L+ is a linear sequence of tokens. If a token t is distant x from +L+ the HREF (u, v) in question, we encode it as a feature (t, x). +L+ Such features will not be useful because there are too many +L+ possible values of x, making the (t, x) space too sparse to +L+ learn well. (How many HREFS will be exactly five tokens +L+ from the term ‘basketball’?)
Clearly, we need to bucket x into a small number of +L+ ranges. Rather than tune arbitrary bucket boundaries by +L+ hand, we argue that DOM offsets are a natural bucketing +L+ scheme provided by the page author. Using the node +L+ numbering scheme described above, each token t on page u +L+ can be annotated w.r.t. the link (u, v) (for simplicity assume +L+ there is only one such link) as (t, d), where d is the DOM +L+ offset calculated above. This is the main set of features +L+ used by the apprentice. We shall see that the apprentice +L+ can learn to limit IdI to less than dmax = 5 in most cases, +L+ which reduces its vocabulary and saves time.
A variety of other feature encodings suggest themselves. +L+ We are experimenting with some in ongoing work (§4), +L+ but decided against some others. For example, we do not +L+ expect gains from encoding specific HTML tag names owing +L+ to the diversity of authoring styles. Authors use &lt;div&gt;, +L+ &lt;span&gt;, &lt;layer&gt; and nested tables for layout control in +L+ non-standard ways; these are best deflated to a nameless +L+ DOM node representation. Similar comments apply to +L+ HREF collections embedded in &lt;ul&gt;, &lt;ol&gt;, &lt;td&gt; and +L+ &lt;dd&gt;. Font and lower/upper case information is useful +L+ for search engines, but would make features even sparser +L+ for the apprentice. Our representation also flattens two- +L+ dimensional tables to their “row-major” representation.
The features we ignore are definitely crucial for other +L+ applications, such as information extraction. We did not +L+ see any cases where this sloppiness led to a large loss rate. +L+ We would be surprised to see tables where relevant links +L+ occurred in the third column and irrelevant links in the fifth, +L+ or pages where they are rendered systematically in different +L+ fonts and colors, but are not otherwise demarcated by the +L+ DOM structure.
2.2.3 Non-textual features
Limiting d may lead us to miss features of u that may be +L+ useful at the whole-page level. One approach would be to use +L+ “d = oo” for all d larger in magnitude than some threshold. +L+ But this would make our apprentice as bulky and slow to +L+ train as the baseline learner.
Instead, we use the baseline learner to abstract u for +L+ the apprentice. Specifically, we use a naive Bayes baseline +L+ learner to classify u, and use the vector of class probabilities +L+ returned as features for the apprentice. These features can +L+ help the apprentice discover patterns such as
“Pages about /Recreation/Boating/Sailing often +L+ link to pages about /Sports/Canoe_and_Kayaking.” 
This also covers for the baseline classifier confusing between +L+ classes with related vocabulary, achieving an effect similar +L+ to context graphs.
Another kind of feature can be derived from co-citation. +L+ If v1 has been fetched and found to be relevant and HREFS +L+ (u, v1) and (u, v2) are close to each other, v2 is likely to +L+ be relevant. Just like textual tokens were encoded as (t, d) +L+ pairs, we can represent co-citation features as (p, d), where +L+ p is a suitable representation of relevance.
Many other features can be derived from the DOM tree +L+ and added to our feature pool. We discuss some options +L+ in §4. In our experience so far, we have found the (t, d) +L+ features to be most useful. For simplicity, we will limit our +L+ subsequent discussion to (t, d) features only.
2.3 Choices of learning algorithms for the +L+ apprentice
Our feature set is thus an interesting mix of categorical, +L+ ordered and continuous features:
9 Term tokens (t, d) have a categorical component t and +L+ a discrete ordered component d (which we may like to +L+ smooth somewhat). Term counts are discrete but can +L+ be normalized to constant document length, resulting +L+ in continuous attribute values.
9 Class names are discrete and may be regarded as
synthetic terms. The probabilities are continuous.
The output we desire is an estimate of Pr(c* Iv), given all the +L+ observations about u and the neighborhood of (u, v) that +L+ we have discussed. Neural networks are a natural choice +L+ to accommodate these requirements. We first experimented +L+ with a simple linear perceptron, training it with the delta +L+ rule (gradient descent) [26]. Even for a linear perceptron, +L+ convergence was surprisingly slow, and after convergence, +L+ the error rate was rather high. It is likely that local +L+ optima were responsible, because stability was generally +L+ poor, and got worse if we tried to add hidden layers or +L+ sigmoids. In any case, convergence was too slow for use +L+ as an online learner. All this was unfortunate, because the +L+ direct regression output from a neural network would be +L+ convenient, and we were hoping to implement a Kohonen +L+ layer for smoothing d.
In contrast, a naive Bayes (NB) classifier worked very +L+ well. A NB learner is given a set of training documents, +L+ each labeled with one of a finite set of classes/topic. A +L+ document or Web page u is modeled as a multiset or bag +L+ of words, {(T,n(u,T))} where T is a feature which occurs +L+ n(u, T) times in u. In ordinary text classification (such as +L+ our baseline learner) the features T are usually single words. +L+ For our apprentice learner, a feature T is a (t, d) pair.
NB classifiers can predict from a discrete set of classes, +L+ but our prediction is a continuous (probability) score. To +L+ bridge this gap, We used a simple two-bucket (low/high +L+ relevance) special case of Torgo and Gama’s technique of +L+ using classifiers for discrete labels for continuous regression +L+ [33], using “equally probable intervals” as far as possible.
152
Torgo and Gama recommend using a measure of centrality, +L+ such as the median, of each interval as the predicted value of +L+ that class. Rennie and McCallum [30] corroborate that 2–3 +L+ bins are adequate. As will be clear from our experiments, the +L+ medians of our ‘low’ and ‘high’ classes are very close to zero +L+ and one respectively (see Figure 5). Therefore, we simply +L+ take the probability of the ‘high’ class as the prediction from +L+ our naive Bayes apprentice.
The prior probability of class c, denoted Pr(c) is the +L+ fraction of training documents labeled with class c. The NB +L+ model is parameterized by a set of numbers 0c,T which is +L+ roughly the rate of occurrence of feature τ in class c, more +L+ exactly,
ITI + Pu T n(u τ,), (1)
where Vc is the set of Web pages labeled with c and T is the +L+ entire vocabulary. The NB learner assumes independence +L+ between features, and estimates
Pr(cIu) a Pr(c) Pr(uIc) ≈ Pr(c) 11 0n (u,T)
c,T. (2)
TEu
Nigam et al. provide further details [22].
3 Experimental study
Our experiments were guided by the following requirements. +L+ We wanted to cover a broad variety of topics, some ‘easy’ and +L+ some ‘di cult’, in terms of the harvest rate of the baseline +L+ crawler. Here is a quick preview of our results.
•	The apprentice classifier achieves high accuracy in +L+ predicting the relevance of unseen pages given (t, d) +L+ features. It can determine the best value of dmax to +L+ use, typically, 4–6.
•	Encoding DOM offsets in features improves the +L+ accuracy of the apprentice substantially, compared +L+ to a bag of ordinary words collected from within the +L+ same DOM offset window.
•	Compared to a baseline crawler, a crawler that is +L+ guided by an apprentice (trained offiine) has a 30% +L+ to 90% lower loss rate. It finds crawl paths never +L+ expanded by the baseline crawler.
•	Even if the apprentice-guided crawler is forced to +L+ stay within the (inferior) Web graph collected by the +L+ baseline crawler, it collects the best pages early on.
•	The apprentice is easy to train online. As soon as it
starts guiding the crawl, loss rates fall dramatically.
•	Compared to (t, d) features, topic- or cocitation-based +L+ features have negligible effect on the apprentice.
To run so many experiments, we needed three highly +L+ optimized and robust modules: a crawler, a HTML-to-DOM +L+ converter, and a classifier.
We started with the w3c-libwww crawling library from +L+ http://www.w3c.org/Library/, but replaced it with our +L+ own crawler because we could effectively overlap DNS +L+ lookup, HTTP access, and disk access using a select over +L+ all socket/file descriptors, and prevent memory leaks visible +L+ in w3c-libwww. With three caching DNS servers, we could +L+ achieve over 90% utilization of a 2Mbps dedicated ISP +L+ connection.
We used the HTML parser libxml2 library to extract +L+ the DOM from HTML, but this library has memory leaks, +L+ and does not always handle poorly written HTML well. We +L+ had some stability problems with HTML Tidy (http: //www. +L+ w3.org/People/Raggett/tidy/), the well-known HTML +L+ cleaner which is very robust to bad HTML. At present we +L+ are using libxml2 and are rolling our own HTML parser and +L+ cleaner for future work.
We intend to make our crawler and HTML parser code +L+ available in the public domain for research use.
For both the baseline and apprentice classifier we used +L+ the public domain BOW toolkit and the Rainbow naive +L+ Bayes classifier created by McCallum and others [20]. Bow +L+ and Rainbow are very fast C implementations which let us +L+ classify pages in real time as they were being crawled.
3.1 Design of the topic taxonomy
We downloaded from the Open Directory (http://dmoz. +L+ org/) an RDF file with over 271954 topics arranged in a +L+ tree hierarchy with depth at least 6, containing a total of +L+ about 1697266 sample URLs. The distribution of samples +L+ over topics was quite non-uniform. Interpreting the tree as +L+ an is-a hierarchy meant that internal nodes inherited all +L+ examples from descendants, but they also had their own +L+ examples. Since the set of topics was very large and many +L+ topics had scarce training data, we pruned the Dmoz tree +L+ to a manageable frontier by following these steps:
1. Initially we placed example URLs in both internal and +L+ leaf nodes, as given by Dmoz.
2.We fixed a minimum per-class training set size of k = +L+ 300 documents.
3.We iteratively performed the following step as long +L+ as possible: we found a leaf node with less than k +L+ example URLs, moved all its examples to its parent, +L+ and deleted the leaf.
4.To each internal node c, we attached a leaf +L+ subdirectory called Other. Examples associated +L+ directly with c were moved to this Other subdirectory.
5. Some topics were populated out of proportion, either +L+ at the beginning or through the above process. We +L+ made the class priors more balanced by sampling +L+ down the large classes so that each class had at most +L+ 300 examples.
The resulting taxonomy had 482 leaf nodes and a total +L+ of 144859 sample URLs. Out of these we could successfully +L+ fetch about 120000 URLs. At this point we discarded the +L+ tree structure and considered only the leaf topics. Training +L+ time for the baseline classifier was about about two hours +L+ on a 729MHz Pentium III with 256kB cache and 512MB +L+ RAM. This was very fast, given that 1.4GB of HTML text +L+ had to be processed through Rainbow. The complete listing +L+ of topics can be obtained from the authors.
3.2 Choice of topics
Depending on the focus topic and prioritization strategy, +L+ focused crawlers may achieve diverse harvest rates. Our
0c,T =
1 + Pu∈VC n(u, τ)
153
early prototype [9] yielded harvest rates typically between +L+ 0.25 and 0.6. Rennie and McCallum [30] reported recall +L+ and not harvest rates. Diligenti et al. [14] focused on very +L+ specific topics where the harvest rate was very low, 4–6%. +L+ Obviously, the maximum gains shown by a new idea in +L+ focused crawling can be sensitive to the baseline harvest +L+ rate.
To avoid showing our new system in an unduly positive +L+ or negative light, we picked a set of topics which were fairly +L+ diverse, and appeared to be neither too broad to be useful +L+ (e.g., /Arts, /Science) nor too narrow for the baseline +L+ crawler to be a reasonable adversary. We list our topics +L+ in Figure 4. We chose the topics without prior estimates of +L+ how well our new system would work, and froze the list +L+ of topics. All topics that we experimented with showed +L+ visible improvements, and none of them showed deteriorated +L+ performance.
3.3 Baseline crawl results
We will skip the results of breadth-first or random crawling +L+ in our commentary, because it is known from earlier work +L+ on focused crawling that our baseline crawls are already +L+ far better than breadth-first or random crawls. Figure 5 +L+ shows, for most of the topics listed above, the distribution +L+ of page relevance after running the baseline crawler to +L+ collect roughly 15000 to 25000 pages per topic. The E +L+ baseline crawler used a standard naive Bayes classifier on +L+ the ordinary term space of whole pages. We see that the +L+ relevance distribution is bimodal, with most pages being +L+ very relevant or not at all. This is partly, but only partly, a +L+ result of using a multinomial naive Bayes model. The naive +L+ Bayes classifier assumes term independence and multiplies +L+ together many (small) term probabilities, with the result +L+ that the winning class usually beats all others by a large +L+ margin in probability. But it is also true that many outlinks +L+ lead to pages with completely irrelevant topics. Figure 5 +L+ gives a clear indication of how much improvement we can +L+ expect for each topic from our new algorithm.
3.4 DOM window size and feature selection
A key concern for us was how to limit the maximum window
width so that the total number of synthesized (t, d) features
remains much smaller than the training data for the baseline
classifier, enabling the apprentice to be trained or upgraded
in a very short time. At the same time, we did not want
to lose out on medium- to long-range dependencies between
significant tokens on a page and the topic of HREF targets
in the vicinity. We eventually settled for a maximum DOM
window size of 5. We made this choice through the following
experiments.
The easiest initial approach was an end-to-end cross- +L+ validation of the apprentice for various topics while +L+ increasing dm x. We observed an initial increase in the +L+ validation accuracy when the DOM window size was +L+ increased beyond 0. However, the early increase leveled +L+ off or even reversed after the DOM window size was +L+ increased beyond 5. The graphs in Figure 6 display these +L+ results. We see that in the Chess category, though the +L+ validation accuracy increases monotonically, the gains are +L+ less pronounced after dm x exceeds 5. For the AI category, +L+ accuracy fell beyond dm x = 4.
Topic	#Good	#Bad
/Arts/Music/Styles/Classical/Composers	24000	13000
/Arts/Performing-Arts/Dance/Folk-Dancing	7410	8300
/Business/Industries.../Livestock/Horses...	17000	7600
/Computers/Artificial-Intelligence	7701	14309
/Computers/Software/Operating-Systems/Linux	17500	9300
/Games/Board-Games/C/Chess	17000	4600
/Health/Conditions-and-Diseases/Cancer	14700	5300
/Home/Recipes/Soups-and-Stews	20000	3600
/Recreation/Outdoors/Fishing/Fly-Fishing	12000	13300
/Recreation/Outdoors/Speleology	6717	14890
/Science/Astronomy	14961	5332
/Science/Earth-Sciences/Meteorology	19205	8705
/Sports/Basketball	26700	2588
/Sports/Canoe-and-Kayaking	12000	12700
/Sports/Hockey/Ice-Hockey	17500	17900
Figure 4: We chose a variety of topics which were neither +L+ too broad nor too narrow, so that the baseline crawler +L+ was a reasonable adversary. #Good (#Bad) show the +L+ approximate number of pages collected by the baseline +L+ crawler which have relevance above (below) 0.5, which +L+ indicates the relative difficulty of the crawling task.
Figure 5: All of the baseline classifiers have harvest rates +L+ between 0.25 and 0.6, and all show strongly bimodal +L+ relevance score distribution: most of the pages fetched are +L+ very relevant or not at all.
It is important to notice that the improvement in +L+ accuracy is almost entirely because with increasing number +L+ of available features, the apprentice can reject negative +L+ (low relevance) instances more accurately, although the +L+ accuracy for positive instances decreases slightly. Rejecting +L+ unpromising outlinks is critical to the success of the +L+ enhanced crawler. Therefore we would rather lose a little +L+ accuracy for positive instances rather than do poorly on the +L+ negative instances. We therefore chose dm x to be either 4 +L+ or 5 for all the experiments.
We verified that adding offset information to text tokens +L+ was better than simply using plain text near the link [8]. +L+ One sample result is shown in Figure 7. The apprentice +L+ accuracy decreases with dm x if only text is used, whereas +L+ it increases if offset information is provided. This highlights
d #pages
100000
10000
1000
100
10
Relevance probability
AI
Astronomy +L+ Basketball +L+ Cancer
Chess
Composers +L+ FlyFishing +L+ FolkDance
Horses
IceHockey
Kayaking
Linux
Meteorology
Soups +L+ Tobacco
154
Chess
Figure 6: There is visible improvement in the accuracy +L+ of the apprentice if dmax is made larger, up to about 5– +L+ 7 depending on topic. The effect is more pronounced on +L+ the the ability to correctly reject negative (low relevance) +L+ outlink instances. ‘Average’ is the microaverage over all +L+ test instances for the apprentice, not the arithmetic mean +L+ of ‘Positive’ and ‘Negative’.
Figure 7: Encoding DOM offset information with textual +L+ features boosts the accuracy of the apprentice substantially.
the importance of designing proper features.
To corroborate the useful ranges of dmax above, we +L+ compared the value of average mutual information gain for +L+ terms found at various distances from the target HREF. +L+ The experiments revealed that the information gain of terms +L+ found further away from the target HREF was generally +L+ lower than those that were found closer, but this reduction +L+ was not monotonic. For instance, the average information
Figure 8: Information gain variation plotted against +L+ distance from the target HREF for various DOM window +L+ sizes. We observe that the information gain is insensitive to +L+ dmax.
gain at d = —2 was higher than that at d = —1; see Figure 8.
For each DOM window size, we observe that the information +L+ gain varies in a sawtooth fashion; this intriguing observation +L+ is explained shortly. The average information gain settled +L+ to an almost constant value after distance of 5 from the +L+ target URL. We were initially concerned that to keep the +L+ computation cost manageable, we would need some cap on +L+ dmax even while measuring information gain, but luckily, +L+ the variation of information gain is insensitive to dmax, as +L+ Figure 8 shows. These observations made our final choice of +L+ dmax easy.
In a bid to explain the occurrence of the unexpected
saw-tooth form in Figure 8 we measured the rate B(t,d) at
which term t occurred at offset d, relative to the total count
of all terms occurring at offset d. (They are roughly the
multinomial naive Bayes term probability parameters.) For
fixed values of d, we calculated the sum of B values of terms
found at those offsets from the target HREF. Figure 9(a)
shows the plot of these sums to the distance(d) for various
categories. The B values showed a general decrease as the
distances from the target HREF increased, but this decrease,
like that of information gain, was not monotonic. The B
values of the terms at odd numbered distances from the
target HREF were found to be lower than those of the
terms present at the even positions. For instance, the sum
of B values of terms occurring at distance —2 were higher +L+ than that of terms at position —1. This observation was
explained by observing the HTML tags that are present +L+ at various distances from the target HREF. We observed +L+ that tags located at odd d are mostly non-text tags, thanks +L+ to authoring idioms such as &lt;li&gt;&lt;a ... &gt;&lt;li&gt;&lt;a ... &gt; and +L+ &lt;a...&gt;&lt;br&gt;&lt;a ... &gt;&lt;br&gt; etc. A plot of the frequency of +L+ HTML tags against the distance from the HREF at which
0	2	4	6	8
d_max
90
85
AI
80
75
70
65
Negative +L+ Positive +L+ Average
0 2 dm6 8 +L+ ax
100
95
90
85
80
75
70
65
Negative +L+ Positive +L+ Average
0	1	2	3	4	5	6	7	8
d_max
Text +L+ Offset
86 +L+ 84 +L+ 82 +L+ 80 +L+ 78
76
AI
Chess
d_max=8 +L+ d_max=5 +L+ d_max=4 +L+ d_max=3
-8	-6	-4	-2	0	2	4	6	8
d
0.0002
0.00018
0.00016
0.00014
0.00012
0.0001
0.00008
0.00006
0.00004
0.00002
AI
-8	-6	-4	-2	0	2	4	6
d
9.00E-05
8.00E-05
6.00E-05
5.00E-05
4.00E-05
7.00E-05
1.00E-04
d_max=8 +L+ d_max=5 +L+ d_max=4 +L+ d_max=3
155
Figure 9: Variation of (a) relative term frequencies and +L+ (b) frequencies of HTML tags plotted against d.
they were found is shown in Figure 9(b). (The &lt;a...&gt; tag +L+ obviously has the highest frequency and has been removed +L+ for clarity.)
These were important DOM idioms, spanning many +L+ diverse Web sites and authoring styles, that we did not +L+ anticipate ahead of time. Learning to recognize these +L+ idioms was valuable for boosting the harvest of the enhanced +L+ crawler. Yet, it would be unreasonable for the user-supplied +L+ baseline black-box predicate or learner to capture crawling +L+ strategies at such a low level. This is the ideal job of +L+ the apprentice. The apprentice took only 3–10 minutes +L+ to train on its (u, v) instances from scratch, despite a +L+ simple implementation that wrote a small file to disk for +L+ each instance of the apprentice. Contrast this with several +L+ hours taken by the baseline learner to learn general term +L+ distribution for topics.
3.5 Crawling with the apprentice trained +L+ off-line
In this section we subject the apprentice to a “field test” as +L+ part of the crawler, as shown in Figure 2. To do this we +L+ follow these steps:
1. Fix a topic and start the baseline crawler from all +L+ example URLs available from the given topic.
2. Run the baseline crawler until roughly 20000–25000 +L+ pages have been fetched.
3. For all pages (u, v) such that both u and v have +L+ been fetched by the baseline crawler, prepare an +L+ instance from (u, v) and add to the training set of +L+ the apprentice.
4. Train the apprentice. Set a suitable value for dmax. +L+ Folk Dancing
0	2000	4000	6000	8000	10000
#Pages fetched
Ice Hockey
Figure 10: Guidance from the apprentice significantly +L+ reduces the loss rate of the focused crawler.
5. Start the enhanced crawler from the same set of pages +L+ that the baseline crawler had started from.
6. Run the enhanced crawler to fetch about the same +L+ number of pages as the baseline crawler.
7. Compare the loss rates of the two crawlers.
Unlike with the reinforcement learner studied by Rennie +L+ and McCallum, we have no predetermined universe of URLs +L+ which constitute the relevant set; our crawler must go +L+ forth into the open Web and collect relevant pages from +L+ an unspecified number of sites. Therefore, measuring recall +L+ w.r.t. the baseline is not very meaningful (although we do +L+ report such numbers, for completeness, in §3.6). Instead, we +L+ measure the loss (the number of pages fetched which had to +L+ be thrown away owing to poor relevance) at various epochs +L+ in the crawl, where time is measured as the number of pages +L+ fetched (to elide fluctuating network delay and bandwidth). +L+ At epoch n, if the pages fetched are v1, ... , vn, then the total +L+ expected loss is (1/n) Pi (1− Pr(c*|vi)).
Figure 10 shows the loss plotted against the number of +L+ pages crawled for two topics: Folk dancing and Ice hockey. +L+ The behavior for Folk dancing is typical; Ice hockey is +L+ one of the best examples. In both cases, the loss goes up +L+ substantially faster with each crawled page for the baseline +L+ crawler than for the enhanced crawler. The reduction of loss +L+ for these topics are 40% and 90% respectively; typically, this +L+ number is between 30% and 60%. In other words, for most
-5 -4 -3 -2 -1	0	1	2	3	4	5
AI
Chess +L+ Horses +L+ Cancer +L+ IceHockey +L+ Linux +L+ Bball+ +L+ Bball-
9000	Tags at various DOM offsets
8000
7000
6000 +L+ 5000 +L+ 4000 +L+ 3000
font
td +L+ img
b +L+ br +L+ p +L+ tr +L+ li
comment +L+ div
table +L+ center
i +L+ span
hr
2000
1000
0
-5 -4 -3 -2 -1 0	1	2 3	4 5 d
0.2 +L+ 0.18 +L+ 0.16 +L+ 0.14 +L+ 0.12 +L+ 0.1 +L+ 0.08 +L+ 0.06 +L+ 0.04
0.02
d
Baseline +L+ Apprentice
Baseline +L+ Apprentice
8000
4000
0
0	4000 8000 12000 16000 20000
#Pages fetched
	6000
	4000
	2000
	0
156
topics, the apprentice reduces the number of useless pages +L+ fetched by one-third to two-thirds.
In a sense, comparing loss rates is the most meaningful +L+ evaluation in our setting, because the network cost of +L+ fetching relevant pages has to be paid anyway, and can be +L+ regarded as a fixed cost. Diligenti et al. show significant +L+ improvements in harvest rate, but for their topics, the loss +L+ rate for both the baseline crawler as well as the context- +L+ focused crawler were much higher than ours.
3.6 URL overlap and recall
The reader may feel that the apprentice crawler has an +L+ unfair advantage because it is first trained on DOM-derived +L+ features from the same set of pages that it has to crawl +L+ again. We claim that the set of pages visited by the baseline +L+ crawler and thea(off-linentrained) enhanced crawler have +L+ small overlap, and the superior results for the crawler guided 4011 8168 2199 +L+ by the apprentice are in large part because of generalizable +L+ learning. Thisgcan be seen from the examples in Figure 11.
Figure 11: The apprentice-guided crawler follows paths +L+ which are quite different from the baseline crawler because +L+ of its superior priority estimation technique. As a result +L+ there is little overlap between the URLs harvested by these +L+ two crawlers.
Given that the overlap between the baseline and the +L+ enhanced crawlers is small, which is ‘better’? As per the +L+ verdict of the baseline classifier, clearly the enhanced crawler +L+ is better. Even so, we report the loss rate of a different +L+ version of the enhanced crawler which is restricted to visiting +L+ only those pages which were visited by the baseline learner. +L+ We call this crawler the recall crawler. This means that in +L+ the end, both crawlers have collected exactly the same set +L+ of pages, and therefore have the same total loss. The test +L+ then is how long can the enhanced learner prevent the loss +L+ from approaching the baseline loss. These experiments are a +L+ rough analog of the ‘recall’ experiments done by Rennie and +L+ McCallum. We note that for these recall experiments, the +L+ apprentice does get the benefit of not having to generalize, +L+ so the gap between baseline loss and recall loss could be +L+ optimistic. Figure 12 compares the expected total loss of +L+ the baseline crawler, the recall crawler, and the apprentice- +L+ guided crawler (which is free to wander outside the baseline +L+ collection) plotted against the number of pages fetched, for a +L+ few topics. As expected, the recall crawler has loss generally
Ice Hockey
0	1000 2000 3000 4000 5000 6000
#Pages fetched
Kayaking
0	5000	10000	15000	20000
#Pages fetched
Figure 12: Recall for a crawler using the apprentice but +L+ limited to the set of pages crawled earlier by the baseline +L+ crawler.
somewhere between the loss of the baseline and the enhanced +L+ crawler.
3.7 Effect of training the apprentice online
Next we observe the effect of a mid-flight correction when +L+ the apprentice is trained some way into a baseline and +L+ switched into the circuit. The precise steps were:
1. Run the baseline crawler for the first n page fetches, +L+ then stop it.
2. Prepare instances and train the apprentice.
3. Re-evaluate the priorities of all unvisited pages v in +L+ the frontier table using the apprentice.
4. Switch in the apprentice and resume an enhanced +L+ crawl.
We report our experience with “Folk Dancing.” The baseline +L+ crawl was stopped after 5200 pages were fetched. Re- +L+ evaluating the priority of frontier nodes led to radical +L+ changes in their individual ranks as well as the priority +L+ distributions. As shown in Figure 13(a), the baseline learner +L+ is overly optimistic about the yield it expects from the +L+ frontier, whereas the apprentice already abandons a large +L+ fraction of frontier outlinks, and is less optimistic about
35%
Baseline +L+ Apprentice +L+ Intersect
Baseline +L+ Apprentice +L+ Intersect
Basketball
4%
FolkDance
9%
47%  +L+ Baseline +L+ Apprentice +L+ Intersect 
3%
39%
34%
Baseline +L+ Apprentice +L+ Intersect
17%
57%
FlyFishing
48%
49%
58%
IceHockey
Baseline
Recall
Apprentice
1000
0
Baseline
Recall
Apprentice
	10000
	5000
	0
157
Folk Dancing
Figure 13: The effect of online training of the apprentice.
The apprentice makes sweeping changes in the +L+ estimated promise of unvisited nodes in the crawl frontier.
(b) Resuming the crawl under the guidance of the +L+ apprentice immediately shows significant reduction in the +L+ loss accumulation rate.
the others, which appears more accurate from the Bayesian +L+ perspective.
Figure 13(b) shows the effect of resuming an enhanced +L+ crawl guided by the trained apprentice. The new (u, v) +L+ instances are all guaranteed to be unknown to the apprentice +L+ now. It is clear that the apprentice’s prioritization +L+ immediately starts reducing the loss rate. Figure 14 shows +L+ an even more impressive example. There are additional mild +L+ gains from retraining the apprentice at later points. It may +L+ be possible to show a more gradual online learning effect +L+ by retraining the classifier at a finer interval, e.g., every +L+ 100 page fetches, similar to Aggarwal et al. In our context, +L+ however, losing a thousand pages at the outset because of +L+ the baseline crawler’s limitation is not a disaster, so we need +L+ not bother.
3.8 Effect of other features
We experimented with two other kinds of feature, which we +L+ call topic and cocitation features.
Our limiting dmax to 5 may deprive the apprentice of +L+ important features in the source page u which are far from +L+ the link (u, v). One indirect way to reveal such features +L+ to the apprentice is to classify u, and to add the names +L+ of some of the top-scoring classes for u to the instance +L+ (u, v). §2.2.3 explains why this may help. This modification +L+ resulted in a 1% increase in the accuracy of the apprentice. +L+ A further increase of 1% was observed if we added all
Figure 14: Another example of training the apprentice +L+ online followed by starting to use it for crawl guidance. +L+ Before guidance, loss accumulation rate is over 30%, after, +L+ it drops to only 6%.
prefixes of the class name. For example, the full name +L+ for the Linux category is /Computers/Software/Operating- +L+ Systems/Linux. We added all of the following to the +L+ feature set of the source page: /, /Computers, /Computers/ +L+ Software, /Computers/Software/Operating-Systems and +L+ /Computers/Software/Operating-Systems/Linux. We also +L+ noted that various class names and some of their prefixes +L+ appeared amongst the best discriminants of the positive and +L+ negative classes.
Cocitation features for the link (u, v) are constructed by +L+ looking for other links (u, w) within a DOM distance of dmax +L+ such that w has already been fetched, so that Pr(c*Iw) is +L+ known. We discretize Pr(c*Iw) to two values HiGH and Low +L+ as in §2.3, and encode the feature as (Low, d) or (HiGH, d). +L+ The use of cocitation features did not improve the accuracy +L+ of the apprentice to any appreciable extent.
For both kinds of features, we estimated that random +L+ variations in crawling behavior (because of fluctuating +L+ network load and tie-breaking frontier scores) may prevent +L+ us from measuring an actual benefit to crawling under +L+ realistic operating conditions. We note that these ideas may +L+ be useful in other settings.
4 Conclusion
We have presented a simple enhancement to a focused +L+ crawler that helps assign better priorities to the unvisited +L+ URLs in the crawl frontier. This leads to a higher rate of +L+ fetching pages relevant to the focus topic and fewer false +L+ positives which must be discarded after spending network, +L+ CPU and storage resources processing them. There is no +L+ need to manually train the system with paths leading to +L+ relevant pages. The key idea is an apprentice learner which +L+ can accurately predict the worth of fetching a page using +L+ DOM features on pages that link to it. We show that the +L+ DOM features we use are superior to simpler alternatives. +L+ Using topics from Dmoz, we show that our new system can +L+ cut down the fraction of false positives by 30–90%.
We are exploring several directions in ongoing work. +L+ We wish to revisit continuous regression techniques for the +L+ apprentice, as well as more extensive features derived from +L+ the DOM. For example, we can associate with a token t the +L+ length $ of the DOM path from the text node containing t to
0	0-.2	.2-.4.4-.6	.6-.8.8-1
Estimated relevance of outlinks
Baseline +L+ Apprentice
(b)	4500	#Pages crawled	5500
Collect instances +L+ for apprentice
Train +L+ apprentice
Apprentice +L+ guides crawl
Folk Dancing
2700 +L+ 2600 +L+ 2500 +L+ 2400 +L+ 2300 +L+ 2200 +L+ 2100
12000 +L+ 10000 +L+ 8000 +L+ 6000 +L+ 4000 +L+ 2000
0

1800
1600
1400
1200
1000
800
600
2000 3000 4000 5000 6000 7000 8000
#Pages fetched
Collect +L+ instances for +L+ apprentice
Classical Composers
Train +L+ apprentice
Apprentice +L+ guides crawl
158
the HREF to v, or the depth of their least common ancestor +L+ in the DOM tree. We cannot use these in lieu of DOM offset, +L+ because regions which are far apart lexically may be close +L+ to each other along a DOM path. (t, f, d) features will be +L+ more numerous and sparser than (t, d) features, and could +L+ be harder to learn. The introduction of large numbers of +L+ strongly dependent features may even reduce the accuracy +L+ of the apprentice. Finally, we wish to implement some form +L+ of active learning where only those instances (u, v) with the +L+ largest I Pr(c* Iu) - Pr(c* Iv) I are chosen as training instances +L+ for the apprentice.
Acknowledgments: Thanks to the referees for suggest- +L+ ing that we present Figure 7.
References
[1] C. C. Aggarwal, F. Al-Garawi, and P. S. Yu. Intelligent +L+ crawling on the World Wide Web with arbitrary predicates. In +L+ WWW2001, Hong Kong, May 2001. ACM. Online at http: +L+ //www10.org/cdrom/papers/110/.
[2] C. Apte, F. Damerau, and S. M. Weiss. Automated learning +L+ of decision rules for text categorization. ACM Transactions on +L+ Information Systems, 1994. IBM Research Report RC18879.
[3] A. Blum and T. M. Mitchell. Combining labeled and unlabeled +L+ data with co-training. In Computational Learning Theory, +L+ pages 92–100,1998.
[4] S. Chakrabarti. Integrating the document object model with +L+ hyperlinks for enhanced topic distillation and information +L+ extraction. In WWW 10, Hong Kong, May 2001. Online at
http://www10.org/cdrom/papers/489.
[5] S. Chakrabarti, B. Dom, R. Agrawal, and P. Raghavan. +L+ Scalable feature selection, classification and signature generation +L+ for organizing large text databases into hierarchical topic +L+ taxonomies. VLDB Journal, Aug. 1998. Online at http:
//www.cs.berkeley.edu/~soumen/VLDB54_3.PDF.
[6] S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, P. Raghavan, +L+ and S. Rajagopalan.	Automatic resource compilation by
analyzing hyperlink structure and associated text. In 7th World- +L+ wide web conference (WWW7), 1998. Online at http://www7. +L+ scu.edu.au/programme/fullpapers/1898/com1898.html.
[7] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced hypertext +L+ categorization using hyperlinks. In SIGMOD Conference. ACM, +L+ 1998. Online at http://www.cs.berkeley.edu/~soumen/sigmod98. +L+ ps.
[8] S. Chakrabarti, B. E. Dom, D. A. Gibson, R. Kumar, +L+ P. Raghavan, S. Rajagopalan, and A. Tomkins. Topic distillation +L+ and spectral filtering. Artificial Intelligence Review, 13(5– +L+ 6):409–435, 1999.
[9] S. Chakrabarti, M. van den Berg, and B. Dom. Focused +L+ crawling: a new approach to topic-specific web resource +L+ discovery. Computer Networks, 31:1623–1640, 1999. First +L+ appeared in the 8th International World Wide Web Conference, +L+ Toronto, May 1999. Available online at http://www8.org/ +L+ w8-papers/5a-search-query/crawling/index.html.
[10] J. Cho, H. Garcia-Molina, and L. Page. Efficient crawling +L+ through URL ordering. In 7th World Wide Web Conference, +L+ Brisbane, Australia, Apr. 1998. Online at http://www7.scu.edu. +L+ au/programme/fullpapers/1919/com1919.htm.
[11] B. D. Davison. Topical locality in the Web. In Proceedings +L+ of the 23rd Annual International Conference on Research and +L+ Development in Information Retrieval (SIGIR 2000), pages +L+ 272–279, Athens, Greece, July 2000. ACM. Online at http:// +L+ www.cs.rutgers.edu/~davison/pubs/2000/sigir/.
[12] P. M. E. De Bra and R. D. J. Post. Information retrieval +L+ in the world-wide web: Making client-based searching feasible. +L+ In Proceedings of the First International World Wide Web +L+ Conference, Geneva, Switzerland, 1994. Online at http://www1. +L+ cern.ch/PapersWWW94/reinpost.ps.
[13] P. M. E. De Bra and R. D. J. Post. Searching for arbitrary +L+ information in the WWW: The fish search for Mosaic. In Second +L+ World Wide Web Conference ’9¢: Mosaic and the Web, +L+ Chicago, Oct. 1994. Online at http://archive.ncsa.uiuc.edu/ +L+ SDG/IT94/Proceedings/Searching/debra/article.html and http: +L+ //citeseer.nj.nec.com/172936.html.
[14] M. Diligenti, F. Coetzee, S. Lawrence, C. L. Giles, and M. Gori. +L+ Focused crawling using context graphs. In A. E. Abbadi, M. L. +L+ Brodie, S. Chakravarthy, U. Dayal, N. Kamel, G. Schlageter, +L+ and K.-Y. Whang, editors, VLDB 2000, Proceedings of +L+ 26th International Conference on Very Large Data Bases, +L+ September 10-1¢, 2000, Cairo, Egypt, pages 527–534. Morgan +L+ Kaufmann, 2000. Online at http://www.neci.nec.com/~lawrence/ +L+ papers/focus-vldb00/focus-vldb00.pdf.
[15] W. A. Gale, K. W. Church, and D. Yarowsky. A method for +L+ disambiguating word senses in a large corpus. Computer and +L+ the Humanities, 26:415–439, 1993.
[16] M. Hersovici, M. Jacovi, Y. S. Maarek, D. Pelleg, M. Shtalhaim, +L+ and S. Ur. The shark-search algorithm—an application: Tailored +L+ Web site mapping. In WWW7, 1998. Online at http://www7.scu. +L+ edu.au/programme/fullpapers/1849/com1849.htm.
[17] T. Joachims, D. Freitag, and T. Mitchell. Web Watcher: A tour +L+ guide for the web. In IJCAI, Aug. 1997. Online at http://www. +L+ cs.cmu.edu/~webwatcher/ijcai97.ps.
[18] H. Leiberman. Letizia: An agent that assists Web browsing. In +L+ International Joint Conference on Artificial Intelligence (IJ- +L+ CAI), Montreal, Aug. 1995. See Website at http://lieber.www. +L+ media.mit.edu/people/lieber/Lieberary/Letizia/Letizia.html.
[19] H. Leiberman, C. Fry, and L. Weitzman. Exploring the Web +L+ with reconnaissance agents. CACM, 44(8):69–75, Aug. 2001.
http://www.acm.org/cacm.
[20] A. McCallum. Bow: A toolkit for statistical language modeling, +L+ text retrieval, classification and clustering. Software available +L+ from http://www.cs.cmu.edu/~mccallum/bow/,1998.
[21] A. McCallum and K. Nigam. A comparison of event models for +L+ naive Bayes text classification. In AAAI/ICML-98 Workshop +L+ on Learning for Teat Categorization, pages 41–48. AAAI Press, +L+ 1998. Online at http://www.cs.cmu.edu/~knigam/.
[22] A. McCallum and K. Nigam. A comparison of event models for +L+ naive Bayes text classification. In AAAI/ICML-98 Workshop +L+ on Learning for Teat Categorization, pages 41–48. AAAI Press, +L+ 1998. Also technical report WS-98-05, CMU; online at http:
//www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf.
[23] F. Menczer. Links tell us about lexical and semantic
Web content. Technical Report Computer Science Abstract +L+ CS.IR/0108004, arXiv.org, Aug. 2001. Online at http://arxiv. +L+ org/abs/cs.IR/0108004.
[24] F. Menczer and R. K. Belew. Adaptive retrieval agents: +L+ Internalizing local context and scaling up to the Web. Machine +L+ Learning, 39(2/3):203–242, 2000. Longer version available as +L+ Technical Report CS98-579, http://dollar.biz.uiowa.edu/~fil/ +L+ Papers/MLJ.ps, University of California, San Diego.
[25] F. Menczer, G. Pant, M. Ruiz, and P. Srinivasan. Evaluating +L+ topic-driven Web crawlers. In SIGIR, New Orleans, Sept. 2001. +L+ ACM.	Online at http://dollar.biz.uiowa.edu/~fil/Papers/
sigir-01.pdf.
[26] T. Mitchell. Machine Learning. McGraw Hill, 1997.
[27] T. Mitchell. Mining the Web. In SIGIR 2001, Sept. 2001. Invited +L+ talk.
[28] S. Mukherjea. WTMS: a system for collecting and analyzing +L+ topic-specific Web information. WWW9/Computer Networks, +L+ 33(1–6):457–471, 2000. Online at http://www9.org/w9cdrom/293/ +L+ 293.html.
[29] S. RaviKumar, P. Raghavan, S. Rajagopalan, D. Sivakumar, +L+ A. Tomkins, and E. Upfal. Stochastic models for the Web graph. +L+ In FOCS, volume 41, pages 57–65. IEEE, nov 2000. Online at
http://www.cs.brown.edu/people/eli/papers/focs00.ps.
[30] J. Rennie and A. McCallum. Using reinforcement learning to +L+ spider the web efficiently. In ICML, 1999. Online at http:// +L+ www.cs.cmu.edu/~mccallum/papers/rlspider-icml99s.ps.gz.
[31] G. Salton and M. J. McGill. Introduction to Modern
Information Retrieval. McGraw-Hill, 1983.
[32] M. Subramanyam, G. V. R. Phanindra, M. Tiwari, and M. Jain. +L+ Focused crawling using TFIDF centroid. Hypertext Retrieval +L+ and Mining (CS610) class project, Apr. 2001. Details available +L+ from manyam@cs.utexas.edu.
[33] L. Torgo and J. Gama. Regression by classification. In D. Borges +L+ and C. Kaestner, editors, Brasilian AI Symposium, volume 1159 +L+ of Lecture Notes in Artificial Intelligence, Curitiba, Brazil, +L+ 1996. Springer-Verlag. Online at http://www.ncc.up.pt/~ltorgo/ +L+ Papers/list_pub.html.
159
