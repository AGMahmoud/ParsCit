A Frequency-based and a Poisson-based Definition of the
Probability of Being Informative
Thomas Roelleke
Department of Computer Science +L+ Queen Mary University of London
thor@dcs.qmul.ac.uk
ABSTRACT
This paper reports on theoretical investigations about the +L+ assumptions underlying the inverse document frequency (idf ). +L+ We show that an intuitive idf-based probability function for +L+ the probability of a term being informative assumes disjoint +L+ document events. By assuming documents to be indepen- +L+ dent rather than disjoint, we arrive at a Poisson-based prob- +L+ ability of being informative. The framework is useful for +L+ understanding and deciding the parameter estimation and +L+ combination in probabilistic retrieval models.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval +L+ models
General Terms
Theory
Keywords
Probabilistic information retrieval, inverse document fre- +L+ quency (idf), Poisson distribution, information theory, in- +L+ dependence assumption
1. INTRODUCTION AND BACKGROUND
The inverse document frequency (idf) is one of the most +L+ successful parameters for a relevance-based ranking of re- +L+ trieved objects. With N being the total number of docu- +L+ ments, and n(t) being the number of documents in which +L+ term t occurs, the idf is defined as follows:
idf(t) := −log n(tt) , 0 &lt;= idf(t) &lt; ∞
Ranking based on the sum of the idf-values of the query +L+ terms that occur in the retrieved documents works well, this +L+ has been shown in numerous applications. Also, it is well +L+ known that the combination of a document-specific term
Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee.
SIGIR’03, July 28–August 1, 2003, Toronto, Canada.
Copyright 2003 ACM 1-58113-646-3/03/0007 ...$5.00.
weight and idf works better than idf alone. This approach +L+ is known as tf-idf, where tf(t, d) (0 &lt;= tf(t, d) &lt;= 1) is +L+ the so-called term frequency of term t in document d. The +L+ idf reflects the discriminating power (informativeness) of a +L+ term, whereas the tf reflects the occurrence of a term.
The idf alone works better than the tf alone does. An ex- +L+ planation might be the problem of tf with terms that occur +L+ in many documents; let us refer to those terms as “noisy” +L+ terms. We use the notion of “noisy” terms rather than “fre- +L+ quent” terms since frequent terms leaves open whether we +L+ refer to the document frequency of a term in a collection or +L+ to the so-called term frequency (also referred to as within- +L+ document frequency) of a term in a document. We asso- +L+ ciate “noise” with the document frequency of a term in a +L+ collection, and we associate “occurrence” with the within- +L+ document frequency of a term. The tf of a noisy term might +L+ be high in a document, but noisy terms are not good candi- +L+ dates for representing a document. Therefore, the removal +L+ of noisy terms (known as “stopword removal”) is essential +L+ when applying tf. In a tf-idf approach, the removal of stop- +L+ words is conceptually obsolete, if stopwords are just words +L+ with a low idf.
From a probabilistic point of view, tf is a value with a +L+ frequency-based probabilistic interpretation whereas idf has +L+ an “informative” rather than a probabilistic interpretation. +L+ The missing probabilistic interpretation of idf is a problem +L+ in probabilistic retrieval models where we combine uncertain +L+ knowledge of different dimensions (e.g.: informativeness of +L+ terms, structure of documents, quality of documents, age +L+ of documents, etc.) such that a good estimate of the prob- +L+ ability of relevance is achieved. An intuitive solution is a +L+ normalisation of idf such that we obtain values in the inter- +L+ val [0; 1]. For example, consider a normalisation based on +L+ the maximal idf-value. Let T be the set of terms occurring +L+ in a collection.
Pf,Q (t is informative) :=  idf(t) 
maxidf
maxidf := max({idf(t)|t ∈ T}), maxidf &lt;= −log(1/N) +L+ minidf := min({idf(t)|t ∈ T}), minidf &gt;= 0
minidf &lt; Pf,Q (t is informative) ≤ 1.0 +L+ maxidf —
This frequency-based probability function covers the interval +L+ [0; 1] if the minimal idf is equal to zero, which is the case +L+ if we have at least one term that occurs in all documents. +L+ Can we interpret Pf� Q , the normalised idf, as the probability +L+ that the term is informative?
When investigating the probabilistic interpretation of the
227
normalised idf, we made several observations related to dis- +L+ jointness and independence of document events. These ob- +L+ servations are reported in section 3. We show in section 3.1 +L+ that the frequency-based noise probability n(t) Nused in the +L+ classic idf-definition can be explained by three assumptions: +L+ binary term occurrence, constant document containment and +L+ disjointness of document containment events. In section 3.2 +L+ we show that by assuming independence of documents, we +L+ obtain 1 — e-1 Pz� 1 — 0.37 as the upper bound of the noise +L+ probability of a term. The value e−1 is related to the loga- +L+ rithm and we investigate in section 3.3 the link to informa- +L+ tion theory. In section 4, we link the results of the previous +L+ sections to probability theory. We show the steps from possi- +L+ ble worlds to binomial distribution and Poisson distribution. +L+ In section 5, we emphasise that the theoretical framework +L+ of this paper is applicable for both idf and tf. Finally, in +L+ section 6, we base the definition of the probability of be- +L+ ing informative on the results of the previous sections and +L+ compare frequency-based and Poisson-based definitions.
2. BACKGROUND
The relationship between frequencies, probabilities and +L+ information theory (entropy) has been the focus of many +L+ researchers. In this background section, we focus on work +L+ that investigates the application of the Poisson distribution +L+ in IR since a main part of the work presented in this paper +L+ addresses the underlying assumptions of Poisson.
[4] proposes a 2-Poisson model that takes into account +L+ the different nature of relevant and non-relevant documents, +L+ rare terms (content words) and frequent terms (noisy terms, +L+ function words, stopwords). [9] shows experimentally that +L+ most of the terms (words) in a collection are distributed +L+ according to a low dimension n-Poisson model. [10] uses a +L+ 2-Poisson model for including term frequency-based proba- +L+ bilities in the probabilistic retrieval model. The non-linear +L+ scaling of the Poisson function showed significant improve- +L+ ment compared to a linear frequency-based probability. The +L+ Poisson model was here applied to the term frequency of a +L+ term in a document. We will generalise the discussion by +L+ pointing out that document frequency and term frequency +L+ are dual parameters in the collection space and the docu- +L+ ment space, respectively. Our discussion of the Poisson dis- +L+ tribution focuses on the document frequency in a collection +L+ rather than on the term frequency in a document.
[7] and [6] address the deviation of idf and Poisson, and +L+ apply Poisson mixtures to achieve better Poisson-based esti- +L+ mates. The results proved again experimentally that a one- +L+ dimensional Poisson does not work for rare terms, therefore +L+ Poisson mixtures and additional parameters are proposed.
[3], section 3.3, illustrates and summarises comprehen- +L+ sively the relationships between frequencies, probabilities +L+ and Poisson. Different definitions of idf are put into con- +L+ text and a notion of “noise” is defined, where noise is viewed +L+ as the complement of idf. We use in our paper a different +L+ notion of noise: we consider a frequency-based noise that +L+ corresponds to the document frequency, and we consider a +L+ term noise that is based on the independence of document +L+ events.
[11], [12], [8] and [1] link frequencies and probability esti- +L+ mation to information theory. [12] establishes a framework +L+ in which information retrieval models are formalised based +L+ on probabilistic inference. A key component is the use of a +L+ space of disjoint events, where the framework mainly uses +L+ terms as disjoint events. The probability of being informa- +L+ tive defined in our paper can be viewed as the probability +L+ of the disjoint terms in the term space of [12].
[8] address entropy and bibliometric distributions. En- +L+ tropy is maximal if all events are equiprobable and the fre- +L+ quency-based Lotka law (N/iλ is the number of scientists +L+ that have written i publications, where N and λ are distri- +L+ bution parameters), Zipf and the Pareto distribution are re- +L+ lated. The Pareto distribution is the continuous case of the +L+ Lotka and Lotka and Zipf show equivalences. The Pareto +L+ distribution is used by [2] for term frequency normalisation. +L+ The Pareto distribution compares to the Poisson distribu- +L+ tion in the sense that Pareto is “fat-tailed”, i. e. Pareto as- +L+ signs larger probabilities to large numbers of events than +L+ Poisson distributions do. This makes Pareto interesting +L+ since Poisson is felt to be too radical on frequent events. +L+ We restrict in this paper to the discussion of Poisson, how- +L+ ever, our results show that indeed a smoother distribution +L+ than Poisson promises to be a good candidate for improving +L+ the estimation of probabilities in information retrieval.
[1] establishes a theoretical link between tf-idf and infor- +L+ mation theory and the theoretical research on the meaning +L+ of tf-idf “clarifies the statistical model on which the different +L+ measures are commonly based”. This motivation matches +L+ the motivation of our paper: We investigate theoretically +L+ the assumptions of classical idf and Poisson for a better +L+ understanding of parameter estimation and combination.
3. FROM DISJOINT TO INDEPENDENT
We define and discuss in this section three probabilities: +L+ The frequency-based noise probability (definition 1), the to- +L+ tal noise probability for disjoint documents (definition 2). +L+ and the noise probability for independent documents (defi- +L+ nition 3).
3.1 Binary occurrence, constant containment +L+ and disjointness of documents
We show in this section, that the frequency-based noise +L+ probability nN(t) in the idf definition can be explained as +L+ a total probability with binary term occurrence, constant +L+ document containment and disjointness of document con- +L+ tainments.
We refer to a probability function as binary if for all events +L+ the probability is either 1.0 or 0.0. The occurrence proba- +L+ bility P(t1d) is binary, if P(t1d) is equal to 1.0 if t E d, and +L+ P(t1d) is equal to 0.0, otherwise.
P(t1d) is binary: P(t1d) = 1.0 V P(t1d) = 0.0
We refer to a probability function as constant if for all +L+ events the probability is equal. The document containment +L+ probability reflect the chance that a document occurs in a +L+ collection. This containment probability is constant if we +L+ have no information about the document containment or +L+ we ignore that documents differ in containment. Contain- +L+ ment could be derived, for example, from the size, quality, +L+ age, links, etc. of a document. For a constant containment +L+ in a collection with N documents, N1 is often assumed as +L+ the containment probability. We generalise this definition +L+ and introduce the constant λ where 0 &lt; λ &lt; N. The con- +L+ tainment of a document d depends on the collection c, this +L+ is reflected by the notation P(d1c) used for the containment
228
of a document.
P(d1c) is constant:	Vd : P(d1c) = λ
N
For disjoint documents that cover the whole event space, +L+ we set λ = 1 and obtain Ed P(d1c) = 1.0. Next, we define +L+ the frequency-based noise probability and the total noise +L+ probability for disjoint documents. We introduce the event +L+ notation t is noisy and t occurs for making the difference +L+ between the noise probability P(t is noisy1c) in a collection +L+ and the occurrence probability P(t occurs 1d) in a document +L+ more explicit, thereby keeping in mind that the noise prob- +L+ ability corresponds to the occurrence probability of a term +L+ in a collection.
DefInItIOn 1. The frequency-based term noise prob- +L+ ability:
Pfr q(t is noisy1c) := n(t)
N
DefInItIOn 2. The total term noise probability for +L+ disjoint documents:
Pdi9(t is noisy1c) := E P(t occurs1d) • P(d1c)
d
Now, we can formulate a theorem that makes assumptions +L+ explicit that explain the classical idf.
TheOrem 1. IDF assumptions: If the occurrence prob- +L+ ability P(t1d) of term t over documents d is binary, and +L+ the containment probability P(d1c) of documents d is con- +L+ stant, and document containments are disjoint events, then +L+ the noise probability for disjoint documents is equal to the +L+ frequency-based noise probability.
Pdi9(t is noisy1c) = Pfr q(t is noisy1c)
PrOOf. The assumptions are:
Vd : (P(t occurs1d) = 1 V P(t occurs1d) = 0) n +L+ P(d1c) = N n
E P(d1c) = 1.0
d +L+ the containment for small documents tends to be smaller +L+ than for large documents. From that point of view, idf +L+ means that P(t n d1c) is constant for all d in which t occurs, +L+ and P(t n d1c) is zero otherwise. The occurrence and con- +L+ tainment can be term specific. For example, set P(t nd1c) = +L+ 1/ND(c) if t occurs in d, where ND(c) is the number of doc- +L+ uments in collection c (we used before just N). We choose a +L+ document-dependent occurrence P(t1d) := 1/NT(d), i. e. the +L+ occurrence probability is equal to the inverse of NT (d), which +L+ is the total number of terms in document d. Next, we choose +L+ the containment P(d1c) := NT(d)/NT(c)•NT(c)/ND(c) where +L+ NT(d)/NT(c) is a document length normalisation (number +L+ of terms in document d divided by the number of terms in +L+ collection c), and NT (c)/ND (c) is a constant factor of the +L+ collection (number of terms in collection c divided by the +L+ number of documents in collection c). We obtain P(tnd1c) = +L+ 1/ND (c).
In a tf-idf-retrieval function, the tf-component reflects +L+ the occurrence probability of a term in a document. This is +L+ a further explanation why we can estimate the idf with a +L+ simple P(t1d), since the combined tf-idf contains the occur- +L+ rence probability. The containment probability corresponds +L+ to a document normalisation (document length normalisa- +L+ tion, pivoted document length) and is normally attached to +L+ the tf-component or the tf-idf-product.
The disjointness assumption is typical for frequency-based +L+ probabilities. From a probability theory point of view, we +L+ can consider documents as disjoint events, in order to achieve +L+ a sound theoretical model for explaining the classical idf. +L+ But does disjointness reflect the real world where the con- +L+ tainment of a document appears to be independent of the +L+ containment of another document? In the next section, we +L+ replace the disjointness assumption by the independence as- +L+ sumption.
3.2 The upper bound of the noise probability +L+ for independent documents
For independent documents, we compute the probability +L+ of a disjunction as usual, namely as the complement of the +L+ probability of the conjunction of the negated events:
P(di V ... V dN) = 1 — P(-di n ... n �dN)
(1 — P(d))
= 1—rl
d
1 +L+ N
=
n(t)
N
= Pfr q(t is noisy1c)
We obtain:
Pdi9(t is noisy1c) = E
dt∈d
The noise probability can be considered as the conjunction +L+ of the term occurrence and the document containment.
The above result is not a surprise but it is a mathemati- +L+ cal formulation of assumptions that can be used to explain +L+ the classical idf. The assumptions make explicit that the +L+ different types of term occurrence in documents (frequency +L+ of a term, importance of a term, position of a term, doc- +L+ ument part where the term occurs, etc.) and the different +L+ types of document containment (size, quality, age, etc.) are +L+ ignored, and document containments are considered as dis- +L+ joint events.
From the assumptions, we can conclude that idf (frequency- +L+ based noise, respectively) is a relatively simple but strict +L+ estimate. Still, idf works well. This could be explained +L+ by a leverage effect that justifies the binary occurrence and +L+ constant containment: The term occurrence for small docu- +L+ ments tends to be larger than for large documents, whereas
P(t is noisy1c) := P(t occurs n (dl V ... V dN)1c)
For disjoint documents, this view of the noise probability +L+ led to definition 2. For independent documents, we use now +L+ the conjunction of negated events.
DefInItIOn 3. The term noise probability for inde- +L+ pendent documents:
Pin(t is noisy1c) := rl (1 — P(t occurs1d) • P(d1c))
d
With binary occurrence and a constant containment P(d1c) := +L+ λ/N, we obtain the term noise of a term t that occurs in n(t) +L+ documents:
Pin(t is noisy1c) = 1 — (1 — λN1n(t)
229
For binary occurrence and disjoint documents, the contain- +L+ ment probability was 1/N. Now, with independent docu- +L+ ments, we can use λ as a collection parameter that controls +L+ the average containment probability. We show through the +L+ next theorem that the upper bound of the noise probability +L+ depends on λ.
TheOrem 2. The upper bound of being noisy: If the +L+ occurrence P(t|d) is binary, and the containment P(d|c) +L+ is constant, and document containments are independent +L+ events, then 1 − e−λ is the upper bound of the noise proba- +L+ bility.
∀t : Pin (t is noisy|c) &lt; 1 − e−λ
PrOOf. The upper bound of the independent noise prob- +L+ ability follows from the limit limN→∞(1 + xN)N = ex (see +L+ any comprehensive math book, for example, [5], for the con- +L+ vergence equation of the Euler function). With x = −λ, we +L+ obtain:
N
lim C1 − λN) = e−λ
N→
For the term noise, we have:
Pin(t is noisy|c) = 1 − C1 − λN)n(t)
Pin (t is noisy |c) is strictly monotonous: The noise of a term +L+ tn is less than the noise of a term tn+1, where tn occurs in +L+ n documents and tn+1 occurs in n + 1 documents. There- +L+ fore, a term with n = N has the largest noise probability. +L+ For a collection with infinite many documents, the upper +L+ bound of the noise probability for terms tN that occur in all +L+ documents becomes:
1−C1−λN)N
= 1−e−λ
By applying an independence rather a disjointness assump- +L+ tion, we obtain the probability e−1 that a term is not noisy +L+ even if the term does occur in all documents. In the disjoint +L+ case, the noise probability is one for a term that occurs in +L+ all documents.
If we view P(d|c) := λ/N as the average containment, +L+ then λ is large for a term that occurs mostly in large docu- +L+ ments, and λ is small for a term that occurs mostly in small +L+ documents. Thus, the noise of a term t is large if t occurs in +L+ n(t) large documents and the noise is smaller if t occurs in +L+ small documents. Alternatively, we can assume a constant +L+ containment and a term-dependent occurrence. If we as- +L+ sume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as +L+ the average probability that t represents a document. The +L+ common assumption is that the average containment or oc- +L+ currence probability is proportional to n(t). However, here +L+ is additional potential: The statistical laws (see [3] on Luhn +L+ and Zipf) indicate that the average probability could follow +L+ a normal distribution, i. e. small probabilities for small n(t) +L+ and large n(t), and larger probabilities for medium n(t).
For the monotonous case we investigate here, the noise of +L+ a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and +L+ the noise of a term with n(t) = N is close to 1− e−λ. In the +L+ next section, we relate the value e−λ to information theory.
3.3 The probability of a maximal informative +L+ signal
The probability e−1 is special in the sense that a signal +L+ with that probability is a signal with maximal information as +L+ derived from the entropy definition. Consider the definition +L+ of the entropy contribution H(t) of a signal t.
H(t) := P(t) · − ln P(t)
We form the first derivation for computing the optimum.
− ln P(t) + P(t) · P(t)
= −(1+lnP(t)) +L+ For obtaining optima, we use:
0 = −(1 + ln P(t))
The entropy contribution H(t) is maximal for P(t) = e−1. +L+ This result does not depend on the base of the logarithm as +L+ we see next:
−1 
P(t) · ln b ·P(t)
1	1 + ln P(t) 
ln b + log P(t)	ln b	)
We summarise this result in the following theorem: +L+ TheOrem 3. The probability of a maximal informa- +L+ tive signal: The probability Pmax = e−1 ≈ 0.37 is the prob-
ability of a maximal informative signal. The entropy of a +L+ maximal informative signal is Hmax = e−1.
PrOOf. The probability and entropy follow from the deriva- +L+ tion above.
The complement of the maximal noise probability is e−λ +L+ and we are looking now for a generalisation of the entropy +L+ definition such that e−λ is the probability of a maximal in- +L+ formative signal. We can generalise the entropy definition +L+ by computing the integral of λ+ln P(t), i. e. this derivation +L+ is zero for e−λ. We obtain a generalised entropy:
J −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t))
The generalised entropy corresponds for λ = 1 to the classi- +L+ cal entropy. By moving from disjoint to independent docu- +L+ ments, we have established a link between the complement +L+ of the noise probability of a term that occurs in all docu- +L+ ments and information theory. Next, we link independent +L+ documents to probability theory.
4. THE LINK TO PROBABILITY THEORY
We review for independent documents three concepts of +L+ probability theory: possible worlds, binomial distribution +L+ and Poisson distribution.
4.1 Possible Worlds
Each conjunction of document events (for each document, +L+ we consider two document events: the document can be +L+ true or false) is associated with a so-called possible world. +L+ For example, consider the eight possible worlds for three +L+ documents (N = 3).
Pin (tN is noisy) = lim
N→∞
lim
N→∞
∂H(t) +L+ ∂P(t)
∂H(t)  +L+ ∂P(t)
= − logb P(t)+
= −C
230
world w	conjunction
w7	d1 ∧ d2 ∧ d3
ws	d1 ∧ d2 ∧ ¬d3
w5	d1 ∧ ¬d2 ∧ d3
w4	d1 ∧ ¬d2 ∧ ¬d3
w3	¬d1 ∧ d2 ∧ d3
w2	¬d1 ∧ d2 ∧ ¬d3
w1	¬d1 ∧ ¬d2 ∧ d3
w0	¬d1 ∧ ¬d2 ∧ ¬d3
With each world w, we associate a probability µ(w), which +L+ is equal to the product of the single probabilities of the doc- +L+ ument events.
world w	µ(w)
	probability
w7	(λ·N)0
	(N)3
ws	·	(1 − N)1
	rrN)2	
w5	\N)2·	(1 − N)1
w4	(N)1 ·	2
		(1 − N)
w3	(N)2·	(1 − N)1
w2	(λ	(1 − N)2
w1	(N)1 ·	
w0	(λ	N)3
The sum over the possible worlds in which k documents are +L+ true and N−k documents are false is equal to the probabil- +L+ ity function of the binomial distribution, since the binomial +L+ coefficient yields the number of possible worlds in which k +L+ documents are true.
4.2 Binomial distribution
The binomial probability function yields the probability +L+ that k of N events are true where each event is true with +L+ the single event probability p.
P(k) := binom(N, k, p) := (N k / pk (1 − p)N− k
The single event probability is usually defined as p := λ/N, +L+ i. e. p is inversely proportional to N, the total number of +L+ events. With this definition of p, we obtain for an infinite +L+ number of documents the following limit for the product of +L+ the binomial coefficient and p k :
lim N k +L+ N→∞k p
N · (N−1) · ... · (N−k +1)
k!
The limit is close to the actual value for k &lt;&lt; N. For large +L+ k, the actual value is smaller than the limit.
The limit of (1−p)N−k follows from the limit limN→∞(1+ W= +L+ ex.
N−k
lim (1 −p)N −k = lim (1 − λN)
N→∞	N→
Again, the limit is close to the actual value for k &lt;&lt; N. For +L+ large k, the actual value is larger than the limit.
4.3 Poisson distribution
For an infinite number of events, the Poisson probability +L+ function is the limit of the binomial probability function.
Ak
binom(N, k, p) = k! · e−λ +L+ P(k) = poisson(k, λ) := k! · e−λ
The probability poisson(0, 1) is equal to e−1, which is the +L+ probability of a maximal informative signal. This shows +L+ the relationship of the Poisson distribution and information +L+ theory.
After seeing the convergence of the binomial distribution, +L+ we can choose the Poisson distribution as an approximation +L+ of the independent term noise probability. First, we define +L+ the Poisson noise probability:
DefInItIOn 4. The Poisson term noise probability: +L+ P,oi(t is noisy|c) := e−λ ·
For independent documents, the Poisson distribution ap- +L+ proximates the probability of the disjunction for large n(t), +L+ since the independent term noise probability is equal to the +L+ sum over the binomial probabilities where at least one of +L+ n(t) document containment events is true.
Pin (t is noisy | c) = n(t) (n(t)1 pk (1 − p)N−k
1. k J
1
Pin (t is noisy | c) ≈ P,oi (t is noisy | c)
We have defined a frequency-based and a Poisson-based prob- +L+ ability of being noisy, where the latter is the limit of the +L+ independence-based probability of being noisy. Before we +L+ present in the final section the usage of the noise proba- +L+ bility for defining the probability of being informative, we +L+ emphasise in the next section that the results apply to the +L+ collection space as well as to the the document space.
k
 N) = e—λ
5. THE COLLECTION SPACE AND THE +L+ DOCUMENT SPACE
Consider the dual definitions of retrieval parameters in +L+ table 1. We associate a collection space D × T with a col- +L+ lection c where D is the set of documents and T is the set +L+ of terms in the collection. Let ND := |D| and NT := |T| +L+ be the number of documents and terms, respectively. We +L+ consider a document as a subset of T and a term as a subset +L+ of D. Let nT(d) := |{t|d ∈ t}| be the number of terms that +L+ occur in the document d, and let nD(t) := | {d|t ∈ d}| be the +L+ number of documents that contain the term t.
In a dual way, we associate a document space L × T with +L+ a document d where L is the set of locations (also referred +L+ to as positions, however, we use the letters L and l and not +L+ P and p for avoiding confusion with probabilities) and T is +L+ the set of terms in the document. The document dimension +L+ in a collection space corresponds to the location (position) +L+ dimension in a document space.
The definition makes explicit that the classical notion of +L+ term frequency of a term in a document (also referred to as +L+ the within-document term frequency) actually corresponds +L+ to the location frequency of a term in a document. For the
lime
—
λ(1−
N→
=lim
N→∞
(λ)k = λk
N	k!
lim
N→∞
λk
k!
n(t)�
k=1
231
space	collection	document
dimensions	documents and terms	locations and terms
document/location frequency	nD(t, c): Number of documents in which term t occurs in collection c	nL(t, d): Number of locations (positions) at which term t occurs in document d
	ND(c): Number of documents in collection c	NL(d): Number of locations (positions) in docu-ment d
term frequency	nT(d,c): Number of terms that document d con- tains in collection c	nT(l,d): Number of terms that location l contains in document d
	NT(c): Number of terms in collection c	NT(d): Number of terms in document d
noise/occurrence containment	P(t1c) (term noise) P(d1c) (document)	P(t1d) (term occurrence) P(l1d) (location)
informativeness conciseness	— ln P(t1c) — ln P(d1c)	— ln P(t1d) — ln P(l 1d)
P(informative) P(concise)	ln(P(t1c))/ ln(P(tm in, c)) ln(P(d1c))/ ln(P(dmin1c))	ln(P(t1d))/ ln(P(tm in, d)) ln(P(l1d))/ln(P(lm in1d))
Table 1: Retrieval parameters
actual term frequency value, it is common to use the max- +L+ imal occurrence (number of locations; let lf be the location +L+ frequency).
tf(t, d) := lf(t, d) :=  Pf, , (t occurs 1d) =  nL (t, d) +L+ Pf,,(t. occurs1d) nL(t,, ,d)
A further duality is between informativeness and concise- +L+ ness (shortness of documents or locations): informativeness +L+ is based on occurrence (noise), conciseness is based on con- +L+ tainment.
We have highlighted in this section the duality between +L+ the collection space and the document space. We concen- +L+ trate in this paper on the probability of a term to be noisy +L+ and informative. Those probabilities are defined in the col- +L+ lection space. However, the results regarding the term noise +L+ and informativeness apply to their dual counterparts: term +L+ occurrence and informativeness in a document. Also, the +L+ results can be applied to containment of documents and lo- +L+ cations.
6. THE PROBABILITY OF BEING INFOR- +L+ MATIVE
We showed in the previous sections that the disjointness +L+ assumption leads to frequency-based probabilities and that +L+ the independence assumption leads to Poisson probabilities. +L+ In this section, we formulate a frequency-based definition +L+ and a Poisson-based definition of the probability of being +L+ informative and then we compare the two definitions.
DefInItIOn 5. The frequency-based probability of be- +L+ ing informative:
We define the Poisson-based probability of being informa- +L+ tive analogously to the frequency-based probability of being +L+ informative (see definition 5).
DefInItIOn 6. The Poisson-based probability of be-
ing informative:
For λ &gt;&gt; 1, we can alter the noise and informativeness Pois- +L+ son by starting the sum from 0, since eλ &gt;&gt; 1. Then, the +L+ minimal Poisson informativeness is poisson(0, λ) = e−λ. We +L+ obtain a simplified Poisson probability of being informative:
λ — ln En (to \k 
Ppoj(t is informative1c) �
ln En(t) λk +L+ k=0 k!  +L+ λ
The computation of the Poisson sum requires an optimi- +L+ sation for large n(t). The implementation for this paper +L+ exploits the nature of the Poisson density: The Poisson den- +L+ sity yields only values significantly greater than zero in an +L+ interval around λ.
Consider the illustration of the noise and informative- +L+ ness definitions in figure 1. The probability functions dis- +L+ played are summarised in figure 2 where the simplified Pois- +L+ son is used in the noise and informativeness graphs. The +L+ frequency-based noise corresponds to the linear solid curve +L+ in the noise figure. With an independence assumption, we +L+ obtain the curve in the lower triangle of the noise figure. By +L+ changing the parameter p := λ/N of the independence prob- +L+ ability, we can lift or lower the independence curve. The +L+ noise figure shows the lifting for the value λ := ln N � +L+ 9.2. The setting λ = ln N is special in the sense that the +L+ frequency-based and the Poisson-based informativeness have +L+ the same denominator, namely ln N, and the Poisson sum +L+ converges to λ. Whether we can draw more conclusions from +L+ this setting is an open question.
We can conclude, that the lifting is desirable if we know +L+ for a collection that terms that occur in relatively few doc-
— ln (e−λ	n(t) λk I
Ek=1 k!
— ln(e−λ • λ)
λ —ln Ek=1 kk 
=
λ — lnλ
For the sum expression, the following limit holds:
lim	n(t)�	λk	=eλ—1
n(t)→∞	k=1	k!	
Ppoj(t is informative1c) :=
— ln n(t)
N 
—ln 1
N
— logN	N)= 1 — logN n(t) = 1 — ln In N
N	N
Pf, ,(t is informative1c) :=
λ
= 1
232
0.8
0.6
0.4
0.2
0
1
frequency
independence: 1/N
independence: ln(N)/N
poisson: 1000
poisson: 2000
poisson: 1 000,2000
frequency
independence: 1/N +L+ 0.8	independence: ln(N)/N
poisson: 1000
poisson: 2000 +L+ poisson: 1000,2000
0.6
0.4
0.2
0
1
0	2000 4000 6000 8000 10000
n(t): Number of documents with term t +L+ 0	2000 4000 6000 8000 10000
n(t): Number of documents with term t
Figure 1: Noise and Informativeness
Probability function		Noise	Informativeness
Frequency PfreQ	Def Interval	n(t)/N	ln(n(t)/N)/ln(1/N)
		1/N &lt; PfreQ &lt; 1.0	0.0 &lt; PfreQ &lt; 1.0
Independence Pin	Def Interval	1 — (1 — p)n (t)	ln(1 — (1 — p)n(t))/ ln(p)
		p &lt; Pin &lt; 1 — e—λ	ln(p) &lt; Pin &lt; 1.0
Poisson Ppoi	Def Interval Def	e—λEn(t) λk	(λ — ln En(t) \k )/(λ — ln λ)
Poisson Ppoi simplified	Interval	k=1 k!	k=1
		e—λ • λ &lt; Ppoi &lt; 1 — e—λ	(λ — ln(eλ — 1))/(λ — ln λ) &lt; Ppoi &lt; 1.0
		e—λ Ek=0 kk	(λ — ln Ek=0 kk )/λ
		e—λ &lt; Ppoi &lt; 1.0	0.0 &lt; Ppoi &lt; 1.0
Figure 2: Probability functions
uments are no guarantee for finding relevant documents, +L+ i. e. we assume that rare terms are still relatively noisy. On +L+ the opposite, we could lower the curve when assuming that +L+ frequent terms are not too noisy, i. e. they are considered as +L+ being still significantly discriminative.
The Poisson probabilities approximate the independence +L+ probabilities for large n(t); the approximation is better for +L+ larger λ. For n(t) &lt; λ, the noise is zero whereas for n(t) &gt; λ +L+ the noise is one. This radical behaviour can be smoothened +L+ by using a multi-dimensional Poisson distribution. Figure 1 +L+ shows a Poisson noise based on a two-dimensional Poisson:
λkλk
poisson(k, λ1, λ2) := π • e—λ1•k� +(1—π)•e- λ2 • 2
k!
The two dimensional Poisson shows a plateau between λ1 = +L+ 1000 and λ2 = 2000, we used here π = 0.5. The idea be- +L+ hind this setting is that terms that occur in less than 1000 +L+ documents are considered to be not noisy (i.e. they are in- +L+ formative), that terms between 1000 and 2000 are half noisy, +L+ and that terms with more than 2000 are definitely noisy.
For the informativeness, we observe that the radical be- +L+ haviour of Poisson is preserved. The plateau here is ap- +L+ proximately at 1/6, and it is important to realise that this +L+ plateau is not obtained with the multi-dimensional Poisson +L+ noise using π = 0.5. The logarithm of the noise is nor- +L+ malised by the logarithm of a very small number, namely +L+ 0.5 • e—1000 + 0.5 • e—2000. That is why the informativeness +L+ will be only close to one for very little noise, whereas for a +L+ bit of noise, informativeness will drop to zero. This effect +L+ can be controlled by using small values for π such that the +L+ noise in the interval [λ1; λ2] is still very little. The setting +L+ π = e—2000/6 leads to noise values of approximately e —2000/6 +L+ in the interval [λ1; λ2], the logarithms lead then to 1/6 for +L+ the informativeness.
The indepence-based and frequency-based informativeness +L+ functions do not differ as much as the noise functions do. +L+ However, for the indepence-based probability of being infor- +L+ mative, we can control the average informativeness by the +L+ definition p := λ/N whereas the control on the frequency- +L+ based is limited as we address next.
For the frequency-based idf, the gradient is monotonously +L+ decreasing and we obtain for different collections the same +L+ distances of idf-values, i. e. the parameter N does not affect +L+ the distance. For an illustration, consider the distance be- +L+ tween the value idf(tn+1) of a term tn+1 that occurs in n+1 +L+ documents, and the value idf(tn) of a term tn that occurs in +L+ n documents.
idf(tn+1) — idf(tn) = ln  n
n+ 1
The first three values of the distance function are:
idf(t2) — idf(t1) = ln(1/(1	+ 1))	=	0.69
idf(t3) — idf(t2) = ln(1/(2	+ 1))	=	0.41
idf(t4) — idf(t3) = ln(1/(3	+ 1))	=	0.29
For the Poisson-based informativeness, the gradient decreases +L+ first slowly for small n(t), then rapidly near n(t) R� λ and +L+ then it grows again slowly for large n(t).
In conclusion, we have seen that the Poisson-based defini- +L+ tion provides more control and parameter possibilities than
233
the frequency-based definition does. Whereas more control +L+ and parameter promises to be positive for the personalisa- +L+ tion of retrieval systems, it bears at the same time the dan- +L+ ger of just too many parameters. The framework presented +L+ in this paper raises the awareness about the probabilistic +L+ and information-theoretic meanings of the parameters. The +L+ parallel definitions of the frequency-based probability and +L+ the Poisson-based probability of being informative made +L+ the underlying assumptions explicit. The frequency-based +L+ probability can be explained by binary occurrence, constant +L+ containment and disjointness of documents. Independence +L+ of documents leads to Poisson, where we have to be aware +L+ that Poisson approximates the probability of a disjunction +L+ for a large number of events, but not for a small number. +L+ This theoretical result explains why experimental investiga- +L+ tions on Poisson (see [7]) show that a Poisson estimation +L+ does work better for frequent (bad, noisy) terms than for +L+ rare (good, informative) terms.
In addition to the collection-wide parameter setting, the +L+ framework presented here allows for document-dependent +L+ settings, as explained for the independence probability. This +L+ is in particular interesting for heterogeneous and structured +L+ collections, since documents are different in nature (size, +L+ quality, root document, sub document), and therefore, bi- +L+ nary occurrence and constant containment are less appro- +L+ priate than in relatively homogeneous collections.
7. SUMMARY
The definition of the probability of being informative trans- +L+ forms the informative interpretation of the idf into a proba- +L+ bilistic interpretation, and we can use the idf -based proba- +L+ bility in probabilistic retrieval approaches. We showed that +L+ the classical definition of the noise (document frequency) in +L+ the inverse document frequency can be explained by three +L+ assumptions: the term within-document occurrence prob- +L+ ability is binary, the document containment probability is +L+ constant, and the document containment events are disjoint. +L+ By explicitly and mathematically formulating the assump- +L+ tions, we showed that the classical definition of idf does not +L+ take into account parameters such as the different nature +L+ (size, quality, structure, etc.) of documents in a collection, +L+ or the different nature of terms (coverage, importance, po- +L+ sition, etc.) in a document. We discussed that the absence +L+ of those parameters is compensated by a leverage effect of +L+ the within-document term occurrence probability and the +L+ document containment probability.
By applying an independence rather a disjointness as- +L+ sumption for the document containment, we could estab- +L+ lish a link between the noise probability (term occurrence +L+ in a collection), information theory and Poisson. From the +L+ frequency-based and the Poisson-based probabilities of be- +L+ ing noisy, we derived the frequency-based and Poisson-based +L+ probabilities of being informative. The frequency-based prob- +L+ ability is relatively smooth whereas the Poisson probability +L+ is radical in distinguishing between noisy or not noisy, and +L+ informative or not informative, respectively. We showed how +L+ to smoothen the radical behaviour of Poisson with a multi- +L+ dimensional Poisson.
The explicit and mathematical formulation of idf- and +L+ Poisson-assumptions is the main result of this paper. Also, +L+ the paper emphasises the duality of idf and tf, collection +L+ space and document space, respectively. Thus, the result +L+ applies to term occurrence and document containment in a +L+ collection, and it applies to term occurrence and position +L+ containment in a document. This theoretical framework is +L+ useful for understanding and deciding the parameter estima- +L+ tion and combination in probabilistic retrieval models. The +L+ links between indepence-based noise as document frequency, +L+ probabilistic interpretation of idf, information theory and +L+ Poisson described in this paper may lead to variable proba- +L+ bilistic idf and tf definitions and combinations as required +L+ in advanced and personalised information retrieval systems.
Acknowledgment: I would like to thank Mounia Lalmas, +L+ Gabriella Kazai and Theodora Tsikrika for their comments +L+ on the as they said “heavy” pieces. My thanks also go to the +L+ meta-reviewer who advised me to improve the presentation +L+ to make it less “formidable” and more accessible for those +L+ “without a theoretic bent”. This work was funded by a +L+ research fellowship from Queen Mary University of London.
8. REFERENCES
[1] A. Aizawa. An information-theoretic perspective of +L+ tf-idf measures. Information Processing and +L+ Management, 39:45–65, January 2003.
[2] G. Amati and C. J. Rijsbergen. Term frequency +L+ normalization via Pareto distributions. In 24th +L+ BCS-IRSG European Colloquium on IR Research, +L+ Glasgow, Scotland, 2002.
[3] R. K. Belew. Finding out about. Cambridge University +L+ Press, 2000.
[4] A. Bookstein and D. Swanson. Probabilistic models +L+ for automatic indexing. Journal of the American +L+ Society for Information Science, 25:312–318, 1974.
[5] I. N. Bronstein. Taschenbuch der Mathematik. Harri +L+ Deutsch, Thun, Frankfurt am Main, 1987.
[6] K. Church and W. Gale. Poisson mixtures. Natural +L+ Language Engineering, 1(2):163–190, 1995.
[7] K. W. Church and W. A. Gale. Inverse document +L+ frequency: A measure of deviations from poisson. In +L+ Third Workshop on Very Large Corpora, ACL +L+ Anthology, 1995.
[8] T. Lafouge and C. Michel. Links between information +L+ construction and information gain: Entropy and +L+ bibliometric distribution. Journal of Information +L+ Science, 27(1):39–49, 2001.
[9] E. Margulis. N-poisson document modelling. In +L+ Proceedings of the 15th Annual International ACM +L+ SIGIR Conference on Research and Development in +L+ Information Retrieval, pages 177–189, 1992.
[10] S. E. Robertson and S. Walker. Some simple effective +L+ approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of the +L+ 17th Annual International ACM SIGIR Conference on +L+ Research and Development in Information Retrieval, +L+ pages 232–241, London, et al., 1994. Springer-Verlag.
[11] S. Wong and Y. Yao. An information-theoric measure +L+ of term specificity. Journal of the American Society +L+ for Information Science, 43(1):54–61, 1992.
[12] S. Wong and Y. Yao. On modeling information +L+ retrieval with probabilistic inference. ACM +L+ Transactions on Information Systems, 13(1):38–68, +L+ 1995.
234
