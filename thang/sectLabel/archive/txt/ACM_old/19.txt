A Similarity Measure for Motion Stream
Segmentation and Recognition*
Chuanjun Li	B. Prabhakaran
Department of Computer Science
The University of Texas at Dallas, Richardson, TX 75083
{chuanjun, praba}@utdallas.edu
ABSTRACT
Recognition of motion streams such as data streams gener- +L+ ated by different sign languages or various captured human +L+ body motions requires a high performance similarity mea- +L+ sure. The motion streams have multiple attributes, and mo- +L+ tion patterns in the streams can have different lengths from +L+ those of isolated motion patterns and different attributes +L+ can have different temporal shifts and variations. To ad- +L+ dress these issues, this paper proposes a similarity measure +L+ based on singular value decomposition (SVD) of motion ma- +L+ trices. Eigenvector differences weighed by the corresponding +L+ eigenvalues are considered for the proposed similarity mea- +L+ sure. Experiments with general hand gestures and human +L+ motion streams show that the proposed similarity measure +L+ gives good performance for recognizing motion patterns in +L+ the motion streams in real time.
Categories and Subject Descriptors: H.2.8 [Database +L+ Management]: Database Applications – Data Mining
General Terms: Algorithm
Keywords: Pattern recognition, gesture, data streams, seg- +L+ mentation, singular value decomposition.
1. INTRODUCTION
Motion streams can be generated by continuously per- +L+ formed sign language words [14] or captured human body +L+ motions such as various dances. Captured human motions +L+ can be applied to the movie and computer game industries +L+ by reconstructing various motions from video sequences [10] +L+ or images [15] or from motions captured by motion capture +L+ systems [4]. Recognizing motion patterns in the streams +L+ with unsupervised methods requires no training process, and +L+ is very convenient when new motions are expected to be +L+ added to the known pattern pools. A similarity measure +L+ with good performance is thus necessary for segmenting and +L+ recognizing the motion streams. Such a similarity measure +L+ needs to address some new challenges posed by real world
*Work supported partially by the National Science Founda- +L+ tion under Grant No. 0237954 for the project CAREER: +L+ Animation Databases.
Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee.
Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$5.00.
motion streams: first, the motion patterns have dozens of at- +L+ tributes, and similar patterns can have different lengths due +L+ to different motion durations; second, different attributes of +L+ similar motions have different variations and different tem- +L+ poral shifts due to motion variations; and finally, motion +L+ streams are continuous, and there are no obvious ”pauses” +L+ between neighboring motions in a stream. A good similarity +L+ measure not only needs to capture the similarity of complete +L+ motion patterns, but also needs to capture the differences +L+ between complete motion patterns and incomplete motion +L+ patterns or sub-patterns in order to segment a stream for +L+ motion recognition.
As the main contribution of this paper, we propose a sim- +L+ ilarity measure to address the above issues. The proposed +L+ similarity measure is defined based on singular value decom- +L+ position of the motion matrices. The first few eigenvectors +L+ are compared for capturing the similarity of two matrices, +L+ and the inner products of the eigenvectors are given differ- +L+ ent weights for their different contributions. We propose to +L+ use only the eigenvalues corresponding to the involved eigen- +L+ vectors of the two motion matrices as weights. This simple +L+ and intuitive weighing strategy gives the same importance to +L+ eigenvalues of the two matrices. We also show that the 95% +L+ variance rule for choosing the number of eigenvectors [13] is +L+ not sufficient for recognizing both isolated patterns and mo- +L+ tion streams. Our experiments demonstrate that at least the +L+ first 6 eigenvectors need to be considered for motion streams +L+ of either 22 attribute or 54 attributes, and the first 6 eigen- +L+ values accounts for more than 99.5% of the total variance in +L+ the motion matrices.
2. RELATED WORK
Multi-attribute pattern similarity search, especially in con- +L+ tinuous motion streams, has been widely studied for sign +L+ language recognition and for motion synthesis in computer +L+ animation. The recognition methods usually include tem- +L+ plate matching by distance measures and hidden Markov +L+ models (HMM).
Template matching by using similarity/distance measures +L+ has been employed for multi-attribute pattern recognition. +L+ Joint angles are extracted in [11] as features to represent dif- +L+ ferent human body static poses for the Mahalanobis distance +L+ measure of two joint angle features. Similarly, momentum, +L+ kinetic energy and force are constructed in [2,5] as activ- +L+ ity measure and prediction of gesture boundaries for various +L+ segments of the human body, and the Mahalanobis distance +L+ function of two composite features are solved by dynamic +L+ programming.
89
Similarity measures are defined for multi-attribute data +L+ in [6,12,16] based on principal component analysis (PCA). +L+ Inner products or angular differences of principal compo- +L+ nents (PCs) are considered for similarity measure defini- +L+ tions, with different weighted strategies for different PCs. +L+ Equal weights are considered for different combinations of +L+ PCs in [6], giving different PCs equal contributions to the +L+ similarity measure. The similarity measure in [12] takes the +L+ minimum of two weighted sums of PC inner products, and +L+ the two sums are respectively weighted by different weights. +L+ A global weight vector is obtained by taking into account all +L+ available isolated motion patterns in [16], and this weight +L+ vector is used for specifying different contributions from dif- +L+ ferent PC inner products to the similarity measure Eros. +L+ The dominating first PC and a normalized eigenvalue vector +L+ are considered in [7,8] for pattern recognition. In contrast, +L+ this paper propose to consider the first few PCs, and the +L+ angular differences or inner products of different PCs are +L+ weighted by different weights which depends on the data +L+ variances along the corresponding PCs.
The HMM technique has been widely used for sign lan- +L+ guage recognition, and different recognition rates have been +L+ reported for different sign languages and different feature se- +L+ lection approaches. Starner et al. [14] achieved 92% and 98% +L+ word accuracy respectively for two systems, the first of the +L+ systems used a camera mounted on a desk and the second +L+ one used a camera in a user’s cap for extracting features +L+ as the input of HMM. Similarly Liang and Ouhyoung [9] +L+ used HMM for postures, orientations and motion primitives +L+ as features extracted from continuous Taiwan sign language +L+ streams and an average 80.4% recognition rate was achieved. +L+ In contrast, the approach proposed in this paper is an un- +L+ supervised approach, and no training as required for HMM +L+ recognizers is needed.
3. SIMILARITY MEASURE FOR MOTION +L+ STREAM RECOGNITION
The joint positional coordinates or joint angular values of +L+ a subject in motion can be represented by a matrix: the +L+ columns or attributes of the matrix are for different joints, +L+ and the rows or frames of the matrix are for different time +L+ instants. Similarity of two motions is the similarity of the +L+ resulting motion matrices, which have the same number of +L+ attributes or columns, and yet can have different number +L+ of rows due to different motion durations. To capture the +L+ similarity of two matrices of different lengths, we propose +L+ to apply singular value decomposition (SVD) to the motion +L+ matrices in order to capture the similarity of the matrix +L+ geometric structures. Hence we briefly present SVD and its +L+ associated properties below before proposing the similarity +L+ measure based on SVD in this section.
3.1 Singular Value Decomposition
The geometric structure of a matrix can be revealed by +L+ the SVD of the matrix. As shown in [3], any real m x n +L+ matrix A can be decomposed into A = UEVT , where U = +L+ [u1, u2, ... , um] E Rm×m and V = [v1, v2, ... , vn] E RnXn +L+ are two orthogonal matrices, and E is a diagonal matrix with +L+ diagonal entries being the singular values of A: v1 &gt; v2 &gt; +L+ . . . &gt; Qmin(m,n) &gt; 0. Column vectors ui and vi are the ith +L+ left and right singular vectors of A, respectively.
It can be shown that the right singular vectors of the sym-
metric n x n matrix M = AT A are identical to the corre- +L+ sponding right singular vectors of A, referred to as eigenvec- +L+ tors of M. The singular values of M, or eigenvalues of M, +L+ are squares of the corresponding singular values of A. The +L+ eigenvector with the largest eigenvalue gives the first prin- +L+ cipal component. The eigenvector with the second largest +L+ eigenvalue is the second principal component and so on.
3.2 Similarity Measure
Since SVD exposes the geometric structure of a matrix, it +L+ can be used for capturing the similarity of two matrices. We +L+ can compute the SVD of M = AT A instead of computing +L+ the SVD of A to save computational time. The reasons are +L+ that the eigenvectors of M are identical to the corresponding +L+ right singular vectors of A, the eigenvalues of M are the +L+ squares of the corresponding singular values of A, and SVD +L+ takes O(n 3) time for the n x n M and takes O(mn2) time +L+ with a large constant for the m x n A, and usually m &gt; n.
Ideally, if two motions are similar, their corresponding +L+ eigenvectors should be parallel to each other, and their cor- +L+ responding eigenvalues should also be proportional to each +L+ other. This is because the eigenvectors are the correspond- +L+ ing principal components, and the eigenvalues reflect the +L+ variances of the matrix data along the corresponding prin- +L+ cipal components. But due to motion variations, all corre- +L+ sponding eigenvectors cannot be parallel as shown in Fig- +L+ ure 1. The parallelness or angular differences of two eigen- +L+ vectors u and v can be described by the absolute value of +L+ their inner products: l cosOl = lu • vl/(lullvl) = lu • vl, where +L+ lul = lvl = 1. We consider the absolute value of the in- +L+ ner products because eigenvectors can have different signs +L+ as shown in [8].
Since eigenvalues are numerically related to the variances +L+ of the matrix data along the associated eigenvectors, the im- +L+ portance of the eigenvector parallelness can be described by +L+ the corresponding eigenvalues. Hence, eigenvalues are to be +L+ used to give different weights to different eigenvector pairs. +L+ Figure 2 shows that the first eigenvalues are the dominat- +L+ ing components of all the eigenvalues, and other eigenval- +L+ ues become smaller and smaller and approach zero. As the +L+ eigenvalues are close to zero, their corresponding eigenvec- +L+ tors can be very different even if two matrices are similar. +L+ Hence not all the eigenvectors need to be incorporated into +L+ the similarity measure.
Since two matrices have two eigenvalues for the corre- +L+ sponding eigenvector pair, these two eigenvalues should have +L+ equal contributions or weights to the eigenvector parallel- +L+ ness. In addition, the similarity measure of two matrices +L+ should be independent to other matrices, hence only eigen- +L+ vectors and eigenvalues of the two matrices should be con- +L+ sidered.
Based on the above discussions, we propose the following +L+ similarity measure for two matrices Q and P:
k
1
  (Q, P) =
2 i=1
where vi and Ai are the ith eigenvalues corresponding to the +L+ ith eigenvectors ui and vi of square matrices of Q and P, +L+ respectively, and 1 &lt; k &lt; n. Integer k determines how many +L+ eigenvectors are considered and it depends on the number +L+ of attributes n of motion matrices. Experiments with hand +L+ gesture motions (n = 22) and human body motions (n =
n
((�i/
i=1
n
Qi+Ai/
i=1
Ai)lui •vil)
90
Figure 1: Eigenvectors of similar patterns. The first +L+ eigenvectors are similar to each other, while other +L+ eigenvectors, such as the second vectors shown in +L+ the bottom, can be quite different.
54) in Section 4 show that k = 6 is large enough without +L+ loss of pattern recognition accuracy in streams. We refer to +L+ this non-metric similarity measure as k Weighted Angular +L+ Similarity (kWAS) , which captures the angular similarities +L+ of the first k corresponding eigenvector pairs weighted by +L+ the corresponding eigenvalues.
It can be easily verified that the value of kWAS ranges over +L+ [0,1]. When all corresponding eigenvectors are normal to +L+ each other, the similarity measure will be zero, and when two +L+ matrices are identical, the similarity measure approaches the +L+ maximum value one if k approaches n.
3.3 Stream Segmentation Algorithm
In order to recognize motion streams, we assume one mo- +L+ tion in a stream has a minimum length l and a maximum +L+ length L. The following steps can be applied to incremen- +L+ tally segment a stream for motion recognition:
1. SVD is applied to all isolated motion patterns P to +L+ obtain their eigenvectors and eigenvalues. Let S be +L+ the incremented stream length for segmentation, and +L+ let L be the location for segmentation. Initially L = l.
2. Starting from the beginning of the stream or the end of +L+ the previously recognized motion, segment the stream +L+ at location L. Compute the eigenvectors and eigenval- +L+ ues of the motion segment Q.
3. Compute kWAS between Q and all motion patterns
P. Update T... to be the highest similarity after the
previous motion’s recognition.
4. If L+S &lt; L, update L = L+S and go to step 2. Other-
wise, the segment corresponding to T,r,.. is recognized
to be the motion pattern which gives the highest simi-
larity T..., update L = l starting from the end of the
last recognized motion pattern and go to step 2.
100
99
95 +L+ 93 +L+ 91
89
87
1	2	3	4	5	6	7	8
Number of Eigenvalues
Figure 2: Accumulated eigenvalue percentages in +L+ total eigenvalues for CyberGlove data and captured +L+ human body motion data. There are 22 eigenvalues +L+ for the CyberGlove data and 54 eigenvalues for the +L+ captured motion data. The sum of the first 2 eigen- +L+ values is more than 95% of the corresponding total +L+ eigenvalues, and the sum of the first 6 eigenvalues is +L+ almost 100% of the total eigenvalues.
4. PERFORMANCE EVALUATION
This section evaluates experimentally the performances +L+ of the similarity measure kWAS proposed in this paper. It +L+ has been shown in [16] that Eros [16] outperforms other +L+ similarity measures mentioned in Section 2 except MAS [8]. +L+ Hence in this section, we compare the performances of the +L+ proposed kWAS with Eros and MAS for recognizing similar +L+ isolated motion patterns and for segmenting and recognizing +L+ motion streams from hand gesture capturing CyberGlove +L+ and human body motion capture system.
4.1 Data Generation
A similarity measure should be able to be used not only +L+ for recognizing isolated patterns with high accuracy, but also +L+ for recognizing patterns in continuous motions or motion +L+ streams. Recognizing motion streams is more challenging +L+ than recognizing isolated patterns. This is because many +L+ very similar motion segments or sub-patterns needs to be +L+ compared in order to find appropriate segmentation loca- +L+ tions, and a similarity measure should capture the difference +L+ between a complete motion or pattern and its sub-patterns. +L+ Hence, both isolated motion patterns and motion streams +L+ were generated for evaluating the performance of kWAS. +L+ Two data sources are considered for data generation: a Cy- +L+ berGlove for capturing hand gestures and a Vicon motion +L+ capture system for capturing human body motions.
4.1.1 CyberGlove Data
A CyberGlove is a fully instrumented data glove that pro- +L+ vides 22 sensors for measuring hand joint angular values to +L+ capture motions of a hand, such as American Sign Language +L+ (ASL) words for hearing impaired. The data for a hand ges- +L+ ture contain 22 angular values for each time instant/frame, +L+ one value for a joint of one degree of freedom. The mo- +L+ tion data are extracted at around 120 frames per second. +L+ Data matrices thus have 22 attributes for the CyberGlove +L+ motions.
One hundred and ten different isolated motions were gen- +L+ erated as motion patterns, and each motion was repeated +L+ for three times, resulting in 330 isolated hand gesture mo- +L+ tions. Some motions have semantic meanings. For example,
2	4	6	8	10	12	14	16	18	20	22
Motion341
Motion342
Motion341
Motion342
2	4	6	8	10	12	14	16	18	20	22
Component of Second Eigenvector
	0.6 0.4 0.2 0 −0.2 −0.4
0.2
0.1
0 +L+ −0.1 +L+ −0.2 +L+ −0.3 +L+ −0.4 +L+ −0.5 +L+ −0.6 +L+ −0.7
Component of First Eigenvector
97
85
CyberGlove Data +L+ MoCap Data
91
the motion for BUS as shown in Table 1 is for the ASL sign +L+ ”bus”. Yet for segmentation and recognition, we only re- +L+ quire that each individual motion be different from others, +L+ and thus some motions are general motions, and do not have +L+ any particular semantic meanings, such as the THUMBUP +L+ motion in Table 1.
The following 18 motions shown in Table 1 were used to +L+ generate continuous motions or streams. Twenty four dif- +L+ ferent motion streams were generated for segmentation and +L+ recognition purpose. There are 5 to 10 motions in a stream +L+ and 150 motions in total in 24 streams, with 6.25 motions in +L+ a stream on average. It should be noted that variable-length +L+ transitional noises occur between successive motions in the +L+ generated streams.
Table 1: Individual motions used for streams
35 60 70 80 90 BUS GOODBYE
HALF IDIOM JAR JUICE KENNEL KNEE
MILK TV SCISSOR SPREAD THUMBUP
4.1.2 Motion Capture Data
The motion capture data come from various motions cap- +L+ tured collectively by using 16 Vicon cameras and the Vicon +L+ iQ Workstation software. A dancer wears a suit of non- +L+ reflective material and 44 markers are attached to the body +L+ suit. After system calibration and subject calibration, global +L+ coordinates and rotation angles of 19 joints/segments can +L+ be obtained at about 120 frames per second for any mo- +L+ tion. Similarity of patterns with global 3D positional data +L+ can be disguised by different locations, orientations or differ- +L+ ent paths of motion execution as illustrated in Figure 3(a). +L+ Since two patterns are similar to each other because of sim- +L+ ilar relative positions of corresponding body segments at +L+ corresponding time, and the relative positions of different +L+ segments are independent of locations or orientations of the +L+ body, we can transform the global position data into local +L+ position data as follows.
Let Xp, Yp, Zp be the global coordinates of one point on +L+ pelvis, the selected origin of the ”moving” local coordinate +L+ system, and a,,3, -y be the rotation angles of the pelvis seg- +L+ ment relative to the global coordinate system axes, respec- +L+ tively. The translation matrix is T as follows:
1	0	0	0
0	1	0	0
0	0	1	0
—Xp	—Yp	—Zp	1
The rotation matrix R = R. x Ry x Rz, where
1	0	0	0
0 cos a — sin a 0 +L+ 0 sin a cos a 0 +L+ 0 0 0 1
cos,3 0 sin,3 0
0	1	0	0
—sin,3 0 cos,3 0
0	0	0	1
Motion Capture Frames	Motion Capture Frames
(a)	(b)
Figure 3: 3D motion capture data for similar motions +L+ executed at different locations and in different orien- +L+ tations: (a) before transformation; (b) after transfor- +L+ mation.
cos-y —sin -y 0 0 +L+ sin -y cos -y 0 0
0 0 1 0
0 0 0 1
Let X, Y, Z be the global coordinates of one point on any +L+ segments, and x, y, z be the corresponding transformed local +L+ coordinates. x, y and z can be computed as follows:
[x y z 1]=[X Y Z 1] x T x R
The transformed data are positions of different segments +L+ relative to a moving coordinate system with the origin at +L+ some fixed point of the body, for example the pelvis. The +L+ moving coordinate system is not necessarily aligned with +L+ the global system, and it can rotate with the body. So data +L+ transformation includes both translation and rotation, and +L+ the transformed data would be translation and rotation in- +L+ variant as shown in Figure 3(b). The coordinates of the +L+ origin pelvis are not included, thus the transformed matri- +L+ ces have 54 columns.
Sixty two isolated motions including Taiqi, Indian dances, +L+ and western dances were performed for generating motion +L+ capture data, and each motion was repeated 5 times, yield- +L+ ing 310 isolated human motions. Every repeated motion has +L+ a different location and different durations, and can face +L+ different orientations. Twenty three motion streams were +L+ generated for segmentation. There are 3 to 5 motions in +L+ a stream, and 93 motions in total in 23 streams, with 4.0 +L+ motions in a stream on average.
4.2 Performance of kWAS for Capturing Sim- +L+ ilarities and Segmenting Streams
We first apply kWAS to isolated motion patterns to show +L+ that the proposed similarity measure kWAS can capture the +L+ similarities of isolated motion patterns. Then kWAS is ap- +L+ plied to motion streams for segmenting streams and recog- +L+ nizing motion patterns in the streams. We experimented +L+ with different k values in order to find out the smallest k +L+ without loss of good performance.
Figure 2 shows the accumulated eigenvalue percentages +L+ averaged on 330 hand gestures and 310 human motions, re- +L+ spectively. Although the first two eigenvalues account for
1500
1000
500
0
−500
−1000
−1500
0	50	100	150	200	250	300	350	400	450
−1000
0	50	100	150	200	250	300	350	400	450
1000
500
0
−500
2000
1500
1000
500
0
0	50	100	150	200	250	300	350	400	450
−1000
0	50	100	150	200	250	300	350	400	450
1000
500
0
−500
T=	
R, =	
Ry =	
Rz =	
92
Number of Nearest Neghbors (Most Simlar Patterns)
Figure 4: Recognition rate of similar CyberGlove +L+ motion patterns. When k is 3, kWAS can find the +L+ most similar motions for about 99.7% of 330 mo- +L+ tions, and can find the second most similar motions +L+ for 97.5% of the them.
Number of Nearest Neighbors (Most aimilar Patterns1
Figure 5: Recognition rate of similar captured mo- +L+ tion patterns. When k is 5, by using kWAS, the most +L+ similar motions of all 310 motions can be found, and +L+ the second most similar motions of 99.8% of the 310 +L+ motions can also be found.
more than 95% of the respective sums of all eigenvalues, +L+ considering only the first two eigenvectors for kWAS is not +L+ sufficient as shown in Figure 4 and Figure 5. For Cyber- +L+ Glove data with 22 attributes, kWAS with k = 3 gives the +L+ same performance as kWAS with k = 22, and for motion +L+ capture data with 54 attributes, kWAS with k = 5 gives the +L+ same performance as kWAS with k = 54. Figure 4 and Fig- +L+ ure 5 illustrate that kWAS can be used for finding similar +L+ motion patterns and outperforms MAS and Eros for both +L+ hand gesture and human body motion data.
The steps in Section 3.3 are used for segmenting streams +L+ and recognizing motions in streams. The recognition accu- +L+ racy as defined in [14] is used for motion stream recognition. +L+ The motion recognition accuracies are shown in Table 2. For +L+ both CyberGlove motion and captured motion data, k = 6 +L+ is used for kWAS, which gives the same accuracy as k = 22 +L+ for CyberGlove data and k = 54 for motion capture data, +L+ respectively.
Figure 6 shows the time taken for updating the candi- +L+ date segment, including updating the matrix, computing the +L+ SVD of the updated matrix, and computing the similarities +L+ of the segment and all motion patterns. The code imple- +L+ mented in C++ was run on one 2.70 GHz Intel processor +L+ of a GenuineIntel Linux box. There are 22 attributes for +L+ the CyberGlove streams, and 54 attributes for the captured
Figure 6: Computation time for stream segment up- +L+ date and similarity computation.
Table 2: Stream Pattern Recognition Accuracy (%)
Similarity Measures	CyberGlove	Motion Capture
	Streams	Streams
Eros	68.7	78.5
MAS	93.3	78.5
kWAS (k=6)	94.0	94.6
motion streams. Hence updating captured motion segments +L+ takes longer than updating CyberGlove motion segments as +L+ shown in Figure 6. The time required by kWAS is close to +L+ the time required by MAS, and is less than half of the time +L+ taken by using Eros.
4.3 Discussions
kWAS captures the similarity of square matrices of two +L+ matrices P and Q, yet the temporal order of pattern execu- +L+ tion is not revealed in the square matrices. As shown in [7], +L+ two matrices with the identical row vectors in different or- +L+ ders have identical eigenvectors and identical eigenvalues. If +L+ different temporal orders of pattern execution yield patterns +L+ with different semantic meanings, we need to further con- +L+ sider the temporal execution order, which is not reflected in +L+ the eigenvectors and eigenvalues and has not been consid- +L+ ered previously in [6,12,16].
Since the first eigenvectors are close or parallel for similar +L+ patterns, we can project pattern A onto its first eigenvector +L+ ul by Aul. Then similar patterns would have similar projec- +L+ tions (called projection vectors hereafter), showing similar +L+ temporal execution orders while the projection variations +L+ for each pattern can be maximized. The pattern projection +L+ vectors can be compared by computing their dynamic time +L+ warping (DTW) distances, for DTW can align sequences +L+ of different lengths and can be solved easily by dynamic +L+ programming [1]. Incorporating temporal order information +L+ into the similarity measure can be done as for MAS in [7] +L+ if motion temporal execution orders cause motion pattern +L+ ambiguity to kWAS.
5. CONCLUSIONS
This paper has proposed a similarity measure kWAS for +L+ motion stream segmentation and motion pattern recogni- +L+ tion. kWAS considers the first few k eigenvectors and com- +L+ putes their angular similarities/differences, and weighs con- +L+ tributions of different eigenvector pairs by their correspond-
100
99
98
97
96
95
94
93
92
91
90
1	2
kWAS (k = 22) +L+ kWAS (k = 5) +L+ kWAS (k = 3) +L+ kWAS (k = 2) +L+ MAS
EROS
99.5
98.5
97.5
96.5
95.5
100
99
98
97
96
95
123	4
kWna (k = 541 +L+ kWna (k = 51 +L+ kWna (k = 41 +L+ kWna (k = 31 +L+ Mna
EROa
20
18
16
14
12
10
8
6
4
2
0
CyberGlove Streams	Motion Capture Streams
MAS
kWAS (k = 6) +L+ EROS
93
ing eigenvalues. Eigenvalues from two motion matrices are +L+ given equal importance to the weights. Experiments with +L+ CyberGlove hand gesture streams and captured human body +L+ motions such as Taiqi and dances show that kWAS can rec- +L+ ognize 100% most similar isolated patterns and can recog- +L+ nize 94% motion patterns in continuous motion streams.
6. REFERENCES
[1] D. Berndt and J. Clifford. Using dynamic time +L+ warping to find patterns in time series. In AAAI-94 +L+ Workshop on Knowledge Discovery in Databases, +L+ pages 229–248, 1994.
[2] V. M. Dyaberi, H. Sundaram, J. James, and G. Qian. +L+ Phrase structure detection in dance. In Proceedings of +L+ the ACM Multimedia Conference 2004, pages 332–335, +L+ Oct. 2004.
[3] G. H. Golub and C. F. V. Loan. Matrix Computations. +L+ The Johns Hopkins University Press, +L+ Baltimore,Maryland, 1996.
[4] L. Ikemoto and D. A. Forsyth. Enriching a motion +L+ collection by transplanting limbs. In Proceedings of the +L+ 2004 ACM SIGGRAPH/Eurographics symposium on +L+ Computer animation, pages 99 – 108, 2004.
[5] K. Kahol, P. Tripathi, S. Panchanathan, and +L+ T. Rikakis. Gesture segmentation in complex motion +L+ sequences. In Proceedings of IEEE International +L+ Conference on Image Processing, pages II – 105–108, +L+ Sept. 2003.
[6] W. Krzanowski. Between-groups comparison of +L+ principal components. J. Amer. Stat. Assoc., +L+ 74(367):703–707, 1979.
[7] C. Li, B. Prabhakaran, and S. Zheng. Similarity +L+ measure for multi-attribute data. In Proceedings of the +L+ 2005 IEEE International Conference on Acoustics, +L+ Speach, and Signal Processing (ICASSP), Mar. 2005.
[8] C. Li, P. Zhai, S.-Q. Zheng, and B. Prabhakaran. +L+ Segmentation and recognition of multi-attribute +L+ motion sequences. In Proceedings of the ACM +L+ Multimedia Conference 2004, pages 836–843, Oct. +L+ 2004.
[9] R. H. Liang and M. Ouhyoung. A real-time continuous +L+ gesture recognition system for sign language. In +L+ Proceedings of the 3rd. International Conference on +L+ Face and Gesture Recognition, pages 558–565, 1998.
[10] K. Pullen and C. Bregler. Motion capture assisted +L+ animation: texturing and synthesis. In SIGGRAPH, +L+ pages 501–508, 2002.
[11] G. Qian, F. Guo, T. Ingalls, L. Olson, J. James, and +L+ T. Rikakis. A gesture-driven multimodal interactive +L+ dance system. In Proceedings of IEEE International +L+ Conference on Multimedia and Expo, June 2004.
[12] C. Shahabi and D. Yan. Real-time pattern isolation +L+ and recognition over immersive sensor data streams. +L+ In Proceedings of the 9th International Conference on +L+ Multi-Media Modeling, pages 93–113, Jan 2003.
[13] A. Singhal and D. E. Seborg. Clustering of +L+ multivariate time-series data. In Proceedings of the +L+ American Control Conference, pages 3931–3936, 2002.
[14] T. Starner, J. Weaver, and A. Pentland. Real-time +L+ american sign language recognition using desk and +L+ wearable computer based video. IEEE Transactions +L+ on Pattern Analysis and Machine Intelligence, +L+ 20(12):1371–1375, 1998.
[15] C. J. Taylor. Reconstruction of articulated objects +L+ from point correspondences in a single image. +L+ Computer Vision and Image Understanding, +L+ 80(3):349–363, 2000.
[16] K. Yang and C. Shahabi. A PCA-based similarity +L+ measure for multivariate time series. In Proceedings of +L+ the Second ACM International Workshop on +L+ Multimedia Databases, pages 65–74, Nov. 2004.
94
