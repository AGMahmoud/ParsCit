A Machine Learning Based Approach for Table Detection
on The Web
Yalin Wang
Intelligent Systems Laboratory
Dept. of Electrical Engineering
Univ. of Washington
Seattle, WA 98195 US
ylwang@u.washington.edu
Jianying Hu
Avaya Labs Research
233, Mount Airy Road
Basking Ridge, NJ 07920 US
jianhu@avaya.com
ABSTRACT
Table is a commonly used presentation scheme, especially +L+ for describing relational information. However, table under- +L+ standing remains an open problem. In this paper, we con- +L+ sider the problem of table detection in web documents. Its +L+ potential applications include web mining, knowledge man- +L+ agement, and web content summarization and delivery to +L+ narrow-bandwidth devices. We describe a machine learning +L+ based approach to classify each given table entity as either +L+ genuine or non-genuine. Various features reflecting the lay- +L+ out as well as content characteristics of tables are studied.
In order to facilitate the training and evaluation of our +L+ table classifier, we designed a novel web document table +L+ ground truthing protocol and used it to build a large ta- +L+ ble ground truth database. The database consists of 1,393 +L+ HTML files collected from hundreds of different web sites +L+ and contains 11,477 leaf &lt;TABLE&gt; elements, out of which +L+ 1,740 are genuine tables. Experiments were conducted us-
ing the cross validation method and an F-measure of 95.89%
was achieved.
Categories and Subject Descriptors
H.4.3 [Information Systems Applications]: Communi- +L+ cations Applications Information browsers
General Terms
Algorithms
Keywords
Table Detection, Layout Analysis, Machine Learning, Deci- +L+ sion tree, Support Vector Machine, Information Retrieval
1. INTRODUCTION
The increasing ubiquity of the Internet has brought about +L+ a constantly increasing amount of online publications. As +L+ a compact and efficient way to present relational informa- +L+ tion, tables are used frequently in web documents. Since +L+ tables are inherently concise as well as information rich, the +L+ automatic understanding of tables has many applications in- +L+ cluding knowledge management, information retrieval, web
Copyright is held by the author/owner(s).
WWW2002, May 7–11,2002, Honolulu, Hawaii, USA. +L+ ACM 1-58113-449-5/02/0005.
mining, summarization, and content delivery to mobile de- +L+ vices. The processes of table understanding in web doc- +L+ uments include table detection, functional and structural +L+ analysis and finally table interpretation [6]. In this paper, +L+ we concentrate on the problem of table detection. The web +L+ provides users with great possibilities to use their own style +L+ of communication and expressions. In particular, people use +L+ the &lt;TABLE&gt; tag not only for relational information display +L+ but also to create any type of multiple-column layout to +L+ facilitate easy viewing, thus the presence of the &lt;TABLE&gt; +L+ tag does not necessarily indicate the presence of a relational +L+ table. In this paper, we define genuine tables to be docu- +L+ ment entities where a two dimensional grid is semantically +L+ significant in conveying the logical relations among the cells +L+ [10]. Conversely, Non-genuine tables are document entities +L+ where &lt;TABLE&gt; tags are used as a mechanism for grouping +L+ contents into clusters for easy viewing only. Figure 1 gives +L+ a few examples of genuine and non-genuine tables. While +L+ genuine tables in web documents could also be created with- +L+ out the use of &lt;TABLE&gt; tags at all, we do not consider such +L+ cases in this article as they seem very rare from our ex- +L+ perience. Thus, in this study, Table detection refers to the +L+ technique which classifies a document entity enclosed by the +L+ &lt;TABLE&gt;&lt;/TABLE&gt; tags as genuine or non-genuine tables.
Several researchers have reported their work on web table +L+ detection [2, 10, 6, 14]. In [2], Chen et al. used heuris- +L+ tic rules and cell similarities to identify tables. They tested +L+ their table detection algorithm on 918 tables from airline in-
formation web pages and achieved an F-measure of 86.50%.
Penn et al. proposed a set of rules for identifying genuinely +L+ tabular information and news links in HTML documents +L+ [10]. They tested their algorithm on 75 web site front-pages
and achieved an F-measure of 88.05%. Yoshida et al. pro-
posed a method to integrate WWW tables according to the +L+ category of objects presented in each table [14]. Their data +L+ set contains 35,232 table tags gathered from the web. They +L+ estimated their algorithm parameters using all of table data +L+ and then evaluated algorithm accuracy on 175 of the tables.
The average F-measure reported in their paper is 82.65%.
These previous methods all relied on heuristic rules and were +L+ only tested on a database that is either very small [10], or +L+ highly domain specific [2]. Hurst mentioned that a Naive +L+ Bayes classifier algorithm produced adequate results but no +L+ detailed algorithm and experimental information was pro- +L+ vided [6].
We propose a new machine learning based approach for
242
Figure 1: Examples of genuine and non-genuine tables.
table detection from generic web documents. In particu- +L+ lar, we introduce a set of novel features which reflect the +L+ layout as well as content characteristics of tables. These +L+ features are used in classifiers trained on thousands of ex- +L+ amples. To facilitate the training and evaluation of the table +L+ classifiers, we designed a novel web document table ground +L+ truthing protocol and used it to build a large table ground +L+ truth database. The database consists of 1,393 HTML files +L+ collected from hundreds of different web sites and contains +L+ 11,477 leaf &lt;TABLE&gt; elements, out of which 1,740 are gen- +L+ uine tables. Experiments on this database using the cross +L+ validation method demonstrate significant performance im- +L+ provements over previous methods.
The rest of the paper is organized as follows. We describe +L+ our feature set in Section 2, followed by a brief discussion +L+ of the classifiers we experimented with in Section 3. In Sec- +L+ tion 4, we present a novel table ground truthing protocol +L+ and explain how we built our database. Experimental re- +L+ sults are then reported in Section 5 and we conclude with +L+ future directions in Section 6.
2. FEATURES FOR WEB TABLE +L+ DETECTION
Feature selection is a crucial step in any machine learning +L+ based methods. In our case, we need to find a combination +L+ of features that together provide significant separation be- +L+ tween genuine and non-genuine tables while at the same time +L+ constrain the total number of features to avoid the curse of +L+ dimensionality. Past research has clearly indicated that lay- +L+ out and content are two important aspects in table under- +L+ standing [6]. Our features were designed to capture both of +L+ these aspects. In particular, we developed 16 features which +L+ can be categorized into three groups: seven layout features, +L+ eight content type features and one word group feature. In +L+ the first two groups, we attempt to capture the global com- +L+ position of tables as well as the consistency within the whole +L+ table and across rows and columns. The last feature looks at +L+ words used in tables and is derived directly from the vector +L+ space model commonly used in Information Retrieval.
Before feature extraction, each HTML document is first +L+ parsed into a document hierarchy tree using Java Swing +L+ XML parser with W3C HTML 3.2 DTD [10]. A &lt;TABLE&gt; +L+ node is said to be a leaf table if and only if there are no +L+ &lt;TABLE&gt; nodes among its children [10]. Our experience in- +L+ dicates that almost all genuine tables are leaf tables. Thus +L+ in this study only leaf tables are considered candidates for +L+ genuine tables and are passed on to the feature extraction +L+ stage. In the following we describe each feature in detail.
2.1 Layout Features
In HTML documents, although tags like &lt;TR&gt; and &lt;TD&gt; +L+ (or &lt;TH&gt;) may be assumed to delimit table rows and table +L+ cells, they are not always reliable indicators of the number +L+ of rows and columns in a table. Variations can be caused +L+ by spanning cells created using &lt;ROWSPAN&gt; and &lt;COLSPAN&gt; +L+ tags. Other tags such as &lt;BR&gt; could be used to move con- +L+ tent into the next row. Therefore to extract layout features +L+ reliably one can not simply count the number of &lt;TR&gt;&apos;s and +L+ &lt;TD&gt;&apos;s. For this purpose, we maintain a matrix to record all
243
the cell spanning information and serve as a pseudo render- +L+ ing of the table. Layout features based on row or column +L+ numbers are then computed from this matrix.
Given a table T, assuming its numbers of rows and columns +L+ are rn and cn respectively, we compute the following layout +L+ features:
•	Average number of columns, computed as the average +L+ number of cells per row:
Here LCcl is defined as: LCcl = 0.5 — D, where D = +L+ min{lcl — mil�mi,1.0}. Intuitively, LCcl measures the +L+ degree of consistency between cl and the mean cell +L+ length, with —0.5 indicating extreme inconsistency and +L+ 0.5 indicating extreme consistency. When most cells +L+ within Ri are consistent, the cumulative measure CLCi +L+ is positive, indicating a more or less consistent row.
3. Take the average across all rows:
ci,
c =
1 +L+ rn
Xrn +L+ i�1
CLCr = 1
r
Xr CLCi . +L+ i�1
where ci is the number of cells in row i, i = 1, ..., rn,
•	Standard deviation of number of columns:
(ci — c) x (ci — c);
•	Average number of rows, computed as the average +L+ number of cells per column:
where ri is the number of cells in column i, i = 1, ..., cn,
•	Standard deviation of number of rows:
(ri — r) x (ri — r).
Since the majority of tables in web documents contain +L+ characters, we compute three more layout features based on +L+ cell length in terms of number of characters:
•	Average overall cell length: cl = en Pin1 cli, where en +L+ is the total number of cells in a given table and cli is +L+ the length of cell i, i = 1, ... , en,
•	Standard deviation of cell length:
(cli — cl) x (cli — cl)�
•	Average Cumulative length consistency, CLC.
The last feature is designed to measure the cell length con- +L+ sistency along either row or column directions. It is inspired +L+ by the fact that most genuine tables demonstrate certain +L+ consistency either along the row or the column direction, +L+ but usually not both, while non-genuine tables often show +L+ no consistency in either direction. First, the average cumu- +L+ lative within-row length consistency, CLCr, is computed as +L+ follows. Let the set of cell lengths of the cells from row i be +L+ Ri, i = 1, ... , r (considering only non-spanning cells):
1. Compute the mean cell length, mi, for row Ri.
2. Compute cumulative length consistency within each +L+ Ri:
CLCi = X LCcl .
clERi
After the within-row length consistency CLCr is com- +L+ puted, the within-column length consistency CLCc is com- +L+ puted in a similar manner. Finally, the overall cumulative +L+ length consistency is computed as CLC = max(CLCr, CLCc).
2.2 Content Type Features
Web documents are inherently multi-media and has more +L+ types of content than any traditional documents. For ex- +L+ ample, the content within a &lt;TABLE&gt; element could include +L+ hyperlinks, images, forms, alphabetical or numerical strings, +L+ etc. Because of the relational information it needs to convey, +L+ a genuine table is more likely to contain alpha or numeri- +L+ cal strings than, say, images. The content type feature was +L+ designed to reflect such characteristics.
We define the set of content types T = {Image, Form, +L+ Hyperlink, Alphabetical, Digit, Empty, Others}. Our content +L+ type features include:
•	The histogram of content type for a given table. This +L+ contributes 7 features to the feature set,
•	Average content type consistency, CTC.
The last feature is similar to the cell length consistency fea- +L+ ture. First, within-row content type consistency CTCr is +L+ computed as follows. Let the set of cell type of the cells +L+ from row i as Ti, i = 1,... , r (again, considering only non- +L+ spanning cells):
1. Find the dominant type, DTi, for Ti.
2. Compute the cumulative type consistency with each +L+ row Ri, i = 1,... ,r:
CTCi = X D,
ctERi
where D = 1 if ct is equal to DTi and D = —1, other- +L+ wise.
3. Take the average across all rows:
CTCr = 1
r
The within-column type consistency is then computed in +L+ a similar manner. Finally, the overall cumulative type con- +L+ sistency is computed as: CTC = max(CTCr, CTCc).
tiv
dC =
1 +L+ rn
Xrn +L+ i�1
ri,
r=
1 +L+ rn
Xcn +L+ i�1
vt uu
dR =
1 +L+ cn
Xcn +L+ i�1
tuuv
dCL =
1 +L+ en
Xen +L+ i�1
Xr CT Ci +L+ i�1
244
2.3 Word Group Feature
If we treat each table as a &quot;mini-document&quot; by itself, ta- +L+ ble classification can be viewed as a document categoriza- +L+ tion problem with two broad categories: genuine tables and +L+ non-genuine tables. We designed the word group feature to +L+ incorporate word content for table classification based on +L+ techniques developed in information retrieval [7, 13].
After morphing [11] and removing the infrequent words, +L+ we obtain the set of words found in the training data, W. +L+ We then construct weight vectors representing genuine and +L+ non-genuine tables and compare that against the frequency +L+ vector from each new incoming table.
Let 3 represent the non-negative integer set. The follow- +L+ ing functions are defined on set W.
•	dfG : W —&gt; 3, where dfG (wi) is the number of genuine +L+ tables which include word wi, i = 1, ..., 1W1;
•	t f G : W —&gt; 3, where t f G (wi) is the number of times
word wi, i =1,...,1W1, appears in genuine tables;
•	dfN : W —&gt; 3, where dfN(wi) is the number of non- +L+ genuine tables which include word wi, i =1,...,1W1;
•	t f N : W —&gt; 3, where t f N (wi) is the number of times +L+ word wi, i =1,...,1W1, appears in non-genuine tables.
•	t fT : W —&gt; 3, where t fT (wi) is the number of times +L+ word wi, wi 2 W appears in a new test table.
To simplify the notations, in the following discussion, we +L+ will use dfGi, t fGi , df N i and t f Ni to represent dfG (wi), t f G (wi), +L+ df N (wi) and t f N (wi), respectively.
Let NG, NN be the number of genuine tables and non- +L+ genuine tables in the training collection, respectively and let +L+ C = max(NG, NN). Without loss of generality, we assume +L+ NG =� 0 and NN =� 0. For each word wi in W, i = 1, ...,1W1, +L+ two weights, pGi and pNi are computed:
N
tfGilog(N� fN +1), when df Ni :A 0 +L+ tfGilog(Ni C+1), when df i = 0 +L+ tfiNlog(NNN fG
G+1), when dfGi00 +L+ tfNilog(NNC+1),	when dfG=0
As can be seen from the formulas, the definitions of these +L+ weights were derived from the traditional t f * idf measures +L+ used in informational retrieval, with some adjustments made +L+ for the particular problem at hand.
Given a new incoming table, let us denote the set includ- +L+ ing all the words in it as Wn. Since W is constructed using +L+ thousands of tables, the words that are present in both W +L+ and Wn are only a small subset of W. Based on the vector +L+ space model, we define the similarity between weight vec- +L+ tors representing genuine and non-genuine tables and the +L+ frequency vector representing the incoming table as the cor- +L+ responding dot products. Since we only need to consider the +L+ words that are present in both W and Wn, we first compute +L+ the effective word set: We = W n Wn. Let the words in +L+ We be represented as wmk, where mk,k = 1, ..., 1We1, are +L+ indexes to the words from set W = fw1, w2, ..., wIWI g. we +L+ define the following vectors:
•	Weight vector representing the genuine table group:
i	pGmJ
GS=
U
where U is the cosine normalization term:
where V is the cosine normalization term:
NN
pmk X pmk .
•	Frequency vector representing the new incoming table:
&apos;i	T T	T
I	(tfml,tfmt,... tfT Wel I .
Finally, the word group feature is defined as the ratio of
the two dot products:
3. CLASSIFICATION SCHEMES
Various classification schemes have been widely used in +L+ document categorization as well as web information retrieval +L+ [13, 8]. For the table detection task, the decision tree classi- +L+ fier is particularly attractive as our features are highly non- +L+ homogeneous. We also experimented with Support Vector +L+ Machines (SVM), a relatively new learning approach which +L+ has achieved one of the best performances in text catego- +L+ rization [13].
3.1 Decision Tree
Decision tree learning is one of the most widely used and +L+ practical methods for inductive inference. It is a method +L+ for approximating discrete-valued functions that is robust +L+ to noisy data.
Decision trees classify an instance by sorting it down the +L+ tree from the root to some leaf node, which provides the clas- +L+ sification of the instance. Each node in a discrete-valued de- +L+ cision tree specifies a test of some attribute of the instance, +L+ and each branch descending from that node corresponds to +L+ one of the possible values for this attribute. Continuous- +L+ valued decision attributes can be incorporated by dynami- +L+ cally defining new discrete-valued attributes that partition +L+ the continuous attribute value into a discrete set of intervals +L+ [9].
An implementation of the continuous-valued decision tree +L+ described in [4] was used for our experiments. The decision +L+ tree is constructed using a training set of feature vectors with +L+ true class labels. At each node, a discriminant threshold
tuuv
IWeI
X
k=1
U=
pGmk X pGmk.
�,
pmt 
V
,
N
pmlWel
V
iNS= pNm� 
V
•	Weight vector representing the non-genuine table group:
i i
,when IT . NS�= 0
i i
	1,	when IT . GS= 0 and
i
	10,	when IT .
i i
IT . NS= 0
i +L+ NS= 0
� � +L+ IT� GS +L+ � � +L+ IT&apos; NS
i	i
GS�=0and IT .
�����
����
wg =
��
�
G
pi =
I
N
pi =
,
G
pmlWel
U
pmt 
U
�,
tuuv
V=
IWeI
X
k=1
245
is chosen such that it minimizes an impurity value. The +L+ learned discriminant function splits the training subset into +L+ two subsets and generates two child nodes. The process is +L+ repeated at each newly generated child node until a stopping +L+ condition is satisfied, and the node is declared as a terminal +L+ node based on a majority vote. The maximum impurity +L+ reduction, the maximum depth of the tree, and minimum +L+ number of samples are used as stopping conditions.
3.2 SVM
Support Vector Machines (SVM) are based on the Struc- +L+ tural Risk Management principle from computational learn- +L+ ing theory [12]. The idea of structural risk minimization +L+ is to find a hypothesis h for which the lowest true error is +L+ guaranteed. The true error of h is the probability that h +L+ will make an error on an unseen and randomly selected test +L+ example.
The SVM method is defined over a vector space where the +L+ goal is to find a decision surface that best separates the data +L+ points in two classes. More precisely, the decision surface by +L+ SVM for linearly separable space is a hyperplane which can +L+ be written as
w�•x�—b=0
where x� is an arbitrary data point and the vector w&quot; and +L+ the constant b are learned from training data. Let D = +L+ (yz, �xz) denote the training set, and yz E {+1, —1} be the +L+ classification for �xz, the SVM problem is to find w� and b +L+ that satisfies the following constraints:
w� •�xz—b&gt;+1 for yz=+1 +L+ w�•�xz—b&lt;—1 for yz=—1
while minimizing the vector 2-norm of �w.
The SVM problem in linearly separable cases can be effi- +L+ ciently solved using quadratic programming techniques, while +L+ the non-linearly separable cases can be solved by either in- +L+ troducing soft margin hyperplanes, or by mapping the orig- +L+ inal data vectors to a higher dimensional space where the +L+ data points become linearly separable [12, 3].
One reason why SVMs are very powerful is that they are +L+ very universal learners. In their basic form, SVMs learn lin- +L+ ear threshold functions. Nevertheless, by a simple &quot;plug-in&quot; +L+ of an appropriate kernel function, they can be used to learn +L+ polynomial classifiers, radial basis function (RBF) networks, +L+ three-layer sigmoid neural nets, etc. [3].
For our experiments, we used the SVMlzght system im- +L+ plemented by Thorsten Joachims.1
4. DATA COLLECTION AND TRUTHING
Since there are no publicly available web table ground +L+ truth database, researchers tested their algorithms in differ- +L+ ent data sets in the past [2, 10, 14]. However, their data +L+ sets either had limited manually annotated table data (e.g., +L+ 918 table tags in [2], 75 HTML pages in [10], 175 manually +L+ annotated table tags in [14]), or were collected from some +L+ specific domains (e.g., a set of tables selected from airline +L+ information pages were used in [2]). To develop our machine +L+ learning based table detection algorithm, we needed to build +L+ a general web table ground truth database of significant size.
1 http://svmlight.joachims.org
4.1 Data Collection
Instead of working within a specific domain, our goal of +L+ data collection was to get tables of as many different varieties +L+ as possible from the web. To accomplish this, we composed +L+ a set of key words likely to indicate documents containing +L+ tables and used those key words to retrieve and download +L+ web pages using the Google search engine. Three directo- +L+ ries on Google were searched: the business directory and +L+ news directory using key words: {table, stock, bonds, +L+ figure, schedule, weather, score, service, results, +L+ value}, and the science directory using key words {table, +L+ results, value}. A total of 2,851 web pages were down- +L+ loaded in this manner and we ground truthed 1,393 HTML +L+ pages out of these (chosen randomly among all the HTML +L+ pages). These 1,393 HTML pages from around 200 web sites +L+ comprise our database.
4.2 Ground Truthing
There has been no previous report on how to systemati- +L+ cally generate web table ground truth data. To build a large +L+ web table ground truth database, a simple, flexible and com- +L+ plete ground truth protocol is required. Figure 4.2(a) shows +L+ the diagram of our ground truthing procedure. We created +L+ a new Document Type Definition(DTD) which is a super- +L+ set of W3C HTML 3.2 DTD. We added three attributes for +L+ &lt;TABLE&gt; element, which are &quot;tabid&quot;, &quot;genuine table&quot; and +L+ &quot;table title&quot;. The possible value of the second attribute is +L+ yes or no and the value of the first and third attributes is a +L+ string. We used these three attributes to record the ground +L+ truth of each leaf &lt;TABLE&gt; node. The benefit of this design +L+ is that the ground truth data is inside HTML file format. +L+ We can use exactly the same parser to process the ground +L+ truth data.
We developed a graphical user interface for web table +L+ ground truthing using the Java [1] language. Figure 4.2(b) +L+ is a snapshot of the interface. There are two windows. Af- +L+ ter reading an HTML file, the hierarchy of the HTML file is +L+ shown in the left window. When an item is selected in the +L+ hierarchy, the HTML source for the selected item is shown +L+ in the right window. There is a panel below the menu bar. +L+ The user can use the radio button to select either genuine +L+ table or non-genuine table. The text window is used to input +L+ table title.
4.3 Database Description
Our final table ground truth database consists of 1,393 +L+ HTML pages collected from around 200 web sites. There +L+ are a total of 14,609 &lt;TABLE&gt; nodes, including 11,477 leaf +L+ &lt;TABLE&gt; nodes. Out of the 11,477 leaf &lt;TABLE&gt; nodes, +L+ 1,740 are genuine tables and 9,737 are non-genuine tables. +L+ Not every genuine table has its title and only 1,308 genuine +L+ tables have table titles. We also found at least 253 HTML +L+ files have unmatched &lt;TABLE&gt;, &lt;/TABLE&gt; pairs or wrong +L+ hierarchy, which demonstrates the noisy nature of web doc- +L+ uments.
5. EXPERIMENTS
A hold-out method is used to evaluate our table classi- +L+ fier. We randomly divided the data set into nine parts. +L+ Each classifier was trained on eight parts and then tested +L+ on the remaining one part. This procedure was repeated +L+ nine times, each time with a different choice for the test
246
HTML File
Hierarchy
Adding attributes
Parser
HTML with attributes and unique +L+ index to each table(ground truth)
Validation
(a)	(b)
Figure 2: (a) The diagram of ground truthing procedure; (b) A snapshot of the ground truthing software.
part. Then the combined nine part results are averaged to +L+ arrive at the overall performance measures [4].
For the layout and content type features, this procedure +L+ is straightforward. However it is more complicated for the +L+ word group feature training. To compute wg for training +L+ samples, we need to further divide the training set into two +L+ groups, a larger one (7 parts) for the computation of the +L+ weights pGi and pNi, i =1�...�jWj, and a smaller one (1
i i i
part) for the computation of the vectors GS, NS, and IT. +L+ This partition is again rotated to compute wg for each table +L+ in the training set.
Table 1: Possible true- and detected-state combina- +L+ tions for two classes.
True Class	Assigned Class	
	genuine table	non-genuine table
genuine table	Ngg	Ngn
non-genuine table	Nng	Nnn
lows:
Ngg	Ngg	R + P 
R	P 	 F Ngg + Ng&apos;	Ngg + Nng	= 2
For comparison among different features and learning al- +L+ gorithms we report the performance measures when the best +L+ F-measure is achieved. First, the performance of various fea- +L+ ture groups and their combinations were evaluated using the +L+ decision tree classifier. The results are given in Table 2.
Table 2: Experimental results using various feature +L+ groups and the decision tree classifier.
	L	T	LT	LTW
R (%)	87.24	90.80	94.20	94.25
P (%)	88.15	95.70	97.27	97.50
F (%)	87.70	93.25	95.73	95.88
L: Layout only.
T: Content type only.
LT: Layout and content type.
LTW: Layout, content type and word group.
The output of each classifier is compared with the ground +L+ truth and a contingency table is computed to indicate the +L+ number of a particular class label that are identified as mem- +L+ bers of one of two classes. The rows of the contingency table +L+ represent the true classes and the columns represent the as- +L+ signed classes. The cell at row r and column c is the number +L+ of tables whose true class is r while its assigned class is c. +L+ The possible true- and detected-state combination is shown +L+ in Table 1. Three performance measures Recall Rate(R), +L+ Precision Rate(P) and F-measure(F) are computed as fol-
As seen from the table, content type features performed +L+ better than layout features as a single group, achieving an +L+ F-measure of 93.25%. However, when the two groups were +L+ combined the F-measure was improved substantially to 95.73%, +L+ reconfirming the importance of combining layout and con- +L+ tent features in table detection. The addition of the word +L+ group feature improved the F-measure slightly more to 95.88%.
Table 3 compares the performances of different learning +L+ algorithms using the full feature set. The leaning algorithms +L+ tested include the decision tree classifier and the SVM al-
247
gorithm with two different kernels — linear and radial basis +L+ function (RBF).
Table 3: Experimental results using different learn- +L+ ing algorithms.
	Tree	SVM (linear)	SVM (RBF)
R (%)	94.25	93.91	95.98
P (%)	97.50	91.39	95.81
F (%)	95.88	92.65	95.89
As seen from the table, for this application the SVM with +L+ radial basis function kernel performed much better than the
one with linear kernel. It achieved an F measure of 95.89%, +L+ comparable to the 95.88% achieved by the decision tree clas-
sifier.
Figure 3 shows two examples of correctly classified tables, +L+ where Figure 3(a) is a genuine table and Figure 3(b) is a +L+ non-genuine table.
Figure 4 shows a few examples where our algorithm failed. +L+ Figure 4(a) was misclassified as a non-genuine table, likely +L+ because its cell lengths are highly inconsistent and it has +L+ many hyperlinks which is unusual for genuine tables. The +L+ reason why Figure 4(b) was misclassified as non-genuine is +L+ more interesting. When we looked at its HTML source code, +L+ we found it contains only two &lt;TR&gt; tags. All text strings +L+ in one rectangular box are within one &lt;TD&gt; tag. Its author +L+ used &lt;p&gt; tags to put them in different rows. This points +L+ to the need for a more carefully designed pseudo-rendering +L+ process. Figure 4(c) shows a non-genuine table misclassi- +L+ fied as genuine. A close examination reveals that it indeed +L+ has good consistency along the row direction. In fact, one +L+ could even argue that this is indeed a genuine table, with +L+ implicit row headers of Title, Name, Company Affiliation +L+ and Phone Number. This example demonstrates one of the +L+ most difficult challenges in table understanding, namely the +L+ ambiguous nature of many table instances (see [5] for a more +L+ detailed analysis on that). Figure 4(d) was also misclassi- +L+ fied as a genuine table. This is a case where layout features +L+ and the kind of shallow content features we used are not +L+ enough deeper semantic analysis would be needed in or- +L+ der to identify the lack of logical coherence which makes it +L+ a non-genuine table.
For comparison, we tested the previously developed rule- +L+ based system [10] on the same database. The initial re- +L+ sults (shown in Table 4 under &quot;Original Rule Based&quot;) were +L+ very poor. After carefully studying the results from the +L+ initial experiment we realized that most of the errors were +L+ caused by a rule imposing a hard limit on cell lengths in gen- +L+ uine tables. After deleting that rule the rule-based system +L+ achieved much improved results (shown in Table 4 under +L+ &quot;Modified Rule Based&quot;). However, the proposed machine +L+ learning based method still performs considerably better in +L+ comparison. This demonstrates that systems based on hand- +L+ crafted rules tend to be brittle and do not generalize well. +L+ In this case, even after careful manual adjustment in a new +L+ database, it still does not work as well as an automatically +L+ trained classifier.


Figure 3: Examples of correctly classified tables. +L+ (a): a genuine table; (b): a non-genuine table.
Table 4: Experimental results of a previously devel- +L+ oped rule based system.
	Original Rule Based	Modified Rule Based
R (%)	48.16	95.80
P (%)	75.70	79.46
F (%)	61.93	87.63
248
(a)	(b)
(c)	(d)
Figure 4: Examples of misclassified tables. (a) and (b): Genuine tables misclassified as non-genuine; (c) and +L+ (d): Non-genuine tables misclassified as genuine.
A direct comparison to other previous results [2, 14] is +L+ not possible currently because of the lack of access to their +L+ system. However, our test database is clearly more general +L+ and far larger than the ones used in [2] and [14], while our +L+ precision and recall rates are both higher.
6. CONCLUSION AND FUTURE WORK
Table detection in web documents is an interesting and +L+ challenging problem with many applications. We present a +L+ machine learning based table detection algorithm for HTML +L+ documents. Layout features, content type features and word +L+ group features were used to construct a novel feature set. +L+ Decision tree and SVM classifiers were then implemented +L+ and tested in this feature space. We also designed a novel ta- +L+ ble ground truthing protocol and used it to construct a large +L+ web table ground truth database for training and testing. +L+ Experiments on this large database yielded very promising +L+ results.
Our future work includes handling more different HTML +L+ styles in pseudo-rendering, detecting table titles of the rec- +L+ ognized genuine tables and developing a machine learning +L+ based table interpretation algorithm. We would also like to +L+ investigate ways to incorporate deeper language analysis for +L+ both table detection and interpretation.
7. ACKNOWLEDGMENT
We would like to thank Kathie Shipley for her help in +L+ collecting the web pages, and Amit Bagga for discussions on +L+ vector space models. +L+ 8. REFERENCES
[1] M. Campione, K. Walrath, and A. Huml. The +L+ java(tm) tutorial: A short course on the basics (the +L+ java(tm) series).
[2] H.-H. Chen, S.-C. Tsai, and J.-H. Tsai. Mining tables +L+ from large scale html texts. In Proc. 18th
International Conference on Computational
Linguistics, Saabrucken, Germany, July 2000.
[3] C. Cortes and V. Vapnik. Support-vector networks. +L+ Machine Learning, 20:273{296, August 1995.
[4] R. Haralick and L. Shapiro. Computer and Robot +L+ Vision, volume 1. Addison Wesley, 1992.
[5] J. Hu, R. Kashi, D. Lopresti, G. Nagy, and
G. Wilfong. Why table ground-truthing is hard. In +L+ Proc. 6th International Conference on Document +L+ Analysis and Recognition (ICDAR01), pages 129{133, +L+ Seattle, WA, USA, September 2001.
[6] M. Hurst. Layout and language: Challenges for table +L+ understanding on the web. In Proc. 1st International +L+ Workshop on Web Document Analysis, pages 27{30, +L+ Seattle, WA, USA, September 2001.
[7] T. Joachims. A probabilistic analysis of the rocchio +L+ algorithm with tfidf for text categorization. In Proc. +L+ 14th International Conference on Machine Learning, +L+ pages 143{151, Morgan Kaufmann, 1997.
[8] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. +L+ Automating the construction of internet portals with +L+ machine learning. In Information Retrieval Journal, +L+ volume 3, pages 127{163, Kluwer, 2000.
249
[9] T. M. Mitchell. Machine Learning. McGraw-Hill, 1997.
[10] G. Penn, J. Hu, H. Luo, and R. McDonald. Flexible +L+ web document analysis for delivery to narrow- +L+ bandwidth devices. In Proc. 6th International +L+ Conference on Document Analysis and Recognition +L+ (ICDAR01), pages 1074{1078, Seattle, WA, USA, +L+ September 2001.
[11] M. F. Porter. An algorithm for suffix stripping. +L+ Program, 14(3):130-137, 1980.
[12] V. N. Vapnik. The Nature of Statistical Learning +L+ Theory, volume 1. Springer, New York, 1995.
[13] Y. Yang and X. Liu. A re-examination of text +L+ categorization methods. In Proc. SIGIR&apos;99, pages +L+ 42{49, Berkeley, California, USA, August 1999.
[14] M. Yoshida, K. Torisawa, and J. Tsujii. A method to +L+ integrate tables of the world wide web. In Proc. 1st +L+ International Workshop on Web Document Analysis, +L+ pages 31{34, Seattle, WA, USA, September 2001.
250
