CHI 2008 Proceedings · Human-Robot Interaction	April 5-10, 2008 · Florence, Italy
Task 2 – Posture
Task Completion Time
A 2 x 12 (Technique X Posture) ANOVA on the task 
completion time for the posture task showed a significant 
Technique X Posture interaction effect (F11,209 = 8.43, p &lt; 
.001), which means that the Technique effect varies with 
Posture or vice versa.
On the average, there was a significant effect for Technique 
(F1,19 = 67.37, p &lt; .001), with mean times reducing from 
2.2s (SD = 0.4s) with keypad, to 1.5s (SD = 0.3s) with 
Wiimote/Nunchuk; On the average, a 32% reduction in task 
completion time between the two conditions. On the 
average, pairwise comparisons showed that there was a 
significant difference (p &lt; .05) between the techniques for 
posture 1, 2, 7, 8, 9, and 10. But, there was on significant 
difference for the other postures. (Figure 11) Also, on the 
average, the test showed a significant effect for Posture 
(F11,209 = 27.77, p &lt; .001).
Figure 11. Pairwise comparisons of the mean task completion 
time for each interaction technique according to posture.
Error
For the keypad interface, participants had made 1.5 (SD = 
1.2) errors on average for both difficulty levels. However, 
none of the participants had made any errors using the 
Wiimote/Nunchuk interface. As anticipated, a paired t-test 
showed a significant difference (t19 = 7.44, p &lt; .001) 
between the techniques.
DISCUSSION
The results presented in the previous section point to the 
Wiimote and the Wiimote/Nunchuk interfaces 
outperforming the keypad interface in terms of task 
completion time in both the robotic navigation and the 
robotic posture tasks. The differences between the 
interfaces, although statically significant, are a little 
underwhelming in their magnitude.
When attempting to explain this for the navigation task, we 
should consider that both interaction techniques use a set of 
abstract key and gesture combinations to represent specific 
robot movements. Since none of the participants have prior 
experience with these input methods, they have to learn and 
memorize the mappings of both techniques in order to 
navigate the AIBO. This abstract mapping between the user 
interface and the robot action added an extra layer of
cognitive load for the participants to process during the 
experiment. Although pressing buttons should not be slower 
than performing gestures, the study showed that the 
participants finished the obstacle course quicker with 
gesture input than with button input. We believe that 
although both interfaces require the participants to think 
about the abstract mapping before carrying out any actions, 
the Wiimote interface provides a slight advantage.
When using the Wiimote, participants do not need to focus 
on their hands while performing a posture. They are 
naturally aware of the spatial location of their hands. For 
the keypad interface, we observed that the participants have 
to constantly shift their attention back and forth between the 
keypad and the AIBO to look for the buttons they want to 
press and to confirm if they triggered the intended action. 
The consequences of shifting attention constantly between 
the interface and the AIBO may result in action overshoot 
(for example, overturning a corner) and can break the 
continuity of the task when participants have to stop the 
AIBO before they decide which action to take for the next 
step. This practical separation of action and perception 
spaces [23] is perhaps the reason for the slower task 
completion time when using the keypad.
Another possible reason for the faster task completion time 
when using the Wiimote/Nunchuk in the navigation task 
may be the zoomorphic rein-like mapping we used. While 
the mapping offered in this condition is not ideal (see our 
previous discussion of its degrees of integration and 
compatibility) the mapping does afford a simple, and arguably 
intuitive interaction metaphor.
Although the study results indicate that gesture input is 
faster for the navigation task, we are not suggesting it 
would always be a better solution than button input for this 
type of tasks. As we mentioned earlier in the pilot study 
section, the keypad mapping that we used was arguably not 
the most intuitive mapping we can come up with. A “W, A, 
S, D” key configuration would probably be more intuitive 
to use since it requires less key combinations and is a 
commonly used mapping in computer games for 
navigational tasks. However, we believe that our results 
demonstrate that when participants are limited to use 
asymmetric two-hand interaction techniques to control a 
robot, gesture input tends to be more intuitive to use than 
button input.
For the navigation tasks we did not expect that there would 
be a significant difference between the numbers of errors 
participants made using the different techniques. However, 
the data showed the opposite. Participants made 43% more 
errors with the keypad interface than with the Wiimote 
interface. Many participants felt that this was due to the 
small key size and the less intuitive mapping between 
buttons and robot actions.
For the posture tasks, we can see that on average there was 
a significant difference in task completion time between the 
postures that required two arms movement and the ones that 
only required one arm movement. By observation, we 
found that when the participants were using the 
Wiimote/Nunchuk interface, they were extremely engaged
128
