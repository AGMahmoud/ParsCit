CHI 2008 Proceedings · Usability Evaluation Considered Harmful?	April 5-10, 2008 · Florence, Italy
approaches, which in turn leads to a judicious weighing of 
the pros and cons of each design. They may test interfaces 
under iterative development – paper prototypes, running 
prototypes, implemented sub-systems – where they would 
produce a prioritized list of usability problems that could be 
rectified in the next design iteration. This emphasis on 
usability evaluation is most obvious when interface groups 
are composed mostly of human factors professionals trained 
in rigorous evaluation methodologies.
Why this is a problem
In education, academia and industry, usability evaluation 
has become a critical and necessary component in the 
design process. Usability evaluation is core because it is 
truly beneficial in many situations. The problem is that 
academics and practitioners often blindly apply usability 
evaluation to situations where – as we will argue in the 
following sections – it gives meaningless or trivial results, 
and can misdirect or even quash future design directions.
USABILITY EVALUATION AS WEAK SCIENCE
In this section, we emphasize concerns regarding how we as 
researchers do usability evaluations to contribute to our 
scientific knowledge. While we may use scientific methods 
to do our evaluation, this does not necessarily mean we are 
always doing effective science.
The Method Forms the Research Question
In the early days of CHI, a huge number of evaluation 
methods were developed for practitioners and academics to 
use. For example, John Gould’s classic article How to 
Design Usable Systems is choc-full of pragmatic discount 
evaluation methodologies [12]. The mid-‘80s to ‘90s also 
saw many good methods developed and formalized by CHI 
researchers: quantitative, qualitative, analytical, informal, 
contextual and so on (e.g., [22]). The general idea was to 
give practitioners a methodology toolbox, where they could 
choose a method that would help them best answer the 
problem they were investigating in a cost-effective manner. 
Yet Barkhuus and Rode note a disturbing trend in the recent 
ACM CHI publications [1]: evaluations are dominated by 
quantitative empirical usability evaluations (about 70%) 
followed by qualitative usability evaluations (about 25%). 
As well, they report that papers about the evaluation 
methods themselves have almost disappeared. The 
implication is that ACM CHI now has a methodology bias, 
where certain kinds of usability evaluation methods are 
considered more ‘correct’ and thus acceptable than others.
The consequence is that people now likely generate 
‘research questions’ that are amenable to a chosen method, 
rather than the other way around. That is, they choose a 
method perceived as ‘favored’ by review committees, and 
then find or fit a problem to match it. Our own anecdotal 
experiences confirm this: a common statement we hear is 
‘if we don’t do a quantitative study, the chances of a paper 
getting in are small’. That is, researchers first choose the 
method (e.g., controlled study) and then concoct a problem 
that fits that method. Alternately, they may emphasize
aspects of an existing problem that lends itself to that 
method, where that aspect may not be the most important 
one that should be considered. Similarly, we noticed 
methodological biases in reviews, where papers using non- 
empirical methodologies (e.g., case studies, field studies) 
are judged more stringently.
Existence Proofs Instead of Risky Hypothesis Testing 
Designs implemented in research laboratories are often 
conceptual ideas usually intended to show an alternate way 
that something can be done. In these cases, the role of 
usability evaluation ideally validates that this alternate 
interface technique is better – hopefully much better – than 
the existing ‘control’ technique. Putting this into terms of 
hypothesis testing, the alternative (desired) hypothesis is in 
very general terms: “When performing a series of tasks, the 
use of the new technique leads to increased human 
performance when compared to the old technique”.
What most researchers then try to do – often without being 
aware of it – is to create a situation favorable to the new 
technique. The implicit logic is that they should be able to 
demonstrate at least one case where the new technique 
performs better than the old technique; if they cannot, then 
this technique is likely not worth pursuing. In other words, 
the usability evaluation is an existence proof.
This seems like science, for hypothesis formation and 
testing are at the core of the scientific method. Yet it is, at 
best, weak science. The scientific method advocates risky 
hypothesis testing: the more the test tries to refute the 
hypothesis, the more powerful it is. If the hypothesis holds 
in spite of attempts to refute it, there is more validity in its 
claims [29, Ch. 9]. In contrast, the existence proof as used 
in HCI is confirmative hypothesis testing, for the evaluator 
is seeking confirmatory evidence. This safe test produces 
only weak validations of an interface technique. Indeed, it 
would be surprising if the researcher could not come up 
with a single scenario where the new technique would 
prove itself somehow ‘better’ than an existing technique.
The Lack of Replication
Rigorous science also demands replication, and the same 
should be true in CHI [14,29]. Replication serves several 
purposes. First, the HCI community should replicate 
usability evaluations to verify claimed results (in case of 
experimental flaws or fabrication). Second, the HCI 
community should replicate for more stringent and more 
risky hypothesis testing. While the original existence proof 
at least shows that an idea has some merit, follow-up tests 
are required to put bounds on it, i.e., to discover the 
limitations as well the strengths of the method [14,29].
The problem is that replications are not highly valued in 
CHI. They are difficult to publish (unless they are 
controversial), and are rarely considered a strong result. 
This is in spite of the fact that the original study may have 
offered only suggestive results. Again, dipping into 
experiences on program committees, the typical referee
113
