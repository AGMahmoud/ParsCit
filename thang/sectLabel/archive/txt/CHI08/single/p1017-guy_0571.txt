CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy
The average score of this list is 2.36 (on a scale of 1=good, 
4=bad). The list that got the best average score is the one 
based on aggregation of all four sources by weight 
combination 7 on Table 2. The average score of this list is 
2.34 and its number of “best” votes is 58. Over 40% of the 
users (48) got in this aggregated list an identical list to the 
one based on the organizational chart. As the score of the 
aggregated list is better, we may conclude that when the 
lists were not identical, the aggregated list received better 
scores. Finally, note that in Table 2, the total number of 
“best” votes given to aggregations (i.e., combinations 5 
through 8) was significantly higher than the combined 
number of “best” votes for all single sources (1 through 4). 
We consider the above findings as supporting hypothesis 
(4) – the value of aggregation.
The results of the second step are shown on the bottom 
right of Table 2. All lists of this step received scores that 
are higher than 2 but lower than 3, and all lists received 
quite a few “best” votes (over 40). It appears that no 
optimal weighting scheme exists that may serve a good 
default for most users. We were therefore unable to prove 
our hypothesis (5). While conducting the interviews during 
experiment 3, we had another chance to see how different 
people prefer different sources. For example, when 
examining the list from Fringe, one of the users said: 
“Completely off. Only 7 people, out of them only 3 are 
familiar” while another said: “Accurate, very accurate 
actually. [ ...] that would be my ideal buddylist”.
The results of the third step are shown on the bottom of 
Table 5. In this step we compared lists from private sources 
with lists from public sources and with lists based on a mix 
of private and public sources. For this step our plugin 
requested access to users’ private data. Only 55 users 
granted us access to their private data. We therefore based 
our analysis on the responses of these 55 users only.
As can be expected, the lists based on private sources 
received better average scores (1.62-1.76) than those based 
solely on public sources. The list with most “best” votes 
(17) on is the list based on the (private) chat system.
	17	18	19	20	21	22	23	24
Bookmarking	0.25		0.16	0.1	0.2			0.1
people tagging	0.25		0.16	0.1	0.2			0.1
blogs	0.25		0.16	0.1	0.2			0.1
org-chart	0.25		0.16	0.1	0.2			0.1
email		0.5	0.16	0.3	0.1	1.0		0.4
chat		0.5	0.16	0.3	0.1		1.0	0.2
average score								
# of “best” votes	0	12	2	9	0	10	17	9
# of score “1”	3	30	6	23	4	25	26	23
Table 5. Weight combinations and results of public and
private sources in the third set of buddylists
However, 17 votes are only 30.9%, indicating that no list 
significantly outvoted the others. The list with best average 
score (1.62) is the list based on combination 18 in Table 5 
(email and chat), supporting the value of aggregating 
information (private in this case) – hypothesis (4). Yet 
another supporting point for hypothesis (4) was received 
during the interviews, when no single user had chosen a list 
based solely on one source. This observation is less strong, 
since the experiment setting encouraged people to play with 
aggregations, yet they clearly had a choice to disable all 
other sources and stay with one, but did not.
Table 2 and Table 5 also show the number of times each of 
the buddylists in this experiment received score 1. List 18 
(email and chat) received score 1 for the largest number of 
times – over 54% of the people granted it a perfect score. 
Other lists which obtained many high scores are list 20 (mix 
of public and private), list 22 (email), list 23 (chat), and list 
24 (anther mix of public and private). It is obvious from the 
table that lists based on private sources receive score 1 
more often, as expected. The value of public sources is 
evident from the bottom line of Table 2: the lists based 
solely on public sources received a perfect score a 
considerable amount of times, supporting hypothesis (2). In 
experiment 3, seven of 11 used some combination of public 
and private data sources (the twelfth user had no access to 
private data) and two of them even preferred the public 
sources slightly over private ones (see Table 6).
Two of the combinations that mixed all six sources (number 
20 and number 24 in Table 5) received 9 “best” votes each. 
Examining our results reveals that each such vote was given 
by a different user, implying that for 18 people, they created 
a buddylist that is preferred over the buddylists based solely 
on private sources. Both these lists received an average 
score of 1.75, which is identical to the average score of the 
list based on chat, and they both received a perfect score 23 
times. This implies that for quite a few people the mix with 
public sources creates buddylists of high quality. Together 
with the fact that in experiment 3 most users chose to 
combine private with public data, we conclude that 
hypothesis (3) is true - information from public sources 
may provide a more complete picture of one’s SN.
Finally, Experiment 3 revealed what people think about 
sharing lists coming from their private data sources (see 
Table 6). Most of the people (10) said that they will be
	1	2	3	4	5	6	7	8	9	10	11	12
prv	85	00	100	48	63	100	100	100	69	48	54	54
pub	15	100	00	52	37	00	00	00	31	52	46	46
shr	2	3	2	2	2	3	2	2	3	3	1	0
Table 6. Experiment 3 results*
* First row shows percentage of private sources, second row percentage of
public sources, third row – sharing list preference (3 – automatically with
anyone, 2- after editing with anyone, 1 – after editing with friends, 0 – will
not share)
1024
