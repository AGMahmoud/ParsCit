//title
Protocols for Collecting Responses
in Multi-hop Radio Networks
//author
Chungki Lee James E. Burns
Mostafa H. Ammar
//pubnum
GIT-CC-92/28
//date
June 1992
//abstract
Abstract
The problem of collecting responses in multi-hop radio networks is considered. A given node, called the source, is to collect a specified number of
responses from nodes in a radio network. The problem arises in several
applications of distributed systems. A deterministic and a randomized protocol for the problem are presented. The two protocols are analyzed and
their performance is compared. Conclusions are drawn about the suitability
of our protocols in various network environments.
//affiliation
College of Computing
Georgia Institute of Technology
//address
Atlanta, Georgia 30332-0280
//page
+PAGE+ 

//title
Classes as Assertions
//author
Neelam Soundarajan
//affiliation
Computer and Information Science
The Ohio State University
//address
Columbus, OH 43210
//email
e-mail: neelam@cis.ohio-state.edu
//abstract
Abstract: How do we formally specify the relation between a base class and a
derived class? This question has two parts, a syntactic one, and a semantic one.
The syntactic part is of course the easier of the two and the answer to that part is
the standard contra/co- variance requirement on the arguments and result of any
base class method redefined in the derived class. Our concern in the current paper
is with the semantic part of the question, i.e., how do we specify the behavioral
relation between the base class and the derived class? We show that the standard
answer -which is the semantic counterpart of contra/co-variance- is too rigid, and
does not allow some natural and common forms of inheritance. We then propose a
more flexible way to specify the relation, and show how different types of behavioral relations between base classes and derived classes may be specified using our
notation.
//page
+PAGE+ 

//title
The SAMOS Active DBMS Prototype
//author
Stella Gatziu , Andreas Geppert , Klaus R. Dittrich
//affiliation
Institut fur Informatik, Universitat uZrich 1
//pubnum
Technical Report 94.16
//abstract
Abstract
We describe SAMOS, an active object-oriented database management system prototype. SAMOS offers a powerful rule definition language, including a small yet powerful set of event definition facilities. It is able to detect primitive and composite events
automatically and efficiently. Upon event detection, SAMOS executes rules attached
to the occurred events.
//intro
1 Introduction 

//title
Location Independent Names for Nomadic Computers
//author
David C. Steere , Mark Morrissey , Peter Geib , Calton Pu , and Jonathan Walpole
//affiliation
Department of Computer Science and Engineering
Oregon Graduate Institute
//abstract
Abstract
Recent advances in the Domain Name System (DNS) and the Dynamic Host Configuration
Protocol (DHCP) have enabled a new approach to supporting mobile users: location independent
naming. In this approach, machines use the same hostname from any internet location, but use an
IP address that corresponds to their current location. We describe a protocol that implements
location independent naming for nomadic computers, i.e., machines that do not need transparent
mobility. Our protocol allows hosts to move across security domains, uses existing protocols, and
preserves existing trust relationships. Therefore, it preserves the performance and security of
normal IP for nomadic computers at the expense of not providing the transparent mobility of
Mobile IP. We contend that this is a reasonable tradeoff for nomadic computing.
//intro
1 Introduction 

//title
Vertex heaviest paths and cycles in quasi-transitive
digraphs
//author
Jtrgen Bang-Jensen
Gregory Gutin
//affiliation
Department of Mathematics and Computer Science
Odense University, 
//address
Denmark
//abstract
Abstract
A digraph D is called a quasi-transitive digraph (QTD) if for any
triple x; ; of distinct vertices of D such that (x; ) and (; ) are
arcs of D there is at least one arc from x to or from to x. Solving
a conjecture by J. Bang-Jensen and J. Huang (J. Graph Theory, to
appear), G. Gutin (Australas. J. Combin., to appear) described polynomial algorithms for finding a Hamiltonian cycle and a Hamiltonian
path (if it exists) in a QTD. The approach taken in that paper cannot
be used to find a longest path or cycle in polynomial time. We present
a principally new approach that leads to polynomial algorithms for
finding vertex heaviest paths and cycles in QTD's with non-negative
weights on the vertices. This, in particular, provides an answer to a
question by N. Alon on longest paths and cycles in QTD's.
//intro
1 Introduction 

//title
Shape Modeling with Front Propagation: A Level Set Approach
//author
Ravikanth Malladi , 1 James A. Sethian , 1 and Baba C. Vemuri 2
//affiliation
1 Lawrence Berkeley Laboratory
and
Department of Mathematics
University of California, 
//address
Berkeley, CA 94720.
//affiliation
2 Department of Computer & Information Sciences
University of Florida, 
//address
Gainesville, FL 32611.
//abstract
Abstract
Shape modeling is an important constituent of computer vision as well as computer graphics
research. Shape models aid the tasks of object representation and recognition. This paper
presents a new approach to shape modeling which retains some of the attractive features of
existing methods, and overcomes some of their limitations. Our techniques can be applied to
model arbitrarily complex shapes, which include shapes with significant protrusions, and to
situations where no a priori assumption about the object's topology is made. A single instance
of our model, when presented with an image having more than one object of interest, has the
ability to split freely to represent each object. This method is based on the ideas developed
by Osher and Sethian to model propagating solid/liquid interfaces with curvature-dependent
speeds. The interface (front) is a closed, nonintersecting, hypersurface flowing along its gradient
field with constant speed or a speed that depends on the curvature. It is moved by solving a
"Hamilton-Jacobi" type equation written for a function in which the interface is a particular
level set. A speed term synthesized from the image is used to stop the interface in the vicinity of
object boundaries. The resulting equation of motion is solved by employing entropy-satisfying
upwind finite difference schemes. We present a variety of ways of computing evolving front,
including narrow bands, reinitializations, and different stopping criteria. The efficacy of the
scheme is demonstrated with numerical experiments on some synthesized images and some low
contrast medical images.
//note
fl1 Supported in part by the Applied Mathematical Sciences Subprogram of the Office of Energy Research, U.S.
Dept. of Energy under Contract DE-AC03-76SD00098 and by the NSF ARPA under grant DMS-8919074.
2 Supported in part by NSF grant ECS-9210648.
//page
+PAGE+ 

//title
Evolution of Recursive Transition Networks for
Natural Language Recognition with Parallel
Distributed Genetic Programming
//author
Riccardo Poli
//affiliation
School of Computer Science
The University of Birmingham
//email
E-mail: R.Poli@cs.bham.ac.uk
//pubnum
Technical Report: CSRP-96-19
//date
December 1996
//abstract
Abstract
This paper describes the application of Parallel Distributed Genetic Programming (PDGP) to the problem of inducing programs for natural language processing.
PDGP is a new form of Genetic Programming (GP) which is suitable for the development of programs with a high degree of parallelism and an efficient and effective
reuse of partial results. Programs are represented in PDGP as graphs with nodes
representing functions and terminals, and links representing the flow of control and
results. PDGP allows the exploration of a large space of possible programs including standard tree-like programs, logic networks, neural networks, finite state
automata, Recursive Transition Networks (RTNs), etc. The paper describes the
representations, the operators and the interpreters used in PDGP, and illustrates
its behaviour on the problem of inducing RTN-based recognisers for natural lan
guage from positive and negative examples.
//intro
1 Introduction 

//title
Does Configuration Management Research Have a Future?
//author
Andre van der Hoek , Dennis Heimbigner , and Alexander L. Wolf
//affiliation
Department of Computer Science, CB 430
University of Colorado
//address
Boulder, Colorado 80309 USA
//email
fandre,dennis,alwg@cs.colorado.edu
//abstract
Abstract
In this position paper we raise the question of whether Configuration Management (CM)
research has a future. The new standard in CM systems|typified by commercial products
such as Adele, ADC, ClearCase, Continuus/CM, and CCC/Harvest|largely satisfies the CM
functionality requirements posed by Dart. This implies that research in the area of CM is either
unnecessary or that we must find new challenges in CM on which to focus. We believe that
these challenges indeed exist. Here we present some areas that we feel are good opportunities
for new or continued CM research, and therefore conclude that CM research does have a future.
//intro
Introduction 

//title
Numerical conformal mapping using cross-ratios
and Delaunay triangulation
//author
Tobin A. Driscoll Stephen A. Vavasis
//date
January 23, 1996
//abstract
Abstract
We propose a new algorithm for computing the Riemann mapping of the
unit disk to a polygon, also known as the Schwarz-Christoffel transformation.
The new algorithm, CRDT, is based on cross-ratios of the prevertices, and also
on cross-ratios of quadrilaterals in a Delaunay triangulation of the polygon.
The CRDT algorithm produces an accurate representation of the Riemann
mapping even in the presence of arbitrary long, thin regions in the polygon,
unlike any previous conformal mapping algorithm. We believe that CRDT can
never fail to converge to the correct Riemann mapping, but the correctness and
convergence proof depend on conjectures that we have so far not been able to
prove. We demonstrate convergence with computational experiments.
The Riemann mapping has applications to problems in two-dimensional
potential theory and to finite-difference mesh generation. We use CRDT to
produce a mapping and solve a boundary value problem on long, thin regions
for which no other algorithm can solve these problems.
//intro
1 Conformal mapping 

//title
A Feedback Mechanism for Query by Navigation
//author
F.C. Berger
Th.P. van der Weide n
//note
Published as: 
//author
F.C. Berger and Th.P. van der
Weide. 
//title
A Feedback Mechanism for Query by
Navigation. 
//pubnum
Technical Report CSI-R9413, 
//affiliation
Computing Science Institute, University of Nijmegen,
//address
Nijmegen, The Netherlands, 
//date
October 1994. 
//abstract
Abstract
The Two-Level Hypermedia Paradigm sees an
Information Retrieval System as consisting of
a document network (the Hyperbase) and a
descriptor (term) network (the Hyperindex).
Query by Navigation is a process whereby the
searcher gives a description of the Information Need by travelling through the descriptor
network. This paper presents a formalism for
expressing the effects of traversing the Hyper-index on the elements of the Hyperindex. This
formalism makes use of probabilities for mod-elling the searcher's behavious. The events
which can occur during the search process
are discussed and modelled. Some important
properties, which are reasonable to demand of
a retrieval system, can be proven to be valid
if this formalism is adopted. A mechanism
for assigning a measure of relevance to documents is presented. This uses the formalism
mentioned above. An example will show the
effectiveness of The aspect of relevance feedback and its role in Query by Navigation is
introduced by examining the different level on
which the searcher can offer information for
weeding out unwanted sections of the search
space. In order to illustrate the workings of
Query by Navigation a small example is included.
//affiliation
Dept. of Information Systems, Faculty of Mathematics and
Informatics, University of Nijmegen, 
//address
Toernooiveld 1, 6525 ED
Nijmegen, The Netherlands
//keyword
Keywords: information retrieval, relevance feedback, user modelling, query formulation
//note
Classification: AMS 68P20; CR H.3.3, H.5.1
//intro
1 Introduction 

//title
Tally NP Sets and Easy Census Functions
//author
Judy Goldsmith 1
//affiliation
Department of Computer Science
University of Kentucky
//address
Lexington, KY 40506, USA
//email
goldsmit@cs.engr.uky.edu
//author
Mitsunori Ogihara 2
//affiliation
Department of Computer Science
University of Rochester
//address
Rochester, NY 14627, USA
//email
ogihara@cs.rochester.edu
//author
Jorg Rothe 3
//affiliation
Institut fur Informatik
Friedrich-Schiller-Universitat Jena
//address
07740 Jena, Germany
//email
rothe@informatik.uni-jena.de
//date
March 19, 1998
//note
1 Supported in part by NSF grant CCR-9315354.
2 Supported in part by NSF CAREER Award CCR-9701911.
3 Supported in part by grants NSF-INT-9513368/DAAD-315-PRO-fo-ab and NSF-CCR-9322513
and by a NATO Postdoctoral Science Fellowship from the Deutscher Akademischer Austausch-dienst ("Gemeinsames Hochschulsonderprogramm III von Bund und Landern"). Current address:
Department of Computer Science, University of Rochester, Rochester, NY 14627, USA. Work done
in part while visiting the University of Kentucky and the University of Rochester.
//page
+PAGE+ 

//title
Intra-Option Learning about Temporally Abstract Actions
//author
Richard S. Sutton
//affiliation
Department of Computer Science
University of Massachusetts
//address
Amherst, MA 01003-4610
//email
rich@cs.umass.edu
//author
Doina Precup
//affiliation
Department of Computer Science
University of Massachusetts
//address
Amherst, MA 01003-4610
//email
dprecup@cs.umass.edu
//author
Satinder Singh
//affiliation
Department of Computer Science
University of Colorado
//address
Boulder, CO 80309-0430
//email
baveja@cs.colorado.edu
//abstract
Abstract
Several researchers have proposed modeling
temporally abstract actions in reinforcement
learning by the combination of a policy and a termination condition, which we refer to as an option. Value functions over options and models of
options can be learned using methods designed
for semi-Markov decision processes (SMDPs).
However, all these methods require an option to
be executed to termination. In this paper we explore methods that learn about an option from
small fragments of experience consistent with
that option, even if the option itself is not executed. We call these methods intra-option learning methods because they learn from experience
within an option. Intra-option methods are sometimes much more efficient than SMDP methods because they can use off-policy temporal-difference mechanisms to learn simultaneously
about all the options consistent with an experience, not just the few that were actually executed. In this paper we present intra-option learning methods for learning value functions over options and for learning multi-time models of the
consequences of options. We present computational examples in which these new methods
learn much faster than SMDP methods and learn
effectively when SMDP methods cannot learn at
all. We also sketch a convergence proof for intra
option value learning.
//intro
1 Introduction 

//title
Hierarchical Inter-Domain Routing Protocol
with On-Demand ToS and Policy Resolution
//author
Cengiz Alaettinoglu , A. Udaya Shankar
//affiliation
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
//address
College Park, Maryland 20742
//pubnum
CS-TR-3299
//date
June 20, 1994
//abstract
Abstract
Traditional inter-domain routing protocols based on superdomains maintain either "strong"
or "weak" ToS and policy constraints for each visible superdomain. With strong constraints,
a valid path may not be found even though one exists. With weak constraints, an invalid
domain-level path may be treated as a valid path.
We present an inter-domain routing protocol based on superdomains, which always finds
a valid path if one exists. Both strong and weak constraints are maintained for each visible
superdomain. If the strong constraints of the superdomains on a path are satisfied, then the
path is valid. If only the weak constraints are satisfied for some superdomains on the path, the
source uses a query protocol to obtain a more detailed "internal" view of these superdomains,
and searches again for a valid path. Our protocol handles topology changes, including node/link
failures that partition superdomains. Evaluation results indicate our protocol scales well to large
internetworks.
//keyword
Categories and Subject Descriptors: C.2.1 [Computer-Communication Networks]: Network Archi
tecture and Design|packet networks; store and forward networks; C.2.2 [Computer-Communication Net
works]: Network Protocols|protocol architecture; C.2.m [Routing Protocols]; F.2.m [Computer Network
Routing Protocols].
//note
This work is supported in part by ARPA and Philips Labs under contract DASG60-92-0055 to Department
of Computer Science, University of Maryland, and by National Science Foundation Grant No. NCR 89-04590. The
views, opinions, and/or findings contained in this report are those of the author(s) and should not be interpreted as
representing the official policies, either expressed or implied, of the Advanced Research Projects Agency, PL, NSF,
or the U.S. Government. Computer facilities were provided in part by NSF grant CCR-8811954.
//page
+PAGE+ 

//note
Pages 61 to 70 of W. Daelemans, A. van den Bosch, and A. Weijters (Editors),
Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural
Language Processing Tasks, April 26, 1997, Prague, Czech Republic
//title
Automatic Phonetic Transcription of Words
Based On Sparse Data
//author
Maria Wolters (i) and Antal van den Bosch (ii)
//affiliation
(i) Institut fur Kommunikationsforschung und Phonetik, Universitat Bonn 
//author
Poppelsdorfer Allee 47, 53113 Bonn, Germany 
//email
mwo@asl1.ikp.uni-bonn.de
//affiliation
(ii) Department of Computer Science, Universiteit Maastricht 
//author
PO Box 616, 6200 MD Maastricht, The Netherlands 
//email
antal@cs.unimaas.nl
//abstract
Abstract
The relation between the orthography and the phonology of a language has
traditionally been modelled by hand-crafted rule sets. Machine-learning (ML)
approaches offer a means to gather this knowledge automatically. Problems
arise when the training material is sparse. Generalising from sparse data
is a well-known problem for many ML algorithms. We present experiments
in which connectionist, instance-based, and decision-tree learning algorithms
are applied to a small corpus of Scottish Gaelic. instance-based learning in the
ib1-ig algorithm yields the best generalisation performance, and that most
algorithms tested perform tolerably well. Given the availability of a lexicon,
even if it is sparse, ML is a valuable and efficient tool for automatic phonetic
transcription of written text.
//intro
1 The Problem 

//title
Loop Optimizations for Acyclic Object-Oriented Queries
//author
Vasilis Samoladas Daniel P. Miranker
//affiliation
The University of Texas at Austin
Department of Computer Sciences
//address
Taylor Hall 2.124
Austin, TX 78712-1188
//email
fvsam,mirankerg@cs.utexas.edu
//phone
Tel: (512)-471-9541
//abstract
Abstract
Nested loop execution of object-oriented queries retains
the promise of maintaining the full generality of the object paradigm, independent of the specifics of any single
object model. Thus, from this starting point we have
developed an object-oriented query optimizer and execution engine. The methods, developed to date for only
acyclic queries, augment nested loops structures with a
simple marking mechanism such that unnecessary loop
iterations are not repeated. In the case of acyclic queries,
the executions are asymptotically optimal. In contrast to
optimal query methods based on semijoin reductions our
method involves no preprocessing step and thus avoids
the extra I/O associated with semijoins and prevents the
formal benefits of semijoin reduction from appearing as a
practical improvement. Empirical results comparing our
query environment with a commercially available product
demonstrate significant performance improvement.
//intro
1 Introduction 

//title
Utility Models for Goal-Directed
Decision-Theoretic Planners
//author
Peter Haddawy 1 , Steve Hanks
//affiliation
Department of Computer Science and Engineering
University of Washington
//address
Seattle, WA 98195
//pubnum
Technical Report 93-06-04
//date
June 15, 1993
//affiliation
1 Department of EE & CS, University of Wisconsin-Milwaukee, 
//address
Milwaukee WI 53201
//page
+PAGE+ 

//note
Appears in Working Notes, Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms
Workshop, Thirteenth National Conference on Artificial Intelligence, Portland, OR: AAAI Press (1996).
//title
Human Expert-Level Performance on a Scientific Image Analysis Task
by a System Using Combined Artificial Neural Networks
//author
Kevin J. Cherkauer
//affiliation
Department of Computer Sciences
University of Wisconsin-Madison
//address
1210 West Dayton Street
Madison, WI 53706, USA
//email
cherkauer@cs.wisc.edu
//abstract
Abstract
This paper presents the Plannett system, which
combines artificial neural networks to achieve expert-
level accuracy on the difficult scientific task of recognizing volcanos in radar images of the surface of the
planet Venus. Plannett uses ANNs that vary along
two dimensions: the set of input features used to train
and the number of hidden units. The ANNs are combined simply by averaging their output activations.
When Plannett is used as the classification module
of a three-stage image analysis system called JAR-
tool, the end-to-end accuracy (sensitivity and specificity) is as good as that of a human planetary geologist on a four-image test suite. JARtool-Plannett
also achieves the best algorithmic accuracy on these
images to date.
//intro
Introduction 

//title
What Tasks Can Be Performed with an Uncalibrated
Stereo Vision System?
//author
J. P. Hespanha , Z. Dodds , G. D. Hager , and A. S. Morse
//affiliation
Center for Computational Vision and Control
c/o Computer Science Department
//address
P.O. Box 208285
Yale University
New Haven, CT, 06520
//phone
Phone: (203) 432-6432
//email
E-mail: (gregory.hager, joao.hespanha, zachary.dodds, as.morse)@yale.edu
//abstract
Abstract
This article studies the following question: "When is it possible to decide, on the basis of images of point features observed by an imprecisely modeled two-camera stereo
vision system, whether or not a prescribed robot positioning task has been accomplished with precision?" It is shown that for a stereo vision system with known epipo-lar geometry, whether or not such a positioning task has been accomplished can be
decided with available data, just in case the task function which specifies the task is a
projective invariant.
//note
Submitted to IJCV special issue on vision research at Yale.
This research was supported by the National Science Foundation, the Army Research Office, and the
Air Force Office of Scientific Research
//page
+PAGE+ 

//title
An Evolving Algebra Abstract Machine
//author
Giuseppe Del Castillo 1 , Igor D - urd -anovic 2 , Uwe Glasser 1
//affiliation
1 Heinz Nixdorf Institut, Universitat-GH Paderborn, 
//address
Furstenallee 11,
33102 Paderborn, Germany, 
//email
fgiusp,glaesserg@uni-paderborn.de 
//affiliation
2 FB Mathematik-Informatik, Universitat-GH Paderborn, 
//address
Warburger Str. 100,
33098 Paderborn, Germany, 
//email
igor@uni-paderborn.de 
//abstract
Abstract. Evolving algebras (EAs) as defined by Yuri Gurevich constitute the basis of a powerful and elegant specification and verification
method which has successfully been applied to the design and analysis of
various kinds of discrete dynamic systems. Aiming at the development
of a comprehensive EA-based specification and design environment, we
introduce the concept of an evolving algebra abstract machine (EAM ) as
a platform for the systematic development of EA tools; for instance, as
required for machine based analysis and execution of EA specifications.
We give a formal definition of the EAM ground model in terms of a
universal evolving algebra, where we validate the correctness of the relation between evolving algebras (their theoretical foundations) and their
EAM representation and interpretation. Our approach covers sequential
as well as distributed evolving algebras.
//intro
Introduction 

//title
Hierarchical Optimization of Optimal Path Finding for
Transportation Applications
//author
Ning Jing
//affiliation
Changsha Institute of Technology
//email
jning@eecs.umich.edu
//author
Yun-Wu Huang
//affiliation
University of Michigan
//email
ywh@eecs.umich.edu
//author
Elke A. Rundensteiner
//affiliation
University of Michigan
//email
rundenst@eecs.umich.edu
//abstract
Abstract
Efficient path query processing is a key requirement for advanced
database applications including GIS (Geographic Information Systems) and ITS (Intelligent Transportation Systems). We study the
problem in the context of automobile navigation systems where a
large number of path requests can be submitted over the transportation network within a short period of time. To guarantee efficient re-sponsefor path queries, we employ a path view materialization strategy for precomputing the best paths. We tackle the following three
issues: (1) memory-resident solutions quickly exceed current computer storage capacity for networks of thousands of nodes, (2) disk-based solutions have been found inefficient to meet the stringent
performance requirements, and (3) path views become too costly
to update for large graphs. We propose the HEP V (Hierarchical
Encoded Path View) approach that addresses these problems while
guaranteeing the optimality of path retrieval. Our experimental results reveal that HEP V is more efficient than previously known
path finding approaches.
//intro
1 Introduction 

//title
Efficient PRAM Simulation on a
Distributed Memory Machine
//author
Richard M. Karp
//affiliation
University of California at Berkeley and
International Computer Science Institute, 
//address
Berkeley, CA 
//author
Michael Luby
//affiliation
International Computer Science Institute, 
//address
Berkeley, CA 
//affiliation
and UC Berkeley
//author
Friedhelm Meyer auf der Heide
//affiliation
Heinz Nixdorf Institute and Computer Science Department, 
//affiliation
University of Paderborn, 
//address
Germany 
//pubnum
TR-93-040 
//date
August 1993 
//abstract
Abstract
We present algorithms for the randomized simulation of a shared memory machine
(PRAM) on a Distributed Memory Machine (DMM). In a PRAM, memory conflicts
occur only through concurrent access to the same cell, whereas the memory of a
DMM is divided into modules, one for each processor, and concurrent accesses to
the same module create a conflict. The delay of a simulation is the time needed to
simulate a parallel memory access of the PRAM. Any general simulation of an m
processor PRAM on a n processor DMM will necessarily have delay at least m=n. A
randomized simulation is called time-processor optimal if the delay is O(m=n) with
high probability. Using a novel simulation scheme based on hashing we obtain a
time-processor optimal simulation with delay O(loglog(n)log (n)). The best previous
simulations use a simpler scheme based on hashing and have much larger delay:
fi(log(n)= loglog(n)) for the simulation of an n processor PRAM on an n processor
DMM, and fi(log(n)) in the case where the simulation is time-processor optimal.
//note
Research partially supported by NSF/DARPA Grant CCR-9005448
Research partially supported by NSF operating grant CCR-9016468 and by grant No. 89-00312 from
the United States-Israel Binational Science Foundation (BSF), Jerusalem, Israel.
Part of work was done during a visit at the International Computer Science Institute at Berkeley;
supported in part by DFG-Forschergruppe "Effiziente Nutzung massiv paralleler Systeme, Teilprojekt 4",
and by the Esprit Basic Research Action Nr. 7141 (ALCOM II).
//page
+PAGE+ 

//title
Towards the Assessment of Logics for
Concurrent Actions
//author
Choong-Ho Yi
//affiliation
Department of Computer and Information Science
Linkoping University
//address
581 83 Linkoping, Sweden
//email
E-mail: choyi@ida.liu.se
//abstract
Abstract
We have introduced concurrency into the framework of Sandewall. The resulting formalism is capable of reasoning about interdependent as well
as independent concurrent actions. Following
Sandewall's systematical method, we have then
applied the entailment criterion PCM to selecting
intended models of common sense theories where
concurrent actions are allowed, and proved that
the criterion leads to only intended models for a
subset of such theories.
//intro
Introduction 

//title
Decision-Theoretic Troubleshooting: A Framework for
Repair and Experiment
//author
John S. Breese
David Heckerman
//email
&lt;breese|heckerma@microsoft.com&gt;
//date
March, 1996
(revised May 1996)
//pubnum
Technical Report
MSR-TR-96-06
//affiliation
Microsoft Research
Advanced Technology Division
Microsoft Corporation
//address
One Microsoft Way
Redmond, WA 98052
//note
Also appears in the Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence,
August, 1996
//page
+PAGE+ 

//title
Real-Time Reliable Multicast Using Proactive Forward Error Correction
//author
Dan Rubenstein , Jim Kurose , and Don Towsley
//affiliation
Computer Science Department
University of Massachusetts
//address
Amherst, MA 01003
//email
fdrubenst, kurose, towsleyg@cs.umass.edu
//pubnum
Technical Report 98-19
//affiliation
Department of Computer Science
//date
March 1998
//abstract
Abstract
Real-Time reliable multicast over a best-effort service network remains a challenging research problem. Most
protocols for reliable multicast use repair techniques that result in significant and variable delay, which can lead to
missed deadlines in real-time scenarios. This paper presents a repair technique that combines forward error correction
(FEC) with automatic repeat request (ARQ). The novel aspect of the technique is its ability to reduce delay in reliable
multicast delivery by sending repairs proactively (i.e., before they are required). The technique requires minimal
state at senders and receivers, and no additional active router functionality beyond what is required by the current
multicast service model. Furthermore, the technique uses only end-to-end mechanisms, where all data and repairs are
transmitted by the data-originating source, leaving receivers free from any burden of sending repairs. We simulate
a simple round-based version of a protocol embodying this technique to show its effectiveness in preventing repair
request implosion, reducing the expected time of reliable delivery of data, and keeping bandwidth usage for repairs
low. We show how a protocol using the technique can be adapted to provide delivery that is reliable before a real-time
deadline with probabilities extremely close to one. Finally, we develop several variations of the protocol that use the
technique in various fashions for high rate data streaming applications, and present results from additional simulations
that examine performance in a variety of Internet-like heterogeneous networks.
//intro
1 Introduction 

//title
Interpretable Neural Networks with BP-SOM
//author
Ton Weijters 1 , Antal van den Bosch 2 , and Jaap van den Herik 3
//affiliation
1 Information Technology, Eindhoven University of Technology, 
//address
The Netherlands
//affiliation
2 ILK / Computational Linguistics, Tilburg University, 
//address
The Netherlands
//affiliation
3 Department of Computer Science, Universiteit Maastricht, 
//address
The Netherlands
//abstract
Abstract. Interpretation of models induced by artificial neural networks is often a difficult task. In this paper we focus on a relatively
novel neural network architecture and learning algorithm, bp-som, that
offers possibilities to overcome this difficulty. It is shown that networks
trained with bp-som show interesting regularities, in that hidden-unit
activations become restricted to discrete values, and that the som part
can be exploited for automatic rule extraction.
//intro
1 Introduction 

//title
Visualisation of Large Networks in 3-D Space:
Issues in
Implementation and Experimental Evaluation
//author
Yan Xiao Paul Milgram
//abstract
Abstract
Three dimensional visualisation has become a
widespread scheme for helping users to access
and manage large information network. In this
report, various techniques for displaying depth
information are reviewed, with an emphasis on
stereoscopic displays. Input devices used to interact with a 3-D space are also examined. Issues in 3-D network visualisation are elicited from
three viewpoints: psychological, task-related and
implementational. Consideration of these issues
leads to the design of a preliminary experimental
programme for evaluating various network visu-alisation techniques.
//intro
1 Introduction 

//title
Trust-region interior-point algorithms for minimization problems
with simple bounds
//author
J. E. Dennis Lus N. Vicente
//abstract
Abstract
Two trust-region interior-point algorithms for the solution of minimization problems
with simple bounds are presented. The algorithms scale the local model in a way proposed
by Coleman and Li [1], but they are new otherwise. The first algorithm is more usual in
that the trust region and the local quadratic model are consistently scaled. The second
algorithm proposed here uses an unscaled trust region. A first-order convergence result for
these algorithms is given and dogleg and conjugate-gradient algorithms to compute trial
steps are introduced. Some numerical examples that show the advantages of the the second
algorithm are presented.
//keyword
Keywords. trust-region methods, interior-point algorithms, Dikin-Karmarkar ellipsoid,
Coleman and Li scaling, simple bounds.
//note
AMS subject classification. 49M37, 90C20, 90C30
//intro
1 Introduction 

//title
Incoercible Multiparty Computation
//note
(Extended Abstract)
//author
Ran Canetti Rosario Gennaro
//date
May 17, 1996
//abstract
Abstract
Current secure multiparty protocols have the following deficiency. The public transcript of the communication can be used as an involuntary commitment of the parties to their inputs and outputs. Thus
parties can be later coerced by some authority to reveal their private data. Previous work that has
pointed this interesting problem out contained only partial treatment.
In this work we present the first general and rigorous treatment of the coercion problem in secure computation. First we present a general definition of protocols that provide resilience to coercion. Our
definition constitutes a natural extension of the general paradigm used for defining secure multiparty
protocols. Next we show that if trapdoor permutations exist then any function can be incoercibly
computed (i.e., computed by a protocol that provides resilience to coercion) in the presence of com-putationally bounded adversaries and only public communication channels. This holds as long as less
than half the parties are coerced (or corrupted). In particular, ours are the first incoercible protocols
without physical assumptions. Also, our protocols constitute an alternative solution to the recently
solved adaptive security problem.
Our techniques are quite surprising and include non-standard use of deniable encryptions.
//affiliation
Laboratory for Computer Science, Massachusetts Institute of Technology, 
//address
545 Technology Square, Cambridge MA 02139,
U.S.A. 
//email
canetti,rosario@theory.lcs.mit.edu
//page
+PAGE+ 

//title
On Algorithms for Simplicial Depth
//author
Andrew Y. Cheng
//affiliation
Department of Industrial Engineering
//author
Ming Ouyang
//affiliation
Department of Computer Science
Rutgers University
//address
New Brunswick, New Jersey 08903
//abstract
ABSTRACT
Simplicial depth is a way to measure how deep a point is among a set of points. Efficient
algorithms to compute it are important to the usefulness of its applications, such as in
multivariate analysis in statistics. A straightforward method takes O(n d+1 ) time when the
points are in d-dimensional space. We discuss an algorithm that takes O(n 2 ) time when the
points are in three-dimensional space, and we generalize it to four-dimensional space with
a time complexity of O(n 4 ). For spaces higher than four-dimensional, there are no known
algorithms faster than the straightforward method.
//intro
1 Simplicial depth 

//title
Unification and Polymorphism in Region Inference
//author
Mads Tofte, 
//affiliation
Department of Computer Science, University of Copenhagen
//author
Lars Birkedal, 
//affiliation
School of Computer Science, Carnegie Mellon University
//note
Dedicated to Robin Milner on the occasion of his 60th birthday.
//abstract
Abstract
Region Inference is a technique for inferring lifetimes of values in strict, higher-order programming languages such as Standard ML. The purpose of this paper is to show how ideas
from Milner's polymorphic type discipline can serve as a basis for region inference, even in the
presence of a limited form of polymorphic recursion.
//intro
1 Introduction 

//title
Distributed Simulation of DEVS-Based Multiformalism Models
//author
Herbert Praehofer and Gernot Reisinger
//affiliation
Institute of Systems Science
Systems Theory and Information Engineering
Johannes Kepler University Linz
//address
A-4040 Linz, Austria
//abstract
Abstract
In this paper we introduce a new approach for parallel, distributed simulation of modular, hierarchical
DEVS and DEVS-based combined discrete/continuous
multiformalism models. The algorithm combines
conservative and optimistic distributed simulation
strategies and is able to optimally exploit lookahead
capabilities of the model. The object oriented implementation in C++ is intended to serve as a powerful
simulator in the STIMS modeling and simulation environment.
//intro
1 Introduction and Motivation 

//title
Transis: A Communication Sub-System
for
High Availability
//author
Yair Amir , Danny Dolev , Shlomo Kramer , Dalia Malki
//affiliation
Computer Science department
The Hebrew University of Jerusalem
//address
Jerusalem, Israel
//pubnum
Technical Report CS91-13
//date
April 30, 1992
//page
+PAGE+ 

//title
Constraint Based Design of ATM
Networks, an Experimental Study
//author
Hongzhou Ma , Inderjeet Singh , Jonathan Turner
//pubnum
wucs-97-15
//date
April 97
//affiliation
Department of Computer Science
//address
Campus Box 1045
//affiliation
Washington University
//address
One Brookings Drive
St. Louis, MO 63130-4899
//abstract
Abstract
This paper describes an experimental study of constraint-based network design. We used a
novel network design tool, implemented in Java, to design representative networks joining
major U.S. cities. The cost of three topologies: Best Star, Minimum Spanning Tree (MST),
and Delaunay Triangulation, are compared, with and without localized traffic constraints.
The best star network gives near optimal result when the traffic is only constrained by source
and sink capacity of switches (flat traffic constraints). With localized traffic constraints, the
most cost effective network has a structure similar to the MST. The cheapest network has a
tree structure when there are only flat traffic constraints, but can have cycles when localized
traffic constraints are present.
//page
+PAGE+ 

//title
Theory and Design of Multidimensional QMF Sub-Band Filters From
1-D Filters and Polynomials Using Transforms
//author
I.A. Shah A.A.C. Kalker
//affiliation
Philips Research Laboratories,
//address
P.O. Box 80.000, 5600 JA Eindhoven, The Netherlands
//email
Net: kalker@prl.philips.nl, shah@prl.philips.nl
//abstract
Abstract
The paper presents the general theory of designing multidimensional Quadrature Mirror Filters (QMF),
for use in sub-band coding (SBC) systems, using the McClellan transform [1]. It was recently shown that
McClellan transform could be used to generate 2-D diamond shape QMF filters [2]. In this paper we will
formalize the proofs of the diamond shape case, and generalize it to other shapes, sampling rasters and
dimensions.
Moreover we show that we do not really need the 1-D QMF filters: it is also possible and even more
convenient to design QMF filter banks by performing transformations on a class of real valued polynomials.
Examples are given of two dimensional diamond and fan-shape filters and three dimensional tetrad filters
designed using this transformation technique.
//intro
1 Introduction 

//title
Modeling and Optimization of a Multiresolution
Image Retrieval System
//author
Antonio Ortega ,
//affiliation
Dept. of Electrical Eng.-Systems
University of Southern California
//address
Los Angeles, California
//author
Zhensheng Zhang,
//affiliation
Dept. of Electrical Engineering and Center for Telecom. Research
Columbia University, 
//address
New York
//author
Martin Vetterli
//affiliation
Dept. of Electrical Engineering and Computer Science
University of California,
//address
Berkeley, California
//date
July 15, 1994
//note
IEEE/ACM Transactions on Networking, Submitted, July 1994
//abstract
Abstract
In this paper, we study the tradeoffs involved in choosing the bit allocation in a
multiresolution remote image retrieval system. Such a system uses a multiresolution
image coding scheme so that a user accessing the database will first see a coarse
version of the images and will be able to accept or discard a given image faster,
without needing to receive all the image data. We formalize the problem of choosing
the bit allocation (e.g., in the two resolution case, how many bits should be given
to the coarse image and the additional information, respectively?) so that the
overall delay in the query is minimized. We provide analytical methods to find the
optimal solution under different configurations and show how a good choice of the
bit allocation results in a significant reduction of the overall delay in the query (by
up to a factor of two in some cases).
//note
This work was presented in part at the IS&T/SPIE Symp. on Electronic Imaging Science & Tech
nology '94, San Jose, CA, Feb. 94 and at Infocom '94, Toronto, Canada, Jun. 94.
Work supported in part by the Fulbright Commission and the Ministry of Education of Spain. This
work was done while at the Dept. of Electrical Eng. and Center for Telecom. Research, Columbia
University.
//page
+PAGE+ 

//title
Automated Decomposition of Model-based Learning Problems
//author
Brian C. Williams and Bill Millar
//affiliation
Recom Technologies, Caelum Research
NASA Ames Research Center, 
//address
MS 269-2
Moffett Field, CA 94305 USA
//email
E-mail: williams, millar@ptolemy.arc.nasa.gov
//abstract
Abstract
A new generation of sensor rich, massively distributed
autonomous systems is being developed that has
the potential for unprecedented performance, such
as smart buildings, reconfigurable factories, adaptive
traffic systems and remote earth ecosystem monitoring. To achieve high performance these massive systems will need to accurately model themselves and
their environment from sensor information. Accomplishing this on a grand scale requires automating the
art of large-scale modeling. This paper presents a
formalization of decompositional, model-based learning
(DML), a method developed by observing a modeler's
expertise at decomposing large scale model estimation
tasks. The method exploits a striking analogy between
learning and consistency-based diagnosis. Moriarty,
an implementation of DML, has been applied to thermal modeling of a smart building, demonstrating a
significant improvement in learning rate.
//intro
Introduction 

//title
Path integral approach to no-Coriolis
approximation in heavy-ion collisions
//author
K. Hagino , 1 N. Takigawa, 1 , A.B. Balantekin 2 , and J.R. Bennett 3
//affiliation
1 Department of Physics, Tohoku University, 
//address
980-77 Sendai, Japan
//affiliation
2 Physics Department, University of Wisconsin, 
//address
Madison, Wisconsin 53706, USA
//affiliation
3 Department of Physics and Astronomy,
University of North Carolina at Chapel Hill, 
//address
Chapel Hill, NC 27599-3255
//date
June 26, 1995
//abstract
Abstract
We use the two time influence functional method of the path integral approach in
order to reduce the dimension of the coupled-channels equations for heavy-ion reactions
based on the no-Coriolis approximation. Our method is superior to other methods in that
it easily enables us to study the cases where the initial spin of the colliding particle is not
zero. It can also be easily applied to the cases where there is a spin-orbit force, and where
the internal degrees of freedom are not necessarily collective coordinates. It also clarifies
the underlying assumption of the approximation.
//page
+PAGE+ 

//title
Digital Communication Over Rayleigh
Fading Channels
//author
T. M. Parks
//date
15 December 1992
//abstract
Abstract
The properties of Rayleigh fading channels are derived and their
effects on various QAM signal constellations are explored. A simplified
channel model for an urban radio environment is justified in order
to simplify the analysis of error performance for the constellations.
Finally, arguments are made for extending the results to more general
channel models.
//page
+PAGE+ 

//note
Appears in KDD-97
//title
MineSet: An Integrated System for Data Mining
//author
Cliff Brunk James Kelly Ron Kohavi
//affiliation
Data Mining and Visualization
Silicon Graphics, Inc.
//address
2011 N. Shoreline Blvd
Mountain View, CA 94043-1389
//email
fbrunk,jkelly,ronnykg@engr.sgi.com
//abstract
Abstract
MineSet TM , Silicon Graphics' interactive system for
data mining, integrates three powerful technologies:
database access, analytical data mining, and data visualization. It supports the knowledge discovery process from data access and preparation through iterative analysis and visualization to deployment. Mine-Set is based on a client-server architecture that scales
to large databases. The database access component
provides a rich set of operators that can be used to
preprocess and transform the stored data into forms
appropriate for visualization and analytical mining.
The 3D visualization capabilities allow direct data visualization for exploratory analysis, including tools
for displaying high-dimensional data containing geographical and hierarchical information. The analytical mining algorithms help identify potentially interesting models of the data, which can be viewed using
visualization tools specialized for the learned models.
Third party vendors can interface to the MineSet tools
for model deployment and for integration with other
packages.
//intro
Introduction 

//date
07/17/97 
//note
10:13 1 of 8
//title
The Abstract Class Pattern
//author
Bobby Woolf
//affiliation
Knowledge Systems Corp.
//address
4001 Weston Pkwy, Cary, NC 27513-2303
//phone
919-677-1119 x541, 
//email
bwoolf@ksccary.com
//title
ABSTRACT CLASS Class Behavioral
//abstract
Intent
Define the interface for a hierarchy of classes while deferring the implementation to subclasses.
Abstract Class lets subclasses redefine the implementation of an interface while preserving the
polymorphism of those classes.
Also Known As
Liskov Substitution Principle [LW93], Design by Contract [Meyer91], Base Class [Auer95] ,
Template Class [Woolf97]
//intro
Motivation
//page
+PAGE+ 

//pubnum
M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 368
//note
Appears in: Fourth European Conference on Computer Vision, Cambridge, UK, April 1996.
//title
Generalized Image Matching:
Statistical Learning of Physically-Based Deformations
//author
Chahab Nastar , Baback Moghaddam and Alex Pentland
//affiliation
Perceptual Computing Section, The Media Laboratory,
Massachusetts Institute of Technology
//address
20 Ames Street, Cambridge MA 02139, U.S.A.
//abstract
Abstract
We describe a novel approach for image matching
based on deformable intensity surfaces. In this
approach, the intensity surface of the image
is modeled as a deformable 3D mesh in the
(x; ; I(x; )) space. Each surface point has 3
degrees of freedom, thus capturing fine surface
changes. A set of representative deformations
within a class of objects (e.g. faces) are statistically learned through a Principal Components Analysis, thus providing a priori knowledge
about object-specific deformations. We demonstrate the power of the approach by examples
such as image matching and interpolation of
missing data. Moreover this approach dramatically reduces the computational cost of solving
the governing equation for the physically based
system by approximately three orders of magni
tude.
//intro
1 Introduction 

//note
Kluwer Academic Publishers, Boston. Manufactured in The Netherlands. 
//title
A Method for Automatic Design Error Location and
Correction in Combinational Logic Circuits
//author
AYMAN M. WAHBA AND DOMINIQUE BORRIONE
//affiliation
Modelisation et Preuves de Circuits, TIMA Laboratory, 
//address
BP 53X, 38041 Grenoble Cedex FRANCE
//email
Ayman.Wahba@imag.fr, Dominique.Borrione@imag.fr
//note
Received ??. Revised ??.
//abstract
Abstract. We present a new diagnostic algorithm, based on backward-propagation, for localising design
errors in combinational logic circuits. Three hypotheses are considered, that cover all single gate replacement and insertion errors. Diagnosis-oriented test patterns are generated in order to rapidly reduce the
suspected area where the error lies. The originality of our method is the use of patterns which do not
detect the error, in addition to detecting patterns. A theorem shows that, in favourable cases, only two
patterns suffice to get a correction. We have implemented the test generation and diagnosis algorithms.
Results obtained on benchmarks show that the error is always found, after the application of a small
number of test patterns, with an execution time proportional to the circuit size.
//keyword
Keywords: design correctness, design debugging, design error diagnosis
//intro
1. Introduction 

//title
Segregating Planners and Their Environments
//author
Scott D. Anderson
Paul R. Cohen
//affiliation
Experimental Knowledge Systems Laboratory
Computer Science Department, LGRC
University of Massachusetts
//address
Amherst MA 01003-4610
//email
fanderson,coheng@cs.umass.edu
//note
To be published in the proceedings of the
Spring Symposium on Integrated Planning Applications
//abstract
Abstract
By implementing agents and environments using a domain-independent, extensible simulation substrate, described in this
paper, agents will have clean interfaces to
their environments. These makes it easier
for agents to be plugged into other environments that have been similarly defined. If
agents can interact with multiple environments, their behaviors and the associated
experimental results will be more general
and interesting.
//intro
1 Introduction 

//title
Kin Recognition, Similarity, and Group Behavior
//author
Maja J Mataric
//affiliation
MIT Artificial Intelligence Laboratory
//address
545 Technology Square #721
Cambridge, MA 02139
//phone
phone: (617) 253-8839
//phone
fax: (617) 253-0039
//email
maja@ai.mit.edu
//abstract
Abstract
This paper presents an approach to describing
group behavior using simple local interactions
among individuals. We propose that for a given
domain a set of basic interactions can be defined
which describes a large variety of group behaviors.
The methodology we present allows for simplified
qualitative analysis of group behavior through the
use of shared goals, kin recognition, and minimal
communication. We also demonstrate how these
basic interactions can be simply combined into
more complex compound group behaviors.
To validate our approach we implemented an array of basic group behaviors in the domain of spatial interactions among homogeneous agents. We
describe some of the experimental results from two
distinct domains: a software environment, and a
collection of 20 mobile robots. We also describe
a compound behavior involving a combination of
the basic interactions. Finally, we compare the
performance of homogeneous groups to those of
dominance hierarchies on the same set of basic behaviors.
//intro
Introduction 

//title
Planning and Proof Planning
//author
Erica Melis 1 and Alan Bundy 2
//abstract
Abstract. The paper adresses proof planning as a specific AI planning. It describes some peculiarities of proof planning and discusses
some possible cross-fertilization of planning and proof planning.
//intro
1 Introduction 

//title
A Data-Flow Graphical User Interface for Querying a Scientific
Database
//author
Bosco S. Tjan , Leonard Breslow , Sait Dogru , Vijay Rajan,
Keith Rieck , James R. Slagle , and Marius O. Poliac
//affiliation
Computer Science Department
University of Minnesota, 
//address
Minneapolis MN 55455
//abstract
Abstract
We describe the design principles and functionality
of a visual query language called SeeQL that represents data retrieval and analysis operations as a data-flow graph. A query is viewed as a sequence of relational algebra and other data transformation operations applied to database tables. The language is well-suited for large-scale scientific database applications,
where data analysis is a major component and the typical queries or data retrieval patterns are unrestricted.
The language provides a flexible yet easy-to-use environment for database access and data analysis for
non-programmer research scientists. We have implemented this language in a system being used in a long-term data-intensive highway pavement research project
(MnRoad) conducted by the Minnesota Department of
Transportation.
//intro
1 Introduction 

//title
Adaptive Markov Chain Monte Carlo through
Regeneration
//author
Walter R. Gilks
//affiliation
Medical Research Council
Biostatistics Unit
//address
Cambridge, CB2 2SR, UK.
//author
Gareth O. Roberts
//affiliation
Statistical Laboratory
University of Cambridge
//address
Cambridge, CB2 1SB, UK.
//author
Sujit K. Sahu
//affiliation
School of Mathematics
University of Wales, Cardiff
//address
Cardiff, CF2 4YH, UK.
//date
January 26, 1998
//abstract
Summary
Markov chain Monte Carlo (MCMC) is used for evaluating expectations of functions of interest
under a target distribution . This is done by calculating averages over the sample path of a
Markov chain having as its stationary distribution. For computational efficiency, the Markov
chain should be rapidly mixing. This can sometimes be achieved only by careful design of the
transition kernel of the chain, on the basis of a detailed preliminary exploratory analysis of . An
alternative approach might be to allow the transition kernel to adapt whenever new features of
are encountered during the MCMC run. However, if such adaptation occurs infinitely often, the
stationary distribution of the chain may be disturbed. We describe a framework, based on the
concept of Markov chain regeneration, which allows adaptation to occur infinitely often, but which
does not disturb the stationary distribution of the chain or the consistency of sample-path averages.
//keyword
Key Words: Adaptive method; Bayesian inference; Gibbs sampling; Markov chain Monte Carlo;
//page
+PAGE+ 

//title
Measuring the Difficulty of Specific Learning Problems
//author
Chris Thornton
//affiliation
Cognitive and Computing Sciences
University of Sussex
//address
Brighton BN1 9QN
//email
Email: Chris.Thornton@cogs.susx.ac.uk
//phone
Tel: (44)273 606755 x 3239
//date
October 21, 1994
//abstract
Abstract
Existing complexity measures from contemporary learning theory cannot be conveniently applied to specific learning problems (e.g., training sets). Moreover, they are typically non-generic,
i.e., they necessitate making assumptions about the way in which the learner will operate. The lack
of a satisfactory, generic complexity measure for learning problems poses difficulties for researchers
in various areas; the present paper puts forward an idea which may help to alleviate these. It
shows that supervised learning problems fall into two, generic, complexity classes only one of which
is associated with computational tractability. By determining which class a particular problem
belongs to, we can thus effectively evaluate its degree of generic difficulty.
//intro
1 Introduction 

//title
A Study of the Structure and Performance
of MMU Handling Software
//author
Yousef A. Khalidi
Vikram P. Joshi
Dock Williams
//pubnum
SMLI TR-94-28 
//date
June 1994
//abstract
Abstract:
Modern operating systems provide a rich set of interfaces for mapping, sharing, and protecting memory. Different
memory management unit (MMU) architectures provide different mechanisms for managing memory translations.
Since the same OS usually runs on different MMU architectures, a software hardware address translation (hat)
layer that abstracts the MMU architecture is normally implemented between MMU hardware and the virtual memory system of the OS. In this paper, we study the impact of the OS and the MMU on the structure and performance
of the hat layer. In particular, we concentrate on the role of the hat layer on the scalability of system performance
on symmetric multiprocessors with 2-12 CPUs. The results show that, unlike single-user applications, multi-user
applications require very careful multi-threading of the hat layer to achieve system performance that scales with
the number of CPUs. In addition, multi-threading the hat can result in better performance in lesser amounts of
physical memory.
//note
email addresses:
//email
yousef.khalidi@eng.sun.com
vikram.joshi@eng.sun.com
dock.williams@eng.sun.com
//address
M/S 29-01
2550 Garcia Avenue
Mountain View, CA 94043
//page
+PAGE+ 

//title
Dynamic Procedure Placement Through Cache Windowing
//author
Carleton Miyamoto
//pubnum
CS 252/265 
//date
Spring 98
//affiliation
University of California, Berkeley
//abstract
Abstract
The relative slowdown of DRAMs with respect to
processor speeds and the widespread use of SMP
machines have bolstered the reliance on processor caches
to provide good performance. As a result, optimizing
machines and software for caches have recently received
more attention. In addition, with the popularity of
extensible computing, which includes the object oriented
programming style, shared libraries, and Java based
computing, creating effective compilers has become
more challenging, with an increased reliance on more
dynamic techniques, such as profiling and runtime code
generation. This paper proposes a dynamic optimization
method called cache windowing to reduce conflict misses
in L1 instruction caches. Using a combination of
hardware and software support, cache windowing
integrates a RollCache (a direct-mapped cache enhanced
to support dynamic cache configuration) and a software
implemented FIFO caching policy. Together, both allow
a program to reposition procedures, dynamically and
efficiently, to eliminate cache conflicts. Experiments
show that this type of caching scheme can achieve miss
rates competitive to a 2-way set associative cache for
various programs. Currently, a high software overhead
exists to support a software caching policy, though
different compiler optimizations, such as inlining, may
help to reduce this. Such a system provides a more robust
runtime architecture that, potentially, may adapt better to
a wider variety of environments.
//intro
1 Introduction 

//title
Qualia Structure and the
Compositional Interpretation of Compounds
//author
Michael Johnston x and Federica Busa
//affiliation
Research Lab for Linguistics and Computation,
Computer Science Department,
Volen Center for Complex Systems,
Brandeis University,
//address
Waltham, MA 02254
//email
johnston@cs.brandeis.edu federica@cs.brandeis.edu
//abstract
Abstract
The analysis of nominal compound constructions has proven to be a recalcitrant problem
for linguistic semantics and poses serious challenges for natural language processing systems.
We argue for a compositional treatment of compound constructions which limits the need for
listing of compounds in the lexicon. We argue that the development of a practical model of
compound interpretation crucially depends on issues of lexicon design. The Generative Lexicon
(Pustejovsky 1995) provides us with a model of the lexicon which couples sufficiently expressive
lexical semantic representations with mechanisms which capture the relationship between those
representations and their syntactic expression. In our approach, the qualia structures of the
nouns in a compound provide relational structure enabling compositional interpretation of the
modification of the head noun by the modifying noun. This brings compound interpretation
under the same rubric as other forms of composition in natural language, including argument
selection, adjectival modification, and type coercion (Pustejovsky (1991,1995), Bouillon 1995).
We examine data from both English and Italian and develop analyses for both languages which use
phrase structure schemata to account for the connections between lexical semantic representation
and syntactic expression. In addition to applications in natural language understanding, machine
translation, and generation, the model of compound interpretation developed here can be applied
to multi-lingual information extraction tasks.
//page
+PAGE+ 

//title
The Undecidability of
Mitchell's Subtyping Relationship
//author
J. B. Wells
//email
jbw@cs.bu.edu
//affiliation
Dept. of Computer Science
Boston University
//address
Boston, MA 02215, U.S.A.
//date
December 10, 1995
//abstract
Abstract
Mitchell defined and axiomatized a subtyping relationship (also known
as containment, coercibility, or subsumption) over the types of System F
(with "!" and "8"). This subtyping relationship is quite simple and does
not involve bounded quantification. Tiuryn and Urzyczyn quite recently
proved this subtyping relationship to be undecidable. This paper supplies a new undecidability proof for this subtyping relationship. First, a
new syntax-directed axiomatization of the subtyping relationship is defined. Then, this axiomatization is used to prove a reduction from the
undecidable problem of semi-unification to subtyping. The undecidability of subtyping implies the undecidability of type checking for System F
extended with Mitchell's subtyping, also known as "F plus eta".
//intro
1 Introduction 

//title
Experience with Rover Navigation for Lunar-Like Terrains
//author
Reid Simmons , Eric Krotkov , Lalitesh Katragadda , and Martial Hebert
//affiliation
Robotics Institute, Carnegie Mellon University
//address
5000 Forbes Avenue, Pittsburgh, PA 15213
//email
reids@cs.cmu.edu
//intro
Introduction 

//title
Fast Soft Shadows
//author
Michael Herf and Paul S. Heckbert
//abstract
Abstract
Presented is a new algorithm to generate soft shadows. It
employs graphics hardware, including texture mapping and
accumulation buffering, to produce shadows resulting from
area light sources quickly.
//affiliation
Computer Science Dept., Carnegie Mellon University, 
//address
Pittsburgh PA 15213-3891, USA. 
//web
http://www.cs.cmu.edu/ph, 
//email
herf+@cmu.edu,
ph@cs.cmu.edu.
//page
+PAGE+ 

//note
To appear in the Proceedings of the 16th ACM Symposium on Operating System Principles
//title
Agile Application-Aware Adaptation for Mobility
//author
Brian D. Noble , M. Satyanarayanan , Dushyanth Narayanan , James Eric Tilton , Jason Flinn , Kevin R. Walker
//affiliation
School of Computer Science
Carnegie Mellon University
//abstract
Abstract
In this paper we show that application-aware adaptation, a
collaborative partnership between the operating system and
applications, offers the most general and effective approach
to mobile information access. We describe the design of
Odyssey, a prototype implementing this approach, and show
how it supports concurrent execution of diverse mobile applications. We identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it.
We present the results of our evaluation of Odyssey, indicating performance improvements up to a factor of 5 on a
benchmark of three applications concurrently using remote
services over a network with highly variable bandwidth.
//intro
1 Introduction 

//note
Submitted to the Future Generation Computer Systems special issue on Data Mining.
//title
Using Neural Networks for Data Mining
//author
Mark W. Craven
//affiliation
School of Computer Science
Carnegie Mellon University
//address
Pittsburgh, PA 15213-3891
//email
mark.craven@cs.cmu.edu
//author
Jude W. Shavlik
//affiliation
Computer Sciences Department
University of Wisconsin-Madison
//address
Madison, WI 53706-1685
//email
shavlik@cs.wisc.edu
//abstract
Abstract
Neural networks have been successfully applied in a wide range of supervised and unsupervised learning applications. Neural-network methods are not commonly used for data-mining
tasks, however, because they often produce incomprehensible models and require long training
times. In this article, we describe neural-network learning algorithms that are able to produce
comprehensible models, and that do not require excessive training times. Specifically, we discuss
two classes of approaches for data mining with neural networks. The first type of approach,
often called rule extraction, involves extracting symbolic models from trained neural networks.
The second approach is to directly learn simple, easy-to-understand networks. We argue that,
given the current state of the art, neural-network methods deserve a place in the tool boxes of
data-mining specialists.
//keyword
Keywords: machine learning, neural networks, rule extraction, comprehensible
models, decision trees, perceptrons
//intro
1 Introduction 

//title
Metamorphosis Networks:
An Alternative to Constructive Methods
//author
Brian V. Bonnlander Michael C. Mozer
//affiliation
Department of Computer Science &
Institute of Cognitive Science
University of Colorado
//address
Boulder, CO 80309-0430
//abstract
Abstract
Given a set of training examples, determining the appropriate number of free parameters is a challenging problem. Constructive
learning algorithms attempt to solve this problem automatically by
adding hidden units, and therefore free parameters, during learning. We explore an alternative class of algorithms|called metamorphosis algorithms|in which the number of units is fixed, but
the number of free parameters gradually increases during learning.
The architecture we investigate is composed of RBF units on a lattice, which imposes flexible constraints on the parameters of the
network. Virtues of this approach include variable subset selection, robust parameter selection, multiresolution processing, and
interpolation of sparse training data.
//intro
1 INTRODUCTION 

//title
Scalability of Hierarchical Meta-Learning
on Partitioned Data
//author
Philip K. Chan
//affiliation
Computer Science
Florida Institute of Technology
//address
Melbourne, FL 32901
//email
pkc@cs.fit.edu
//phone
FAX: (407) 984-8461
//author
Salvatore J. Stolfo
//affiliation
Department of Computer Science
Columbia University
//address
New York, NY 10027
//email
sal@cs.columbia.edu
//phone
(212) 939-7080
//date
May 8, 1997
//abstract
Abstract
In this paper we study the issue of how to scale machine learning algorithms, that
typically are designed to deal with main-memory based datasets, to efficiently learn
models from large distributed databases. We have explored an approach called meta-learning that is related to the traditional approaches of data reduction commonly
employed in distributed database query processing systems. We explore the scalability
of learning arbiter and combiner trees from partitioned data. Arbiter and combiner
trees integrate classifiers trained in parallel from small disjoint subsets. Previous work
demonstrated the efficacy of these meta-learning architectures in terms of accuracy
of the computed meta-classifiers. Here we discuss the computational performance
of constructing arbiter and combiner trees in terms of speedup and scalability as a
function of database size and number of partitions. The performance of serial learning
algorithms is evaluated. We then analyze the performance of the algorithms used to
construct combiner and arbiter trees in parallel. Our empirical results validate these
analyses and indicate that the techniques can effectively scale up to large datasets with
millions of records using cheap commodity hardware.
//keyword
Keywords: speedup, scalability, arbiter and combiner trees, meta-learning, parallel/distributed
processing, inductive learning
//note
This work was partially funded by grants from NSF (IRI-96-32225 & CDA-96-25374), ARPA (F30602
96-1-0311), and NYSSTF (423115-445).
//page
+PAGE+ 

//note
DIMACS Series in Discrete Mathematics
and Theoretical Computer Science
Volume 00, 19xx
//title
Some applications of generalized FFTs
//author
Daniel N. Rockmore
//abstract
Abstract. Generalized FFTs are efficient algorithms for computing a Fourier
transform of a function defined on finite group, or a bandlimited function defined on a compact group. The development of such algorithms has been accompanied and motivated by a growing number of both potential and realized
applications. This paper will attempt to survey some of these applications.
Appendices include some more detailed examples.
//intro
1. A brief history 

//title
A Protocol for Efficient Transfer of Data over Fiber/Cable Systems
//author
Dolors Sala John O. Limb
//affiliation
School of Electrical College of Computing
and Computer Engineering
Georgia Institute of Technology
//address
Atlanta, GA 30332-0280
//email
E-mail: (dolors,limb)@cc.gatech.edu
//abstract
Abstract
A revolution is occurring in the scope and range
of information, communication and education services
that will be made available to schools, libraries, town-halls, clinics and, most importantly, residences. These
services will be provided initially, primarily over hybrid fiber-cable systems, either by telephone companies
or cable companies. The old cable plant is being upgraded and used in totally new ways.
The topology and physical characteristics of the upstream channel present new challenges for efficient
channel access. We present a media access protocol
that efficiently transfers data on this channel. A primary goal in the design was to keep the portion of the
protocol resident in the station as simple as possible.
Thus we use centralized control located in the cable
head-end and minimize intelligence in the station. We
refer to this protocol as Centralized Priority Reservation or CPR. A station wishing to transmit sends a request to the head-end using a contention channel. The
head-end acknowledges the request and then schedules
the request, informing the station by means of a grant
message when to transmit.
The protocol performs well under heavy load. Performance is affected little by the number of stations,
the speed of the system and the physical length of the
system.
//intro
1 Introduction 

//title
Using Queue Time Predictions for Processor Allocation
//author
Allen B. Downey
//affiliation
University of California at Berkeley
San Diego Supercomputer Center
//abstract
Abstract
When a moldable job is submitted to a space-sharing
parallel computer, it must choose whether to begin execution on a small, available cluster or wait in queue for
more processors to become available. To make this decision, it must predict how long it will have to wait for
the larger cluster. We propose statistical techniques for
predicting these queue times, and develop an allocation
strategy that uses these predictions. We present a workload model based on observed workloads at the San Diego
Supercomputer Center and the Cornell Theory Center,
and use this model to drive simulations of various allocation strategies. We find that prediction-based allocation
not only improves the turnaround time of individual jobs;
it also improves the utilization of the system as a whole.
//intro
1 Introduction 

//title
A Ray Tracing Method for Illumination Calculation in
Diffuse-Specular Scenes
//author
Peter Shirley
//affiliation
Department of Computer Science
University of Illinois
//address
1304 West Springfield Avenue
Urbana, Illinois 61801
USA
//abstract
Abstract
Several ways of improving the realism of the results
of traditional ray tracing are presented. The essential physical quantities of spectral radiant power and
spectral radiance and their use in lighting calculations
are discussed. Global illumination terms are derived
by employing illumination ray tracing for calculation of
quickly changing indirect lighting components, and ra-diosity ray tracing for slowly changing indirect lighting
components. Direct lighting is calculated during the
viewing phase allowing the use of bump maps. Finally,
a method is introduced that reduces the total number
of shadow rays to no more than the total number of
viewing rays for a given picture.
//keyword
Keywords: Bump Mapping, Illumination, Radiosity,
Radiance, Ray Tracing, Realism, Stratified Sampling,
Texture Mapping.
//intro
1 Introduction 

//title
Aditi-Prolog language manual +L 
//author
James Harland
David B. Kemp
Tim S. Leask
Kotagiri Ramamohanarao
John A. Shepherd
Zoltan Somogyi
Peter J. Stuckey
Jayen Vaghani
//abstract
Abstract
Aditi is a deductive database system under development at the Collaborative Information
Technology Research Institute by researchers from the University of Melbourne. The main
language in which users interact with Aditi is Aditi-Prolog. This document is a reference
manual for Aditi-Prolog.
//intro
1 Introduction 

//title
Contingency Selection in Plan Generation
//author
Nilufer Onder
//affiliation
Department of Computer Science
University of Pittsburgh
//address
Pittsburgh, PA 15260
//note
nilufer@cs.pitt.edu
//author
Martha E. Pollack
//affiliation
Department of Computer Science
and Intelligent Systems Program
University of Pittsburgh
//address
Pittsburgh, PA 15260
//note
pollack@cs.pitt.edu
//abstract
Abstract
A key question in conditional planning is: how many,
and which of the possible execution failures should be
planned for? One cannot, in general, plan for all the
possible failures because the search space is too large.
One cannot ignore all the possible failures, or one will
fail to produce sufficiently flexible plans. In this paper,
we describe an approach to conditional planning that
attempts to identify the contingencies that contribute
the most to a plan's overall utility. Plan generation
proceeds by handling the most important contingencies first, extending the plan to include actions that
will be taken in case the contingency fails. We discuss
the representational issues that must be addressed in
order to implement such an algorithm, and present an
example which illustrates our approach.
//intro
Introduction 

//title
Navigation in Three Dimensional Spaces
//pubnum
CS-590Z
//author
Carlos Gonzalez Ochoa Aleman
//date
May 23, 1994
//abstract
Abstract
Current graphic hardware have helped to develop scientific visualization tools,
but this progress has not level with the magnitude of data genereted in some areas needing to be visualized. Techniques to navigate data have been developed,
including new hardware and algorithms to improve the rendering speed and quality.
This paper will describe the issues of navigation, current display and interaction
technology, and algorithms. At the end a set of problems yet to be solved will be
discussed
//intro
1 Introduction 

//title
Carlsberg: A Distributed Execution Environment
Providing Coherent Shared Memory and
Integrated Message Passing
//note
A Position/Work-in-Progress Paper presented at
Nordic Workshop on Programming Environment Research, NWPER'94,
Lund, Sweden, June, 1994
//author
Povl T. Koch Robert J. Fowler
//affiliation
Department of Computer Science, University of Copenhagen (DIKU)
//address
Universitetsparken 1, 2100 Copenhagen, Denmark
//phone
Tel: +45 35 32 14 18 Fax: +45 35 32 14 01 
//email
E-mail: koch,fowler@diku.dk
//abstract
Abstract
The Carlsberg prototype is a distributed operating system designed to provide efficient support for distributed-parallel applications on a cluster of high-performance workstations. A unique feature of Carlsberg is the integration of
coherent shared memory, multithreading, and message passing in one system.
In this paper we discuss the motivation for the Carlsberg system and we present
aspects of its design.
//intro
1 Introduction 

//title
Intelligent Model Selection for Hillclimbing Search in
Computer-Aided Design
//author
Thomas Ellman John Keane Mark Schwabacher
//affiliation
Department of Computer Science, Hill Center for Mathematical Sciences
Rutgers University, 
//address
New Brunswick, NJ 08903
//email
fellman,keane,schwabacg@cs.rutgers.edu
//abstract
Abstract
Models of physical systems can differ according to
computational cost, accuracy and precision, among
other things. Depending on the problem solving
task at hand, different models will be appropriate. Several investigators have recently developed
methods of automatically selecting among multiple models of physical systems. Our research is
novel in that we are developing model selection
techniques specifically suited to computer-aided de
sign. Our approach is based on the idea that artifact performance models for computer-aided design
should be chosen in light of the design decisions
they are required to support. We have developed
a technique called "Gradient Magnitude Model Selection" (GMMS), which embodies this principle.
GMMS operates in the context of a hillclimbing
search process. It selects the simplest model that
meets the needs of the hillclimbing algorithm in
which it operates. We are using the domain of sailing yacht design as a testbed for this research. We
have implemented GMMS and used it in hillclimb-ing search to decide between a computationally expensive potential-flow program and an algebraic
approximation to analyze the performance of sailing yachts. Experimental tests show that GMMS
makes the design process faster than it would be if
the most expensive model were used for all design
evaluations. GMMS achieves this performance improvement with little or no sacrifice in the quality
of the resulting design.
//intro
1. Introduction 

//title
Learning to Select Useful Landmarks
//author
Russell Greiner
//affiliation
Siemens Corporate Research
//address
Princeton, NJ 08540
//email
greiner@learning.siemens.com
//author
Ramana Isukapalli
//affiliation
Department of Computer Science
Rutgers University
//email
ramana@cs.rutgers.edu
//note
Appears in the
Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94),
Seattle, Washington, July 1994.
//abstract
Abstract
To navigate effectively, an autonomous agent must be
able to quickly and accurately determine its current
location. Given an initial estimate of its position (perhaps based on dead-reckoning) and an image taken of
a known environment, our agent first attempts to locate a set of landmarks (real-world objects at known
locations), then uses their angular separation to obtain an improved estimate of its current position. Unfortunately, some landmarks may not be visible, or
worse, may be confused with other landmarks, resulting in both time wasted in searching for invisible landmarks, and in further errors in the agent's estimate of
its position. To address these problems, we propose a
method that uses previous experiences to learn a selection function that, given the set of landmarks that
might be visible, returns the subset which can reliably
be found correctly, and so provide an accurate registration of the agent's position. We use statistical techniques to prove that the learned selection function is,
with high probability, effectively at a local optimal in
the space of such functions. This report also presents
empirical evidence, using real-world data, that demonstrate the effectiveness of our approach.
//intro
1. Introduction 

//title
Jade: A High-Level, Machine-Independent Language for Parallel
Programming
//author
Martin C. Rinard, Daniel J. Scales and Monica S. Lam
//affiliation
Computer Systems Laboratory
Stanford University, 
//address
CA 94305
//intro
1 Introduction 

//title
Almost All Regular Graphs are
Hamiltonian
//author
R. W. Robinson
//affiliation
Computer Science Department
University of Georgia
//address
Athens, GA 30602, U.S.A.
//author
N. C. Wormald
//affiliation
Department of Mathematics
University of Melbourne
//address
Parkville, VIC 3052, Australia
//abstract
Abstract
In a previous paper the authors showed that almost all labelled
cubic graphs are hamiltonian. In the present paper, this result is
used to show that almost all r-regular graphs are hamiltonian for any
fixed r 3, by an analysis of the distribution of 1-factors in random
regular graphs. Moreover, almost all such graphs are r-edge-colourable
if they have an even number of vertices. Similarly, almost all r-regular
bipartite graphs are hamiltonian and r-edge-colourable for fixed r 3.
//note
Research supported by the Australian Research Council
//page
+PAGE+ 

//title
Quantum Computing | A treatise
//author
Prabhat Kumar
//date
October 24, 1996
//abstract
Abstract
Quantum computing has witnessed a surge of activity recently owing
to some very exciting discoveries on both the theoretical and practical
fronts. In this overview, we sketch an account of the developments in
this scientifically intriguing field, starting in the early 80's when the first
questions about the computability of quantum processes were raised,
and which led to the formal definitions of a Quantum Computer and a
Quantum Complexity Theory. Peter Shor's recent remarkable discovery of quantum algorithms to solve the problems of integer factoring
and discrete log computing, which are believed to be extremely hard to
solve efficiently on classical computers, is a compelling demonstration
of the suspected superiority of quantum computing over the classical
model that is in use today. We discuss one of his algorithms and the
implications it has for classical cryptography. We discuss some of the
latest work in this field which has brought us yet closer to achieving a
physical realization of a quantum computer. Whenever it happens, if it
happens, it would be yet another revolution in the field of computing,
and maybe the biggest one to date.
//intro
1 Birth of Quantum Computing 

//note
To appear in Proceedings of the Twenty Third ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, St. Petersburg
Beach, Florida, January 21-24, 1996. c fl1996 ACM (see notice below).
//title
Is it a Tree, a DAG, or a Cyclic Graph?
A Shape Analysis for Heap-Directed Pointers in C
//author
Rakesh Ghiya and Laurie J. Hendren
//affiliation
School of Computer Science, McGill University
//address
Montreal, Quebec, CANADA H3A 2A7
//email
fghiya,hendreng@cs.mcgill.ca
//abstract
Abstract
This paper reports on the design and implementation of a practical shape analysis for C. The purpose of the analysis is to aid in the disambiguation of
heap-allocated data structures by estimating the shape
(Tree, DAG, or Cyclic Graph) of the data structure accessible from each heap-directed pointer. This shape
information can be used to improve dependence testing and in parallelization, and to guide the choice of
more complex heap analyses.
The method has been implemented as a context-sensitive interprocedural analysis in the McCAT compiler. Experimental results and observations are given
for 16 benchmark programs. These results show that
the analysis gives accurate and useful results for an
important group of applications.
//intro
1 Introduction and Related 

//note
To be presented at the 17th IEEE Real-Time Systems Symposium, December 1996.
//title
A Framework for Implementing Objects and Scheduling Tasks in Lock-Free
Real-Time Systems
//author
James H. Anderson and Srikanth Ramamurthy
//affiliation
Department of Computer Science, University of North Carolina at Chapel Hill
//abstract
Abstract
We present an integrated framework for developing real-time systems in which lock-free algorithms are employed to
implement shared objects. There are two key objectives of
our work. The first is to enable functionality for object sharing in lock-free real-time systems that is comparable to that
in lock-based systems. Our main contribution toward this
objective is an efficient approach for implementing multi-object lock-free operations and transactions. A second key
objective of our work is to improve upon previously proposed
scheduling conditions for tasks that share lock-free objects.
When developing such conditions, the key issue is to bound
the cost of operation interferences. We present a general
approach for doing this, based on linear programming.
//intro
1. Introduction 

//title
Stability and Chaos in an Inertial Two
Neuron System
//author
Diek W. Wheeler 1 and W. C. Schieve
//affiliation
Ilya Prigogine Center for Studies in Statistical Mechanics and
Complex Systems
and
Physics Department, The University of Texas,
//address
Austin, TX 78712
//abstract
Abstract.
Inertia is added to a continuous-time, Hopfield [1] effective-neuron system.
We explore the effects on the stability of the fixed points of the system. A two
neuron system with one or two inertial terms added is shown to exhibit chaos.
The chaos is confirmed by Lyapunov exponents, power spectra, and phase space
plots.
//intro
INTRODUCTION 

//title
Automated Proof Support for
Reasoning about Distributed Mobile
Programs
//degree
A thesis
submitted in partial fulfilment
of the requirements for
the degree
of
Bachelor of Technology
in
Computer Science and Engineering
//author
by
B Karthikeyan
T R Vishwanath
//degree
under the guidance of
Dr Sanjiva Prasad
//affiliation
Department of Computer Science & Engineering
Indian Institute of Technology, Delhi
//date
May 1997
//page
+PAGE+ 

//title
Scheduling and Page Migration for Multiprocessor Compute Servers
//author
Rohit Chandra , Scott Devine , Ben Verghese,
Anoop Gupta , and Mendel Rosenblum
//affiliation
Computer Systems Laboratory
Stanford University, 
//address
Stanford CA 94305
//abstract
Abstract
Several cache-coherent shared-memory multiprocessors have been
developed that are scalable and offer a very tight coupling between
the processing resources. They are therefore quite attractive for
use as compute servers for multiprogramming and parallel application workloads. Process scheduling and memory management,
however, remain challenging due to the distributed main memory found on such machines. This paper examines the effects of
OS scheduling and page migration policies on the performance
of such compute servers. Our experiments are done on the Stan-ford DASH, a distributed-memory cache-coherent multiprocessor.
We show that for our multiprogramming workloads consisting of
sequential jobs, the traditional Unix scheduling policy does very
poorly. In contrast, a policy incorporating cluster and cache affinity along with a simple page-migration algorithm offers up to twofold performance improvement. For our workloads consisting of
multiple parallel applications, we compare space-sharing policies
that divide the processors among the applications to time-slicing
policies such as standard Unix or gang scheduling. We show
that space-sharing policies can achieve better processor utilization
due to the operating point effect, but time-slicing policies benefit
strongly from user-level data distribution. Our initial experience
with automatic page migration suggests that policies based only
on TLB miss information can be quite effective, and useful for
addressing the data distribution problems of space-sharing sched-ulers.
//intro
1 Introduction 

//title
Demand Interprocedural Dataflow Analysis
//author
Susan Horwitz , Thomas Reps , and Mooly Sagiv
//affiliation
University of Wisconsin
//abstract
Abstract
An exhaustive dataflow-analysis algorithm associates with each point in a program a set of dataflow facts
that are guaranteed to hold whenever that point is reached during program execution. By contrast, a
demand dataflow-analysis algorithm determines whether a single given dataflow fact holds at a single given
point.
This paper presents a new demand algorithm for interprocedural dataflow analysis. The algorithm has
four important properties:
g It provides precise (meet-over-all-interprocedurally-valid-paths) solutions to a large class of problems.
g It has a polynomial worst-case cost for both a single demand and a sequence of all possible demands.
g The worst-case total cost of the sequence of all possible demands is no worse than the worst-case cost
of a single run of the current best exhaustive algorithm.
g Experimental results show that in many situations (e.g., when only a small number of demands are
made, or when most demands are answered yes) the demand algorithm is superior to the current best
exhaustive algorithm.
//keyword
CR Categories and Subject Descriptors: D.2.2 [Software Engineering]: Tools and Techniques; D.3.4
[Programming Languages]: Processors compilers, optimization; E.1 [Data Structures] graphs; F.2.2
[Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems computations on discrete structures; G.2.2 [Discrete Mathematics]: Graph Theory graph algorithms
General Terms: Algorithms, Experimentation, Theory
Additional Key Words and Phrases: demand dataflow analysis, distributive dataflow framework, graph
reachability, interprocedural dataflow analysis, interprocedurally realizable path, interprocedurally valid
path, meet-over-all-valid-paths solution
//note
On leave from IBM Scientific Center, Haifa, Israel.
This work was supported in part by a David and Lucile Packard Fellowship for Science and Engineering, by the National Science
Foundation under grants CCR-8958530 and CCR-9100424, by the Defense Advanced Research Projects Agency under ARPA Order
No. 8856 (monitored by the Office of Naval Research under contract N00014-92-J-1937), by the Air Force Office of Scientific
Research under grant AFOSR-91-0308, and by a grant from Xerox Corporate Research.
Part of this work was done while the authors were visiting the University of Copenhagen.
A preliminary version of this paper appeared in SIGSOFT 95: Proceedings of the Third ACM SIGSOFT Symposium on Foundations
of Software Engineering (Washington DC, October 10-13, 1995) [15]
Authors' address: 
//affiliation
Computer Sciences Department; Univ. of Wisconsin; 
//address
1210 West Dayton Street; Madison, WI 53706; USA.
//email
Electronic mail: -horwitz, reps, sagiv-@cs.wisc.edu.
//page
+PAGE+ 

//title
High-Order Accurate Schemes for Incompressible Viscous Flow
//author
John C. Strikwerday
//affiliation
Computer Sciences Department
University of Wisconsin-Madison
//abstract
Abstract
We present new finite difference schemes for the incompressible Navier-Stokes equations. The schemes are based on two spatial differencing methods, one is fourth-order
accurate and the other is sixth-order accurate. The temporal differencing is based on
backward differencing formulas. The schemes use non-staggered grids and satisfy regularity estimates, guaranteeing smoothness of the solutions. The schemes are computationally
efficient. Computational results demonstrating the accuracy are presented.
//keyword
Keywords: incompressible Navier-Stokes, finite difference schemes, GMRES.
//note
AMS(MOS) classifications: 65M05, 65N05, 76D05
//intro
1. Introduction. 

//title
Moufang Quasigroups
//author
Kenneth Kunen 1
//affiliation
University of Wisconsin, 
//address
Madison, WI 53706, U.S.A.
//email
kunen@cs.wisc.edu
//date
September 5, 1995
//abstract
ABSTRACT
Each of the Moufang identities in a quasigroup implies that the
quasigroup is a loop.
//intro
x1. Introduction. 

//title
Measuring the Performance of Communication
Middleware on High-Speed Networks
//author
Aniruddha Gokhale and Douglas C. Schmidt
//email
gokhale@cs.wustl.edu and schmidt@cs.wustl.edu
//affiliation
Department of Computer Science, Washington University
//address
St. Louis, MO 63130, USA
//note
An earlier version of this paper appeared in the Proceedings
of the SIGCOMM Conference, 1996, Stanford University,
August, 1996.
//abstract
Abstract
Conventional implementations of communication middle-ware (such as CORBA and traditional RPC toolkits) incur
considerable overhead when used for performance-sensitive
applications over high-speed networks. As gigabit networks
become pervasive, inefficient middleware will force programmers to use lower-level mechanisms to achieve the necessary
transfer rates. This is a serious problem for mission/life-critical applications (such as satellite surveillance and medical imaging).
This paper compares the performance of several widely
used communication middleware mechanisms on a high-speed ATM network. The middleware ranged from lower-level mechanisms (such as socket-based C interfaces and
C++ wrappers for sockets) to higher-level mechanisms (such
as RPC, hand-optimized RPC and two implementations of
CORBA - Orbix 2.0.1 and ORBeline 2.0). These measurements reveal that the lower-level C and C++ implementations outperform the CORBA implementations significantly
(the best CORBA throughput for remote transfer was roughly
75 to 80 percent of the best C/C++ throughput for sending scalar data types and only around 33 percent for sending structs containing binary fields), and the hand-optimized
RPC code performs slightly better than the CORBA implementations. Our goal in precisely pinpointing the sources of
overhead for communication middleware is to develop scalable and flexible CORBA implementations that can deliver
gigabit data rates to applications.
//keyword
Keywords: Communication middleware, distributed object computing, CORBA, high-speed networks.
//intro
1 Introduction and Motivation 

//title
Defining and Measuring Conflicts
in Optimistic Replication
//author
John Heidemann Ashvin Goel Gerald Popek
//affiliation
University of California, Los Angeles
//pubnum
Technical report UCLA-CSD-950033
//abstract
Abstract
Optimistic replication is often viewed as essential for
large scale systems and for supporting mobile computing. In optimistic replication, updates can be made concurrently to different file replicas, resulting in multiple
versions of the file. To recover from these conflicting
updates, after-the fact conflict resolution actions are
required to recombine multiple versions into one. This
paper defines these concepts and discusses approaches
to measure them in optimistically replicated systems.
Measurement of the number of conflicting updates
and conflict resolution is important to judge the practicality of optimistic replication. An environment
where conflicting updates are frequent will not be attractive since users cannot assume they have up-to-date
data. Although many conflicts can be automatically
resolved, some conflicts require user intervention; such
conflicts cannot be too common. This paper shows an
approach to measure the number of conflicting updates.
From this measurement we derive the actual amount of
work done by the user or system to resolve conflicts
and the minimum amount of work required to resolve
conflicts.
//intro
1 Introduction 

//title
Using Linguistic Phenomena to Motivate a Set of Rhetorical
Relations
//author
Alistair Knott
//affiliation
Department of Artificial Intelligence, University of Edinburgh
//address
80 South Bridge, Edinburgh EH1 1HN, Scotland
//email
Email: A.Knott@ed.ac.uk
//author
Robert Dale
//affiliation
Human Communication Research Centre, University of Edinburgh
//address
2 Buccleuch Place, Edinburgh EH8 9LW, Scotland
//email
Email: R.Dale@ed.ac.uk
//date
May 5, 1993
//note
Running head: Motivating Rhetorical Relations
//page
+PAGE+ 

//title
What's Decidable about Hybrid Automata?
//author
Thomas A. Henzinger 2 Peter W. Kopke 2 Anuj Puri 3 Pravin Varaiya 3
//abstract
Abstract. Hybrid automata model systems with both
digital and analog components, such as embedded control programs. Many verification tasks for such programs
can be expressed as reachability problems for hybrid automata. By improving on previous decidability and undecidability results, we identify the precise boundary between decidability and undecidability of the reachability
problem for hybrid automata.
On the positive side, we give an (optimal) PSPACE
reachability algorithm for the case of initialized rectangular automata, where all analog variables follow trajectories within piecewise-linear envelopes and are reinitialized
whenever the envelope changes. Our algorithm is based
on a translation of an initialized rectangular automaton
into a timed automaton that defines the same timed language. The translation has practical significance for verification, because it guarantees the termination of symbolic
procedures for the reachability analysis of initialized rectangular automata.
On the negative side, we show that several slight generalizations of initialized rectangular automata lead to an
undecidable reachability problem. In particular, we prove
that the reachability problem is undecidable for timed automata with a single stopwatch.
//intro
1 Introduction 

//title
Spheres of Control:
An Approach to Advanced Recovery
//author
C. Wallace N. Soparkar
//affiliation
Electrical Engineering & Computer Science
The University of Michigan
//address
Ann Arbor, MI 48109-2122
USA
//email
fwallace,soparkarg@eecs.umich.edu
//abstract
Abstract
Recovery from failures and erroneous executions is a crucial but complicated issue for concurrently accessed data systems. Increasingly sophisticated techniques are being developed to improve performance
and functionality of recovery protocols. To better understand and analyze recovery schemes, we reexamine the concept of spheres of control [Dav78], using it as a unifying framework for specifying diverse
recovery models simply and precisely. We constrain sphere-of-control formulations appropriately to capture transaction-oriented recovery in both centralized and distributed environments and with different
types of schedules, as well as semantics-based recovery and compensation. In addition, we discuss how
the operational semantics methodology of evolving algebras [Gur95] can model spheres of control formally
and refine them to lower levels of abstraction.
//intro
1 Introduction 

//title
Video Server on an ATM Connected Cluster of Workstations
//author
Olav Sandsta , Stein Langrgen , and Roger Midtstraum
//affiliation
Department of Computer and Information Science
Norwegian University of Science and Technology
//address
N-7034 Trondheim, Norway
//email
folavsa, steinl, rogerg@idi.ntnu.no
//abstract
Abstract
Video servers are important for applications which make
use of digital video. The video servers should provide better
functionality than most of today's video servers offer, - e.g.,
support of flexible and instant user interactions, delivery of
multiple video formats and support of virtual video documents. In this paper we discuss the requirements that video
servers should fulfill and we describe the design and implementation of the Elvira video server. The Elvira video server
is built on a cluster of standard UNIX workstations interconnected by an ATM switch. The capacity of the Elvira server
is evaluated and we show the effects of different strategies
for allocation of video data across nodes and disks.
//intro
1. Introduction 

//note
To appear in Expert Systems with Applications: An International Journal, Vol.10(1996)
//title
Efficient Rule Induction from Noise Data
//author
Huan Liu
//affiliation
Department of Information Systems and Computer Science
National University of Singapore
//address
Kent Ridge, Singapore 0511
//email
liuh@iscs.nus.sg
//phone
Tel: (+65)-772-6563; Fax: (+65) 779-4580
//note
Acknowledgments
Many thanks to Rudy Setiono and Tiow Seng Tan for providing valuable com
ments and help.
//page
+PAGE+ 

//note
To appear in the Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98)
//title
Learning to Predict User Operations for Adaptive Scheduling
//author
Melinda T. Gervasio and Wayne Iba and Pat Langley
//affiliation
Institute for the Study of Learning and Expertise
//address
2164 Staunton Court, Palo Alto, California 94306
//email
fgervasio,iba,langleyg@isle.org
//abstract
Abstract
Mixed-initiative systems present the challenge of finding an effective level of interaction between humans
and computers. Machine learning presents a promising approach to this problem in the form of systems
that automatically adapt their behavior to accommodate different users. In this paper, we present an empirical study of learning user models in an adaptive
assistant for crisis scheduling. We describe the problem domain and the scheduling assistant, then present
an initial formulation of the adaptive assistant's learning task and the results of a baseline study. After this,
we report the results of three subsequent experiments
that investigate the effects of problem reformulation
and representation augmentation. The results suggest
that problem reformulation leads to significantly better accuracy without sacrificing the usefulness of the
learned behavior. The studies also raise several interesting issues in adaptive assistance for scheduling.
//intro
Introduction 

//title
ON COMPUTABLE BELIEFS OF RATIONAL MACHINES
//author
Nimrod Megiddo
//abstract
Abstract. Traditional decision theory has assumed that agents have complete, consistent and readily available beliefs and preferences. Obviously, even if
an expert system has complete and consistent beliefs, it cannot have them readily
available. Moreover, some beliefs about beliefs are not even approximately computable. It is shown that if all players have complete and consistent beliefs, they
can compute approximate beliefs about beliefs of any order by considering events
arbitrarily close in some well-defined sense to the ones in question.
//intro
1. Introduction 

//title
Rule Combination
in
Inductive Learning
//author
Luis Torgo
//affiliation
LIACC
//address
R.Campo Alegre, 823 - 2.
4100 PORTO
PORTUGAL
//phone
Telf. : (+351) 2 600 16 72 - Ext. 115
Fax : (+351) 2 600 3654
//email
email : ltorgo@ciup1.ncc.up.pt
//abstract
Abstract. This paper describes the work on methods for combining rules
obtained by machine learning systems. Three methods for obtaining the
classification of examples with those rules are compared. The advantages and
disadvantages of each method are discussed and the results obtained on three
real world domains are commented. The methods compared are: selection of
the best rule; PROSPECTOR-like probabilistic approximation for rule
combination; and MYCIN-like approximation. Results show significant
differences between methods indicating that the problemsolving strategy is
important for accuracy of learning systems.
//intro
1 Introduction 

//title
Designing Distributed Applications
with Mobile Code Paradigms
//author
Antonio Carzaniga
//affiliation
Politecnico di Milano
//address
Piazza Leonardo da Vinci, 32
20133 Milano, Italy
//phone
+39-2-2399-3638
//email
carzaniga@elet.polimi.it
//author
Gian Pietro Picco
//affiliation
Politecnico di Torino
//address
Corso Duca degli Abruzzi, 24
10129 Torino, Italy
//phone
+39-11-564-7008
//email
picco@athena.polito.it
//author
Giovanni Vigna
//affiliation
Politecnico di Milano
//address
Piazza Leonardo da Vinci, 32
20133 Milano, Italy
//phone
+39-2-2399-3666
//email
vigna@elet.polimi.it
//abstract
ABSTRACT
Large scale distributed systems are becoming of
paramount importance, due to the evolution of technology and to the interest of market. Their development,
however, is not yet supported by a sound technological and methodological background, as the results developed for small size distributed systems often do not
scale up. Recently, mobile code languages (MCLs) have
been proposed as a technological answer to the problem.
In this work, we abstract away from the details of these
languages by deriving design paradigms exploiting code
mobility that are independent of any particular technology. We present such design paradigms, together
with a discussion of their features, their application domain, and some hints about the selection of the correct
paradigm for a given distributed application.
//keyword
Keywords
Mobile code, design paradigms, distributed applications.
//intro
INTRODUCTION 

//title
Sensor-based Registration and Stacking
of Electronic Substrate Layers
//author
Andrew E. Brennemann, 1 Robert Hammer, 2
William V. Jecusco II, 3
and Ralph L. Hollis 4
//affiliation
IBM Research Division
Thomas J. Watson Research Center
//address
Yorktown Heights, New York, USA
//abstract
Abstract
Substrates for most of today's electronic products contain many wiring layers
which are individually fabricated, mechanically registered with one another,
and laminated together. Alignment tolerances of 0.05 mm to 0.1 mm are
sufficient to register the vertical connection pads or vias on each layer. More
aggressive designs of the future will, however, require manufacturing accuracies of at least an order of magnitude better to accommodate much finer
wire widths and pin spacings. Conventional equipment relying on mechanical
"pin-in-slot" methods will likely be inadequate, and a new approach will be
needed.
We describe here a sensor-based approach for registration and stacking
of electronic substrate sublaminates that replaces pin-in-slot methods, yet
does not require accurate automation equipment. A pilot work cell for this
approach is presented, which has an IBM 7576 coarse-positioning robot, a
specially-developed fine-positioning robot, optical sensors, and several routine
low accuracy fixtures. A novel robot bracing method was used to minimize
environmental vibration during sublaminate stacking.
Pairs of test sublaminates, each containing an identical pattern of 100
m holes, were aligned, stacked and bonded. The accuracy of registration
//note
1 Retired, 
//address
4 Morningside Court, Ossining, NY, 10562. 

//title
Improving Programming-by-Demonstration With Better Semantic Expression
//degree
Thesis Proposal
//author
Richard McDaniel
//date
November 14, 1995
//abstract
Abstract
The domain of applications that can be created with programming-by-demonstration
(PBD) can be extended by improving the developers ability to communicate with the system. The
techniques provided in this thesis will allow nonprogrammers to create a new variety of complete,
interactive applications including many board games and educational software using PBD.
A PBD software tool uses inferencing to induce programs by watching the developer demonstrate examples that show how the application should behave. Current systems reduce their scope
or resort to having the developer program because they do not provide sufficient ways to express
behaviors and the factors that affect them. Therefore, the goal of this thesis is to develop understandable forms of annotated expression and manipulation that help a system infer a broader range
of behavior. To test these ideas, this proposal introduces a new system called Gamut that will
present the techniques in a unified software tool.
The first technique replaces the macro recorder method for demonstrating behavior used
in other PBD systems with a technique called nudges. The developer demonstrates by correcting
the system at important points during program execution and also using two nudge commands to
communicate important situations. First, the Do Something! nudge causes the system to reconsider
past learned behavior and try to generalize its knowledge to fit the current situation. Using the
Stop That! nudge will point out improper behavior and generate negative examples.
Second, Gamut will use a new deck-of-playing-cards metaphor to express concepts such
as randomness, sequencing, and data storage. By constructing an appropriate deck, shufing, sorting, and playing cards at key moments, developers can incorporate many effects not available without programming in other systems.
Third, Gamut will improve communication about behaviors by making them more manipulable than in previous systems. Behaviors will be represented as small icons near the objects they
affect. Using the familiar cut, copy, and paste commands, the developer can transfer behavior
between objects. Determining how to make a behavior operate in the new context will be inferred
automatically. An objects state from the recent past will be represented as temporal ghosts in
which objects become dimmed, translucent images. Many sorts of behavior refer to prior states
such as a previous position or an old property value. The ghost objects will allow the developer to
make explicit connections.
Finally, to reduce the number of options the system must explore, the developer will be
able to give hints by highlighting important objects and properties. A new inferencing algorithm
will be created that will take advantage of the hints.
By combining these techniques, Gamut will provide a rich medium for expressing developer intentions, fostering greater communication between the PBD system and the developer and
enabling the developer to create highly interactive software with minimal programming expertise.
//page
+PAGE+ 

//title
Problem Solving for Redesign
//author
Anita Pos 1 and Hans Akkermans 1 and Remco Straatman 2
//affiliation
1 University of Twente (UT)
Department of Computer Science
//address
P.O. Box 217
NL-7500 AE Enschede
The Netherlands
//email
E-mail: fpos,akkermang@cs.utwente.nl
//affiliation
2 University of Amsterdam (UvA)
Department of Social Science Informatics (SWI)
//address
Roetersstraat 15
1081 WB Amsterdam
The Netherlands
//email
E-mail: remco@swi.psy.uva.nl
//abstract
Abstract. A knowledge-level analysis of complex tasks like diagnosis and design can give us a better understanding of these tasks in terms of the goals they
aim to achieve and the different ways to achieve these goals. In this paper we
present a knowledge-level analysis of redesign. Redesign is viewed as a family of
methods based on some common principles, and a number of dimensions along
which redesign problem solving methods can vary are distinguished. By examining the problem-solving behavior of a number of existing redesign systems and approaches, we came up with a collection of problem-solving methods for redesign
and developed a task-method structure for redesign.
In constructing a system for redesign a large number of knowledge-related choices
and decisions are made. In order to describe all relevant choices in redesign problem solving, we have to extend the current notion of possible relations between
tasks and methods in a PSM architecture. The realization of a task by a problem-solving method, and the decomposition of a problem-solving method into subtasks are the most common relations in a PSM architecture. However, we suggest
to extend these relations with the notions of task refinement and method refinement. These notions represent intermediate decisions in a task-method structure,
in which the competence of a task or method is refined without immediately paying attention to its operationalization in terms of subtasks. Explicit representation
of this kind of intermediate decisions helps to make and represent decisions in a
more piecemeal fashion.
//intro
1 Introduction 

//title
Perfect Simulation of some Point Processes
for the Impatient User
//author
Elke Thonnes
//affiliation
Department of Statistics, University of Warwick
//date
February 9, 1998
//abstract
Abstract
Recently Propp and Wilson [14] have proposed an algorithm, called
Coupling from the Past (CFTP), which allows not only an approximate but perfect (i.e. exact) simulation of the stationary distribution
of certain finite state space Markov chains. Perfect Sampling using
CFTP has been successfully extended to the context of point processes, amongst other authors, by Haggstrom et al. [5]. In [5] Gibbs
sampling is applied to a bivariate point process, the penetrable spheres
mixture model [19]. However, in general the running time of CFTP
in terms of number of transitions is not independent of the state sampled. Thus an impatient user who aborts long runs may introduce a
subtle bias, the user impatience bias. Fill [3] introduced an exact sampling algorithm for finite state space Markov chains which, in contrast
to CFTP, is unbiased for user impatience. Fill's algorithm is a form
of rejection sampling and similar to CFTP requires sufficient mono-tonicity properties of the transition kernel used. We show how Fill's
version of rejection sampling can be extended to an infinite state space
context to produce an exact sample of the penetrable spheres mixture
process and related models. Following [5] we use Gibbs sampling and
make use of the partial order of the mixture model state space. Thus
//note
Research supported by EPSRC earmarked studentship and University of Warwick
graduate award. Postal address: 
//affiliation
Dept. of Statistics, University of Warwick, 
//address
Coventry,
CV4 7AL, UK
//page
+PAGE+ 

