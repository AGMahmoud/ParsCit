GIT-CC-92/60
+L+
A
Model-Based
Approach
to
+L+
Analogical
Reasoning
and
Learning
in
Design
+L+
Sambasiva
R.
Bhatta
+L+
bhatta@cc.gatech.edu
+L+
A
THESIS
PROPOSAL
+L+
Presented
to
+L+
The
Academic
Faculty
+L+
In
Partial
Fulfillment
+L+
of
the
Requirements
for
the
Degree
+L+
Doctor
of
Philosophy
+L+
in
Information
and
Computer
Science
+L+
The
Committee:
+L+
Dr.
Ashok
Goel
(Advisor)
+L+
Dr.
Richard
Catrambone
(Psy)
+L+
Dr.
T.
Govindaraj
(ISyE)
+L+
Dr.
Janet
Kolodner
+L+
Dr.
Ashwin
Ram
+L+
Georgia
Institute
of
Technology
+L+
November
1992
+L+
This
work
has
been
supported
by
research
grants
from
the
Office
of
Naval
Research
(contract
+L+
N00014-92-J-1234)
and
NSF,
a
CER
grant
from
NSF
(grant
CCR-86-19886),
and
equipment
+L+
donated
by
IBM,
NCR,
and
Symbolics.
+L+
+PAGE+

IEEE
TRANSACTIONS
ON
PARALLEL
AND
DISTRIBUTED
SYSTEMS,
VOL.
XX,
NO.
Y,
MONTH
1999
1
+L+
A
Trip-based
Multicasting
Model
in
+L+
Wormhole-routed
Networks
with
Virtual
Channels
+L+
Yu-Chee
Tseng
,
Dhabaleswar
K.
Panda
,
Member,
IEEE,
and
Ten-Hwang
Lai
,
Member,
IEEE
+L+
Abstract|
This
paper
focuses
on
efficient
multicasting
in
+L+
wormhole-routed
networks.
A
trip-based
model
is
proposed
+L+
to
support
adaptive,
distributed,
and
deadlock-free
multiple
+L+
multicast
on
any
network
with
arbitrary
topology
using
at
+L+
most
two
virtual
channels
per
physical
channel.
This
model
+L+
significantly
generalizes
the
path-based
model
proposed
earlier
[21],
[22],
which
works
only
for
Hamiltonian
networks
+L+
and
can
not
be
applicable
to
networks
with
arbitrary
topology
resulted
due
to
system
faults.
Fundamentals
of
the
trip-based
model,
including
the
necessary
and
sufficient
condition
to
be
deadlock-free,
and
the
use
of
appropriate
number
+L+
of
virtual
channels
to
avoid
deadlock
are
investigated.
The
+L+
potential
of
this
model
is
illustrated
by
applying
it
to
hyper-cubes
with
faulty
nodes.
Simulation
results
indicate
that
the
+L+
proposed
model
can
implement
multiple
multicast
on
faulty
+L+
hypercubes
with
negligible
performance
degradation.
+L+
Keywords|
Routing
algorithm,
Interprocessor
communication,
Multicast,
Virtual
channel,
Wormhole-routing,
+L+
Path-based
routing,
Collective
communication,
and
Fault
+L+
tolerance.
+L+
I.
Introduction

Submitted
to
SIGGRAPH95.
+L+
Space
Deformation
using
Ray
Deectors
+L+
Yair
Kurzion
and
Roni
Yagel
+L+
Department
of
Computer
and
Information
Science
+L+
The
Ohio
State
University
+L+
Abstract
+L+
In
this
paper
we
introduce
a
new
approach
to
the
deformation
of
surface
and
raster
+L+
models
in
two
and
three
dimensions.
Rather
then
deforming
the
objects
in
the
+L+
model,
we
deform
the
rays
used
to
render
the
scene.
The
mechanism
to
specify
the
+L+
deformation,
which
we
call
a
deector,
is
a
vector
of
gravity
positioned
in
space.
+L+
This
gravity
vector
bends
any
ray
that
travels
through
its
field
of
gravity.
Images
+L+
generated
by
these
curved
rays
give
the
impression
of
a
deformed
space.
Unlike
+L+
previous
methods
that
deform
all
the
objects
in
the
scene,
our
approach
deforms
+L+
only
those
parts
of
the
model
that
contribute
to
the
final
image.
In
addition,
using
+L+
deectors,
our
approach
can
deform
any
object
type
that
can
be
rendered
by
a
ray
+L+
casting
algorithm,
providing
a
unified
solution
to
space
deformation.
+L+
1.
Introduction

Efficient
Rasterization
of
Implicit
Functions
+L+
Torsten
Mller
and
Roni
Yagel
+L+
Department
of
Computer
and
Information
Science
+L+
The
Ohio
State
University
+L+
Columbus,
Ohio
+L+
-moeller,
yagel-@cis.ohio-state.edu
+L+
Abstract
+L+
Implicit
curves
are
widely
used
in
computer
graphics
because
of
their
powerful
features
for
modeling
and
their
ability
for
general
function
description.
The
most
popular
rasterization
techniques
for
implicit
curves
are
space
subdivision
and
curve
+L+
tracking.
In
this
paper
we
are
introducing
an
efficient
curve
tracking
algorithm
that
+L+
is
also
more
robust
then
existing
methods.
We
employ
the
Predictor-Corrector
+L+
Method
on
the
implicit
function
to
get
a
very
accurate
curve
approximation
in
a
+L+
short
time.
Speedup
is
achieved
by
adapting
the
step
size
to
the
curvature.
In
addition,
we
provide
mechanisms
to
detect
and
properly
handle
bifurcation
points,
+L+
where
the
curve
intersects
itself.
Finally,
the
algorithm
allows
the
user
to
trade-off
+L+
accuracy
for
speed
and
vice
a
versa.
We
conclude
by
providing
examples
that
dem-onstrate
the
capabilities
of
our
algorithm.
+L+
1.
Introduction

Alleviating
Consumption
Channel
Bottleneck
+L+
in
Wormhole-Routed
k-ary
n-cube
Systems
1
+L+
Debashis
Basak
and
Dhabaleswar
K.
Panda
+L+
Dept.
of
Computer
and
Information
Science
+L+
The
Ohio
State
University
+L+
Columbus,
OH
43210-1277
+L+
Tel:
(614)-292-5199,
Fax:
(614)-292-2911
+L+
Email:
fbasak,pandag@cis.ohio-state.edu
+L+
Abstract
+L+
This
paper
identifies
performance
degradation
in
wormhole
routed
k-ary
n-cube
networks
due
+L+
to
limited
number
of
router-to-processor
consumption
channels
at
each
node.
Many
recent
research
+L+
in
wormhole
routing
have
advocated
the
advantages
of
adaptive
routing
and
virtual
channel
flow
+L+
control
schemes
to
deliver
better
network
performance.
This
paper
indicates
that
the
advantages
+L+
associated
with
these
schemes
can
not
be
realized
with
limited
consumption
capacity.
To
alleviate
+L+
such
performance
bottleneck,
a
new
network
interface
design
using
multiple
consumption
channels
+L+
is
proposed.
To
match
virtual
multiplexing
on
network
channels,
we
also
propose
each
consumption
+L+
channel
to
support
multiple
virtual
consumption
channels.
The
impact
of
message
arrival
rate
at
+L+
a
node
on
the
required
number
of
consumption
channel
is
studied
analytically.
It
is
shown
that
+L+
wormhole
networks
with
higher
routing
adaptivity,
dimensionality,
degree
of
hot-spot
traffic,
and
+L+
number
of
virtual
lanes
have
to
take
advantage
of
multiple
consumption
channels
to
deliver
better
+L+
performance.
The
interplay
between
system
topology,
routing
algorithm,
number
of
virtual
lanes,
+L+
messaging
overheads,
and
communication
traffic
is
studied
through
simulation
to
derive
the
effective
+L+
number
of
consumption
channels
required
in
a
system.
Using
the
on-going
technological
trend,
it
is
+L+
shown
that
wormhole-routed
systems
can
use
up
to
2-4
consumption
channels
per
node
to
deliver
+L+
better
system
performance.
+L+
Keywords:
Parallel
computer
architecture,
wormhole
routing,
k-ary
n-cube,
consumption
Channel,
virtual
Channel,
deterministic
routing,
adaptive
routing,
hot-spot
traffic,
performance
evaluation,
and
interprocessor
communication.
+L+
1
This
research
is
supported
in
part
by
NSF
Grant
MIP-9309627,
Faculty
Early
Career
Development
Award
MIP-9502294,
and
an
Ohio
State
University
Presidential
Fellowship.
A
preliminary
version
of
this
paper[4]
has
been

A
Certificate
Path
Generation
Algorithm
for
+L+
Authenticated
Signaling
in
ATM
Networks
+L+
Jun
Xu
Mukesh
Singhal
+L+
Department
of
Computer
and
Information
Science
+L+
The
Ohio
State
University
+L+
Columbus,
OH
43210
+L+
fjun,singhalg@cis.ohio-state.edu
+L+
Abstract
+L+
ATM
Forum
specifies
public
key
cryptography
to
be
the
default
ATM
authentication
mechanism
and
directory
services
like
X.509
to
be
the
infrastructure
for
public
+L+
key
distribution
and
certification.
Authenticated
signaling,
widely
acknowledged
+L+
as
a
necessary
security
feature
of
ATM
network,
requires
the
signaling
message
to
+L+
be
authenticated
with
a
digital
signature
signed
by
the
private
key
of
the
calling
+L+
party.
To
verify
the
digital
signature,
the
called
party
needs
to
obtain
the
public
+L+
key
of
the
calling
party
and
a
proof
of
the
calling
party's
ownership
to
that
public
+L+
key.
In
X.509,
the
standard
form
of
such
a
proof
is
a
chain
of
public
key
certificates,
called
the
certificate
path
between
two
parties.
Certificate
exchange
protocol
+L+
(CEP),
proposed
by
ATM
Forum,
requires
that
another
bi-directional
connection
+L+
be
established
between
two
parties
to
exchange
public
keys
and
certificate
paths
+L+
before
an
authenticated
connection
can
be
set
up,
which
is
not
an
ideal
approach.
+L+
We
propose
an
algorithm
which
is
embedded
into
ATM
routing
protocols
to
generate
a
certificate
path
inside
a
signaling
message
on-the-fly
as
the
signaling
message
+L+
travels
through
the
ATM
network.
In
this
approach,
all
that
a
calling
party
needs
+L+
to
know
for
authentication
purpose
is
its
own
public
key
certificate
and
the
ATM
+L+
network
builds
the
rest
of
the
certificate
path
for
it.
Related
issues
like
distribution
+L+
of
public
key
certificates
and
optimization
of
CA
hierarchy
are
also
addressed
in
+L+
this
paper.
+L+
Keywords:
ATM,
P-NNI,
authenticated
signaling,
certificate
path.
+L+
This
work
was
partially
supported
by
NSA
Grant
MDA904-96-1-0111.
+L+
+PAGE+

A
Neural
Network
for
Attentional
Spotlight
+L+
Wee
Kheng
Leow
and
Risto
Miikkulainen
+L+
Technical
Report
AI91-165
+L+
Department
of
Computer
Sciences,
+L+
University
of
Texas
at
Austin,
Austin,
Texas
78712
+L+
leow@cs.utexas.edu,
risto@cs.utexas.edu
+L+
Abstract
+L+
According
to
space-based
theory,
visual
attention
is
limited
to
a
local
region
in
space
+L+
called
the
attentional
field.
Visual
information
within
the
attentional
field
is
enhanced
+L+
for
further
processing
while
information
outside
is
suppressed.
There
is
evidence
that
+L+
enhancement
and
suppression
are
achieved
with
dynamic
weighting
of
network
activity.
+L+
This
paper
discusses
a
neural
network
that
generates
the
appropriate
weights,
called
the
+L+
attentional
spotlight,
given
the
size
and
the
position
of
the
intended
attentional
field.
+L+
The
network
has
three
layers.
A
shunting
feedback
network
serves
as
the
output
layer
+L+
and
performs
a
critical
task
which
cannot
be
accomplished
by
feedforward
networks.
+L+
1
Introduction

Integrating
Topological
and
Metric
Maps
for
Mobile
Robot
Navigation:
+L+
A
Statistical
Approach
+L+
Sebastian
Thrun
1
,
Steffen
Gutmann
2
,
Dieter
Fox
3
,
Wolfram
Burgard
3
,
and
Benjamin
J.
Kuipers
4
+L+
1
Computer
Science
Department
2
Institut
fur
Informatik
3
Institut
fur
Informatik
III
4
Computer
Science
Department
+L+
Carnegie
Mellon
University
Universitat
Freiburg
University
of
Bonn
University
of
Texas
at
Austin
+L+
Pittsburgh,
PA
15213
D-79110
Freiburg,
Germany
D-53117
Bonn,
Germany
Austin,
TX
78712
+L+
submitted
to
AAAI-98
+L+
Abstract
+L+
The
problem
of
concurrent
mapping
and
localization
has
received
considerable
attention
in
the
mobile
robotics
community.
With
few
exceptions,
existing
approaches
can
largely
be
+L+
grouped
into
two
distinct
paradigms:
topological
and
metric.
+L+
This
paper
proposes
a
method
that
integrates
both
paradigms.
+L+
It
poses
the
mapping
problem
as
a
statistical
maximum
likelihood
problem,
and
devises
an
efficient
algorithm
for
search
+L+
in
likelihood
space.
Based
on
that,
it
presents
an
novel
mapping
algorithm
that
integrates
two
phases:
a
topological
and
a
+L+
metric
mapping
phase.
The
topological
mapping
phase
solves
a
+L+
global
position
alignment
problem
between
potentially
indistinguishable,
significant
places.
The
subsequent
metric
mapping
+L+
phase
produces
a
fine-grained
metric
map
of
the
environment
+L+
in
floating-point
resolution.
Experimental
results
in
cyclic
environments
of
sizes
up
to
80
by
25
meters
demonstrate
the
+L+
appropriateness
of
this
approach.
+L+
Introduction

Massively
Parallel
Computation
for
Three-Dimensional
+L+
Monte
Carlo
Semiconductor
Device
Simulation
+L+
Henry
Sheng
,
Roberto
Guerrieri
+L+
and
Alberto
Sangiovanni-Vincentelli
+L+
Department
of
Electrical
Engineering
and
Computer
Sciences
+L+
University
of
California,
Berkeley,
CA
94720,
U.S.A.
+L+
Dipartimento
di
Elettronica
e
Informatica
+L+
Universita
di
Bologna,
Italy
+L+
Abstract
+L+
This
work
presents
a
study
of
the
applicability
of
a
massively
parallel
computing
paradigm
+L+
to
Monte
Carlo
techniques
for
device
simulation.
A
unique
mapping
of
Monte
Carlo
to
SIMD
+L+
fine-grained
parallelism
has
been
developed,
decoupling
the
problem
into
separate
computational
domains.
For
MOSFET
simulation,
this
novel
mapping
allows
estimated
speeds
of
+L+
over
200,000
scatterings
processed
per
second
on
a
65,536
processor
Connection
Machine,
+L+
nearly
a
factor
of
six
over
the
fastest
known
to
date.
+L+
1
Introduction

On
Hamiltonian
Triangulations
in
Simple
Polygons
+L+
Giri
NARASIMHAN
1
+L+
e-mail:
giri@next1.msci.memphis.edu
+L+
Dept.
of
Mathematical
Sciences
+L+
The
University
of
Memphis
+L+
Memphis
TN
38152
+L+
Abstract
+L+
A
simple
polygon
P
is
said
to
have
a
Hamilitonian
Triangulation
if
it
has
a
triangulation
+L+
whose
dual
graph
contains
a
Hamiltonian
path.
Such
triangulations
are
useful
in
fast
+L+
rendering
engines
in
Computer
Graphics.
Arkin
et
al.
[AHMS]
observed
that
a
polygon
+L+
has
a
Hamiltonian
triangulation
if
and
only
if
it
is
Discretely
Straight
Walkable,
a
concept
+L+
that
is
a
discrete
version
of
Straight
Walkability
concept
as
introduced
by
Icking
and
Klein
+L+
[IK].
Using
this
characterization,
Arkin
et
al.
also
showed
an
algorithm
to
recognize
such
+L+
polygons
in
time
that
is
linear
in
the
size
of
the
visibility
polygon
of
the
given
polygon
P
.
+L+
The
size
of
the
visbility
polygon
of
P
could
be
quadratic
in
the
size
of
P
and
hence
their
+L+
algorithm
could
be
very
inefficient
even
for
nearly
convex
polygons.
+L+
We
give
a
new
characterization
of
polygons
with
Hamiltonian
triangulations.
We
use
+L+
this
characterization
to
present
the
following
algorithms:
+L+
*
An
O(n
log
n)-time
algorithm
to
recognize
polygons
with
a
Hamiltonian
triangulation.
+L+
*
An
O(n
log
n)-time
algorithm
to
construct
such
a
triangulation.
+L+
*
Given
vertices
p
and
q
on
a
simple
polygon,
an
O(n)-time
algorithm
to
determine
+L+
whether
the
polygon
is
discretely
straight
walkable
with
respect
to
the
two
vertices.
+L+
*
An
O(n
log
n)-time
algorithm
to
list
out
all
pairs
of
points
on
a
simple
polygon
with
+L+
respect
to
which
the
polygon
is
discretely
straight
walkable.
+L+
References
+L+
[IK]
C.
Icking
and
R.
Klein,
"The
two
guards
problem,"
Proc.
7th
Annual
ACM
Symp.
on
+L+
Computational
Geometry,
1991,
pp.
166-175.
+L+
[AHMS]
E.M.
Arkin,
M.
Held,
J.S.B.
Mitchell,
S.S.
Skienna,
"Hamiltonian
Triangulations
+L+
for
Fast
Rendering,"
Proc.
of
the
2nd
ESA,
1994.
+L+
1
Supported
in
part
by
NSF
Grant
CCR-940-9752

On
the
Robustness
of
the
Damped
V
-cycle
+L+
of
the
Wavelet
Frequency
Decomposition
+L+
Multigrid
Method
+L+
Andreas
Rieder
1fl
+L+
Xiaodong
Zhou
1
+L+
December
15,
1993
+L+
Abstract
+L+
The
V
-cycle
of
the
wavelet
variation
of
the
"Frequency
decomposition
multigrid
method"
of
Hackbusch
[Numer.
Math.,
56,
pp.
+L+
229-245,
1989]
is
considered.
+L+
It
is
shown
that
its
convergence
speed
is
not
affected
by
the
presence
of
anisotropy
provided
that
the
corresponding
coarse
grid
correction
is
damped
sufficiently
strong.
Our
analysis
is
based
on
properties
+L+
of
wavelet
packets
which
are
supplied
and
proved.
+L+
Numerical
approximations
to
the
speed
of
convergence
illustrate
+L+
the
theoretical
results.
+L+
Key
words:
wavelets,
wavelet
packets,
robust
multilevel
methods,
V
-cycle
+L+
Subject
classification:
AMS(MOS)
65F10,
65N30
+L+
1
supported
partially
by
AFOSR
under
grant
number
90-0334
which
was
funded
by
+L+
DARPA
+L+
partially
supported
by
a
Feodor
Lynen-Fellowship
of
the
Alexander
von
Humboldt
+L+
Foundation
+L+
+PAGE+

Neuronal
Goals:
Efficient
Coding
and
Coincidence
Detection
+L+
Nathan
Intrator
+L+
School
of
Mathematical
Sciences
+L+
Tel
Aviv
University
+L+
nin@cns.brown.edu
+L+
Abstract|
Barlow's
seminal
work
on
minimal
entropy
codes
and
unsupervised
learning
is
+L+
reiterated.
In
particular,
the
need
to
transmit
the
probability
of
events
is
put
in
a
practical
+L+
neuronal
framework
for
detecting
suspicious
events.
A
variant
of
the
BCM
learning
rule
[15]
+L+
is
presented
together
with
some
mathematical
results
suggesting
optimal
minimal
entropy
+L+
coding.
+L+
Key
words:
Sparse
coding,
Non-Gaussian
distributions,
BCM
Theory,
Minimal
Entropy
+L+
1
Introduction

Evolving
Visually
Guided
Robots
+L+
D.
Cliff
,
P.
Husbands
,
I.
Harvey
+L+
CSRP
220,
July
1992
+L+
Cognitive
Science
Research
Paper
+L+
Serial
No.
CSRP
220
+L+
The
University
of
Sussex
+L+
School
of
Cognitive
and
Computing
Sciences
+L+
Falmer
+L+
Brighton
BN1
9QH
+L+
England,
U.K.
+L+
A
version
of
this
paper
appears
in:
+L+
Proceedings
of
SAB92,
+L+
the
Second
International
Conference
on
Simulation
of
Adaptive
Behaviour
+L+
J.-A.
Meyer,
H.
Roitblat,
and
S.
Wilson,
editors,
+L+
MIT
Press
Bradford
Books,
Cambridge,
MA,
1993.
+L+
+PAGE+

Copyright
1993
ACM
0-8186-4340-4/93/0011
Permission
to
copy
for
non-commercial
use
granted
by
the
Association
for
Computing
Machinery.
+L+
MPI:
A
Message
Passing
Interface
+L+
The
MPI
Forum
+L+
This
paper
presents
an
overview
of
mpi,
a
proposed
+L+
standard
message
passing
interface
for
MIMD
distributed
memory
concurrent
computers.
The
design
+L+
of
mpi
has
been
a
collective
effort
involving
researchers
+L+
in
the
United
States
and
Europe
from
many
organizations
and
institutions.
mpi
includes
point-to-point
+L+
and
collective
communication
routines,
as
well
as
support
for
process
groups,
communication
contexts,
and
+L+
application
topologies.
While
making
use
of
new
ideas
+L+
where
appropriate,
the
mpi
standard
is
based
largely
+L+
on
current
practice.
+L+
1
Introduction

Reactive
Functional
Programming
+L+
Richard
B.
Kieburtz
+L+
Oregon
Graduate
Institute
of
Science
&
Technology
+L+
P.O.
Box
91000,
Portland,
OR
97291-1000
USA
+L+
October
13,
1997
+L+
Abstract
+L+
Reactive
systems
respond
to
concurrent,
possibly
unsynchronized
streams
of
input
events.
Programming
+L+
reactive
systems
is
challenging
without
language
support
for
event-triggered
actions.
It
is
even
more
+L+
challenging
to
reason
about
reactive
systems.
This
paper
explores
a
new
conceptual
basis
for
applying
+L+
functional
programming
techniques
to
the
design
and
formal
verification
of
reactive
systems.
The
+L+
mathematical
foundation
for
this
approach
is
based
upon
signature
coalgebras
and
derived
proof
rules
+L+
for
coinduction.
The
concepts
are
illustrated
with
an
example
that
has
been
used
with
the
language
+L+
Esterel.
+L+
1
Introduction

Comparison
of
Statistical
and
Neural
Classifiers
+L+
and
Their
Applications
to
+L+
Optical
Character
Recognition
and
Speech
Classification
+L+
Ethem
Alpaydn
,
Fikret
Gurgen
+L+
Department
of
Computer
Engineering
+L+
Bogazi~ci
University
+L+
TR-80815
_
Istanbul
Turkey
+L+
falpaydin,gurgeng@boun.edu.tr
+L+
Neural
Network
Systems
Techniques
and
Applications
(in
print)
+L+
C.
T.
Leondes
(Ed.),
c
flACADEMIC
Press
+L+
October
24,
1996
+L+
Abstract
+L+
We
give
a
review
of
basic
statistical
and
neural
techniques
for
classification.
Statistical
techniques
are
based
on
the
idea
of
estimating
class-conditional
likelihoods
and
using
Bayes
rule
+L+
to
convert
these
to
posterior
class
probabilities
whereas
neural
techniques
estimate
directly
the
+L+
posteriors.
Statistical
techniques
include
(i)
Parametric
(Gaussian)
Bayes
classifiers,
(ii)
Non-parametric
kernel-based
density
estimators
like
k-nearest
neighbor
and
Parzen
windows,
and
+L+
(iii)
mixtures
of
(Gaussian)
densities
(a
special
case
of
which
is
the
Learning
Vector
Quantization).
As
neural
classifiers,
we
include
simple
perceptrons
and
multilayer
perceptrons
with
+L+
sigmoid
and
Gaussian
hidden
units.
The
neural
and
statistical
techniques
are
quite
similar
in
+L+
many
respects
and
many
approaches
have
been
discovered
independently
twice,
once
in
1960s
+L+
by
statisticians
and
once
in
1980s
by
the
neural
network
researchers.
One
of
the
aims
of
this
article
is
to
make
this
link
more
apparent.
We
also
discuss
two,
most
popular,
pattern
recognition
+L+
applications:
Optical
character
recognition
and
speech
recognition.
Though
they
seem
different,
+L+
in
many
respects,
the
two
applications
are
similar
and
in
the
past,
almost
the
same
techniques
+L+
have
been
applied
for
their
implementation.
We
implement
the
well
known
statistical
and
neural
+L+
classification
techniques
for
two
datasets
of
these
applications
and
compare
them
in
terms
of
+L+
generalization
accuracy,
memory
requirement
and
learning
time.
We
especially
advise
to
take
+L+
into
account
statistics
of
the
sample
even
if
a
neural
classifier
is
to
be
used.
The
similarity
+L+
between
statistical
and
neural
techniques
is
greater
than
generally
agreed
and
simple
statistical
+L+
methods
like
k-NN
perform
generally
quite
well
and
much
of
the
functionality
of
neural
networks
like
distributed
parallel
computation
can
be
obtained
by
such
methods
without
requiring
+L+
complicated
computation
and
precise
error
minimization
procedures.
+L+
Keywords|
Statistical
pattern
recognition,
artificial
neural
networks,
optical
character
recog
+L+
nition,
speech
recognition,
Bayes
decision
theory,
nonparametric
estimation.
+L+
+PAGE+

DIMACS
Technical
Report
97-15
+L+
April
1997
+L+
(Revised
August
1997)
+L+
Crowds:
Anonymity
for
Web
Transactions
+L+
by
+L+
Michael
K.
Reiter
1
Aviel
D.
Rubin
2
+L+
AT&T
Labs|Research,
Murray
Hill,
New
Jersey,
USA
+L+
freiter,rubing@research.att.com
+L+
1
Permanent
Member
+L+
2
Permanent
Member
+L+
DIMACS
is
a
partnership
of
Rutgers
University,
Princeton
University,
AT&T
Labs,
Bellcore,
and
Bell
Labs.
+L+
DIMACS
is
an
NSF
Science
and
Technology
Center,
funded
under
contract
STC-91-19999;
and
also
receives
+L+
support
from
the
New
Jersey
Commission
on
Science
and
Technology.
+L+
+PAGE+

Kit:
A
Study
in
+L+
Operating
System
Verification
+L+
William
R.
Bevier
+L+
Technical
Report
28
August,
1988
+L+
Computational
Logic
Inc.
+L+
1717
W.
6th
St.
Suite
290
+L+
Austin,
Texas
78703
+L+
(512)
322-9951
+L+
This
research
was
supported
in
part
by
the
U.S.
+L+
Government.
The
views
and
conclusions
contained
in
this
+L+
document
are
those
of
the
author
and
should
not
be
+L+
interpreted
as
representing
the
official
policies,
either
+L+
expressed
or
implied,
of
the
Defense
Advanced
Research
+L+
Projects
Agency
or
the
U.S.
Government.
This
work
was
+L+
sponsored
in
part
at
Computational
Logic,
Inc.
by
the
+L+
Defense
Advanced
Research
Projects
Agency,
ARPA
+L+
Orders
6082
and
9151,
and
at
the
University
of
Texas
at
+L+
Austin
by
the
Defense
Advanced
Research
Projects
+L+
Agency,
ARPA
Order
5246,
issued
by
the
Space
and
+L+
Naval
Warfare
Systems
Command
under
Contract
+L+
N00039-85-K-0085.
+L+
+PAGE+

LISP
AND
SYMBOLIC
COMPUTATION:
An
International
Journal,
5,
191-221,
1992
+L+
c
1992
Kluwer
Academic
Publishers
Manufactured
in
The
Netherlands
+L+
Callee-save
Registers
in
Continuation-passing
Style
+L+
ANDREW
W.
APPEL
(appel@princeton.edu)
+L+
ZHONG
SHAO
(zsh@princeton.edu)
+L+
Department
of
Computer
Science,
Princeton
University,
Princeton,
NJ
08544-2087
+L+
Keywords:
Register
Allocation,
Continuation-passing
Style,
Procedure
Call
+L+
Abstract.
Continuation-passing
style
(CPS)
is
a
good
abstract
representation
to
use
+L+
for
compilation
and
optimization:
it
has
a
clean
semantics
and
is
easily
manipulated.
+L+
We
examine
how
CPS
expresses
the
saving
and
restoring
of
registers
in
source-language
+L+
procedure
calls.
In
most
CPS-based
compilers,
the
context
of
the
calling
procedure
is
+L+
saved
in
a
"continuation
closure"|a
single
variable
that
is
passed
as
an
argument
to
the
+L+
function
being
called.
This
closure
is
a
record
containing
bindings
of
all
the
free
variables
+L+
of
the
continuation;
that
is,
registers
that
hold
values
needed
by
the
caller
"after
the
call"
+L+
are
written
to
memory
in
the
closure,
and
fetched
back
after
the
call.
+L+
Consider
the
procedure-call
mechanisms
used
by
conventional
compilers.
In
particular,
+L+
registers
holding
values
needed
after
the
call
must
be
saved
and
later
restored.
The
+L+
responsibility
for
saving
registers
can
lie
with
the
caller
(a
"caller-saves"
convention)
+L+
or
with
the
called
function
("callee-saves").
In
practice,
to
optimize
memory
traffic,
+L+
compilers
find
it
useful
to
have
some
caller-saves
registers
and
some
callee-saves.
+L+
"Conventional"
CPS-based
compilers
that
pass
a
pointer
to
a
record
containing
all
+L+
the
variables
needed
after
the
call
(i.e.,
the
continuation
closure),
are
using
a
caller-saves
+L+
convention.
We
explain
how
to
express
callee-save
registers
in
Continuation-Passing
+L+
Style,
and
give
measurements
showing
the
resulting
improvement
in
execution
time.
+L+
1.
Introduction

Discovering
Compressive
Partial
Determinations
+L+
in
Mixed
Numerical
and
Symbolic
Domains
+L+
Bernhard
Pfahringer
and
Stefan
Kramer
+L+
Austrian
Research
Institute
for
Artificial
Intelligence
+L+
Schottengasse
3
+L+
A-1010
Vienna,
Austria
+L+
fbernhard,
stefang@ai.univie.ac.at
+L+
Abstract
+L+
Partial
determinations
are
an
interesting
+L+
form
of
dependency
between
attributes
in
a
+L+
relation.
They
generalize
functional
dependencies
by
allowing
exceptions.
We
modify
a
known
MDL
formula
for
evaluating
+L+
such
partial
determinations
to
allow
for
its
+L+
use
in
an
admissible
heuristic
in
exhaustive
+L+
search.
Furthermore
we
describe
an
efficient
+L+
preprocessing-based
approach
for
handling
+L+
numerical
attributes.
An
empirical
investigation
tries
to
evaluate
the
viability
of
the
+L+
presented
ideas.
+L+
1
Introduction

A
Knowledge-Sharing
Strategy
+L+
David
Goldstein
and
Albert
Esterline
+L+
North
Carolina
A&T
State
University
+L+
Greensboro,
North
Carolina
+L+
goldstn
@ncat.edu,
esterlin@ncat.edu
+L+
Introduction

Core
Selection
Methods
for
+L+
Multicast
Routing
+L+
Kenneth
L.
Calvert
Ellen
W.
Zegura
+L+
Michael
J.
Donahoo
+L+
GIT-CC-95/15
+L+
Abstract
+L+
Multicast
routing
is
an
important
topic
of
both
theoretical
and
practical
+L+
interest.
Some
recently-proposed
multicast
routing
algorithms
involve
the
+L+
designation
of
one
or
more
network
nodes
as
the
"center"
of
the
routing
+L+
tree
for
each
multicast
group
address.
The
choice
of
this
designated
router
+L+
(which
we
refer
to
as
the
"core")
influences
the
shape
of
the
multicast
routing
+L+
tree,
and
thus
influences
performance
of
the
routing
scheme.
In
this
paper
we
+L+
investigate
the
relationship
between
the
choice
of
core
and
three
performance
+L+
measures.
Specifically,
we
compare
various
methods
of
selecting
a
core
with
+L+
respect
to
their
effect
on
bandwidth,
delay,
and
traffic
concentration.
We
+L+
conclude
that
simple
methods
are
adequate
for
widely
distributed
groups,
+L+
but
that
the
addition
of
group
information
can
be
leveraged
to
improve
+L+
performance
especially
when
the
group
is
small
or
exhibits
a
high
degree
+L+
of
locality.
We
also
conclude
that
core
choice
has
a
significant
impact
on
+L+
traffic
concentration,
in
fact
traffic
concentration
effects
can
be
ameliorated
+L+
by
appropriate
core
choice
policies.
+L+
Keywords:
Multicast
routing,
Scalability,
Network
modeling
+L+
College
of
Computing
+L+
Georgia
Institute
of
Technology
+L+
Atlanta,
Georgia
30332-0280
+L+
+PAGE+

Implementing
Schema-theoretic
Models
of
+L+
Animal
Behavior
in
Robotic
Systems
+L+
Khaled
S.
Ali
and
Ronald
C.
Arkin
+L+
Mobile
Robot
Laboratory
+L+
College
of
Computing
+L+
Georgia
Institute
of
Technology
+L+
Atlanta,
GA,
30332-0281
USA
+L+
fkali,arking@cc.gatech.edu
+L+
Abstract
+L+
Formal
models
of
animal
sensorimotor
behavior
can
+L+
provide
effective
methods
for
generating
robotic
intelligence.
In
this
paper
we
describe
how
schema-theoretic
+L+
models
of
the
praying
mantis
are
implemented
on
a
+L+
hexapod
robot
equipped
with
a
real-time
color
vision
+L+
system.
The
model
upon
which
the
implementation
+L+
is
based
was
developed
by
ethologists
studying
man-tids.
This
implementation
incorporates
a
wide
range
+L+
of
behaviors,
including
obstacle
avoidance,
prey
acquisition,
predator
avoidance,
mating,
and
chantlitaxia
+L+
behaviors.
+L+
1
Introduction

Hypertextual
Concurrent
Control
+L+
of
a
Lisp
Kernel
+L+
P.
David
Stotts
Richard
Furuta
+L+
Department
of
Computer
and
Department
of
Computer
Science
and
+L+
Information
Sciences
Institute
for
Advanced
Computer
Studies
+L+
University
of
Florida
University
of
Maryland
+L+
Gainesville,
FL
32611
College
Park,
MD
20742
+L+
Abstract
+L+
Using
the
Trellis
human/computer
interaction
model
as
an
implementation
vehicle,
we
demonstrate
+L+
how
to
use
concurrency-supporting
hypertext
to
provide
visual
displays
of
the
execution
flows
through
+L+
a
parallel
Lisp
program.
In
addition
to
displays,
the
hypertext
interface
allows
injection
of
control
+L+
flow
into
an
otherwise
functional
computation,
and
therefore
provides
reader
control
over
the
order
of
+L+
evaluation
of
expressions.
The
resulting
system,
termed
Trellis,
can
be
thought
of
as
a
concurrent
control
+L+
flow
browser
for
composing
functional
computations,
providing
a
visual
implementation
of
kernel-control
+L+
decomposition.
The
advantages
of
Trellis
are
ease
of
exploring
program
side
effects;
ease
of
debugging
+L+
parallel
code;
aid
in
teaching
functional
languages;
and
the
ability
to
construct
hypertext
documents
+L+
that
have
parallel
execution
semantics
and
flexible
browsing
behaviors.
+L+
Key
words:
functional
programming,
parallelism,
kernel-control
decomposition,
Lisp,
hypertext,
exe
+L+
cution
visualization.
+L+
1
Introduction

Selection
Predicate
Indexing
for
Active
Databases
+L+
Using
Interval
Skip
Lists
+L+
Eric
N.
Hanson
+L+
Theodore
Johnson
+L+
Computer
and
Information
Sciences
Department
+L+
University
of
Florida
+L+
Gainesville,
FL
32611
+L+
fhanson,tedg@cis.ufl.edu
+L+
TR94-017
+L+
15
April
1994
+L+
(revised
13
October
1994)
+L+
Abstract
+L+
A
new,
efficient
selection
predicate
indexing
scheme
for
active
database
systems
is
introduced.
+L+
The
selection
predicate
index
proposed
uses
an
interval
index
on
an
attribute
of
a
relation
or
+L+
object
collection
when
one
or
more
rule
condition
clauses
are
defined
on
that
attribute.
The
+L+
selection
predicate
index
uses
a
new
type
of
interval
index
called
the
interval
skip
list
(IS-list).
+L+
The
IS-list
is
designed
to
allow
efficient
retrieval
of
all
intervals
that
overlap
a
point,
while
allowing
+L+
dynamic
insertion
and
deletion
of
intervals.
IS-list
algorithms
are
described
in
detail.
The
IS-list
+L+
allows
efficient
on-line
searches,
insertions,
and
deletions,
yet
is
much
simpler
to
implement
than
+L+
other
comparable
interval
index
data
structures
such
as
the
priority
search
tree
and
balanced
+L+
interval
binary
search
tree
(IBS-tree).
IS-lists
require
only
one
third
as
much
code
to
implement
+L+
as
balanced
IBS-trees.
The
combination
of
simplicity,
performance,
and
dynamic
updateability
+L+
of
the
IS-list
is
unmatched
by
any
other
interval
index
data
structure.
This
makes
the
IS-list
a
+L+
good
interval
index
structure
for
implementation
in
an
active
database
predicate
index.
+L+
1
Introduction

Quality
Management
of
Information
Systems
Development
+L+
Geoff
Beckworth,
+L+
email:
gbeck@deakin.edu.au
+L+
Brian
Garner
+L+
email:
brian@deakin.edu.au
+L+
School
of
Computing
and
Mathematics
+L+
Deakin
University,
Geelong,
Victoria,
3217,
Australia.
+L+
Abstract
+L+
The
role
of
the
systems
analyst
in
the
implementation
process
has
changed
+L+
dramatically
in
recent
times
because
of
changes
to
organisational
boundaries,
the
+L+
need
to
align
IT
with
business
objectives
and
the
complexity
of
the
systems
now
+L+
required.
Some
organisations
are
finding
themselves
in
a
continually
changing
+L+
environment
and
being
involved
in
multi-organisational
structures.
Establishing
+L+
strategies
and
requirements
for
these
organisations
requires
a
new
understanding
+L+
of
the
implementation
process.
Implementation
is
concerned
with
behavioural
+L+
phenomena
since
people
are
involved
from
the
inception
of
the
idea,
as
well
as
+L+
being
involved
in
the
development
process.
They
are
also
affected
by
the
changes
+L+
which
the
new
system
brings
to
the
organisation.
The
research
is
attempting
to
+L+
understand
the
critical
factors
associated
with
the
implementation
process
and
+L+
consequently
develop
an
appropriate
model.
+L+
1.
Introduction

In
press:
The
Neurobiology
of
Computation:
Proceedings
of
the
Annual
Compu--tational
Neuroscience
Meeting.
J.M.
Bower,
ed.
Kluwer
Academic
Publishers,
+L+
Boston.
+L+
UNSUPERVISED
LEARNING
OF
+L+
INVARIANT
REPRESENTATIONS
OF
FACES
+L+
THROUGH
TEMPORAL
ASSOCIATION
+L+
Marian
Stewart
Bartlett
;
+L+
Terrence
J.
Sejnowski
;
+L+
marni@salk.edu,
terry@salk.edu
+L+
Departments
of
Cognitive
Science
and
Psychology,
UCSD
+L+
Howard
Hughes
Medical
Institute
+L+
The
Salk
Institute,
La
Jolla,
CA,
92037
+L+
Abstract
+L+
The
appearance
of
an
object
or
a
face
changes
continuously
as
the
observer
+L+
moves
through
the
environment
or
as
a
face
changes
expression
or
pose.
Recognizing
an
object
or
a
face
despite
these
image
changes
is
a
challenging
problem
+L+
for
computer
vision
systems,
yet
we
perform
the
task
quickly
and
easily.
This
+L+
simulation
investigates
the
ability
of
an
unsupervised
learning
mechanism
to
+L+
acquire
representations
that
are
tolerant
to
such
changes
in
the
image.
The
+L+
learning
mechanism
finds
these
representations
by
capturing
temporal
relationships
between
2-D
patterns.
Previous
models
of
temporal
association
learning
+L+
have
used
idealized
input
representations.
The
input
to
this
model
consists
of
+L+
graylevel
images
of
faces.
A
two-layer
network
learned
face
representations
that
+L+
incorporated
changes
of
pose
up
to
30
ffi
.
A
second
network
learned
representations
that
were
independent
of
facial
expression.
+L+
Introduction

Displaying
3D
Images:
Algorithms
for
+L+
Single
Image
Random
Dot
Stereograms
+L+
Harold
W.
Thimbleby
,
Stuart
Inglis
,
and
Ian
H.
Witten
*
+L+
Abstract
+L+
This
paper
describes
how
to
generate
a
single
image
which,
when
viewed
in
the
+L+
appropriate
way,
appears
to
the
brain
as
a
3D
scene.
The
image
is
a
stereogram
composed
+L+
of
seemingly
random
dots.
A
new,
simple
and
symmetric
algorithm
for
generating
such
+L+
images
from
a
solid
model
is
given,
along
with
the
design
parameters
and
their
influence
+L+
on
the
display.
The
algorithm
improves
on
previously-described
ones
in
several
ways:
it
+L+
is
symmetric
and
hence
free
from
directional
(right-to-left
or
left-to-right)
bias,
it
corrects
+L+
a
slight
distortion
in
the
rendering
of
depth,
it
removes
hidden
parts
of
surfaces,
and
it
+L+
also
eliminates
a
type
of
artifact
that
we
call
an
echo.
+L+
Random
dot
stereograms
have
one
remaining
problem:
difficulty
of
initial
viewing.
If
+L+
a
computer
screen
rather
than
paper
is
used
for
output,
the
problem
can
be
ameliorated
by
+L+
shimmering,
or
time-multiplexing
of
pixel
values.
We
also
describe
a
simple
+L+
computational
technique
for
determining
what
is
present
in
a
stereogram
so
that,
if
+L+
viewing
is
difficult,
one
can
ascertain
what
to
look
for.
+L+
Keywords:
Single
image
random
dot
stereograms,
SIRDS,
autostereograms,
+L+
stereoscopic
pictures,
optical
illusions
+L+
Department
of
Psychology,
University
of
Stirling,
Stirling,
Scotland.
Phone
(+44)
786-467679;
fax
+L+
786-467641;
email
hwt@compsci.stirling.ac.uk
+L+
Department
of
Computer
Science,
University
of
Waikato,
Hamilton,
New
Zealand.
Phone
(+64
7)
+L+
856-2889;
fax
838-4155;
email
singlis@waikato.ac.NZ.
+L+
Department
of
Computer
Science,
University
of
Waikato,
Hamilton,
New
Zealand.
Phone
(+64
7)
+L+
838-4246;
fax
838-4155;
email
ihw@waikato.ac.NZ.
+L+
*
Please
address
all
correspondence
to
Ian
H.
Witten
+L+
+PAGE+

Development
of
an
Intelligent
Monitoring
and
Control
System
for
a
+L+
Heterogeneous
Numerical
Propulsion
System
Simulation
+L+
Abdollah
A.
Afjeh
*
,
Patrick
T.
Homer
,
Henry
Lewandowski
,
+L+
John
A.
Reed
*
,
and
Richard
D.
Schlichting
+L+
Cleveland
State
University
,
The
University
of
Arizona
,
University
of
Toledo
*
+L+
Abstract
+L+
The
NASA
Numerical
Propulsion
System
Simulation
+L+
(NPSS)
project
is
exploring
the
use
of
computer
simulation
+L+
to
facilitate
the
design
of
new
jet
engines.
Several
key
issues
+L+
raised
in
this
research
are
being
examined
in
an
NPSS-related
research
project:
zooming,
monitoring
and
control,
+L+
and
support
for
heterogeneity.
The
design
of
a
simulation
+L+
executive
that
addresses
each
of
these
issues
is
described.
+L+
In
this
work,
the
strategy
of
zooming,
which
allows
codes
+L+
that
model
at
different
levels
of
fidelity
to
be
integrated
+L+
within
a
single
simulation,
is
applied
to
the
fan
component
+L+
of
a
turbofan
propulsion
system.
A
prototype
monitoring
+L+
and
control
system
has
been
designed
for
this
simulation
to
+L+
support
experimentation
with
expert
system
techniques
for
+L+
active
control
of
the
simulation.
An
interconnection
system
+L+
provides
a
transparent
means
of
connecting
the
heterogeneous
systems
that
comprise
the
prototype.
+L+
1.
Introduction

USENIX
Summer
Conference
+L+
June
11-15,
1990
+L+
Anaheim,
California
+L+
Why
Aren't
+L+
Operating
Systems
+L+
Getting
Faster
As
+L+
Fast
as
Hardware?
+L+
John
K.
Ousterhout
University
of
California
at
Berkeley
+L+
ABSTRACT
+L+
This
paper
evaluates
several
hardware
platforms
and
operating
systems
using
a
set
of
benchmarks
+L+
that
stress
kernel
entry/exit,
file
systems,
and
other
things
related
to
operating
systems.
The
+L+
overall
conclusion
is
that
operating
system
performance
is
not
improving
at
the
same
rate
as
the
+L+
base
speed
of
the
underlying
hardware.
The
most
obvious
ways
to
remedy
this
situation
are
to
+L+
improve
memory
bandwidth
and
reduce
operating
systems'
tendency
to
wait
for
disk
operations
to
+L+
complete.
+L+
1.
Introduction

Submitted
to
EuroGP-98,
Paris,
16-17
April,
1998
+L+
Genetic
Programming
Bloat
with
Dynamic
Fitness
+L+
W.
B.
Langdon
and
R.
Poli
+L+
School
of
Computer
Science,
University
of
Birmingham,
Birmingham
B15
2TT,
UK
+L+
fW.B.Langdon,R.Polig@cs.bham.ac.uk
http://www.cs.bham.ac.uk/~wbl,
~rmp
+L+
Tel:
+44
(0)
121
414
4791,
Fax:
+44
(0)
121
414
4281
+L+
Technical
Report:
CSRP-97-29,
3
December
1997
+L+
Abstract
+L+
In
artificial
evolution
individuals
which
perform
as
their
parents
are
usually
rewarded
identically
+L+
to
their
parents.
We
note
that
Nature
is
more
dynamic
and
there
may
be
a
penalty
to
pay
for
doing
+L+
the
same
thing
as
your
parents.
We
report
two
sets
of
experiments
where
static
fitness
functions
+L+
are
firstly
augmented
by
a
penalty
for
unchanged
offspring
and
secondly
the
static
fitness
case
+L+
is
replaced
by
randomly
generated
dynamic
test
cases.
We
conclude
genetic
programming,
when
+L+
evolving
artificial
ant
control
programs,
is
surprisingly
little
effected
by
large
penalties
and
program
+L+
growth
is
observed
in
all
our
experiments.
+L+
1
Introduction

Technical
Report
CSRP-98-13
+L+
School
of
Computer
Science,
The
University
of
Birmingham
+L+
GP-Music:
An
Interactive
Genetic
Programming
System
for
+L+
Music
Generation
with
Automated
Fitness
Raters
+L+
Brad
Johanson
+L+
Stanford
University
+L+
Rains
Apt.
9A
+L+
704
Campus
Dr.
+L+
Stanford,
CA.
94305
+L+
bjohanso@stanford.edu
+L+
650-497-7543
+L+
Riccardo
Poli
+L+
University
of
Birmingham
+L+
School
of
Computer
Science
+L+
The
University
of
Birmingham
+L+
Birmingham
B15
2TT
+L+
R.Poli@cs.bham.ac.uk
+L+
+44-121-414-3739
+L+
Abstract
+L+
In
this
paper
we
present
the
GP-Music
System,
an
interactive
system
which
allows
users
to
evolve
short
musical
sequences
+L+
using
interactive
genetic
programming,
and
its
extensions
aimed
at
making
the
system
fully
automated.
The
basic
GP-system
works
by
using
a
genetic
programming
algorithm,
a
small
set
of
functions
for
creating
musical
sequences,
and
a
user
+L+
interface
which
allows
the
user
to
rate
individual
sequences.
With
this
user
interactive
technique
it
was
possible
to
generate
+L+
pleasant
tunes
over
runs
of
20
individuals
over
10
generations.
As
the
user
is
the
bottleneck
in
interactive
systems,
the
+L+
system
takes
rating
data
from
a
users
run
and
uses
it
to
train
a
neural
network
based
automatic
rater,
or
auto
rater,
which
+L+
can
replace
the
user
in
bigger
runs.
Using
this
auto
rater
we
were
able
to
make
runs
of
up
to
50
generations
with
500
+L+
individuals
per
generation.
The
best
of
run
pieces
generated
by
the
auto
raters
were
pleasant
but
were
not,
in
general,
as
+L+
nice
as
those
generated
in
user
interactive
runs.
+L+
1
Introduction

A
General
Approach
to
+L+
Performance
Analysis
and
Optimization
+L+
of
Asynchronous
Circuits
+L+
Thesis
by
+L+
Tak
Kwan
Lee
+L+
In
Partial
Fulfillment
of
the
Requirements
+L+
for
the
Degree
of
+L+
Doctor
of
Philosophy
+L+
California
Institute
of
Technology
+L+
Pasadena,
California
+L+
1995
+L+
(Submitted
May
18,
1995)
+L+
+PAGE+

From
the
Proc.
of
the
1996
Inter.
Conf.
on
Requirements
Engineering,
Colorado
Springs,
Colorado,
April
15-18,
1996
+L+
A
Facilitator
Method
for
Upstream
Design
Activities
+L+
with
Diverse
Stakeholders
+L+
Regina
M.
Gonzales
and
Alexander
L.
Wolf
+L+
Software
Engineering
Research
Laboratory
+L+
Department
of
Computer
Science
+L+
University
of
Colorado
+L+
Boulder,
CO
80309
USA
+L+
fregina.gonzales,alwg@cs.colorado.edu
+L+
Abstract
+L+
This
paper
presents
a
method
that
can
be
used
for
+L+
the
elicitation
and
specification
of
requirements
and
+L+
high-level
design.
It
supports
stakeholder-based
modeling,
rapid
feasibility
feedback
to
marketing,
and
the
+L+
interpersonal
dynamics
that
are
necessary
to
develop
+L+
a
product.
The
method
centers
on
the
role
of
the
facilitator,
an
independent
agent
whose
purpose
is
to
build
+L+
the
Integrated
System
Model
(ISM).
The
ISM
is
the
result
of
merging
the
independent
system
views
from
all
+L+
stakeholders
at
any
given
abstraction
level.
Formulation
of
this
method
was
based
on
the
real-world
experience
of
developing
a
complex,
high-technology
medical
+L+
product
with
critical
time-to-market
pressures.
It
has
+L+
proven
to
be
a
practical
approach
to
the
evolution
of
+L+
requirements
definition
and
provides
a
necessary
link
+L+
to
the
marketing
aspect
of
a
product.
+L+
1
Introduction

DIMACS
Series
in
Discrete
Mathematics
+L+
and
Theoretical
Computer
Science
+L+
Volume
00,
19xx
+L+
Global
Optimization
Methods
for
Protein
Folding
Problems
+L+
Richard
H.
Byrd
,
Elizabeth
Eskow
,
Andre
van
der
Hoek
,
Robert
B.
+L+
Schnabel
,
Chung-Shang
Shao
,
and
Zhihong
Zou
+L+
Abstract.
The
problem
of
finding
the
naturally
occurring
structure
of
a
protein
is
believed
to
correspond
to
minimizing
the
free,
or
potential,
energy
of
+L+
the
protein.
This
is
generally
a
very
difficult
global
optimization
problem,
with
+L+
a
large
number
of
parameters
and
a
huge
number
of
local
minimizers
including
many
with
function
values
near
that
of
the
global
minimizer.
This
paper
+L+
presents
a
new
global
optimization
method
for
such
problems.
The
method
+L+
consists
of
an
initial
phase
that
locates
some
reasonably
low
local
minimizers
of
+L+
the
energy
function,
followed
by
the
main
phase
that
progresses
from
the
best
+L+
current
local
minimizers
to
even
lower
local
minimizers.
The
method
combines
+L+
portions
that
work
on
small
subsets
of
the
parameters,
including
small-scale
+L+
global
optimizations
using
stochastic
methods,
with
local
minimizations
involving
all
the
parameters.
In
computational
tests
on
the
protein
polyalanine
+L+
with
up
to
58
amino
acids
(116
internal
parameters),
the
method
appears
to
+L+
be
very
successful
in
finding
the
lowest
energy
structures.
The
largest
case
+L+
is
particularly
significant
because
the
lowest
energy
structures
that
are
found
+L+
include
ones
that
exhibit
interesting
tertiary
as
opposed
to
just
secondary
+L+
structure.
+L+
1.
Introduction

Markov
Decision
Processes
in
Large
State
Spaces
+L+
Lawrence
K.
Saul
and
Satinder
P.
Singh
+L+
lksaul@psyche.mit.edu,
singh@psyche.mit.edu
+L+
Center
for
Biological
and
Computational
Learning
+L+
Massachusetts
Institute
of
Technology
+L+
79
Amherst
Street,
E10-243
+L+
Cambridge,
MA
02139
+L+
Abstract
+L+
In
this
paper
we
propose
a
new
framework
for
+L+
studying
Markov
decision
processes
(MDPs),
+L+
based
on
ideas
from
statistical
mechanics.
The
+L+
goal
of
learning
in
MDPs
is
to
find
a
policy
+L+
that
yields
the
maximum
expected
return
over
+L+
time.
In
choosing
policies,
agents
must
therefore
weigh
the
prospects
of
short-term
versus
+L+
long-term
gains.
We
study
a
simple
MDP
in
+L+
which
the
agent
must
constantly
decide
between
exploratory
jumps
and
local
reward
mining
in
state
space.
The
number
of
policies
to
+L+
choose
from
grows
exponentially
with
the
size
+L+
of
the
state
space,
N
.
We
view
the
expected
returns
as
defining
an
energy
landscape
over
policy
space.
Methods
from
statistical
mechanics
+L+
are
used
to
analyze
this
landscape
in
the
thermodynamic
limit
N
!
1.
We
calculate
the
+L+
overall
distribution
of
expected
returns,
as
well
+L+
as
the
distribution
of
returns
for
policies
at
a
+L+
fixed
Hamming
distance
from
the
optimal
one.
+L+
We
briefly
discuss
the
problem
of
learning
optimal
policies
from
empirical
estimates
of
the
+L+
expected
return.
As
a
first
step,
we
relate
our
+L+
findings
for
the
entropy
to
the
limit
of
high-temperature
learning.
Numerical
simulations
+L+
support
the
theoretical
results.
+L+
1
Introduction

A
Simple
Algorithm
for
Nearest
Neighbor
Search
+L+
in
High
Dimensions
+L+
Sameer
A.
Nene
and
Shree
K.
Nayar
+L+
Department
of
Computer
Science
+L+
Columbia
University
+L+
New
York,
N.Y.
10027
+L+
October,
1995
+L+
Technical
Report
No.
CUCS-030-95
+L+
+PAGE+

The
Extruded
Generalized
Cylinder:
A
Deformable
Model
for
+L+
Object
Recovery
+L+
Thomas
O'Donnell
?
Xi-Sheng
Fang
?
+L+
Terrence
E.
Boult
?
Alok
Gupta
+L+
?
Dept.
of
Computer
Science
Siemens
Corporate
Research,
Inc.
+L+
Columbia
University
755
College
Road
East
+L+
New
York,
N.Y.
10027
Princeton,
N.J.
08540
+L+
Email:
odonnell@cs.columbia.edu
alok@scr.siemens.com
+L+
February
12,
1997
+L+
Abstract
+L+
There
is
increasing
interest
in
the
recovery
of
generalized
cylinders
(GCs)
with
curved
spines.
However,
+L+
existing
formulations
of
such
GCs,
for
example
those
+L+
based
on
the
Frenet-Serret
frame
or
the
tube
model,
+L+
suffer
serious
drawbacks:
discontinuities,
a
lack
of
expressive
power,
"narrowing"
in
the
plane
normal
to
+L+
the
spine,
non-intuitive
twisting
behavior,
and/or
off-axis
nonorthogonality
of
their
local
coordinate
systems.
+L+
We
discuss
some
of
the
problems
associated
with
the
+L+
non-orthogonality
of
the
coordinate
system
based
on
+L+
the
Frenet-Serret
frame.
This
non-orthogonality
is
induced
by
torsion
effects
and
we
show
how
to
correct
for
+L+
it.
We
then
introduce
a
new
model,
the
extruded
GC
+L+
(EGC)
model,
which
overcomes
all
the
problems
mentioned
above.
For
complex
axes,
the
EGC
model
is
also
+L+
simpler
to
understand
and
use
than
existing
models.
+L+
The
EGC
model
is
further
extended
by
including
local
surface
deformations.
Recovery
of
the
deformable
+L+
EGC
via
a
physically-motivated
paradigm
is
demonstrated
on
pre-segmented
data
from
a
human
carotid
+L+
artery.
+L+
1
Introduction
and
Previous

Predictive
Dynamic
Load
Balancing
of
Parallel
and
Distributed
Rule
+L+
and
Query
Processing
+L+
Hasanat
M.
Dewan
Salvatore
J.
Stolfo
+L+
Mauricio
Hernandez
Jae-Jun
Hwang
+L+
Department
of
Computer
Science
+L+
Columbia
University,
New
York,
NY
10027
+L+
CUCS-025-94
+L+
(This
paper
appeared
in
the
Proceedings
of
the
1994
ACM
SIGMOD
Conference.)
+L+
Abstract
+L+
Expert
Databases
are
environments
that
support
the
processing
of
rule
programs
against
a
disk
resident
database.
+L+
They
occupy
a
position
intermediate
between
active
and
deductive
databases,
with
respect
to
the
level
of
abstraction
+L+
of
the
underlying
rule
language.
The
operational
semantics
+L+
of
the
rule
language
influences
the
problem
solving
strategy,
+L+
while
the
architecture
of
the
processing
environment
determines
efficiency
and
scalability.
+L+
In
this
paper,
we
present
elements
of
the
PARADISER
+L+
architecture
and
its
kernel
rule
language,
PARULEL.
The
+L+
PARADISER
environment
provides
support
for
parallel
and
+L+
distributed
evaluation
of
rule
programs,
as
well
as
static
+L+
and
dynamic
load
balancing
protocols
that
predictively
+L+
balance
a
computation
at
runtime.
This
combination
of
+L+
features
results
in
a
scalable
database
rule
and
complex
+L+
query
processing
architecture.
We
validate
our
claims
by
+L+
analyzing
the
performance
of
the
system
for
two
realistic
+L+
test
cases.
In
particular,
we
show
how
the
performance
of
a
+L+
parallel
implementation
of
transitive
closure
is
significantly
+L+
improved
by
predictive
dynamic
load
balancing.
+L+
1
Introduction

Submitted
for
review
to
Discrete
and
Computational
Geometry,
September
1997.
+L+
On
the
Area
Bisectors
of
a
Polygon
+L+
Karl-Friedrich
Bohringer
+L+
Cornell
University
+L+
Bruce
Randall
Donald
+L+
Dartmouth
College
+L+
Dan
Halperin
x
+L+
Tel
Aviv
University
+L+
Abstract
+L+
We
consider
the
family
of
lines
that
are
area
bisectors
of
a
polygon
(possibly
with
+L+
holes)
in
the
plane.
We
say
that
two
bisectors
of
a
polygon
P
are
combinatorially
+L+
distinct
if
they
induce
different
partitionings
of
the
vertices
of
P
.
We
derive
an
algebraic
+L+
characterization
of
area
bisectors.
We
then
show
that
there
are
simple
polygons
with
n
+L+
vertices
that
have
(n
2
)
combinatorially
distinct
area
bisectors
(matching
the
obvious
+L+
upper
bound),
and
present
an
output-sensitive
algorithm
for
computing
an
explicit
+L+
representation
of
all
the
bisectors
of
a
given
polygon.
Our
study
is
motivated
by
the
+L+
development
of
novel,
flexible
feeding
devices
for
parts
positioning
and
orienting.
The
+L+
question
of
determining
all
the
bisectors
of
polygonal
parts
arises
in
connection
with
+L+
the
development
of
efficient
part
positioning
strategies
when
using
these
devices.
+L+
Work
on
this
paper
by
Karl-Friedrich
Bohringer
and
Bruce
Randall
Donald
has
been
supported
in
+L+
part
by
the
National
Science
Foundation
under
grants
No.
IRI-8802390,
IRI-9000532,
IRI-9201699,
and
+L+
by
a
Presidential
Young
Investigator
award
to
Bruce
Donald,
in
part
by
NSF/ARPA
Special
Grant
for
+L+
Experimental
Research
No.
IRI-9403903,
and
in
part
by
the
Air
Force
Office
of
Sponsored
Research,
the
+L+
Mathematical
Sciences
Institute,
Intel
Corporation,
and
AT&T
Bell
laboratories.
Work
on
this
paper
by
+L+
Dan
Halperin
has
been
supported
in
part
by
an
Alon
Fellowship,
by
ESPRIT
IV
LTR
Project
No.
21957
+L+
(CGAL),
by
the
USA-Israel
Binational
Science
Foundation,
and
by
the
Hermann
Minkowski
-
Minerva
+L+
Center
for
Geometry
at
Tel
Aviv
University.
A
preliminary
and
abridged
version
of
the
paper
appeared
in
+L+
proc.
13th
ACM
Symp.
on
Computational
Geometry,
Nice,
1997,
pp.
457-459.
+L+
Robotics
&
Vision
Laboratory,
Department
of
Computer
Science,
Cornell
University.
Author's
current
+L+
address:
ALPHA
laboratory,
Dept.
of
Ind.
Eng.
and
Op.
Research,
University
of
California,
Berkeley.
Email
+L+
address:
karl@IEOR.Berkeley.EDU.
+L+
Dept.
of
Computer
Science,
Dartmouth
College,
6211
Sudikoff
Laboratory,
Hanover,
NH
03755-3510.
+L+
brd@cs.dartmouth.edu,
http://www.cs.dartmouth.edu/
brd/.
+L+
x
Department
of
Computer
Science,
Tel
Aviv
University,
Tel
Aviv
69978,
ISRAEL.
Email
address:
+L+
halperin@math.tau.ac.il.
Part
of
the
work
on
this
paper
was
carried
out
while
D.H.
was
at
the
Robotics
+L+
Laboratory,
Department
of
Computer
Science,
Stanford
University.
+L+
+PAGE+

Asymptotically
Tight
Bounds
for
Performing
+L+
BMMC
Permutations
on
Parallel
Disk
Systems
+L+
Thomas
H.
Cormen
+L+
Thomas
Sundquist
+L+
Leonard
F.
Wisniewski
+L+
Department
of
Mathematics
and
Computer
Science
+L+
Dartmouth
College
+L+
Abstract
+L+
We
give
asymptotically
equal
lower
and
upper
bounds
for
the
number
of
parallel
I/O
operations
required
to
perform
bit-matrix-multiply/complement
(BMMC)
permutations
on
parallel
+L+
disk
systems.
In
a
BMMC
permutation
on
N
records,
where
N
is
a
power
of
2,
each
(lg
N
)-bit
+L+
source
address
x
maps
to
a
corresponding
(lg
N)-bit
target
address
by
the
matrix
equation
+L+
=
A
x
c,
where
matrix
multiplication
is
performed
over
GF
(2).
The
characteristic
matrix
A
+L+
is
(lg
N
)fi(lg
N
)
and
nonsingular
over
GF
(2).
Under
the
Vitter-Shriver
parallel-disk
model
with
+L+
N
records,
D
disks,
B
records
per
block,
and
M
records
of
memory,
we
show
a
universal
lower
+L+
bound
of
+L+
BD
+L+
1
+
rank
+L+
lg(M=B)
+L+
parallel
I/Os
for
performing
a
BMMC
permutation,
where
+L+
is
the
lower
left
lg(N=B)
fi
lg
B
submatrix
of
the
characteristic
matrix.
We
also
present
an
algo
+L+
rithm
that
uses
at
most
2N
+L+
BD
+L+
rank
+L+
lg(M=B)
+L+
+
2
+L+
parallel
I/Os,
which
asymptotically
matches
the
+L+
lower
bound
and
improves
upon
the
BMMC
and
bit-permute/complement
(BPC)
algorithms
in
+L+
[4].
When
rank
is
low,
this
method
is
an
improvement
over
the
general-permutation
bound
of
+L+
fi
+L+
N
+L+
lg(N=B)
+L+
We
introduce
a
new
subclass
of
BMMC
permutations,
called
memoryload-dispersal
(MLD)
+L+
permutations,
which
can
be
performed
in
one
pass.
This
subclass,
which
is
used
in
the
BMMC
+L+
algorithm,
extends
the
catalog
of
one-pass
permutations
appearing
in
[4].
+L+
Although
many
BMMC
permutations
of
practical
interest
fall
into
subclasses
that
might
be
+L+
explicitly
invoked
within
the
source
code,
we
show
how
to
detect
in
at
most
N=BD+
+L+
l
+L+
D
+L+
parallel
I/Os
whether
a
given
vector
of
target
addresses
specifies
a
BMMC
permutation.
Thus,
+L+
one
can
determine
efficiently
at
run
time
whether
a
permutation
to
be
performed
is
BMMC
and
+L+
then
avoid
the
general-permutation
algorithm
and
save
parallel
I/Os
by
using
our
algorithm.
+L+
1
Introduction

To
appear
in
Parallel
Computing,
1997.
+L+
Available
at
URL
ftp://ftp.cs.dartmouth.edu/kotz/papers/nieuwejaar:jgalley.ps.Z
+L+
The
Galley
Parallel
File
System
+L+
Nils
Nieuwejaar
,
David
Kotz
+L+
fnils,dfkg@cs.dartmouth.edu
+L+
Department
of
Computer
Science,
Dartmouth
College,
Hanover,
NH
03755-3510
+L+
Most
current
multiprocessor
file
systems
are
designed
to
use
multiple
disks
+L+
in
parallel,
using
the
high
aggregate
bandwidth
to
meet
the
growing
I/O
+L+
requirements
of
parallel
scientific
applications.
Many
multiprocessor
file
+L+
systems
provide
applications
with
a
conventional
Unix-like
interface,
allowing
the
application
to
access
multiple
disks
transparently.
This
interface
conceals
the
parallelism
within
the
file
system,
increasing
the
ease
+L+
of
programmability,
but
making
it
difficult
or
impossible
for
sophisticated
programmers
and
libraries
to
use
knowledge
about
their
I/O
needs
+L+
to
exploit
that
parallelism.
In
addition
to
providing
an
insufficient
interface,
most
current
multiprocessor
file
systems
are
optimized
for
a
different
+L+
workload
than
they
are
being
asked
to
support.
We
introduce
Galley,
a
+L+
new
parallel
file
system
that
is
intended
to
efficiently
support
realistic
+L+
scientific
multiprocessor
workloads.
We
discuss
Galley's
file
structure
and
+L+
application
interface,
as
well
as
the
performance
advantages
offered
by
+L+
that
interface.
+L+
Key
words:
Parallel
I/O.
Multiprocessor
file
system.
Performance
evaluation.
IBM
+L+
SP-2.
Scientific
Computing.
+L+
1
Introduction

Relating
Comprehension
and
Production
+L+
in
the
Acquisition
of
Morphology
+L+
Michael
Gasser
+L+
Indiana
University
+L+
Abstract
+L+
Most
theories
of
language
processing
and
acquisition
make
the
assumption
that
+L+
perception
and
comprehension
are
related
to
production,
but
few
have
anything
say
+L+
about
how.
This
paper
describes
a
performance-oriented
connectionist
model
of
+L+
the
acquisition
of
morphology
in
which
production
builds
on
representations
which
+L+
develop
during
the
learning
of
word
recognition.
Using
artificial
language
stimuli
+L+
embodying
simple
suffixation,
prefixation,
and
template
rules,
I
demonstrate
that
+L+
the
model
generalizes
to
novel
combinations
of
roots
and
inflections
for
both
word
+L+
recognition
and
production.
I
argue
that
the
capacity
of
connectionist
networks
to
+L+
develop
intermediate
distributed
representations
which
not
only
enable
the
solving
+L+
of
the
task
at
hand
but
also
facilitate
another
task
offers
a
plausible
account
of
how
+L+
comprehension
and
production
come
to
share
phonological
knowledge
as
words
are
+L+
learned.
+L+
Introduction

Coir:
A
Thread-Model
for
Supporting
Task-
and
Data-
Parallelism
+L+
in
Object-Oriented
Parallel
Languages
+L+
Neelakantan
Sundaresan
Dennis
Gannon
+L+
nsundare@cs.indiana.edu
gannon@cs.indiana.edu
+L+
Computer
Science
Department
+L+
215
Lindley
Hall
+L+
Indiana
University
+L+
Bloomington,
IN
47405
+L+
Abstract
+L+
Data-
and
task-parallelism
are
two
important
parallel
programming
models.
Object-oriented
paradigm
+L+
in
parallelism
provides
a
good
way
of
abstracting
out
various
aspects
of
computations
and
computing
resources.
Using
an
object-oriented
language
like
C++,
one
can
compose
data
and
control
representations
+L+
into
a
single
active
object.
+L+
We
propose
a
thread
model
of
parallelism
that
addresses
both
data
and
task
parallelism.
Computation
+L+
and
communication
can
be
overlapped
by
suspending
a
thread
of
computation
which
is
waiting
for
an
+L+
event
and
running
an
eligible
thread
of
computation
in
its
place.
Threads
naturally
subsume
task-parallelism.
Threads
are
encapsulated
into
thread
objects
may
be
grouped
into
rope
objects
[22,
20],
that
+L+
span
the
parallel
machine
domain,
for
collective
computation
and
communication.
Thus
data-parallelism
+L+
can
be
supported.
Since
rope
objects
are
parallel
objects,
they
can
be
customized,
interestingly,
in
a
+L+
serial
or
a
parallel
manner.
Spatial
transparency
of
objects
is
achieved
by
global
pointer
templates.
+L+
We
present
results
from
a
prototype
system
running
on
the
SGI
Challenge
and
the
Intel
Paragon.
+L+
keywords:
task-parallelism,
data-parallelism,
thread,
rope,
object-oriented
paradigm
+L+
+PAGE+

Finding
Genes
in
DNA
with
a
Hidden
Markov
Model
+L+
John
Henderson
Steven
Salzberg
Kenneth
H.
Fasman
+L+
Jan.
31,
1996,
revised
Aug.
28,
1996
+L+
Abstract
+L+
This
study
describes
a
new
Hidden
Markov
Model
(HMM)
system
for
segmenting
uncharacterized
genomic
DNA
sequences
into
exons,
introns,
and
intergenic
+L+
regions.
Separate
HMM
modules
were
designed
and
trained
for
specific
regions
of
+L+
DNA:
exons,
introns,
intergenic
regions,
and
splice
sites.
The
models
were
then
+L+
tied
together
to
form
a
biologically
feasible
topology.
The
integrated
HMM
was
+L+
trained
further
on
a
set
of
eukaryotic
DNA
sequences,
and
tested
by
using
it
to
+L+
segment
a
separate
set
of
sequences.
The
resulting
HMM
system,
which
is
called
+L+
VEIL
(Viterbi
Exon-Intron
Locator),
obtains
an
overall
accuracy
on
test
data
of
+L+
92%
of
total
bases
correctly
labelled,
with
a
correlation
coefficient
of
0.68.
Using
the
more
stringent
test
of
exact
exon
prediction,
VEIL
correctly
located
both
+L+
ends
of
46%
of
the
exons.
Moreover,
more
than
50%
of
the
exons
it
predicts
are
+L+
exactly
correct.
These
results
compare
favorably
to
the
best
previous
results
for
+L+
gene
structure
prediction,
and
demonstrate
the
benefits
of
using
HMMs
for
this
+L+
problem.
+L+
1
Introduction

A
Unifying
Framework
+L+
for
+L+
Conceptual
Data
Modelling
Concepts
+L+
P.J.M.
Frederiks
,
A.H.M.
ter
Hofstede
,
E.
Lippe
+L+
Department
of
Information
Systems
+L+
University
of
Nijmegen
+L+
Toernooiveld
1
+L+
NL-6525
ED
Nijmegen
+L+
The
Netherlands
+L+
fpaulf,arthur,ernstlg@cs.kun.nl
+L+
Published
as:
P.J.M.
Frederiks,
A.H.M.
ter
Hofstede,
and
E.
Lippe.
A
Unifying
Framework
+L+
for
Conceptual
Data
Modelling
Concepts.
Technical
Report
CSI-R9410,
Computing
Science
+L+
Institute,
University
of
Nijmegen,
Nijmegen,
The
Netherlands,
September
1994.
+L+
Abstract
+L+
For
succesful
information
systems
development,
conceptual
data
modelling
is
essential.
+L+
Nowadays
many
techniques
for
conceptual
data
modelling
exist,
examples
are
NIAM,
FORM,
+L+
PSM,
many
(E)ER
variants,
IFO,
and
FDM.
In-depth
comparisons
of
concepts
of
these
techniques
is
very
difficult
as
the
mathematical
formalisations
of
these
techniques,
if
existing
at
+L+
all,
are
very
different.
As
such
there
is
a
need
for
a
unifying
formal
framework
providing
a
+L+
sufficiently
high
level
of
abstraction.
In
this
paper
the
use
of
category
theory
for
this
purpose
+L+
is
addressed.
Well-known
conceptual
data
modelling
concepts
are
discussed
from
a
category
+L+
theoretic
point
of
view.
Advantages
and
disadvantages
of
the
approach
chosen
will
be
outlined.
+L+
Keywords:
Conceptual
Data
Modelling,
Category
Theory,
Meta
Modelling
+L+
Classification:
68P99
(AMS-1991),
H.1.0.
(CR-1991)
+L+
1
Introduction

Middle
Scale
Robot
Navigation
A
Case
Study
+L+
Carl
Owen
and
Ulrich
Nehmzow
+L+
Department
of
Computer
Science
+L+
University
of
Manchester
+L+
Manchester
M13
9PL
+L+
United
Kingdom
+L+
owenc@cs.man.ac.uk
+L+
u.nehmzow@cs.man.ac.uk
+L+
7/4/97
+L+
Abstract
+L+
In
this
paper
we
present
results
of
experiments
carried
out
with
a
route
learning
system
for
a
+L+
mobile
robot,
conducted
in
a
`real
world'
environment
covering
distances
of
several
hundred
metres.
+L+
The
system
uses
no
odometry
and
is
based
on
a
self-organising
mapbuilding
process
using
perceptual
+L+
landmarks.
+L+
A
performance
metric
is
defined
and
used
to
measure
the
robot's
ability
to
traverse
the
route.
+L+
1
Introduction

Error-Correcting
Output
Codes:
+L+
A
General
Method
for
Improving
+L+
Multiclass
Inductive
Learning
Programs
+L+
Thomas
G.
Dietterich
and
Ghulum
Bakiri
+L+
Department
of
Computer
Science
+L+
Oregon
State
University
+L+
Corvallis,
OR
97331-3202
+L+
Abstract
+L+
Multiclass
learning
problems
involve
finding
a
definition
for
an
unknown
function
f(x)
whose
range
is
a
+L+
discrete
set
containing
k
&gt;
2
values
(i.e.,
k
"classes").
+L+
The
definition
is
acquired
by
studying
large
collections
+L+
of
training
examples
of
the
form
hx
i
;
f(x
i
)i.
Existing
+L+
approaches
to
this
problem
include
(a)
direct
application
of
multiclass
algorithms
such
as
the
decision-tree
+L+
algorithms
ID3
and
CART,
(b)
application
of
binary
+L+
concept
learning
algorithms
to
learn
individual
binary
+L+
functions
for
each
of
the
k
classes,
and
(c)
application
+L+
of
binary
concept
learning
algorithms
with
distributed
+L+
output
codes
such
as
those
employed
by
Sejnowski
and
+L+
Rosenberg
in
the
NETtalk
system.
This
paper
compares
these
three
approaches
to
a
new
technique
in
+L+
which
BCH
error-correcting
codes
are
employed
as
a
+L+
distributed
output
representation.
We
show
that
these
+L+
output
representations
improve
the
performance
of
ID3
+L+
on
the
NETtalk
task
and
of
backpropagation
on
an
+L+
isolated-letter
speech-recognition
task.
These
results
+L+
demonstrate
that
error-correcting
output
codes
provide
a
general-purpose
method
for
improving
the
performance
of
inductive
learning
programs
on
multiclass
+L+
problems.
+L+
Introduction

Analysis
of
Algorithms
Generalizing
B-Spline
+L+
Subdivision
+L+
Jorg
Peters
Ulrich
Reif
+L+
January
27,
1997
+L+
Abstract
+L+
A
new
set
of
tools
for
verifying
smoothness
of
surfaces
generated
by
stationary
+L+
subdivision
algorithms
is
presented.
The
main
challenge
here
is
the
verification
of
+L+
injectivity
of
the
characteristic
map.
The
tools
are
sufficiently
versatile
and
easy
+L+
to
wield
to
allow,
as
an
application,
a
full
analysis
of
algorithms
generalizing
bi-quadratic
and
bicubic
B-spline
subdivision.
In
the
case
of
generalized
biquadratic
+L+
subdivision
the
analysis
yields
a
hitherto
unknown
sharp
bound
strictly
less
than
+L+
one
on
the
second
largest
eigenvalue
of
any
smoothly
converging
subdivision.
+L+
Keywords:
subdivision,
arbitrary
topology,
characteristic
map,
Doo-Sabin
Algorithm,
Catmull-Clark
algorithm,
B-spline
+L+
AMS
subject
classification:
65D17,
65D07,
68U07
+L+
Abbreviated
title:
Generalized
B-Spline
Subdivision
+L+
1
Introduction

Utterance
Units
in
Spoken
Dialogue
+L+
David
R.
Traum
1
and
Peter
A.
Heeman
2
+L+
Abstract.
In
order
to
make
spoken
dialogue
systems
more
sophisticated,
designers
need
to
better
understand
the
conventions
that
people
+L+
use
in
structuring
their
speech
and
in
interacting
with
their
fellow
con-versants.
In
particular,
it
is
crucial
to
discriminate
the
basic
building
+L+
blocks
of
dialogue
and
how
they
affect
the
way
people
process
language.
Many
researchers
have
proposed
the
utterance
unit
as
the
+L+
primary
object
of
study,
but
defining
exactly
what
this
is
has
remained
a
difficult
issue.
To
shed
light
on
this
question,
we
consider
+L+
grounding
behavior
in
dialogue,
and
examine
co-occurrences
between
+L+
turn-initial
grounding
acts
and
utterance
unit
signals
that
have
been
+L+
proposed
in
the
literal,
namely
prosodic
boundary
tones
and
pauses.
+L+
Preliminary
results
indicate
high
correlation
between
grounding
and
+L+
boundary
tones,
with
a
secondary
correlation
for
longer
pauses.
We
+L+
also
consider
some
of
the
dialogue
processing
issues
which
are
impacted
by
a
definition
of
utterance
unit.
+L+
1
INTRODUCTION

Scalable
Atomic
Primitives
for
Distributed
+L+
Shared
Memory
Multiprocessors
+L+
(Extended
Abstract)
+L+
Maged
M.
Michael
+L+
Department
of
Computer
Science
+L+
University
of
Rochester
+L+
Rochester,
NY
14627-0226
+L+
USA
+L+
Michael
L.
Scott
+L+
Department
of
Computer
Science
+L+
University
of
Rochester
+L+
Rochester,
NY
14627-0226
+L+
USA
+L+
Abstract
+L+
Our
research
addresses
the
general
topic
of
atomic
update
of
shared
data
+L+
structures
on
large-scale
shared-memory
multiprocessors.
In
this
paper
+L+
we
consider
alternative
implementations
of
the
general-purpose
single-address
atomic
primitives
fetch
and
,
compare
and
swap,
load
linked,
+L+
and
store
conditional.
These
primitives
have
proven
popular
on
small-scale
bus-based
machines,
but
have
yet
to
become
widely
available
on
+L+
large-scale,
distributed
shared
memory
machines.
We
propose
several
alternative
hardware
implementations
of
these
primitives,
and
then
analyze
+L+
the
performance
of
these
implementations
for
various
data
sharing
patterns.
Our
results
indicate
that
good
overall
performance
can
be
obtained
+L+
by
implementing
compare
and
swap
in
the
cache
controllers,
and
by
pro
+L+
viding
an
additional
instruction
to
load
an
exclusive
copy
of
a
cache
line.
+L+
1
INTRODUCTION

Priors
for
Infinite
Networks
+L+
Radford
M.
Neal
+L+
Technical
Report
CRG-TR-94-1
+L+
Department
of
Computer
Science
+L+
University
of
Toronto
+L+
10
King's
College
Road
+L+
Toronto,
Canada
M5S
1A4
+L+
E-mail:
radford@cs.toronto.edu
+L+
1
March
1994
+L+
Abstract
+L+
Bayesian
inference
begins
with
a
prior
distribution
for
model
parameters
that
is
+L+
meant
to
capture
prior
beliefs
about
the
relationship
being
modeled.
For
multilayer
+L+
perceptron
networks,
where
the
parameters
are
the
connection
weights,
the
prior
+L+
lacks
any
direct
meaning
|
what
matters
is
the
prior
over
functions
computed
+L+
by
the
network
that
is
implied
by
this
prior
over
weights.
In
this
paper,
I
show
+L+
that
priors
over
weights
can
be
defined
in
such
a
way
that
the
corresponding
+L+
priors
over
functions
reach
reasonable
limits
as
the
number
of
hidden
units
in
the
+L+
network
goes
to
infinity.
When
using
such
priors,
there
is
thus
no
need
to
limit
the
+L+
size
of
the
network
in
order
to
avoid
"overfitting".
The
infinite
network
limit
also
+L+
provides
insight
into
the
properties
of
different
priors.
A
Gaussian
prior
for
hidden-to-output
weights
results
in
a
Gaussian
process
prior
for
functions,
which
can
be
+L+
smooth,
Brownian,
or
fractional
Brownian,
depending
on
the
hidden
unit
activation
+L+
function
and
the
prior
for
input-to-hidden
weights.
Quite
different
effects
can
be
+L+
obtained
using
priors
based
on
non-Gaussian
stable
distributions.
In
networks
with
+L+
more
than
one
hidden
layer,
a
combination
of
Gaussian
and
non-Gaussian
priors
+L+
appears
most
interesting.
+L+
+PAGE+

To
appear
in
Jordan,
MI,
Kearns
MJ,
and
Solla,
SA
Advances
in
Neural
Information
+L+
Processing
Systems
10.
MIT
Press:
Cambridge,
MA,
1998.
+L+
Hierarchical
Non-linear
Factor
Analysis
+L+
and
Topographic
Maps
+L+
Zoubin
Ghahramani
and
Geoffrey
E.
Hinton
+L+
Dept.
of
Computer
Science,
University
of
Toronto
+L+
Toronto,
Ontario,
M5S
3H5,
Canada
+L+
http://www.cs.toronto.edu/neuron/
+L+
fzoubin,hintong@cs.toronto.edu
+L+
Abstract
+L+
We
first
describe
a
hierarchical,
generative
model
that
can
be
+L+
viewed
as
a
non-linear
generalisation
of
factor
analysis
and
can
+L+
be
implemented
in
a
neural
network.
The
model
performs
perceptual
inference
in
a
probabilistically
consistent
manner
by
using
+L+
top-down,
bottom-up
and
lateral
connections.
These
connections
+L+
can
be
learned
using
simple
rules
that
require
only
locally
available
information.
We
then
show
how
to
incorporate
lateral
connections
into
the
generative
model.
The
model
extracts
a
sparse,
+L+
distributed,
hierarchical
representation
of
depth
from
simplified
+L+
random-dot
stereograms
and
the
localised
disparity
detectors
in
+L+
the
first
hidden
layer
form
a
topographic
map.
When
presented
+L+
with
image
patches
from
natural
scenes,
the
model
develops
topo
+L+
graphically
organised
local
feature
detectors.
+L+
1
Introduction

Gap-Definable
Counting
Classes
+L+
Stephen
A.
Fenner
+L+
Computer
Science
Department
+L+
University
of
Southern
Maine
+L+
96
Falmouth
Street
+L+
Portland,
Maine
04103
+L+
Lance
J.
Fortnow
+L+
Stuart
A.
Kurtz
+L+
Computer
Science
Department
+L+
University
of
Chicago
+L+
1100
East
Fifty-eighth
Street
+L+
Chicago,
Illinois
60637
+L+
July
12,
1992
+L+
Work
done
while
the
first
author
was
a
graduate
student
at
the
University
of
Chicago
Computer
Science
Depart
+L+
ment,
supported
in
part
by
a
University
of
Chicago
Fellowship.
+L+
Supported
by
NSF
Grant
CCR-9009936
+L+
+PAGE+

Q-Learning
for
Bandit
Problems
+L+
Michael
O.
Duff
+L+
Department
of
Computer
Science
+L+
University
of
Massachusetts
+L+
Amherst,
MA
01003
+L+
duff@cs.umass.edu
+L+
Abstract
+L+
Multi-armed
bandits
may
be
viewed
as
+L+
decompositionally-structured
Markov
decision
processes
(MDP's)
with
potentially
very-large
state
sets.
A
particularly
elegant
+L+
methodology
for
computing
optimal
policies
+L+
was
developed
over
twenty
ago
by
Gittins
+L+
[Gittins
&
Jones,
1974].
Gittins'
approach
+L+
reduces
the
problem
of
finding
optimal
policies
for
the
original
MDP
to
a
sequence
of
+L+
low-dimensional
stopping
problems
whose
solutions
determine
the
optimal
policy
through
+L+
the
so-called
"Gittins
indices."
Katehakis
+L+
and
Veinott
[Katehakis
&
Veinott,
1987]
have
+L+
shown
that
the
Gittins
index
for
a
process
+L+
in
state
i
may
be
interpreted
as
a
particular
+L+
component
of
the
maximum-value
function
+L+
associated
with
the
"restart-in-i"
process,
+L+
a
simple
MDP
to
which
standard
solution
+L+
methods
for
computing
optimal
policies,
such
+L+
as
successive
approximation,
apply.
This
paper
explores
the
problem
of
learning
the
Git-tins
indices
on-line
without
the
aid
of
a
process
model;
it
suggests
utilizing
process-state-specific
Q-learning
agents
to
solve
their
respective
restart-in-state-i
subproblems,
and
+L+
includes
an
example
in
which
the
online
reinforcement
learning
approach
is
applied
to
+L+
a
problem
of
stochastic
scheduling|one
instance
drawn
from
a
wide
class
of
problems
+L+
that
may
be
formulated
as
bandit
problems.
+L+
1
INTRODUCTION

Humans
Plus
Agents
+L+
Maintain
Schedules
Better
+L+
than
Either
Alone
+L+
Tim
Oates
and
Paul
R.
Cohen
+L+
Computer
Science
Technical
Report
94-03
+L+
Experimental
Knowledge
Systems
Laboratory
+L+
Department
of
Computer
Science,
Box
34610
+L+
Lederle
Graduate
Research
Center
+L+
University
of
Massachusetts
+L+
Amherst,
MA
01003-4610
+L+
Abstract
+L+
Tracking
and
evaluating
the
progress
of
large,
complex
plans
or
+L+
schedules
as
they
unfold
in
real
time
is
extremely
difficult
for
humans.
+L+
In
this
paper
we
present
a
mixed-initiative
system
for
the
task
of
schedule
maintenance
in
a
simulated
shipping
network.
A
schedule
maintenance
agent
monitors
the
network,
predicting
the
occurrence
of
states
+L+
that
may
result
in
reduced
throughput
and
formulating
schedule
modifications
to
avoid
those
states.
The
goal
is
to
maximize
throughput
+L+
while
minimizing
disruptions
to
the
original
schedule.
We
present
results
of
experiments
in
which
human
subjects
attempt
to
obtain
that
+L+
goal
both
with
and
without
the
aid
of
the
agent.
We
found
that
the
human
and
the
agent
working
together
are
able
to
achieve
better
results
+L+
than
either
one
working
alone.
In
addition
to
looking
at
global
performance
measures
such
as
throughput,
we
analyze
individual
schedule
+L+
modification
decisions
made
by
subjects
in
an
attempt
to
assign
credit
+L+
for
the
improvements
in
performance.
+L+
This
research
is
supported
by
ARPA-AFOSR
contract
F30602-91-C-0076.
+L+
+PAGE+

Issues
in
Design-to-time
Real-time
Scheduling
+L+
Alan
Garvey
+L+
Department
of
Computer
Science
+L+
Pacific
Lutheran
University
+L+
Tacoma,
WA
98447
+L+
Email:
garveyaj@plu.edu
+L+
Victor
Lesser
+L+
Computer
Science
Department
+L+
University
of
Massachusetts
+L+
Amherst,
MA
01003
+L+
Email:
lesser@cs.umass.edu
+L+
Abstract
+L+
Design-to-time
real-time
scheduling
is
an
alternative
to
the
many
flexible
computation
approaches
that
are
based
on
anytime
algorithms.
+L+
It
builds
schedules
at
runtime
that
dynamically
+L+
combine
solutions
to
subproblems,
taking
advantage
of
the
time
available
to
achieve
the
best
+L+
results
it
can.
In
this
paper
we
look
in
detail
at
+L+
a
few
issues
related
to
design-to-time,
including
where
the
approximations
we
rely
on
come
+L+
from,
how
uncertainty
affects
the
scheduling
+L+
process
and
the
interface
between
the
sched-uler
and
its
invoker.
+L+
Introduction

Integrated
Signal
Processing
+L+
and
Signal
Understanding
1
+L+
Victor
Lesser
,
Hamid
Nawab
,
+L+
Malini
Bhandaru
,
Norman
Carver,
+L+
Zarko
Cvetanovic
,
Izaskun
Gallastegi,
+L+
Frank
Klassner
+L+
COINS
Technical
Report
91-34
+L+
November
1991
+L+
Electrical
and
Computer
Engineering
Dept.
+L+
Boston
University
+L+
44
Cummington
Street
+L+
Boston,
Massachusetts
02125
+L+
Abstract
+L+
This
report
outlines
the
IPUS
paradigm,
named
for
Integrated
Processing
and
Understanding
of
Signals,
which
permits
sophisticated
interaction
between
theory-based
problem
+L+
solving
in
signal
processing
and
heuristic
problem-solving
in
signal
interpretation.
The
need
+L+
for
such
a
paradigm
arises
in
signal
understanding
domains
that
require
the
processing
of
+L+
complicated
interacting
signals
under
variable
signal-to-noise
ratios.
One
such
application
is
+L+
sound
understanding,
in
the
context
of
which
we
report
on
a
testbed
experiment
illustrating
+L+
the
functionality
of
key
IPUS
architecture
components.
+L+
1
This
work
was
supported
by
the
Office
of
Naval
Research
under
University
Research
Initiative

DTP:
An
Efficient
Transport
Protocol
+L+
Dheeraj
Sanghi
and
Ashok
K.
Agrawala
+L+
Department
of
Computer
Science,
University
of
Maryland,
College
Park,
+L+
MD
20742,
USA
+L+
Abstract
+L+
We
recently
introduced
a
new
flow
control
scheme,
called
send-time
control,
which
is
+L+
based
on
a
deterministic
model
of
virtual
circuits
in
a
computer
network.
In
this
scheme,
+L+
the
time
at
which
a
packet
is
sent
by
a
source
is
computed
from
estimates
of
round-trip
+L+
time,
traffic
in
the
network
and
bottleneck
service
time.
In
this
paper,
we
describe
a
new
+L+
transport
protocol,
called
DTP,
which
uses
send-time
control
as
its
flow
control
scheme.
+L+
Preliminary
measurements
of
coast-to-coast
connections
over
the
Internet
show
significant
+L+
performance
improvement
over
TCP,
which
is
the
most
commonly
used
transport
protocol
+L+
in
the
Internet
today.
+L+
Keyword
Codes:
C.2.2
+L+
Keywords:
Computer-Communication
Networks,
Network
Protocols
+L+
1.
Introduction

A
SERVER
OF
DISTRIBUTED
DISK
PAGES
+L+
USING
A
CONFIGURABLE
SOFTWARE
BUS
+L+
Charles
Falkenberg
,
Paul
Hagger
and
Steve
Kelley
+L+
Institute
for
Advanced
Computer
Studies
and
+L+
The
Department
of
Computer
Science
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
ABSTRACT
+L+
As
network
latency
drops
below
disk
latency,
access
time
to
a
remote
disk
will
begin
+L+
to
approach
local
disk
access
time.
The
performance
of
I/O
may
then
be
improved
+L+
by
spreading
disk
pages
across
several
remote
disk
servers
and
accessing
disk
pages
+L+
in
parallel.
To
research
this
we
have
prototyped
a
data
page
server
called
a
Page
+L+
File.
This
persistent
data
type
provides
a
set
of
methods
to
access
disk
pages
stored
+L+
on
a
cluster
of
remote
machines
acting
as
disk
servers.
The
goal
is
to
improve
the
+L+
throughput
of
database
management
system
or
other
I/O
intensive
application
by
+L+
accessing
pages
from
remote
disks
and
incurring
disk
latency
in
parallel.
This
report
+L+
describes
the
conceptual
foundation
and
the
methods
of
access
for
our
prototype.
+L+
With
oversight
by
Office
of
Naval
Research,
this
research
is
supported
by
ARPA/SISTO
in
+L+
conjunction
with
the
Domain
Specific
Software
Architectures
project.
+L+
+PAGE+

Finite
State
Machines
and
Recurrent
Neural
Networks
-
+L+
Automata
and
Dynamical
Systems
Approaches
+L+
Peter
Tino
a;b
,
Bill
G.
Horne
b
,
C.
Lee
Giles
b;c
+L+
a
Department
of
Computer
Science
and
Engineering
+L+
Slovak
Technical
University
+L+
Ilkovicova
3,
812
19
Bratislava,
Slovakia
+L+
Email:
tino@decef.elf.stuba.sk
+L+
b
NEC
Research
Institute
+L+
4
Independence
Way
+L+
Princeton,
NJ
08540
+L+
Email:
+L+
ftino,horne,gilesg@research.nj.nec.com
+L+
c
Institute
for
Advanced
Computer
Studies
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
Technical
Report
+L+
UMIACS-TR-95-1
and
CS-TR-3396
+L+
Institute
for
Advanced
Computer
Studies
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
+PAGE+

The
Relative
Importance
of
+L+
Concurrent
Writers
and
Weak
Consistency
Models
+L+
Pete
Keleher
+L+
Department
of
Computer
Science
+L+
University
of
Maryland
+L+
College
Park,
MD
20742-3255
+L+
keleher@cs.umd.edu
+L+
Abstract
+L+
This
paper
presents
a
detailed
comparison
of
the
relative
importance
of
allowing
concurrent
writers
+L+
versus
the
choice
of
the
underlying
consistency
model.
Our
comparison
is
based
on
single-
and
multiple-writer
versions
of
a
lazy
release
consistent
(LRC)
protocol,
and
a
single-writer
sequentially
consistent
+L+
protocol,
all
implemented
in
the
CVM
software
distributed
shared
memory
system.
+L+
We
find
that
in
our
environment,
which
we
believe
to
be
representative
of
distributed
systems
today
+L+
and
in
the
near
future,
the
consistency
model
has
a
much
higher
impact
on
overall
performance
than
the
+L+
choice
of
whether
to
allow
concurrent
writers.
The
multiple
writer
protocol
performs
an
average
of
9%
+L+
better
than
the
single
writer
LRC
protocol,
but
34%
better
than
the
single-writer
sequentially
consistent
+L+
protocol.
Set
against
this,
MW-LRC
required
an
average
of
72%
memory
overhead,
compared
to
10%
+L+
overhead
for
the
single-writer
protocols.
+L+
1
Introduction

Exploiting
the
Temporal
Structure
of
MPEG
Video
+L+
for
the
Reduction
of
Bandwidth
Requirements
+L+
Marwan
Krunz
and
Satish
Tripathi
+L+
Institute
for
Advanced
Computer
Studies
+L+
Department
of
Computer
Science
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
Email:
krunz@cs.umd.edu
+L+
Abstract
+L+
We
propose
a
new
bandwidth
allocation
scheme
for
VBR
video
traffic
in
ATM
networks.
+L+
The
scheme
is
tailored
to
MPEG-coded
video
sources
that
require
stringent
and
deterministic
+L+
quality-of-service
guarantees.
By
exploiting
the
temporal
structure
of
MPEG
sources,
we
show
+L+
that
our
scheme
results
in
an
effective
bandwidth
which,
in
most
cases,
is
less
than
the
source
+L+
peak
rate.
The
reduction
in
the
bandwidth
requirement
is
achieved
without
sacrificing
any
+L+
perceived
QoS.
Efficient
procedures
are
provided
for
the
computation
of
the
effective
bandwidth
+L+
under
heterogeneous
MPEG
sources.
The
effective
bandwidth
strongly
depends
on
the
arrangement
of
the
multiplexed
streams
which
is
a
measure
of
the
degree
of
synchronization
between
the
+L+
GOP
patterns
of
different
streams.
Assuming
that
all
possible
arrangements
are
equi-probable,
+L+
we
derive
an
expression
for
the
asymptotic
tail
distribution
of
the
effective
bandwidth.
From
+L+
the
tail
distribution,
we
compute
several
performance
measures
for
the
call
blocking
probability
+L+
when
the
allocation
is
made
based
on
the
effective
bandwidth.
In
the
case
of
homogeneous
+L+
sources,
we
give
a
closed-form
expression
for
the
`best'
arrangement
that
results
in
the
`optimal'
+L+
effective
bandwidth.
Numerical
examples
based
on
real
MPEG
traces
are
used
to
demonstrate
+L+
the
advantages
of
our
scheme.
+L+
Keywords:
bandwidth
allocation,
MPEG,
statistical
multiplexing,
CAC.
+L+
This
research
was
partially
supported
by
the
NSF
grant
#
CCR
9318933.
+L+
+PAGE+

In:
Multimedia
Systems,
Volume
2,
Number
6,
(January
1995)
pages
267-279.
+L+
An
Empirical
Study
of
Delay
Jitter
Management
Policies
*
+L+
Donald
L.
Stone
Kevin
Jeffay
+L+
University
of
North
Carolina
at
Chapel
Hill
+L+
Department
of
Computer
Science
+L+
Chapel
Hill,
NC
27599-3175
USA
+L+
stone,
jeffay-@cs.unc.edu
+L+
July
1994
+L+
Abstract:
This
paper
presents
an
empirical
study
of
several
policies
for
managing
the
effect
+L+
of
delay
jitter
on
the
playout
of
audio
and
video
in
computer-based
conferences.
The
problem
+L+
addressed
is
that
of
managing
the
fundamental
tradeoff
between
display
with
low
latency
and
+L+
display
with
few
gaps.
We
describe
a
particular
policy
called
queue
monitoring
which
+L+
observes
delay
jitter
over
time
and
dynamically
adjusts
display
latency
in
order
to
support
+L+
low-latency
conferences
with
an
acceptable
gap
rate.
Queue
monitoring
is
evaluated
by
+L+
comparing
it
with
two
policies
from
the
literature
in
a
study
based
on
measurements
from
a
+L+
computer-based
conferencing
system.
Our
results
show
that
queue
monitoring
performs
as
+L+
well
or
better
than
the
other
policies
over
the
range
of
observed
network
loads.
More
+L+
importantly,
we
show
that
queue
monitoring
performs
better
on
those
network
loads
for
+L+
which
the
other
policies
exhibit
poor
performance.
+L+
1.
Introduction

Sync:
A
System
for
Mobile
Collaborative
Applications
+L+
Jonathan
P.
Munson
and
Prasun
Dewan
+L+
Department
of
Computer
Science,
University
of
North
Carolina
at
Chapel
Hill
+L+
March
14,
1997
+L+
ABSTRACT
+L+
Sync
is
a
new
Java-based
framework
for
developing
collaborative
applications
for
wireless
mobile
+L+
systems.
Sync
is
based
on
objectoriented
replication
and
offers
high-level
synchronization-aware
classes
+L+
based
on
existing
Java
classes.
Programmers
may
also
extend
the
Sync-provided
classes
to
create
new
+L+
replicated
classes,
either
to
add
functionality
or
to
modify
a
classs
merge
policy.
Sync
supports
fully
+L+
disconnected
operation
and
employs
centralized,
asynchronous
synchronization.
Application
programmers
+L+
use
the
Sync
framework
to
define
conflicts
and
specify
conflict
resolution
on
the
basis
of
the
applications
+L+
structure
and
semantics.
+L+
We
discuss
the
general
needs
of
wireless
mobile
applications,
and
present
a
high-function
example
+L+
application
that
would
be
useful
to
mobile
users,
to
be
used
for
illustration
throughout
the
paper.
Next
we
+L+
discuss
related
work,
and
evaluate
each
work
relative
to
its
ability
to
support
the
example
application.
We
+L+
then
present
the
Sync
framework,
motivating
each
feature
with
its
use
in
the
example
application.
+L+
INTRODUCTION

Strange
Bedfellows:
Issues
in
Object
Naming
Under
Unix
+L+
Douglas
B.
Orr
,
Robert
W.
Mecklenburg
and
Ravindra
Kuramkote
+L+
Department
of
Computer
Science
+L+
University
of
Utah
+L+
Salt
Lake
City,
UT
84112
USA
+L+
E-mail:
dbo@cs.utah.edu,
mecklen@cs.utah.edu,
kuramkot@cs.utah.edu
+L+
Abstract
+L+
Naming
plays
a
key
role
in
the
design
of
any
system
that
exports
services
or
resources.
Object
systems
+L+
may
export
many
different
categories
of
names:
instances,
components
of
records,
types,
etc.
Operating
+L+
systems
export
the
names
of
files,
devices,
and
services.
Integrating
an
object
base
with
existing
operating
system
facilities
can
improve
accessibility
of
the
+L+
object
base
resources.
We
consider
the
benefits
and
+L+
pitfalls
of
integrating
an
object
base
namespace
with
+L+
the
Unix
namespace.
1
+L+
1
Introduction

Testing
the
FM9001
Microprocessor
+L+
Kenneth
L.
Albin
,
Bishop
C.
Brock,
+L+
Warren
A.
Hunt
,
Jr.,
Lawrence
M.
Smith
+L+
Technical
Report
90
January
6,
1995
+L+
Computational
Logic,
Inc.
+L+
1717
West
Sixth
Street,
Suite
290
+L+
Austin,
Texas
78703-4776
+L+
TEL:
+1
512
322
9951
+L+
EMAIL:
hunt@cli.com
+L+
This
work
was
supported
in
part
at
Computational
Logic,
Inc.
and
by
the
+L+
Defense
Advanced
Research
Projects
Agency,
ARPA
Orders
6082
and
9151.
The
+L+
views
and
conclusions
contained
in
this
document
are
those
of
the
author(s)
and
+L+
should
not
be
interpreted
as
representing
the
official
policies,
either
expressed
or
+L+
implied,
of
Computational
Logic,
Inc.
+L+
Copyright
c
1995
Computational
Logic,
Inc.
+L+
+PAGE+

Secure
Group
Communications
Using
Key
Graphs
+L+
Chung
Kei
Wong
Mohamed
Gouda
Simon
S.
Lam
+L+
Department
of
Computer
Sciences
+L+
University
of
Texas
at
Austin
+L+
Austin,
TX
78712-1188
+L+
fckwong,gouda,lamg@cs.utexas.edu
+L+
Abstract
+L+
Many
emerging
applications
(e.g.,
teleconference,
real-time
+L+
information
services,
pay
per
view,
distributed
interactive
+L+
simulation,
and
collaborative
work)
are
based
upon
a
group
+L+
communications
model,
i.e.,
they
require
packet
delivery
+L+
from
one
or
more
authorized
senders
to
a
very
large
number
+L+
of
authorized
receivers.
As
a
result,
securing
group
communications
(i.e.,
providing
confidentiality,
integrity,
and
authenticity
of
messages
delivered
between
group
members)
+L+
will
become
a
critical
networking
issue.
+L+
In
this
paper,
we
present
a
novel
solution
to
the
scalability
problem
of
group/multicast
key
management.
We
+L+
formalize
the
notion
of
a
secure
group
as
a
triple
(U;
K;
R)
+L+
where
U
denotes
a
set
of
users,
K
a
set
of
keys
held
by
the
+L+
users,
and
R
a
user-key
relation.
We
then
introduce
key
+L+
graphs
to
specify
secure
groups.
For
a
special
class
of
key
+L+
graphs,
we
present
three
strategies
for
securely
distributing
rekey
messages
after
a
join/leave,
and
specify
protocols
+L+
for
joining
and
leaving
a
secure
group.
The
rekeying
strategies
and
join/leave
protocols
are
implemented
in
a
prototype
+L+
group
key
server
we
have
built.
We
present
measurement
+L+
results
from
experiments
and
discuss
performance
comparisons.
We
show
that
our
group
key
management
service,
using
any
of
the
three
rekeying
strategies,
is
scalable
to
large
+L+
groups
with
frequent
joins
and
leaves.
In
particular,
the
+L+
average
measured
processing
time
per
join/leave
increases
+L+
linearly
with
the
logarithm
of
group
size.
+L+
1
Introduction

Products
of
Domain
Models
+L+
Don
Batory
+L+
Department
of
Computer
Sciences
+L+
The
University
of
Texas
+L+
Austin,
Texas
78712
+L+
batory@cs.utexas.edu
+L+
Abstract
+L+
We
argue
that
domain
models
should
produce
four
basic
products:
identification
of
reusable
software
components,
definition
of
software
architectures
that
explain
how
components
can
be
composed,
a
demonstration
of
architecture
scalability,
and
a
direct
relationship
of
these
results
to
+L+
software
generation
of
target
systems.
+L+
1
Introduction

Verifying
adder
circuits
using
powerlists
+L+
William
Adams
+L+
Department
of
Computer
Sciences
+L+
The
University
of
Texas
at
Austin
+L+
Austin,
TX
78712-1188
+L+
USA
+L+
e-mail:
will@cs.utexas.edu
+L+
March
29,
1994
+L+
Abstract
+L+
We
define
the
ripple-carry
and
the
carry-lookahead
adder
circuits
in
the
+L+
powerlist
notation
and
we
use
the
powerlist
algebra
to
prove
that
these
+L+
circuits
correctly
implement
addition
for
natural
numbers
represented
as
+L+
bit
vectors.
+L+
0
Introduction

Algorithms
for
Fence
Design
+L+
Robert-Paul
Berretty
University
of
Utrecht,
Utrecht,
The
Netherlands
+L+
Ken
Goldberg
University
of
California
at
Berkeley,
CA,
USA
+L+
Mark
H.
Overmars
University
of
Utrecht,
Utrecht,
The
Netherlands
+L+
A.
Frank
van
der
Stappen
University
of
Utrecht,
Utrecht,
The
Netherlands
+L+
Abstract
+L+
A
common
task
in
automated
manufacturing
processes
is
to
orient
parts
prior
to
+L+
assembly.
We
address
sensorless
orientation
of
a
polygonal
part
on
a
conveyor
belt
by
+L+
a
sequence
of
stationary
fences
across
this
belt.
Since
fences
can
only
push
against
the
+L+
motion
of
the
belt,
it
is
a
challenging
problem
to
compute
fence
designs
which
orients
+L+
a
given
part.
In
this
paper,
we
give
several
polynomial-time,
algorithms
to
compute
+L+
fence
designs
which
are
optimal
with
respect
to
various
criteria.
We
address
both
+L+
frictionless
and
frictional
fences.
We
also
compute
modular
fence
designs
in
which
+L+
the
fence
angles
are
restricted
to
a
discrete
set
of
angles
instead
of
an
interval.
+L+
1
Introduction

Software
Engineering
+L+
Beginning
In
+L+
The
First
Computer
Science
Course
1
+L+
Jane
C.
Prey
James
P.
Cohoon
<</sep>,>
Greg
Fife
+L+
Department
of
Computer
Science
+L+
School
of
Engineering
and
Applied
Sciences
+L+
University
of
Virginia
+L+
Charlottesville,
VA
22903
+L+
Abstract.
The
demand
for
computing
and
computing
power
is
increasing
at
a
rapid
pace.
With
this
demand,
+L+
the
ability
to
develop,
enhance
and
maintain
software
is
a
top
priority.
Educating
students
to
do
competent
+L+
work
in
software
development,
enhancement
and
maintenance
has
become
a
complex
problem.
Software
+L+
engineering
concepts
are
typically
not
introduced
in
beginning
computer
science
courses.
Students
do
not
see
+L+
software
engineering
until
the
third
or
fourth
year
of
the
curriculum.
We
do
not
believe
students
can
acquire
+L+
an
adequate
software
engineering
foundation
with
the
present
approach.
We
believe
an
emphasis
on
software
+L+
engineering
should
begin
in
the
very
first
course
and
continue
throughout
the
curriculum.
We
are
redesigning
+L+
our
curriculum
to
reect
this.
The
first
course
of
the
new
curriculum
is
complete.
This
article
focuses
on
two
+L+
of
the
laboratory
activities
we
have
developed
which
deal
with
specific
software
engineering
concepts.
+L+
Introduction

Shade:
A
Fast
Instruction-Set
Simulator
+L+
for
Execution
Profiling
+L+
Bob
Cmelik
+L+
Sun
Microsystems,
Inc.
+L+
rfc@eng.sun.com
+L+
David
Keppel
+L+
University
of
Washington
+L+
pardo@cs.washington.edu
+L+
Abstract
+L+
Tracing
tools
are
used
widely
to
help
analyze,
design,
and
tune
+L+
both
hardware
and
software
systems.
This
paper
describes
a
tool
+L+
called
Shade
which
combines
efficient
instruction-set
simulation
+L+
with
a
flexible,
extensible
trace
generation
capability.
Efficiency
+L+
is
achieved
by
dynamically
compiling
and
caching
code
to
simulate
and
trace
the
application
program.
The
user
may
control
the
+L+
extent
of
tracing
in
a
variety
of
ways;
arbitrarily
detailed
application
state
information
may
be
collected
during
the
simulation,
but
+L+
tracing
less
translates
directly
into
greater
efficiency.
Current
+L+
Shade
implementations
run
on
SPARC
systems
and
simulate
the
+L+
SPARC
(Versions
8
and
9)
and
MIPS
I
instruction
sets.
This
+L+
paper
describes
the
capabilities,
design,
implementation,
and
performance
of
Shade,
and
discusses
instruction
set
emulation
in
+L+
general.
+L+
1.
Introduction

A
Portable
Parallel
N-body
Solver
+L+
E
Christopher
Lewis
Calvin
Lin
Lawrence
Snyder
George
Turkiyyah
+L+
Abstract
+L+
We
present
parallel
solutions
for
direct
and
fast
n-body
solvers
written
in
the
ZPL
+L+
language.
We
describe
the
direct
solver,
compare
its
performance
against
a
sequential
+L+
C
program,
and
show
performance
results
for
two
very
different
parallel
machines:
the
+L+
KSR-2
and
the
Paragon.
We
also
discuss
the
implementation
of
the
fast
solver
in
ZPL,
+L+
including
factors
pertinent
to
data
movement.
+L+
1
Introduction

User-Level
Threads
and
Interprocess
Communication
+L+
Michael
J.
Feeley
,
Jeffrey
S.
Chase
,
and
Edward
D.
Lazowska
+L+
Department
of
Computer
Science
and
Engineering,
FR-35
+L+
University
of
Washington
+L+
Seattle,
WA
98195
+L+
Technical
Report
93-02-03
+L+
Abstract
+L+
User-level
threads
have
performance
and
flexibility
advantages
over
both
Unix-like
processes
+L+
and
kernel
threads.
However,
the
performance
of
user-level
threads
may
suffer
in
multipro-grammed
environments,
or
when
threads
block
in
the
kernel
(e.g.,
for
I/O).
These
problems
+L+
can
be
particularly
severe
in
tasks
that
communicate
frequently
using
IPC
(e.g.,
multithreaded
+L+
servers),
due
to
interactions
between
the
user-level
thread
scheduler
and
the
operating
system
+L+
IPC
primitives.
Efficient
IPC
typically
involves
processor
handoff
that
blocks
the
caller
and
+L+
unblocks
a
thread
in
the
callee;
when
combined
with
user-level
threads,
this
can
cause
problems
+L+
for
both
caller
and
callee,
particularly
if
the
caller
thread
should
subsequently
block.
+L+
In
this
paper
we
describe
a
new
user-level
thread
package,
called
OThreads,
designed
to
+L+
support
blocking
and
efficient
IPC
for
a
system
based
on
traditional
kernel
threads.
We
discuss
+L+
the
extent
to
which
these
problems
can
be
solved
at
the
user
level
without
kernel
changes
+L+
such
as
scheduler
activations.
Our
conclusion
is
that
problems
caused
by
application-controlled
+L+
blocking
and
IPC
can
be
resolved
in
the
user-level
thread
package,
but
that
problems
due
+L+
to
multiprogramming
workload
and
unanticipated
blocking
such
as
page
faults
require
kernel
+L+
changes
such
as
scheduler
activations.
+L+
1
Introduction

Interface
Timing
Verification
with
+L+
Combined
Max
and
Linear
Constraints
+L+
Elizabeth
Walkup
,
Gaetano
Borriello
+L+
Department
of
Computer
Science
and
Engineering
+L+
University
of
Washington
+L+
Seattle,
WA
98195
+L+
Technical
Report
94-03-04
+L+
June
3,
1994
+L+
+PAGE+

Fast
Rendering
of
Subdivision
Surfaces
+L+
Kari
Pulli
+L+
University
of
Washington
+L+
Seattle,
WA
+L+
Mark
Segal
+L+
Silicon
Graphics
Inc.
+L+
Abstract
+L+
Subdivision
surfaces
provide
a
curved
surface
representation
that
is
useful
in
a
number
of
applications,
including
modeling
surfaces
of
arbitrary
topological
type
[5]
,
fitting
scattered
data
[6]
,
and
geometric
compression
+L+
and
automatic
level-of-detail
generation
using
wavelets
[8]
.
Subdivision
surfaces
also
provide
an
attractive
representation
for
fast
rendering,
since
they
can
directly
represent
complex
surfaces
of
arbitrary
topology.
This
direct
+L+
representation
contrasts
with
traditional
approaches
such
as
trimmed
NURBS,
in
which
tesselating
trim
regions
+L+
dominates
rendering
time,
and
algebraic
implicit
surfaces,
in
which
rendering
requires
resultants,
root
finders,
or
+L+
other
computationally
expensive
techniques.
+L+
We
present
a
method
for
subdivision
surface
triangulation
that
is
fast,
uses
minimum
memory,
and
is
simpler
in
+L+
structure
than
a
naive
rendering
method
based
on
direct
subdivision.
These
features
make
the
algorithm
amenable
+L+
to
implementation
on
dedicated
geometry
engine
processors,
allowing
high
rendering
performance
on
appropri
+L+
ately
equipped
graphics
hardware.
+L+
CR
Categories
and
Subject
Descriptors:
I.3.6
[Computer
Graphics]:
Methodology
and
Techniques.
+L+
Additional
Key
Words:
subdivision
surfaces,
surface
rendering.
+L+
1
Introduction

Random
Striping
for
+L+
News
on
Demand
Servers
+L+
Juan
Alemany
and
Jayram
S.
Thathachar
+L+
Technical
Report
UW-CSE-97-02-02
+L+
February,
1997
+L+
Department
of
Computer
Science
and
Engineering
+L+
University
of
Washington
+L+
Box
352350
+L+
Seattle,
WA
98195
+L+
+PAGE+

The
Error
in
Polynomial
Tensor-Product,
+L+
and
Chung-Yao,
Interpolation
+L+
Carl
de
Boor
+L+
Abstract.
A
formula
for
the
error
in
Chung-Yao
interpolation
announced
earlier
is
proved
(by
induction).
In
the
process,
a
bivariate
divided
difference
identity
of
independent
interest
is
proved.
Also,
an
inductive
proof
of
an
error
formula
for
polynomial
interpolation
by
tensor-products
is
given.
The
main
tool
is
a
(convenient
notation
for
a)
multi-variate
divided
difference.
+L+
Surface
Fitting
and
Multiresolution
Methods
35
+L+
A.
Le
Mehaute,
C.
Rabut,
and
L.
L.
Schumaker
(eds.),
pp.
35-50.
+L+
Copyright
o
c
1997
by
Vanderbilt
University
Press,
Nashville,
TN.
+L+
ISBN
0-8265-1294-1.
+L+
All
rights
of
reproduction
in
any
form
reserved.
+L+
+PAGE+

Recovering
Shape
by
Purposive
Viewpoint
Adjustment
+L+
Kiriakos
N.
Kutulakos
Charles
R.
Dyer
+L+
Computer
Sciences
Department
+L+
University
of
Wisconsin
+L+
Madison,
Wisconsin
53706
+L+
Technical
Report
#1035
+L+
August
1991
+L+
Abstract
+L+
We
present
an
approach
for
recovering
surface
shape
from
the
occluding
contour
using
an
active
(i.e.,
moving)
observer.
It
is
based
on
a
relation
between
the
geometries
of
+L+
a
surface
in
a
scene
and
its
occluding
contour:
If
the
viewing
direction
of
the
observer
+L+
is
along
a
principal
direction
for
a
surface
point
whose
projection
is
on
the
contour,
+L+
surface
shape
(i.e.,
curvature)
at
the
surface
point
can
be
recovered
from
the
contour.
+L+
Unlike
previous
approaches
for
recovering
shape
from
the
occluding
contour,
we
use
an
+L+
observer
that
purposefully
changes
viewpoint
in
order
to
achieve
a
well-defined
geometric
relationship
with
respect
to
a
3D
shape
prior
to
its
recognition.
We
show
that
there
+L+
is
a
simple
and
efficient
viewing
strategy
that
allows
the
observer
to
align
their
viewing
+L+
direction
with
one
of
the
two
principal
directions
for
a
point
on
the
surface.
This
strategy
depends
on
only
curvature
measurements
on
the
occluding
contour
and
therefore
+L+
demonstrates
that
recovering
quantitative
shape
information
from
the
contour
does
not
+L+
require
knowledge
of
the
velocities
or
accelerations
of
the
observer.
Experimental
results
+L+
demonstrate
that
our
method
can
be
easily
implemented
and
can
provide
reliable
shape
+L+
information
from
the
occluding
contour.
+L+
The
support
of
the
National
Science
Foundation
under
Grant
No.
IRI-9002582
is
gratefully
acknowledged.
+L+
+PAGE+

Team
Learning
of
Formal
Languages
+L+
Sanjay
Jain
+L+
Dept.
of
Info.
Systems
&
Computer
Science
+L+
National
University
of
Singapore
+L+
Singapore
0511,
Republic
of
Singapore
+L+
sanjay@iscs.nus.sg
+L+
Arun
Sharma
+L+
School
of
Computer
Science
and
Engineering
+L+
The
University
of
New
South
Wales
+L+
Sydney,
NSW
2052,
Australia
+L+
arun@cse.unsw.edu.au
+L+
Abstract
+L+
A
team
of
learning
machines
is
a
multiset
of
+L+
learning
machines.
A
team
is
said
to
successfully
learn
a
concept
just
in
case
each
member
+L+
of
some
nonempty
subset,
of
predetermined
+L+
size,
of
the
team
learns
the
concept.
+L+
Team
learning
of
computer
programs
for
+L+
computable
functions
from
their
graphs
has
+L+
been
studied
extensively.
However,
team
+L+
learning
of
languages
turns
out
to
be
a
+L+
more
suitable
theoretical
model
for
studying
+L+
computational
limits
on
multi-agent
machine
+L+
learning.
The
main
reason
for
this
is
that
+L+
language
learning
can
model
both
learning
+L+
from
positive
data
and
learning
from
positive
+L+
and
negative
data,
whereas
function
learning
+L+
models
only
learning
from
positive
and
negative
data.
+L+
Some
theoretical
results
about
learnability
of
+L+
formal
languages
by
teams
of
algorithmic
machines
are
surveyed.
Some
new
results
about
+L+
restricted
classes
of
languages
are
presented.
+L+
These
results
are
mainly
about
two
issues:
redundancy
and
aggregation.
The
issue
of
redundancy
deals
with
the
impact
of
increasing
+L+
the
size
of
a
team
and
increasing
the
number
+L+
of
machines
required
to
be
successful.
The
+L+
issue
of
aggregation
deals
with
conditions
under
which
a
team
may
be
replaced
by
a
single
+L+
machine
without
any
loss
in
learning
ability.
+L+
Scenarios
which
can
be
modeled
by
team
+L+
learning
are
also
presented.
+L+
1
INTRODUCTION

Clustering
via
Concave
Minimization
+L+
P.
S.
Bradley
and
O.
L.
Mangasarian
W.
N.
Street
+L+
Computer
Sciences
Department
Computer
Science
Department
+L+
University
of
Wisconsin
Oklahoma
State
University
+L+
1210
West
Dayton
Street
205
Mathematical
Sciences
+L+
Madison,
WI
53706
Stillwater,
OK
74078
+L+
email:
paulb@cs.wisc.edu,
olvi@cs.wisc.edu
email:nstreet@cs.okstate.edu
+L+
Abstract
+L+
The
problem
of
assigning
m
points
in
the
n-dimensional
real
space
+L+
R
n
to
k
clusters
is
formulated
as
that
of
determining
k
centers
in
+L+
R
n
such
that
the
sum
of
distances
of
each
point
to
the
nearest
+L+
center
is
minimized.
If
a
polyhedral
distance
is
used,
the
problem
+L+
can
be
formulated
as
that
of
minimizing
a
piecewise-linear
concave
+L+
function
on
a
polyhedral
set
which
is
shown
to
be
equivalent
to
+L+
a
bilinear
program:
minimizing
a
bilinear
function
on
a
polyhedral
set.
A
fast
finite
k-Median
Algorithm
consisting
of
solving
+L+
few
linear
programs
in
closed
form
leads
to
a
stationary
point
of
+L+
the
bilinear
program.
Computational
testing
on
a
number
of
real-world
databases
was
carried
out.
On
the
Wisconsin
Diagnostic
+L+
Breast
Cancer
(WDBC)
database,
k-Median
training
set
correctness
was
comparable
to
that
of
the
k-Mean
Algorithm,
however
its
+L+
testing
set
correctness
was
better.
Additionally,
on
the
Wisconsin
+L+
Prognostic
Breast
Cancer
(WPBC)
database,
distinct
and
clinically
important
survival
curves
were
extracted
by
the
k-Median
+L+
Algorithm,
whereas
the
k-Mean
Algorithm
failed
to
obtain
such
+L+
distinct
survival
curves
for
the
same
database.
+L+
1
Introduction

High-Bandwidth
Address
Translation
+L+
for
Multiple-Issue
Processors
+L+
Todd
M.
Austin
Gurindar
S.
Sohi
+L+
Computer
Sciences
Department
+L+
University
of
Wisconsin-Madison
+L+
1210
W.
Dayton
Street
+L+
Madison,
WI
53706
+L+
faustin,sohig@cs.wisc.edu
+L+
Abstract
+L+
In
an
effort
to
push
the
envelope
of
system
performance,
microprocessor
designs
are
continually
exploiting
higher
levels
of
+L+
instruction-level
parallelism,
resulting
in
increasing
bandwidth
demands
on
the
address
translation
mechanism.
Most
current
microprocessor
designs
meet
this
demand
with
a
multi-ported
TLB.
While
+L+
this
design
provides
an
excellent
hit
rate
at
each
port,
its
access
latency
and
area
grow
very
quickly
as
the
number
of
ports
is
increased.
+L+
As
bandwidth
demands
continue
to
increase,
multi-ported
designs
+L+
will
soon
impact
memory
access
latency.
+L+
We
present
four
high-bandwidth
address
translation
mechanisms
+L+
with
latency
and
area
characteristics
that
scale
better
than
a
multi-ported
TLB
design.
We
extend
traditional
high-bandwidth
memory
+L+
design
techniques
to
address
translation,
developing
interleaved
and
+L+
multi-level
TLB
designs.
In
addition,
we
introduce
two
new
designs
+L+
crafted
specifically
for
high-bandwidth
address
translation.
Piggyback
ports
are
introduced
as
a
technique
to
exploit
spatial
locality
in
+L+
simultaneous
translation
requests,
allowing
accesses
to
the
same
virtual
memory
page
to
combine
their
requests
at
the
TLB
access
port.
+L+
Pretranslation
is
introduced
as
a
technique
for
attaching
translations
+L+
to
base
register
values,
making
it
possible
to
reuse
a
single
translation
many
times.
+L+
We
perform
extensive
simulation-based
studies
to
evaluate
our
+L+
designs.
We
vary
key
system
parameters,
such
as
processor
model,
+L+
page
size,
and
number
of
architected
registers,
to
see
what
effects
+L+
these
changes
have
on
the
relative
merits
of
each
approach.
A
number
of
designs
show
particular
promise.
Multi-level
TLBs
with
as
+L+
few
as
eight
entries
in
the
upper-level
TLB
nearly
achieve
the
performance
of
a
TLB
with
unlimited
bandwidth.
Piggyback
ports
+L+
combined
with
a
lesser-ported
TLB
structure,
e.g.,
an
interleaved
or
+L+
multi-ported
TLB,
also
perform
well.
Pretranslation
over
a
single-ported
TLB
performs
almost
as
well
as
a
same-sized
multi-level
+L+
TLB
with
the
added
benefit
of
decreased
access
latency
for
physically
indexed
caches.
+L+
1
Introduction

OPTIMAL
PROCESSOR
ASSIGNMENT
FOR
PARALLEL
+L+
DATABASE
DESIGN
+L+
SHAHRAM
GHANDEHARIZADEH
,
ROBERT
R.
MEYER
,
GARY
L.
SCHULTZ
AND
+L+
JONATHAN
YACKEL
+L+
Abstract.
The
computing
time
benefits
of
parallelism
in
database
systems
(achieved
by
using
multiple
processors
to
execute
a
query)
must
be
weighed
against
communication,
startup,
and
+L+
termination
overhead
costs
that
increase
as
a
function
of
the
number
of
processors
used.
We
consider
problems
of
minimizing
overhead
subject
to
allocating
data
among
the
processors
according
+L+
to
specified
loads.
We
present
lower
bounds
for
these
combinatorial
problems
and
demonstrate
how
+L+
processors
may
be
optimally
assigned
for
some
problem
classes.
+L+
1.
Introduction.
In
highly-parallel
database
machines
(e.g.,
Gamma
[2],
Bubba

1995
Computer
Science
Department
MQP
+L+
Review
+L+
Robert
E.
Kinicki
+L+
Craig
E.
Wills
+L+
Computer
Science
Department
+L+
Worcester
Polytechnic
Institute
+L+
Worcester,
MA
01609
+L+
WPI-CS-TR-95-01
+L+
August
1,
1995
+L+
Abstract
+L+
This
report
presents
results
of
a
peer
review
of
MQPs
conducted
within
+L+
the
Computer
Science
Department
during
the
Summer
of
1995
as
part
of
a
+L+
campus-wide
MQP
review.
The
goal
of
the
report
is
to
assess
whether
the
+L+
department
MQPs
are
accomplishing
their
educational
goals.
The
report
+L+
identifies
problems
that
need
to
be
addressed
and
trends
that
need
to
be
+L+
continued
to
make
the
MQPs
a
worthwhile
learning
experience.
It
reflects
+L+
data
and
evaluations
for
27
MQPs,
involving
43
computer
science
students,
+L+
that
were
completed
between
the
Summer
of
1994
and
the
Spring
of
1995.
+L+
The
report
also
makes
comparisons
to
similar
reviews
done
in
1991
and
1993.
+L+
Overall,
the
large
majority
of
the
projects
are
meeting
the
educational
+L+
goals
of
the
department
as
good
learning
experiences.
The
reviews
indicate
+L+
the
overall
quality
of
the
projects
is
good,
about
the
same
as
in
1993
and
+L+
a
little
better
than
1991.
The
report
draws
a
number
of
conclusions
about
+L+
the
success
of
the
projects
based
upon
the
data
collected
and
evaluations
+L+
done
for
this
review.
The
report
concludes
with
recommendations
for
future
+L+
projects.
+L+
+PAGE+

In
Proc.
8th
IASTED
Int'l
Conf.
on
Parallel
and
Distributed
Computing
and
Systems
(Chicago,
IL,
USA),
+L+
c
IASTED/ACTA
Press
(Anaheim/Calgary/Z
urich),
pp.
144-148,
Oct.
1996.
[ISBN:
0-88986-213-3]
+L+
HPF
and
MPI
Implementation
of
the
NAS
Parallel
Benchmarks
+L+
Supported
by
Integrated
Program
Engineering
Tools
+L+
Christian
Cl
emencon
Karsten
M.
Decker
Vaibhav
R.
Deshpande
+L+
Akiyoshi
Endo
Josef
Fritscher
Paulo
A.
R.
Lorenzo
+L+
Norio
Masuda
Andreas
Muller
Roland
R
uhl
+L+
William
Sawyer
Brian
J.
N.
Wylie
Frank
Zimmermann
+L+
Centro
Svizzero
di
Calcolo
Scientifico
(CSCS/SCSC)
and
+L+
NEC
European
Supercomputer
Systems,
Swiss
Branch
+L+
CH-6928
Manno,
Switzerland
+L+
http://www.cscs.ch/Official/Project
CSCS-NEC.html
+L+
Abstract:
High
Performance
Fortran
(HPF)
compilers
+L+
and
communication
libraries
with
the
standardized
Message
Passing
Interface
(MPI)
are
becoming
widely
available,
easing
the
development
of
portable
parallel
applications
on
distributed-memory
parallel
processor
systems.
+L+
The
recently
developed
Annai
tool
environment
supports
+L+
programming,
debugging
and
tuning
of
both
HPF-
and
+L+
MPI-based
applications.
Considering
code
development
+L+
and
subsequent
maintenance
time
to
be
as
important
as
ultimate
performance,
we
address
how
sequential
Fortran-77
+L+
versions
of
the
familiar
NAS
Parallel
Benchmark
kernels
+L+
can
be
expediently
parallelized
with
appropriate
tool
support.
While
automatic
parallelization
of
scientific
applications
written
in
traditional
sequential
languages
remains
+L+
largely
impractical,
Annai
provides
users
with
high-level
+L+
language
extensions
and
integrated
program
engineering
+L+
support
tools.
In
this
paper,
Annai
support
is
demonstrated
primarily
focusing
on
the
MG
(multigrid)
kernel,
+L+
with
complementary
examples
selected
from
the
other
four
+L+
kernels.
Respectable
performance
and
good
scalability
+L+
in
most
cases
are
obtained
with
this
straightforward
par-allelization
strategy,
even
without
recourse
to
platform-specific
optimizations
or
major
program
transformations.
+L+
Keywords:
HPF
&
MPI
parallelization;
parallel
program
+L+
engineering
tools.
+L+
1
Introduction

Confluent
Preorder
Parsing
+L+
CS-TR-95-03
+L+
HO,
Kei
Shiu
Edward
and
CHAN,
Lai
Wan
+L+
Department
of
Computer
Science
+L+
The
Chinese
University
of
Hong
Kong
+L+
Shatin,
N.T.,
Hong
Kong
+L+
email
:
ho052@cs.cuhk.hk
and
lwchan@cs.cuhk.hk
+L+
KEYWORDS:
Neural
Networks,
Connectionist
Syntactic
Parsing,
RAAM,
Holistic
Transformation,
Confluent
+L+
Preorder
Parser,
Linearization
of
a
Hierarchical
Parse
Tree,
Parsing
Erroneous
Sentences,
Syntactic
+L+
Disambiguation
+L+
Abstract
+L+
In
this
paper,
syntactic
parsing
is
discussed
in
the
context
of
connectionism.
A
new
model
-
the
Confluent
+L+
Preorder
Parser
(CPP),
is
proposed
which
exemplifies
the
holistic
parsing
paradigm.
Holistic
parsing
has
the
+L+
advantage
that
little
assumption
has
to
be
made
concerning
the
detailed
parsing
algorithm,
which
is
often
+L+
unknown
or
debatable,
especially
when
human
language
understanding
is
concerned.
In
the
CPP,
syntactic
+L+
parsing
is
achieved
by
transforming
in
a
oneshot
manner,
from
the
connectionist
representation
of
the
sentence
+L+
to
the
connectionist
representation
of
the
preorder
traversal
of
its
parse
tree,
instead
of
the
parse
tree
itself.
As
+L+
revealed
by
the
simulation
experiments,
generalization
performance
is
excellent
(as
high
as
90%).
Besides,
the
+L+
CPP
is
also
capable
of
parsing
erroneous
sentences
and
resolving
syntactic
ambiguities.
A
systematic
study
is
+L+
conducted
to
explore
the
range
of
factors
which
can
affect
the
effectiveness
of
it.
This
error-recovery
capability
+L+
is
especially
useful
in
natural
language
processing
when
incomplete
or
even
ungrammatical
sentences
are
to
be
+L+
dealt
with.
+L+
1.
Introduction

DRAFT
June
2,
1996:
+L+
Learning
stable
concepts
in
domains
with
hidden
changes
in
context
+L+
Michael
Harries
+L+
Department
of
Artificial
Intelligence
+L+
School
of
Computer
Science
and
Engineering
+L+
University
of
NSW,
Australia
+L+
mbh@cse.unsw.edu.au
+L+
Kim
Horn
+L+
Predictive
Strategies
Unit
+L+
Australian
Gilt
Securities
Limited
+L+
Australia
+L+
kim@ags.com.au
+L+
Abstract
+L+
This
paper
presents
Splice,
a
batch
meta-learning
system,
designed
to
learn
locally
stable
concepts
in
domains
with
hidden
changes
+L+
in
context.
The
majority
of
machine
learning
+L+
algorithms
assume
that
target
concepts
remain
stable
over
time.
In
many
domains
this
+L+
assumption
is
invalid.
For
example,
financial
prediction,
medical
diagnosis,
and
network
performance
are
domains
in
which
target
concepts
may
not
remain
stable.
Unstable
target
concepts
are
often
due
to
changes
+L+
in
a
hidden
context.
Existing
works
on
learning
in
the
presence
of
hidden
changes
in
con
+L+
text
use
an
incremental
learning
approach.
+L+
1
INTRODUCTION

A
Space
of
Presentation
Emphasis
Techniques
for
Visualizing
Graphs
+L+
Emanuel
G.
Noik
+L+
Computer
Systems
Research
Institute
+L+
University
of
Toronto
+L+
6
King's
College
Road
+L+
Toronto,
Ontario,
Canada
m4s
1a1
+L+
e-mail:
noik@db.toronto.edu
+L+
Telephone:
(416)
978
8609
+L+
Abstract
+L+
The
graph
topo-visual
formalism
has
been
shown
to
+L+
be
well-suited
to
the
task
of
visualizing
complex
relations
on
a
set
of
elements.
Unfortunately,
most
visual
+L+
formalisms
do
not
scale
very
well.
This
observation
is
+L+
particularly
true
of
graphs,
which
even
when
hand-drawn
+L+
by
an
artist,
are
seldom
meaningful
when
the
number
of
+L+
nodes
or
links
exceeds
a
very
modest
threshold
typically
only
a
few
hundred
elements.
This
severe
limitation
+L+
has
prompted
many
researchers
to
seek
alternative
visualization
techniques
that
may
eliminate,
or,
at
the
very
+L+
least,
raise
this
threshold.
+L+
In
this
paper
we
analyze
these
recent
efforts,
describe
+L+
an
abstract
space
of
presentation
emphasis
techniques,
+L+
and
locate
the
current
approaches
within
this
space.
The
+L+
contributions
of
this
paper
are
several:
(1)
a
significant
+L+
portion
of
recent
work
is
collected
and
reviewed;
(2)
a
+L+
common
set
of
criteria
and
a
taxonomy
of
graph
views
+L+
are
proposed;
these,
(3)
permit
a
more
direct
comparison
+L+
of
previous
work;
which
helps
to,
(4)
identify
common
+L+
shortcomings
and
limitations;
which
in
turn,
(5)
suggest
+L+
future
directions.
+L+
Keywords:
presentation
emphasis
techniques,
fisheye
+L+
views,
relational
data
visualization,
graphs,
nested
+L+
graphs.
+L+
1
Introduction

The
Semantics
of
the
C
Programming
Language
+L+
Yuri
Gurevich
and
James
K.
Huggins
+L+
EECS
Department,
University
of
Michigan,
Ann
Arbor,
MI
48109-2122,
USA
+L+
February
19,
1993
+L+
This
paper
first
appeared
in
[GH2],
and
incorporates
the
corrections
indicated
in
[GH3].
+L+
0
Introduction

Defining
the
Java
Virtual
Machine
as
Platform
+L+
for
Provably
Correct
Java
Compilation
?
+L+
Egon
Borger
1
Wolfram
Schulte
2
+L+
1
Universita
di
Pisa,
Dipartimento
di
Informatica,
I-56125
Pisa,
Italy
+L+
boerger@di.unipi.it
+L+
2
Universitat
Ulm,
Fakultat
fur
Informatik,
D-89069
Ulm,
Germany
+L+
wolfram@informatik.uni-ulm.de
+L+
Abstract.
We
provide
concise
abstract
code
for
running
the
Java
Virtual
Machine
(JVM)
to
execute
compiled
Java
programs,
and
define
a
+L+
general
compilation
scheme
of
Java
programs
to
JVM
code.
These
definitions,
together
with
the
definition
of
an
abstract
interpreter
of
Java
+L+
programs
given
in
our
previous
work
[3],
allow
us
to
prove
that
any
+L+
compiler
that
satisfies
the
conditions
stated
in
this
paper
compiles
Java
+L+
code
correctly.
In
addition
we
have
validated
our
JVM
and
compiler
+L+
specification
through
experimentation.
+L+
The
modularity
of
our
definitions
for
Java,
the
JVM
and
the
compilation
+L+
scheme
exhibit
orthogonal
language,
machine
and
compiler
components,
+L+
which
fit
together
and
provide
the
basis
for
a
stepwise
and
provably
correct
design-for-reuse.
As
a
by-product
we
provide
a
challenging
realistic
+L+
case
study
for
mechanical
verification
of
a
compiler
correctness
proof.
+L+
1
Introduction

Reasoning
about
Other
Agents:
Philosophy,
+L+
Theory,
and
Implementation.
+L+
Piotr
J.
Gmytrasiewicz
and
Edmund
H.
Durfee
+L+
Department
of
Computer
Science
+L+
Hebrew
University,
Jerusalem,
Israel
+L+
Department
of
Electrical
Engineering
and
Computer
Science
+L+
University
of
Michigan,
Ann
Arbor,
Michigan
48109
+L+
piotr@cs.huji.ac.il,
durfee@engin.umich.edu
+L+
Abstract
+L+
Drawing
on
on
our
work
in
the
area
of
Distributed
Artificial
Intelligence,
we
propose
the
rudiments
of
a
view
of
multiagent
reasoning
that
relates
current
philosophical
+L+
intuitions,
theoretical
foundations,
and
preliminary
implementation.
The
philosophical
+L+
position
we
take
is
a
combination
of
Daniel
Dennett's
philosophy
of
the
ladder
of
per-sonhood
(consisting
of
rationality,
intentionality,
stance,
reciprocity,
communication,
+L+
and
consciousness)
on
one
hand,
and
the
utilitarian
philosophy
of
selfish
utility
maximization
on
the
other
hand.
The
theories
we
incorporate
are
logics
of
knowledge
and
+L+
belief,
which
in
addressing
the
multiagent
issues
can
be
developed
based
on
a
recursive
+L+
version
of
the
Kripke
structure,
and
the
related
fields
of
utility,
decision
and
game
+L+
theories.
Our
preliminary
implementation,
the
Recursive
Modeling
Method
(RMM),
+L+
lets
an
agent
coordinate
its
actions
with
the
actions
of
other
agents,
cooperate
with
+L+
them
when
appropriate,
and
rationally
choose
an
optimal
form
of
communication
with
+L+
them.
+L+
1
Introduction

Using
MICE
to
Study
Intelligent
Dynamic
Coordination
+L+
Edmund
H.
Durfee
and
Thomas
A.
Montgomery
+L+
Department
of
Electrical
Engineering
and
Computer
Science
+L+
University
of
Michigan
+L+
Ann
Arbor,
Michigan
48109
+L+
(313)
936-1563
+L+
durfee@caen.engin.umich.edu
+L+
Abstract
+L+
We
describe
a
flexible
experimental
testbed,
called
MICE,
for
distributed
artificial
intelligence
+L+
research.
We
argue
that
the
adoption
of
MICE
(or
some
other
standard
testbed)
by
the
distributed
+L+
artificial
intelligence
community
can
draw
together
the
community
and
permit
a
much
greater
level
+L+
of
exchange
of
ideas,
formalisms,
and
techniques.
MICE
allows
an
experimenter
to
specify
the
+L+
constraints
and
characteristics
of
an
environment
in
which
agents
are
simulated
to
act
and
interact,
+L+
and
does
not
assume
any
particular
implementation
of
an
agent's
reasoning
architecture.
MICE
+L+
therefore
provides
a
platform
for
investigating
and
evaluating
alternative
reasoning
architectures
+L+
and
coordination
mechanisms
in
many
different
simulated
environments.
We
outline
the
design
+L+
of
MICE
and
illustrate
its
flexibility
by
describing
simulated
environments
that
model
the
coordination
issues
in
domains
such
as
predators
chasing
prey,
predators
attacking
each
other,
agents
+L+
fighting
a
fire,
and
diverse
robots
that
are
working
together.
In
addition,
we
note
that
MICE's
+L+
ability
to
simulate
multi-agent
environments
makes
it
an
ideal
platform
for
studying
reasoning
in
+L+
dynamic
worlds;
we
can
associate
functionality
to
arbitrary
objects
in
order
to
trigger
changes
in
+L+
the
environment.
We
conclude
by
discussing
the
status
of
MICE
and
how
we
are
using
MICE
in
+L+
our
current
research.
+L+
0
This
research
was
sponsored,
in
part,
by
the
University
of
Michigan
under
a
Rackham
Faculty
Research
+L+
Grant,
and
by
a
Bell
Northern
Research
Postgraduate
Award.
+L+
+PAGE+

Observational
Uncertainty
in
Plan
Recognition
Among
Interacting
+L+
Robots
+L+
Marcus
J.
Huber
+L+
Edmund
H.
Durfee
+L+
Distributed
Intelligent
Agents
Group
(DIAG)
+L+
Artificial
Intelligence
Laboratory
+L+
The
University
of
Michigan
+L+
Ann
Arbor,
Michigan
48109-2110
+L+
marcush@engin.umich.edu,
durfee@engin.umich.edu
+L+
May
16,
1994
+L+
Abstract
+L+
Plan
recognition
is
the
process
of
observing
another
agent's
behavior(s)
and
inferring
what,
and
+L+
possibly
why,
the
agent
is
acting
as
it
is.
Plan
recognition
becomes
a
very
important
means
of
acquiring
+L+
such
information
about
other
agents
in
situations
and
domains
where
explicit
communication
is
either
+L+
very
costly,
dangerous,
or
impossible.
Performing
plan
recognition
in
a
physical
domain
(i.e.
the
real
+L+
world)
forces
the
world's
ubiquitous
uncertainty
upon
the
observing
agent
because
of
the
necessity
to
+L+
use
real
sensors
to
make
the
observations.
We
have
developed
a
multiple
resolution,
hierarchical
plan
+L+
recognition
system
to
coordinate
the
motion
of
two
interacting
mobile
robots.
Uncertainty
arises
in
the
+L+
system
from
dead
reckoning
errors
that
accumulate
while
the
robots
are
moving,
as
well
as
by
errors
+L+
in
the
computer
vision
system
that
is
used
to
detect
the
other
agent's
behaviors.
Based
upon
belief
+L+
networks,
the
plan
recognition
system
gracefully
degrades
in
performance
as
the
level
of
uncertainty
+L+
about
observations
increase.
+L+
1
Introduction

Hierarchical
Path
Views:
A
Model
Based
on
Fragmentation
and
+L+
Transportation
Road
Types
+L+
Yun-Wu
Huangy,
Ning
Jingz,
and
Elke
A.
Rundensteinery
+L+
e-mail:
[ywh
j
jning
j
rundenst]
@eecs.umich.edu
+L+
Dept.
of
Electrical
Engineering
and
Computer
Science,
Univ.
of
Michigan,
Ann
Arbor,
MI48109
+L+
Dept.
of
Electrical
Engineering,
Changsha
Institute
of
Technology,
Changsha,
Hunan,
China
+L+
Abstract
+L+
Efficient
path
query
processing
necessary
for
route
guidance
has
been
identified
as
one
of
the
key
requirements
+L+
for
Intelligent
Transportation
Systems
(ITS)
applications.
+L+
While
precomputing
the
view
of
all
shortest
paths
provides
+L+
the
most
efficient
path
retrieval,
the
view
maintenance
and
+L+
storage
costs
become
unrealistic
for
large
ITS
networks.
Based
+L+
on
ITS
road
type
classification,
we
propose
a
hierarchical
+L+
path
view
approach,
in
which
the
path
view
maintenance
+L+
and
storage
costs
are
dramatically
reduced
at
the
cost
of
+L+
negligible
loss
of
path
optimality.
Comparing
with
the
traditional
ITS
path
finding
approaches
that
use
A
or
hierarchical
A
,
our
hierarchical
approach
is
superior
in
three
+L+
areas:
1)
path
search
is
more
efficient,
2)
the
connecting
+L+
point
from
the
low-level
roads
to
the
high-level
roads
and
+L+
vice
versa
are
dynamically
determined
based
on
the
most
+L+
recent
traffic,
3)
within
one
region,
the
high-level
traffic
can
+L+
be
dynamically
rerouted
through
the
low-level
roads.
In
+L+
this
paper,
we
conduct
experiments
to
gain
insight
into
the
+L+
performance
of
our
proposed
algorithms
and
model,
as
well
+L+
as
to
contrast
the
difference
in
computational
resource
requirements
between
the
hierarchical
path
view
and
the
A
+L+
algorithms.
+L+
1
INTRODUCTION

In
Proceedings
of
the
Twelfth
Conference
on
Uncertainty
in
Artificial
Intelligence
(UAI-96),
+L+
Portland,
OR,
USA,
August
1996
+L+
Optimal
Factory
Scheduling
using
Stochastic
Dominance
A*
+L+
Peter
R.
Wurman
and
Michael
P.
Wellman
+L+
University
of
Michigan
+L+
Artificial
Intelligence
Laboratory
+L+
1101
Beal
Avenue
+L+
Ann
Arbor,
MI,
48109-2110
+L+
fpwurman,
wellmang@umich.edu
+L+
Abstract
+L+
We
examine
a
standard
factory
scheduling
+L+
problem
with
stochastic
processing
and
setup
+L+
times,
minimizing
the
expectation
of
the
+L+
weighted
number
of
tardy
jobs.
Because
+L+
the
costs
of
operators
in
the
schedule
are
+L+
stochastic
and
sequence
dependent,
standard
+L+
dynamic
programming
algorithms
such
as
+L+
A*
may
fail
to
find
the
optimal
schedule.
+L+
The
SDA*
(Stochastic
Dominance
A*)
algorithm
remedies
this
difficulty
by
relaxing
the
+L+
pruning
condition.
We
present
an
improved
+L+
state-space
search
formulation
for
these
problems
and
discuss
the
conditions
under
which
+L+
stochastic
scheduling
problems
can
be
solved
+L+
optimally
using
SDA*.
In
empirical
testing
+L+
on
randomly
generated
problems,
we
found
+L+
that
in
70%,
the
expected
cost
of
the
optimal
stochastic
solution
is
lower
than
that
of
+L+
the
solution
derived
using
a
deterministic
ap
+L+
proximation,
with
comparable
search
effort.
+L+
1
INTRODUCTION

Dynamic
Generation
and
Refinement
of
Concept
Hierarchies
for
+L+
Knowledge
Discovery
in
Databases
+L+
Jiawei
Han
and
Yongjian
Fu
+L+
School
of
Computing
Science
+L+
Simon
Fraser
University
+L+
Burnaby,
B.C.,
Canada
V5A
1S6
+L+
fhan,
yongjiang@cs.sfu.ca
+L+
Abstract
+L+
Concept
hierarchies
organize
data
and
concepts
in
hierarchical
forms
or
in
certain
partial
order,
which
+L+
helps
expressing
knowledge
and
data
relationships
in
databases
in
concise,
high
level
terms,
and
thus,
plays
+L+
an
important
role
in
knowledge
discovery
processes.
Concept
hierarchies
could
be
provided
by
knowledge
+L+
engineers,
domain
experts
or
users,
or
embedded
in
some
data
relations.
However,
it
is
sometimes
desirable
to
automatically
generate
some
concept
hierarchies
or
adjust
some
given
hierarchies
for
particular
+L+
learning
tasks.
In
this
paper,
the
issues
of
dynamic
generation
and
refinement
of
concept
hierarchies
are
+L+
studied.
The
study
leads
to
some
algorithms
for
automatic
generation
of
concept
hierarchies
for
numerical
attributes
based
on
data
distributions
and
for
dynamic
refinement
of
a
given
or
generated
concept
+L+
hierarchy
based
on
a
learning
request,
the
relevant
set
of
data
and
database
statistics.
These
algorithms
+L+
have
been
implemented
in
the
DBLearn
knowledge
discovery
system
and
tested
against
large
relational
+L+
databases.
The
experimental
results
show
that
the
algorithms
are
efficient
and
effective
for
knowledge
+L+
discovery
in
large
databases.
+L+
Keywords:
Knowledge
discovery
in
large
databases,
discovery
methods,
KDD
system
implementation,
al
+L+
gorithms,
dynamic
generation
and
refinement
of
concept
hierarchies.
+L+
1
Introduction

SUPPORTING
TECHNOLOGY
TRANSFER
OF
FORMAL
TECHNICAL
REVIEW
+L+
THROUGH
A
COMPUTER
SUPPORTED
COLLABORATIVE
REVIEW
SYSTEM
+L+
Philip
M.
Johnson
+L+
Department
of
Information
and
Computer
Sciences
+L+
University
of
Hawaii
+L+
Honolulu,
HI
96822
+L+
(808)
956-3489
+L+
johnson@hawaii.edu
+L+
Abstract
+L+
Formal
technical
review
(FTR)
is
an
essential
component
of
all
modern
software
quality
assessment,
assurance,
+L+
and
improvement
techniques,
and
is
acknowledged
to
be
+L+
the
most
cost-effective
form
of
quality
improvement
when
+L+
practiced
effectively.
However,
traditional
FTR
methods
+L+
such
as
inspection
are
very
difficult
to
adopt
in
organizations:
they
introduce
substantial
new
up-front
costs,
+L+
training,
overhead,
and
group
process
obstacles.
Sustained
commitment
from
high-level
management
along
+L+
with
substantial
resources
is
often
necessary
for
successful
+L+
technology
transfer
of
FTR.
+L+
Since
1991,
we
have
been
designing
and
evaluating
+L+
a
series
of
versions
of
a
system
called
CSRS:
an
instrumented,
computer-supported
cooperative
work
environment
for
formal
technical
review.
The
current
version
of
+L+
CSRS
includes
an
FTR
method
definition
language,
which
+L+
allows
organizations
to
design
their
own
FTR
method,
+L+
and
to
evolve
it
over
time.
This
paper
describes
how
our
+L+
approach
to
computer
supported
FTR
can
address
some
+L+
of
the
issues
in
technology
transfer
of
FTR.
+L+
1
Introduction

Combinatory
Differential
Fields:
+L+
An
Algebraic
Approach
to
+L+
Approximate
Computation
and
+L+
Constructive
Analysis
+L+
Karl
Aberer
+L+
TR-91-061
+L+
November
1991
+L+
Abstract
+L+
The
algebraic
structure
of
combinatory
differential
fields
is
constructed
to
provide
a
semantics
for
computations
in
analysis.
In
this
setting
programs,
approximations,
limits
and
operations
of
analysis
are
represented
+L+
as
algebraic
terms.
Analytic
algorithms
can
be
derived
by
algebraic
methods.
The
main
tool
in
this
construction
are
combinatory
models
which
are
inner
algebras
of
Engeler
graph
models.
As
an
universal
domain
+L+
of
denotational
semantics
the
lattice
structure
of
the
graph
models
allows
to
give
a
striking
simple
semantics
+L+
for
computations
with
approximations.
As
models
of
combinatory
algebra
they
provide
all
essential
computational
constructs,
including
recursion.
Combinatory
models
are
constructed
as
extensions
of
first
order
+L+
theories.
The
classical
first
order
theory
to
describe
analysis
is
the
theory
of
differential
fields.
It
turns
out
+L+
that
two
types
of
computational
constructs,
namely
composition
and
piecewise
definition
of
functions,
are
+L+
preferably
introduced
as
extensions
of
the
differential
fields
theory.
Combinatory
differential
fields
are
then
+L+
the
combinatory
models
of
these
enriched
differential
fields.
We
show
for
basic
algorithms
of
computational
+L+
analysis
how
their
combinatory
counterparts
are
derived
in
the
algebraic
setting.
We
illustrate
how
these
+L+
algorithms
are
suitable
to
be
implemented
in
a
computer
algebra
environment
like
mathematica.
+L+
Part
of
this
work
was
done
while
the
author
was
at
ETH
Zurich.
Submitted
to
Journal
of
Symbolic
+L+
Computation.
+L+
International
Computer
Science
Institute,
Berkeley,
CA
94704.
email:
aberer@icsi.berkeley.edu.
Supported
by
Schweizerische
Gesellschaft
zur
Forderung
der
Informatik
und
ihrer
Anwendungen
+L+
+PAGE+

An
Efficient
Parallel
Algorithm
+L+
for
Computing
a
Maximal
+L+
Independent
Set
in
a
+L+
Hypergraph
of
Dimension
3
+L+
Elias
Dahlhaus
1
+L+
Marek
Karpinski
2
+L+
Peter
Kelsen
3
+L+
TR-92-071
+L+
October,
1992
+L+
Abstract
+L+
The
paper
considers
the
problem
of
computing
a
maximal
independent
set
+L+
in
a
hypergraph
(see
[3]
and
[7]).
We
present
an
efficient
deterministic
NC
+L+
algorithm
for
finding
a
maximal
independent
set
in
a
hypergraph
of
dimension
+L+
3:
the
algorithm
runs
in
time
O(log
4
n)
time
on
n
+
m
processors
of
an
+L+
EREW
PRAM
and
is
optimal
up
to
a
polylogarithmic
factor.
Our
algorithm
+L+
adapts
the
technique
of
Goldberg
and
Spencer
([5])
for
finding
a
maximal
+L+
independent
set
in
a
graph
(or
hypergraph
of
dimension
2).
It
is
the
first
+L+
efficient
NC
algorithm
for
finding
a
maximal
independent
set
in
a
hypergraph
+L+
of
dimension
greater
than
2.
+L+
1<affiliation>
Department
of
Computer
Science,
University
of
Bonn,
5300
Bonn
1.

Modeling
a
Copier
Paper
Path:
+L+
A
Case
Study
in
Modeling
+L+
Transportation
Processes
+L+
Vineet
Gupta
Peter
Struss
+L+
TR-95-019
+L+
Abstract
+L+
We
present
a
compositional
model
of
paper
transportation
in
a
photocopier
that
is
meant
to
+L+
support
different
problem
solving
tasks
like
simulation
and
diagnosis,
and
to
be
applicable
to
+L+
a
variety
of
configurations.
Therefore,
we
try
to
avoid
making
hard-wired
implicit
assumptions
+L+
about
design
principles
and
possible
scenarios.
In
order
to
simplify
our
analysis,
the
model
+L+
abstracts
away
from
the
physical
forces
and
reasons
only
about
velocities.
Nonetheless,
it
+L+
succeeds
in
determining
essential
features
of
the
motion
of
the
sheet
of
paper
like
buckling
+L+
and
tearing.
The
framework
provided
is
quite
generic
and
can
be
used
as
a
starting
point
for
+L+
developing
models
of
other
transportation
domains.
+L+
Xerox
Palo
Alto
Research
Center,
3333
Coyote
Hill
Road,
Palo
Alto
CA
94304
USA.
(vgupta@parc.xerox.com)
+L+
Technical
University
of
Munich,
Orleansstr.
34,
D-81667
Munich,
Germany.
(struss@informatik.tu-muenchen.de)
+L+
+PAGE+

Smoothing
and
Multiplexing
+L+
Tradeoffs
for
Deterministic
+L+
Performance
Guarantees
to
VBR
+L+
Video
+L+
Edward
W.
Knightly
and
Paola
Rossaro
+L+
Also
with
EECS
Department,
U.C.
Berkeley
+L+
TR-95-033
+L+
Abstract
+L+
The
burstiness
of
variable
bit
rate
traffic
makes
it
difficult
to
both
efficiently
utilize
network
resources
and
provide
end-to-end
network
performance
guarantees
to
the
traffic
sources.
+L+
Generally,
smoothing
or
shaping
traffic
sources
at
the
entrance
of
the
network
reduces
their
+L+
burstiness
to
allow
higher
utilization
within
the
network.
However,
this
buffering
introduces
+L+
an
additional
delay
so
that,
in
effect,
lossless
smoothing
trades
queueing
delay
inside
the
+L+
network
for
smoothing
delay
at
the
network
edge.
In
this
paper,
we
consider
the
net
effect
+L+
of
smoothing
on
end-to-end
performance
guarantees
where
a
no-loss,
no-delay-violation
deterministic
guarantee
is
provided
with
the
D-BIND
traffic
model.
We
analytically
quantify
+L+
these
tradeoffs
and
provide
a
set
of
general
rules
for
determining
under
which
conditions
+L+
smoothing
provides
a
net
gain.
We
also
empirically
investigate
these
tradeoffs
using
traces
+L+
of
MPEG
compressed
video.
+L+
+PAGE+

INTERNATIONAL
COMPUTER
SCIENCE
INSTITUTE
+L+
I
1947
Center
St.
*
Suite
600
*
Berkeley,
California
94704-1198
*
(510)
643-9153
*
FAX
(510)
643-7684
+L+
Managing
ABR
Capacity
in
+L+
Reservation-based
Slotted
+L+
Networks
+L+
Roya
Ulrich
and
Pieter
Kritzinger
+L+
fulrich@icsi.berkeley.edu,
psk@cs.uct.ac.zag
+L+
The
Networks
Group
+L+
International
Computer
Science
Institute,
and
+L+
The
Computer
Science
Depatrment
+L+
University
of
Cape
Town
+L+
TR-96-006
+L+
January
1996
+L+
Abstract
+L+
For
slotted
networks
carrying
full
multi-media
traffic
to
work
successfully,
it
is
essential
that
connection
setup
and
management
is
done
well
under
all
traffic
conditions.
+L+
Major
challenges
remain
with
the
current
state
of
the
technology,
however,
particularly
on
how
one
copes
with
traffic
bursts.
Existing
reservation-based
networks
do
not
+L+
allow
the
user
to
dynamically
adjust
his
bandwidth
requirements
on
demand.
In
this
+L+
paper
we
propose
a
new
scheme,
called
the
reservoir
scheme,
which
allows
dynamic
+L+
and
distributed
resource
allocation.
The
basic
idea
behind
the
scheme
is
to
reserve
+L+
bandwidth
with
a
guaranteed
bit
rate
for
each
virtual
circuit.
The
user
is
allowed
to
+L+
decentrally
allocate
additional
bandwidth
from
an
Available
Bit
Rate
(ABR)
reservoir
to
satisfy
dynamic
changes
of
Variable
Bit
Rate
(VBR)
traffic.
The
duration
and
+L+
bandwidth
of
this
dynamic
access
are
negotiated
in
the
call
setup
phase
and
do
not
+L+
require
any
renegotiation
with
the
service
provider
so
that
this
solution
overcomes
the
+L+
rigidity
of
current
static
bandwidth
reservation
schemes.
The
additional
management
+L+
requirements
are
low
compared
to
other
dynamic
bandwidth
reservation
schemes.
We
+L+
also
describe
an
analytic
model
and
simulation
which
we
used
to
determine
whether
+L+
it
would
be
practical
to
apply
the
proposed
scheme
in
a
slotted
network.
+L+
Pieter
Kritzinger
is
in
the
Computer
Science
Depatrment,
University
of
Cape
Town,
private
+L+
Bag,
Rondbosch
7700
+L+
+PAGE+

INTERNATIONAL
COMPUTER
SCIENCE
INSTITUTE
+L+
I
1947
Center
St.
*
Suite
600
*
Berkeley,
California
94704-1198
*
(510)
643-9153
*
FAX
(510)
643-7684
+L+
A
Management
Platform
for
+L+
Global
Area
ATM
Networks
+L+
Roya
Ulrich
+L+
ulrich@icsi.berkeley.edu
+L+
TR-96-018
+L+
Abstract
+L+
Technological
progress
has
made
providing
numerous
new
services
to
large
number
+L+
of
users
possible.
Concurrently,
we
also
experience
an
increased
interest
in
real-time
+L+
and
interactive
applications,
e.
g.
teleseminaring,
video
conferencing
and
application
+L+
sharing,
in
particular,
because
of
the
worldwide
and
decentralized
character
of
today's
+L+
research
and
development
organizations.
+L+
The
International
Computer
Science
Institute
(ICSI)
is
a
participant
of
the
first
+L+
transatlantic
ATM
link
which
is
an
integral
part
of
the
Multimedia
Applications
+L+
on
Intercontinental
Highways
(MAY)
Project.
Additionally,
ICSI
is
attached
to
the
+L+
Bay
Area
Gigabit
Network
(BAGNet)
providing
ATM
connectivity
at
the
best-effort
+L+
basis.
Both
projects
provide
platforms
to
identify
the
key
research
and
development
+L+
topics
in
cooperative
real-time
communication.
+L+
The
technical
report
gives
a
brief
introduction
to
the
ATM
infrastructure
at
ICSI
and
+L+
addresses
challenging
management
issues
of
multimedia
applications
in
such
global
+L+
area
ATM
networks.
We
explore
three
management
areas:
performance,
configuration,
+L+
and
fault
management
with
respect
to
the
user's
point
of
view.
Finally,
we
introduce
+L+
a
management
platform
and
tools
we
have
been
developing
which
help
the
user
to
+L+
better
predict
the
quality
of
service
provided
and
to
recover
from
faults
occurred
in
+L+
the
system
or
during
a
transmission.
+L+
+PAGE+

On-line
Load
Balancing
for
+L+
Related
Machines
+L+
Piotr
Berman
+L+
Moses
Charikar
+L+
Marek
Karpinski
+L+
TR-97-007
+L+
January
1997
+L+
Abstract
+L+
We
consider
the
problem
of
scheduling
permanent
jobs
on
related
machines
in
an
+L+
on-line
fashion.
We
design
a
new
algorithm
that
achieves
the
competitive
ratio
of
3
+
+L+
p
+L+
8
5:828
for
the
deterministic
version,
and
3:31=
ln
2:155
4:311
for
its
randomized
+L+
variant,
improving
the
previous
competitive
ratios
of
8
and
2e
5:436.
We
also
prove
+L+
lower
bounds
of
2:4380
on
the
competitive
ratio
of
deterministic
algorithms
and
1:8372
+L+
on
the
competitive
ratio
of
randomized
algorithms
for
this
problem.
+L+
Dept.
of
Computer
Science
&
Eng.,
Pennsylvania
State
University,
University
Park,
PA16802,
USA
+L+
Email:berman@cse.psu.edu
+L+
Department
of
Computer
Science,
Stanford
University,
Stanford,
CA
94305-9045.
Supported
by
Stanford
+L+
School
of
Engineering
Groswith
Fellowship,
an
ARO
MURI
Grant
DAAH04-96-1-0007
and
NSF
Award
+L+
CCR-9357849,
with
matching
funds
from
IBM,
Schlumberger
Foundation,
Shell
Foundation,
and
Xerox
+L+
Corporation.
E-mail:
moses@cs.stanford.edu.
+L+
Dept.
of
Computer
Science,
University
of
Bonn,
53117
Bonn,
and
International
Computer
Science
Institute,
Berkeley.
This
research
was
partially
supported
by
the
DFG
Grant
KA
673/4-1,
by
the
ESPRIT
BR
+L+
Grants
7097
and
EC-US
030.
Email:marek@cs.uni-bonn.de
+L+
+PAGE+

On
Learning
Soccer
Strategies
+L+
Rafa
l
Sa
lustowicz
,
Marco
Wiering
,
Jurgen
Schmidhuber
+L+
IDSIA,
Corso
Elvezia
36,
6900
Lugano,
Switzerland
+L+
e-mail:
frafal,
marco,
juergeng@idsia.ch
+L+
In
W.
Gerstner,
A.
Germond,
M.
Hasler,
and
J.-D.
Nicoud,
editors,
+L+
Proceedings
of
the
Seventh
International
Conference
on
Artificial
+L+
Neural
Networks
(ICANN'97),
volume
1327
of
Lecture
Notes
in
Computer
+L+
Science,
pages
769-774.
Springer-Verlag
Berlin
Heidelberg,
1997.
+L+
Abstract.
We
use
simulated
soccer
to
study
multiagent
learning.
Each
+L+
team's
players
(agents)
share
action
set
and
policy
but
may
behave
differently
due
to
position-dependent
inputs.
All
agents
making
up
a
team
+L+
are
rewarded
or
punished
collectively
in
case
of
goals.
We
conduct
simulations
with
varying
team
sizes,
and
compare
two
learning
algorithms:
+L+
TD-Q
learning
with
linear
neural
networks
(TD-Q)
and
Probabilistic
+L+
Incremental
Program
Evolution
(PIPE).
TD-Q
is
based
on
evaluation
+L+
functions
(EFs)
mapping
input/action
pairs
to
expected
reward,
while
+L+
PIPE
searches
policy
space
directly.
PIPE
uses
an
adaptive
probability
+L+
distribution
to
synthesize
programs
that
calculate
action
probabilities
+L+
from
current
inputs.
Our
results
show
that
TD-Q
has
difficulties
to
learn
+L+
appropriate
shared
EFs.
PIPE,
however,
does
not
depend
on
EFs
and
+L+
finds
good
policies
faster
and
more
reliably.
+L+
1
Introduction

Induction
as
Knowledge
Integration
+L+
Benjamin
D.
Smith
+L+
Jet
Propulsion
Laboratory
+L+
California
Institute
of
Technology
+L+
4800
Oak
Grove
Drive
M/S
525-3660
+L+
Pasadena,
CA
91109-8099
+L+
smith@aig.jpl.nasa.gov
+L+
Paul
S.
Rosenbloom
+L+
Information
Sciences
Institute
&
Computer
Science
Dept.
+L+
University
of
Southern
California
+L+
4676
Admiralty
Way
+L+
Marina
del
Rey,
CA
90292
+L+
rosenbloom@isi.edu
+L+
Abstract
+L+
Two
key
issues
for
induction
algorithms
are
the
accuracy
of
the
learned
hypothesis
and
the
computational
+L+
resources
consumed
in
inducing
that
hypothesis.
One
+L+
of
the
most
promising
ways
to
improve
performance
+L+
along
both
dimensions
is
to
make
use
of
additional
+L+
knowledge.
Multi-strategy
learning
algorithms
tackle
+L+
this
problem
by
employing
several
strategies
for
handling
different
kinds
of
knowledge
in
different
ways.
+L+
However,
integrating
knowledge
into
an
induction
algorithm
can
be
difficult
when
the
new
knowledge
differs
significantly
from
the
knowledge
the
algorithm
+L+
already
uses.
In
many
cases
the
algorithm
must
be
+L+
rewritten.
+L+
This
paper
presents
KII,
a
Knowledge
Integration
+L+
framework
for
Induction,
that
provides
a
uniform
+L+
mechanism
for
integrating
knowledge
into
induction.
+L+
In
theory,
arbitrary
knowledge
can
be
integrated
with
+L+
this
mechanism,
but
in
practice
the
knowledge
representation
language
determines
both
the
knowledge
+L+
that
can
be
integrated,
and
the
costs
of
integration
+L+
and
induction.
By
instantiating
KII
with
various
set
+L+
representations,
algorithms
can
be
generated
at
different
trade-off
points
along
these
dimensions.
+L+
One
instantiation
of
KII,
called
RS-KII,
is
presented
+L+
that
can
implement
hybrid
induction
algorithms,
depending
on
which
knowledge
it
utilizes.
RS-KII
is
+L+
demonstrated
to
implement
AQ-11
(Michalski
1978),
+L+
as
well
as
a
hybrid
algorithm
that
utilizes
a
domain
+L+
theory
and
noisy
examples.
Other
algorithms
are
also
+L+
possible.
+L+
Introduction

Generality
and
Difficulty
in
Genetic
Programming:
+L+
Evolving
a
Sort
+L+
Kenneth
E.
Kinnear,
Jr.
+L+
Adaptive
Computing
Technology
+L+
62
Picnic
Rd.
+L+
Boxboro,
MA
01719
USA
+L+
kim.kinnear@adapt.com
+L+
Abstract
+L+
Genetic
Programming
is
applied
to
the
task
of
+L+
evolving
general
iterative
sorting
algorithms.
A
+L+
connection
between
size
and
generality
was
discovered.
Adding
inverse
size
to
the
fitness
measure
along
with
correctness
not
only
decreases
+L+
the
size
of
the
resulting
evolved
algorithms,
but
+L+
also
dramatically
increases
their
generality
and
+L+
thus
the
effectiveness
of
the
evolution
process.
In
+L+
addition,
a
variety
of
differing
problem
formulations
are
investigated
and
the
relative
probability
+L+
of
success
for
each
is
reported.
An
example
of
an
+L+
evolved
sort
from
each
problem
formulation
is
+L+
presented,
and
an
initial
attempt
is
made
to
+L+
understand
the
variations
in
difficulty
resulting
+L+
from
these
differing
problem
formulations.
+L+
1
Introduction

ACM
Sigplan
Notices
27,3
(March
1992),66-70.
+L+
Copyright
1991
by
Nimble
Computer
Corporation
1
+L+
The
Treadmill:
+L+
Real-Time
Garbage
Collection
Without
Motion
Sickness
+L+
Henry
G.
Baker
+L+
Nimble
Computer
Corporation,
16231
Meadow
Ridge
Way,
Encino,
CA
91436
+L+
(818)
501-4956
(818)
986-1360
FAX
+L+
A
simple
real-time
garbage
collection
algorithm
is
presented
which
does
not
copy,
thereby
avoiding
+L+
some
of
the
problems
caused
by
the
asynchronous
motion
of
objects.
This
in-place
"treadmill"
+L+
garbage
collection
scheme
has
approximately
the
same
complexity
as
other
nonmoving
garbage
+L+
collectors,
thus
making
it
usable
in
a
high-level
language
implementation
where
some
pointers
+L+
cannot
be
traced.
The
treadmill
is
currently
being
used
in
a
Lisp
system
built
in
Ada.
+L+
INTRODUCTION

Appears
in
the
Proceedings
of
the
ACM
SIGMOD
International
Conference
on
Management
of
Data,
San
Jose,
CA,
May
1995
+L+
Efficient
Optimistic
Concurrency
Control
+L+
Using
Loosely
Synchronized
Clocks
+L+
Atul
Adya
Robert
Gruber
Barbara
Liskov
Umesh
Maheshwari
+L+
Laboratory
for
Computer
Science,
+L+
Massachusetts
Institute
of
Technology,
+L+
545
Technology
Square,
Cambridge,
MA
02139
+L+
fadya,gruber,liskov,umeshg@lcs.mit.edu
+L+
Abstract
+L+
This
paper
describes
an
efficient
optimistic
concurrency
control
+L+
scheme
for
use
in
distributed
database
systems
in
which
objects
are
+L+
cached
and
manipulated
at
client
machines
while
persistent
storage
+L+
and
transactional
support
are
provided
by
servers.
The
scheme
+L+
provides
both
serializability
and
external
consistency
for
committed
+L+
transactions;
it
uses
loosely
synchronized
clocks
to
achieve
global
+L+
serialization.
It
stores
only
a
single
version
of
each
object,
and
+L+
avoids
maintaining
any
concurrency
control
information
on
a
per-
+L+
object
basis;
instead,
it
tracks
recent
invalidations
on
a
per-client
+L+
basis,
an
approach
that
has
low
in-memory
space
overhead
and
no
+L+
per-object
disk
overhead.
In
addition
to
its
low
space
overheads,
+L+
the
scheme
also
performs
well.
The
paper
presents
a
simulation
+L+
study
that
compares
the
scheme
to
adaptive
callback
locking,
the
+L+
best
concurrency
control
scheme
for
client-server
object-oriented
+L+
database
systems
studied
to
date.
The
study
shows
that
our
+L+
scheme
outperforms
adaptive
callback
locking
for
low
to
moderate
+L+
contention
workloads,
and
scales
better
with
the
number
of
clients.
+L+
For
high
contention
workloads,
optimism
can
result
in
a
high
abort
+L+
rate;
the
scheme
presented
here
is
a
first
step
toward
a
hybrid
scheme
+L+
that
we
expect
to
perform
well
across
the
full
range
of
workloads.
+L+
1
Introduction

Context-Insensitive
Alias
Analysis
+L+
Reconsidered
+L+
Erik
Ruf
+L+
erikruf@microsoft.com
+L+
May
16,
1995
+L+
Technical
Report
+L+
MSR-TR-95-20
+L+
Microsoft
Research
+L+
Advanced
Technology
Division
+L+
Microsoft
Corporation
+L+
One
Microsoft
Way
+L+
Redmond,
WA
98052
+L+
This
report
is
a
preprint
of
the
paper
"Context-Insensitive
Alias
Analysis
Reconsidered,"
to
appear
in
ACM
SIGPLAN
+L+
'95
Conference
on
Programming
Language
Design
and
Implementation
(PLDI'95),
La
Jolla,
California,
June
1995.
+L+
Copyright
c
1995
by
the
Association
for
Computing
Machinery,
Inc.
Permission
to
make
digital
or
hard
copies
of
all
or
+L+
part
of
this
work
for
personal
or
classroom
use
is
granted
without
fee
provided
that
copies
are
not
made
or
distributed
+L+
for
profit
or
commercial
advantage
and
that
copies
bear
this
notice
and
the
full
citation
on
the
first
page.
Copyrights
+L+
for
components
of
this
work
owned
by
others
than
ACM
must
be
honored.
Abstracting
with
credit
is
permitted.
To
+L+
copy
otherwise,
to
republish,
to
post
on
servers,
or
to
redistribute
to
lists,
requires
prior
specific
permission
and/or
a
+L+
fee.
Request
permissions
from
Publications
Dept,
ACM
Inc.,
fax
+
1
(212)
869-0481,
or
permissions@acm.org.
+L+
+PAGE+

DEPARTMENT
OF
STATISTICS
+L+
University
of
Wisconsin
+L+
1210
West
Dayton
St.
+L+
Madison,
WI
53706
+L+
TECHNICAL
REPORT
NO.
910
+L+
December
21,
1993
+L+
Behavior
Near
Zero
of
the
Distribution
of
GCV
Smoothing
+L+
Parameter
Estimates
1
+L+
by
+L+
Grace
Wahba
and
Yuedong
Wang
+L+
1
Supported
by
the
National
Science
Foundation
under
Grant
DMS-9121003
and
the
National
Eye
Institute
under
+L+
Grant
R01
EY09946.
e-mail
wahba@stat.wisc.edu,
wang@stat.wisc.edu
+L+
+PAGE+

Machine
Translation,
10:1-2,
139-180
(1995)
+L+
c
1995
Kluwer
Academic
Publishers,
Boston.
Manufactured
in
The
Netherlands.
+L+
Toward
a
Lexicalized
Grammar
for
Interlinguas
+L+
CLARE
VOSS
voss@cs.umd.edu
+L+
BONNIE
J.
DORR
bonnie@cs.umd.edu
+L+
Department
of
Computer
Science,
University
of
Maryland,
College
Park,
MD
20742
+L+
Received
September
1,1994;
Revised
July
15,
1995
+L+
Abstract.
In
this
paper
we
present
one
aspect
of
our
research
on
machine
translation
(MT):
+L+
capturing
the
grammatical
and
computational
relation
between
(i)
the
interlingua
(IL)
as
defined
+L+
declaratively
in
the
lexicon
and
(ii)
the
IL
as
defined
procedurally
by
way
of
algorithms
that
+L+
compose
and
decompose
pivot
IL
forms.
We
begin
by
examining
the
interlinguas
in
the
lexicons
of
+L+
a
variety
of
current
IL-based
approaches
to
MT.
This
brief
survey
makes
it
clear
that
no
consensus
+L+
exists
among
MT
researchers
on
the
level
of
representation
for
defining
the
IL.
In
the
section
that
+L+
follows,
we
explore
the
consequences
of
this
missing
formal
framework
for
MT
system
builders
who
+L+
develop
their
own
lexical-IL
entries.
The
lack
of
software
tools
to
support
rapid
IL
respecification
+L+
and
testing
greatly
hampers
their
ability
to
modify
representations
to
handle
new
data
and
new
+L+
domains.
Our
view
is
that
IL-based
MT
research
needs
both
(a)
the
formal
framework
to
specify
+L+
possible
IL
grammars
and
(b)
the
software
support
tools
to
implement
and
test
these
grammars.
+L+
With
respect
to
(a),
we
propose
adopting
a
lexicalized
grammar
approach,
tapping
research
+L+
results
from
the
study
of
tree
grammars
for
natural
language
syntax.
With
respect
to
(b),
we
+L+
sketch
the
design
and
functional
specifications
for
parts
of
ILustrate,
the
set
of
software
tools
+L+
that
we
need
to
implement
and
test
the
various
IL
formalisms
that
meet
the
requirements
of
a
+L+
lexicalized
grammar.
In
this
way,
we
begin
to
address
a
basic
issue
in
MT
research,
how
to
define
+L+
and
test
an
interlingua
as
a
computational
language
|
without
building
a
full
MT
system
for
+L+
each
possible
IL
formalism
that
might
be
proposed.
+L+
Keywords:
interlingua,
machine
translation,
lexicon,
lexicalized
grammar
+L+
1.
Introduction

GROWING
RADIAL
BASIS
FUNCTION
NETWORKS
+L+
E.
BLANZIERI
,
P.
KATENKAMP
flfl
and
A.
GIORDANA
flflfl
+L+
Centro
di
Scienza
Cognitiva,
Universita
di
Torino,
Via
Lagrange
3,
10100
Torino,
Italy.
e-mail:
+L+
blanzier@psych.unito.it
+L+
flfl
Institute
for
Real
Time
Systems
and
Robotics,
University
of
Karlsruhe,
Germany.
+L+
flflfl
Dipartimento
di
Informatica,
Universita
di
Torino,
C.so
Svizzera
185,
10149
Torino,
Italy.
email:
attilio@di.unito.it
+L+
Abstract.
This
paper
presents
and
evaluates
two
algorithms
for
incrementally
constructing
Radial
+L+
Basis
Function
Networks,
a
class
of
neural
networks
which
looks
more
suitable
for
adtaptive
control
+L+
applications
than
the
more
popular
backpropagation
networks.
The
first
algorithm
has
been
derived
+L+
by
a
previous
method
developed
by
Fritzke,
while
the
second
one
has
been
inspired
by
the
CART
+L+
algorithm
developed
by
Breiman
for
generation
regression
trees.
Both
algorithms
proved
to
work
+L+
well
on
a
number
of
tests
and
exhibit
comparable
performances.
An
evaluation
on
the
standard
case
+L+
study
of
the
Mackey-Glass
temporal
series
is
reported.
+L+
Key
Words.
Machine
Learning,
Robotics,
Neural
Nets
+L+
1
INTRODUCTION

On
Scheduling
Two
Classes
of
Real
Time
Traffic
With
Identical
+L+
Deadlines
+L+
Sridhar
Pingali
+L+
Dept.
of
Electrical
and
Computer
Engineering
+L+
University
of
Masschusetts
+L+
Amherst,
MA
01003
+L+
James
F.
Kurose
+L+
Dept.
of
Computer
and
Information
Science
+L+
Univeristy
of
Massachusetts
+L+
Amherst,
MA
01003
+L+
Abstract
+L+
The
problem
of
scheduling
two
classes
of
real-time
traffic
with
correlated
time
constraints
+L+
is
considered.
Three
scheduling
disciplines
are
studied:
a
priority
discipline
which
gives
strict
+L+
priority
to
one
class
of
traffic,
a
threshold-based
scheme
in
which
priority
is
given
to
one
class
+L+
of
traffic
when
the
minimum
laxity
of
its
queued
packets
falls
below
some
threshold,
and
a
+L+
"balancing"
scheme
which
assigns
priority
on
the
basis
of
the
differences
in
minimum
laxities
in
+L+
the
two
classes
of
traffic.
Analytic
results
are
obtained
by
using
a
discrete
time
model
to
obtain
+L+
the
state
occupancy
probabilities
for
the
system.
Here,
the
state
is
defined
using
the
laxities
of
+L+
the
queued
real
time
packets.
Parameters
are
defined
to
study
the
tradeoff
in
the
performance
+L+
of
the
two
classes
of
traffic.
Results
are
obtained
to
demonstrate
how
the
balancing
scheme
+L+
permits
us
to
achieve
significant
improvement
in
the
performance
of
one
class
of
traffic
with
+L+
only
minimal
effect
on
the
performance
of
other
class.
A
video
application
is
suggested
for
this
+L+
work.
+L+
1
Introduction

To
appear
in
Proc.
IEEE
INFOCOM,
March
1996
+L+
The
Effectiveness
of
Affinity-Based
Scheduling
in
Multiprocessor
Networking
+L+
James
D.
Salehi
,
James
F.
Kurose
,
and
Don
Towsley
+L+
Computer
Science
Department,
University
of
Massachusetts,
Amherst
MA
01003,
USA
+L+
-salehi,kurose,towsley-@cs.umass.edu
+L+
Abstract
+L+
Techniques
for
avoiding
the
high
memory
overheads
found
on
+L+
many
modern
shared-memory
multiprocessors
are
of
increasing
+L+
importance
in
the
development
of
high-performance
multiprocessor
protocol
implementations.
One
such
technique
is
processor-cache
affinity
scheduling,
which
can
significantly
lower
packet
+L+
latency
and
substantially
increase
protocol
processing
throughput
+L+
[20].
In
this
paper,
we
evaluate
several
aspects
of
the
effectiveness
of
affinity-based
scheduling
in
multiprocessor
network
+L+
protocol
processing,
under
packet-level
and
connection-level
par-allelization
approaches.
Specifically,
we
evaluate
the
performance
+L+
of
the
scheduling
technique
1)
when
a
large
number
of
streams
are
+L+
concurrently
supported,
2)
when
processing
includes
copying
of
+L+
uncached
packet
data,
3)
as
applied
to
send-side
protocol
processing,
and
4)
in
the
presence
of
stream
burstiness
and
source
locality,
two
well-known
properties
of
network
traffic.
We
find
that
+L+
affinity-based
scheduling
performs
well
under
these
conditions,
+L+
emphasizing
its
robustness
and
general
effectiveness
in
multiprocessor
network
processing.
In
addition,
we
explore
a
technique
+L+
which
improves
the
caching
behavior
and
available
packet-level
+L+
concurrency
under
connection-level
parallelism,
and
find
performance
improves
dramatically.
+L+
1
Introduction

Optimal
Smoothing
of
Stored
Video
and
the
Impact
on
+L+
Network
Resource
Requirements
fly
+L+
James
D.
Salehi
,
Zhi-Li
Zhang
,
James
F.
Kurose
,
and
Don
Towsley
+L+
Department
of
Computer
Science
+L+
University
of
Massachusetts
+L+
Amherst,
MA
01003,
U.S.A
+L+
(413)
545-3179
(voice),
(413)
545-1249
(fax)
+L+
fsalehi,zhzhang,kurose,towsleyg@cs.umass.edu
+L+
Abstract
+L+
VBR
compressed
video
is
known
to
exhibit
significant,
multiple-time-scale
bit
rate
variability.
In
this
paper,
we
consider
the
transmission
of
stored
video
from
a
server
to
a
client
across
a
+L+
high
speed
network,
and
explore
how
the
client
buffer
space
can
be
used
most
effectively
toward
+L+
reducing
the
variability
of
the
transmitted
bit
rate.
+L+
We
present
two
basic
results.
First,
we
show
how
to
achieve
the
greatest
possible
reduction
in
+L+
rate
variability
when
sending
stored
video
to
a
client
with
given
buffer
size.
We
formally
establish
+L+
the
optimality
of
our
optimal
smoothing
approach,
and
illustrate
its
performance
over
a
set
of
+L+
long
MPEG-1
encoded
video
traces.
Second,
we
evaluate
the
impact
of
optimal
smoothing
on
the
+L+
network
resources
needed
for
video
transport,
under
two
network
service
models:
Deterministic
+L+
Guaranteed
service
[1,
11]
and
Renegotiated
CBR
(RCBR)
service
[9,
8].
Under
both
models,
we
+L+
find
the
impact
of
optimal
smoothing
to
be
dramatic.
+L+
An
earlier
version
of
this
paper
appeared
at
the
1996
ACM
SIGMETRICS
conference.
+L+
This
work
was
supported
by
NSF
under
grant
NCR-9206908
and
by
ARPA
under
ESD/AVS
contract
F-19628-92-C
+L+
0089.
+L+
+PAGE+

Appears
in:
"Supercomputing
'94,"
Nov.
1994.
+L+
Reprinted
by
permission
of
IEEE.
+L+
Paging
Tradeoffs
in
Distributed-Shared-Memory
Multiprocessors
+L+
Douglas
C.
Burger
,
Rahmat
S.
Hyder
,
Barton
P.
Miller
,
David
A.
Wood
+L+
Computer
Sciences
Department
+L+
University
of
Wisconsin-Madison
+L+
1210
W.
Dayton
Street
+L+
Madison,
WI
53706
USA
+L+
wwt@cs.wisc.edu
+L+
Abstract
+L+
Massively
parallel
processors
have
begun
using
commodity
operating
systems
that
support
demand-paged
+L+
virtual
memory.
To
evaluate
the
utility
of
virtual
+L+
memory,
we
measured
the
behavior
of
seven
shared-memory
parallel
application
programs
on
a
simulated
+L+
distributed-shared-memory
machine.
Our
results
(i)
+L+
confirm
the
importance
of
gang
CPU
scheduling,
(ii)
+L+
show
that
a
page-faulting
processor
should
spin
rather
+L+
than
invoke
a
parallel
context
switch,
(iii)
show
that
+L+
our
parallel
programs
frequently
touch
most
of
their
+L+
data,
and
(iv)
indicate
that
memory,
not
just
CPUs,
+L+
must
be
"gang
scheduled".
Overall,
our
experiments
+L+
demonstrate
that
demand
paging
has
limited
value
+L+
on
current
parallel
machines
because
of
the
applications'
synchronization
and
memory
reference
patterns
+L+
and
the
machines'
high
page-fault
and
parallel-context-switch
overheads.
+L+
1
Introduction

Supporting
Irregular
Distributions
in
FORTRAN
90D/HPF
+L+
Compilers
+L+
Ravi
Ponnusamy
yz
Yuan-Shin
Hwang
Raja
Das
+L+
Joel
Saltz
Alok
Choudhary
Geoffrey
Fox
+L+
UMIACS
and
Computer
Science
Department
Northeast
Parallel
Architectures
Center
+L+
University
of
Maryland
Syracuse
University
+L+
College
Park,
MD
20742
Syracuse,
NY
13244
+L+
Abstract
+L+
This
paper
presents
methods
that
make
it
possible
to
efficiently
support
irregular
problems
using
data
+L+
parallel
languages.
The
approach
involves
the
use
of
a
portable,
compiler-independent,
runtime
support
+L+
library
called
CHAOS.
The
CHAOS
runtime
support
library
contains
procedures
that
+L+
*
support
static
and
dynamic
distributed
array
partitioning,
+L+
*
partition
loop
iterations
and
indirection
arrays,
+L+
*
remap
arrays
from
one
distribution
to
another,
and
+L+
*
carry
out
index
translation,
buffer
allocation
and
communication
schedule
generation.
+L+
The
CHAOS
runtime
procedures
are
used
by
a
prototype
Fortran
90D
compiler
as
runtime
support
for
irregular
problems.
This
paper
also
presents
performance
results
of
compiler-generated
and
+L+
hand-parallelized
versions
of
two
stripped
down
applications
codes.
The
first
code
is
derived
from
+L+
an
unstructured
mesh
computational
fluid
dynamics
flow
solver
and
the
second
is
derived
from
the
+L+
molecular
dynamics
code
CHARMM.
+L+
A
method
is
described
that
makes
it
possible
to
emulate
irregular
distributions
in
HPF
by
reordering
elements
of
data
arrays
and
renumbering
indirection
arrays.
The
results
suggest
that
an
HPF
+L+
compiler
could
use
reordering
and
renumbering
extrinsic
functions
to
obtain
performance
comparable
+L+
to
that
achieved
by
a
compiler
for
a
language
(such
as
Fortran
90D)
that
directly
supports
irregular
+L+
distributions.
+L+
This
work
was
sponsored
in
part
by
ARPA
(NAG-1-1485),
NSF
(ASC
9213821),
and
ONR
(SC292-1-22913).
+L+
+PAGE+

High
Performance
Verification
Algorithms
+L+
by
+L+
Jagesh
V.
Sanghavi
+L+
B.Tech.
(Indian
Institute
of
Technology,
Bombay)
1989
+L+
M.S.
(University
of
California
at
Berkeley,
California)
1993
+L+
A
dissertation
submitted
in
partial
satisfaction
of
the
+L+
requirements
for
the
degree
of
+L+
Doctor
of
Philosophy
+L+
in
+L+
Engineering
Electrical
Engineering
+L+
and
Computer
Sciences
+L+
in
the
+L+
GRADUATE
DIVISION
+L+
of
the
+L+
UNIVERSITY
of
CALIFORNIA
at
BERKELEY
+L+
Committee
in
charge:
+L+
Professor
Alberto
L.
Sangiovanni-Vincentelli
+L+
Professor
Robert
K.
Brayton
+L+
Professor
Phillip
Colella
+L+
1996
+L+
+PAGE+

THE
PAPIA2
MACHINE:
HARDWARE
AND
SOFTWARE
ARCHITECTURE
+L+
A.
Biancardi
,
V.
Cantoni
,
M.
Ferretti
and
M.
Mosconi
+L+
Dipartimento
di
Informatica
e
Sistemistica
+L+
University
of
Pavia
+L+
Via
Abbiategrasso
209
+L+
I-27100
PAVIA,
ITALY
+L+
Tel:
int
+39.382.391350
+L+
ABSTRACT
+L+
This
paper
presents
the
overall
structure
of
PAPIA2,
a
pyramid
+L+
system
belonging
to
the
family
of
massive
parallel
machines.
It
embeds
+L+
the
topology
of
the
quad-pyramid
into
a
highly
regular,
fault
tolerant,
+L+
eight-connected
proces
sor
array
by
means
of
specially
reconfigurable
+L+
near-neighbor
interconnections.
The
system
comes
with
a
fully-fledged
+L+
software
environment
designed
to
optimize
the
use
of
machine
resources.
+L+
The
highly
interactive
graphic
tools
help
in
understanding
the
machine's
+L+
capabilities,
provide
a
valuable
testbed
for
the
machine
instruction
set,
+L+
and
offer
a
suitable
context
for
monitoring
program
execution.
+L+
1.
Introduction

Simulations
with
an
Evolvable
Fitness
Formula
+L+
Henrik
Hautop
Lund
Domenico
Parisi
+L+
Institute
of
Psychology
+L+
National
Research
Council,
Viale
Marx
15,
00137
Rome,
Italy
+L+
tel.:
(+39)
6
88
94
596
+L+
-
DAIMI
+L+
University
of
Aarhus,
Ny
Munkegade,
8000
Aarhus
C.,
Denmark
+L+
tel.:
(+45)
89
42
32
21
+L+
e-mail:
hhl@daimi.aau.dk
domenico@gracco.irmkant.rm.cnr.it
+L+
Abstract
+L+
The
concept
of
a
fitness
formula
as
a
property
of
an
organism
is
proposed.
In
+L+
artificial
life
simulations
with
organisms
living
in
an
environment,
the
fitness
formula
+L+
can
be
interpreted
as
the
ability
of
organisms
to
extract
energy
from
potential
food
+L+
sources
distributed
in
the
environment.
In
simulations
where
the
goal
of
the
genetic
+L+
algorithm
is
that
of
developing
systems
which
exhibit
a
certain
type
of
behavior
in
a
+L+
particular
environment,
the
fitness
formula
becomes
an
independent
variable
which
can
+L+
be
manipulated
in
order
to
obtain
the
desired
behavior.
The
fitness
formula
can
be
+L+
viewed
as
an
evolvable
trait
of
organisms,
and
therefore
not
fixed
and
decided
by
the
+L+
researcher.
Simulations
with
fixed
and
evolvable
fitness
formulae
show
that
the
fitness
+L+
formula,
the
sensory
apparatus,
and
the
behavior
of
organisms
may
co-evolve
and
be
+L+
co-adapted.
+L+
+PAGE+

Layered,
Server-based
Support
+L+
for
Object-Oriented
Application
Development
+L+
Guruduth
Banavar
Douglas
Orr
Gary
Lindstrom
+L+
Department
of
Computer
Science
+L+
University
of
Utah,
Salt
Lake
City,
UT
84112
USA
+L+
fbanavar,dbo,lindstromg@cs.utah.edu
+L+
Abstract
+L+
This
paper
advocates
the
idea
that
the
physical
modularity
(file
structure)
of
application
components
supported
by
conventional
OS
environments
can
be
elevated
to
the
level
of
logical
modularity,
which
in
turn
+L+
can
directly
support
application
development
in
an
+L+
object-oriented
manner.
We
demonstrate
this
idea
+L+
through
a
system-wide
server
that
manages
the
manipulation
of
such
components
effectively.
The
server
+L+
is
designed
to
be
a
fundamental
operating
system
service
responsible
for
binding
and
mapping
component
+L+
instances
into
client
address
spaces.
+L+
We
show
how
this
model
solves
some
longstanding
+L+
problems
with
the
management
of
application
components
in
existing
application
development
environments.
We
demonstrate
that
this
model's
effectiveness
derives
from
its
support
for
the
cornerstones
of
+L+
OO
programming:
classes
and
their
instances,
encapsulation,
and
several
forms
of
inheritance.
+L+
1
Introduction

To
Appear
in
the
ACM
Multimedia
Journal
+L+
Dynamic
Management
of
Guaranteed
Performance
Multimedia
Connections
+L+
Colin
Parris
,
Hui
Zhang
,
and
Domenico
Ferrari
+L+
parris,
hzhang,
ferrari@tenet.Berkeley.EDU
+L+
Computer
Science
Division
+L+
University
of
California
at
Berkeley
+L+
Berkeley,
CA
94720
+L+
Keywords:
Multimedia,
Network
Management,
Quality
of
Service,
High
Speed
Networks.
+L+
Abstract
+L+
Most
of
the
solutions
proposed
to
support
real-time
(i.e.
guaranteed
performance)
communication
services
in
packet-switching
networks
adopt
a
connection-oriented
and
reservation-oriented
approach.
In
such
an
approach,
resource
allocation
+L+
and
route
selection
decisions
are
made
before
the
start
of
the
communication
on
the
basis
of
resource
availability
and
real-time
network
load
at
that
time,
and
are
usually
kept
for
the
duration
of
the
communication.
This
rather
static
resource
+L+
management
approach
has
certain
limitations:
it
does
not
take
into
account
(a)
the
dynamics
of
the
communicating
clients;
+L+
(b)
the
dynamics
of
the
network
state;
and
(c)
the
tradeoff
between
quality
of
service
and
network
availability,
thus
affecting
+L+
the
availability
and
flexibility
of
the
real-time
network
services.
Availability
is
the
ability
of
the
network
to
accommodate
+L+
as
many
real-time
clients
as
possible,
while
flexibility
is
the
ability
to
adapt
the
real-time
services
to
changing
network
state
+L+
and
client
demands.
In
this
paper,
we
present
the
Dynamic
Connection
Management
(DCM)
scheme,
which
addresses
these
+L+
issues
by
providing
the
network
with
the
capability
to
dynamically
modify
the
performance
parameters
and
the
routes
of
+L+
any
existing
real-time
connection.
With
these
capabilities,
DCM
can
be
used
to
increase
the
availability
and
flexibility
of
+L+
the
guaranteed
performance
service
offered
to
the
clients.
+L+
1
Introduction

MASSACHUSETTS
INSTITUTE
OF
TECHNOLOGY
+L+
ARTIFICIAL
INTELLIGENCE
LABORATORY
+L+
and
+L+
CENTER
FOR
BIOLOGICAL
AND
COMPUTATIONAL
LEARNING
+L+
DEPARTMENT
OF
BRAIN
AND
COGNITIVE
SCIENCES
+L+
A.I.
Memo
No.
1565
February
2,
1996
+L+
C.B.C.L.
Memo
No.
132
+L+
Probabilistic
Independence
Networks
for
Hidden
+L+
Markov
Probability
Models
+L+
Padhraic
Smyth
,
David
Heckerman
,
and
Michael
Jordan
+L+
A<abstract>
bstract
+L+
Graphical
techniques
for
modeling
the
dependencies
of
random
variables
have
been
explored
in
a
variety
+L+
of
different
areas
including
statistics,
statistical
physics,
artificial
intelligence,
speech
recognition,
image
+L+
processing,
and
genetics.
Formalisms
for
manipulating
these
models
have
been
developed
relatively
+L+
independently
in
these
research
communities.
In
this
paper
we
explore
hidden
Markov
models
(HMMs)
+L+
and
related
structures
within
the
general
framework
of
probabilistic
independence
networks
(PINs).
The
+L+
paper
contains
a
self-contained
review
of
the
basic
principles
of
PINs.
It
is
shown
that
the
well-known
+L+
forward-backward
(F-B)
and
Viterbi
algorithms
for
HMMs
are
special
cases
of
more
general
inference
+L+
algorithms
for
arbitrary
PINs.
Furthermore,
the
existence
of
inference
and
estimation
algorithms
for
+L+
more
general
graphical
models
provides
a
set
of
analysis
tools
for
HMM
practitioners
who
wish
to
explore
+L+
a
richer
class
of
HMM
structures.
Examples
of
relatively
complex
models
to
handle
sensor
fusion
and
+L+
coarticulation
in
speech
recognition
are
introduced
and
treated
within
the
graphical
model
framework
+L+
to
illustrate
the
advantages
of
the
general
approach.
+L+
Copyright
c
Massachusetts
Institute
of
Technology,
1996
+L+
This
report
describes
research
done
at
the
Department
of
Information
and
Computer
Science,
University
of
+L+
California,
Irvine,
the
Jet
Propulsion
Laboratory,
California
Institute
of
Technology,
Microsoft
Research,
the
+L+
Center
for
Biological
and
Computational
Learning,
and
the
Artificial
Intelligence
Laboratory
of
the
Massachusetts
+L+
Institute
of
Technology.
The
authors
can
be
contacted
as
pjs@aig.jpl.nasa.gov,
heckerma@microsoft.com,
+L+
and
jordan@psyche.mit.edu.
Support
for
CBCL
is
provided
in
part
by
a
grant
from
the
NSF
(ASC-9217041).
+L+
Support
for
the
laboratory's
artificial
intelligence
research
is
provided
in
part
by
the
Advanced
Research
Projects
+L+
Agency
of
the
Dept.
of
Defense.
MIJ
gratefully
acknowledges
discussions
with
Steffen
Lauritzen
on
the
application
+L+
of
the
IPF
algorithm
to
UPINs.
+L+
+PAGE+

MASSACHUSETTS
INSTITUTE
OF
TECHNOLOGY
+L+
ARTIFICIAL
INTELLIGENCE
LABORATORY
+L+
and
+L+
CENTER
FOR
BIOLOGICAL
INFORMATION
PROCESSING
+L+
WHITAKER
COLLEGE
+L+
A.I.
Memo
No.
1164
October
1989
+L+
C.B.I.P.
Paper
No.
45
+L+
Networks
and
the
Best
Approximation
Property
+L+
Federico
Girosi
and
Tomaso
Poggio
+L+
Abstract
+L+
Networks
can
be
considered
as
approximation
schemes.
Multilayer
networks
of
the
+L+
backpropagation
type
can
approximate
arbitrarily
well
continuous
functions
(Cybenko,
+L+
1989;
Funahashi,
1989;
Stinchcombe
and
White,
1989).
We
prove
that
networks
derived
from
regularization
theory
and
including
Radial
Basis
Functions
(Poggio
and
+L+
Girosi,
1989),
have
a
similar
property.
From
the
point
of
view
of
approximation
theory,
however,
the
property
of
approximating
continuous
functions
arbitrarily
well
is
not
+L+
sufficient
for
characterizing
good
approximation
schemes.
More
critical
is
the
property
+L+
of
best
approximation.
The
main
result
of
this
paper
is
that
multilayer
networks,
of
the
+L+
type
used
in
backpropagation,
are
not
best
approximation.
For
regularization
networks
+L+
(in
particular
Radial
Basis
Function
networks)
we
prove
existence
and
uniqueness
of
+L+
best
approximation.
+L+
c
Massachusetts
Institute
of
Technology,1994
+L+
This
paper
describes
research
done
within
the
Center
for
Biological
Information
Processing,
in
the
Department
of
Brain
and
Cognitive
Sciences,
and
at
the
Artificial
Intelligence
+L+
Laboratory.
This
research
is
sponsored
by
a
grant
from
the
Office
of
Naval
Research
+L+
(ONR),
Cognitive
and
Neural
Sciences
Division;
by
the
Artificial
Intelligence
Center
of
+L+
Hughes
Aircraft
Corporation;
by
the
Alfred
P.
Sloan
Foundation;
by
the
National
Science
Foundation.
Support
for
the
A.
I.
Laboratory's
artificial
intelligence
research
is
provided
by
the
Advanced
Research
Projects
Agency
of
the
Department
of
Defense
under
+L+
Army
contract
DACA76-85-C-0010,
and
in
part
by
ONR
contract
N00014-85-K-0124.
+L+
+PAGE+

Jonathan
E.
Hazan
+L+
and
Richard
G.
Morgan
+L+
Technical
Report
no.
3/92
+L+
Artificial
Intelligence
Systems
Research
Group
+L+
Computer
Science
Division
+L+
School
of
Engineering
and
Computer
Science
+L+
University
of
Durham,
DH1
3LE,
UK
+L+
12th
July
1993
+L+
Abstract
+L+
Programmers
using
imperative
languages
have
a
number
of
well-established
debugging
tools
available
to
them;
functional
programmers
have
few,
if
any,
tools
available.
+L+
Many
of
the
tools
and
techniques
developed
for
debugging
functional
programs
are
+L+
based
on
those
for
imperative
programming
and
lack
a
theoretical
basis
relevant
to
+L+
functional
programming.
In
addition,
the
techniques
used
are
typically
very
time-consuming.
A
theoretical
foundation
on
which
to
base
the
study
of
errors
and
debugging
in
functional
programming
is
presented
in
this
report.
Using
this
theoretical
+L+
foundation,
a
set
of
program
transformation
schemes
has
been
developed
which
facilitate
the
location
of
the
type
of
error
which
results
in
an
evaluation-time
error
message
+L+
and
the
termination
of
evaluation.
A
brief
description
of
the
practical
experience
ob
+L+
tained
using
the
tool
is
also
presented.
+L+
The
authors
can
be
contacted
by
emailing
J.E.Hazan@durham.ac.uk.
FAX:
+44
(0)91
374
3741.
This
+L+
research
is
funded
by
a
grant
from
the
Science
and
Engineering
Research
Council
of
Great
Britain.
+L+
+PAGE+

SEQUOIA
2000
--
A
REFLECTION
ON
THE
FIRST
THREE
YEARS
+L+
Michael
Stonebraker
+L+
EECS
Department
+L+
University
of
California,
Berkeley
+L+
Abstract
+L+
This
paper
describes
the
SEQUOIA
2000
project
+L+
and
its
implementation
efforts
during
the
first
three
years.
+L+
Included
are
the
objectives
we
had,
how
we
chose
to
+L+
address
them
and
some
of
the
lessons
we
learned
from
+L+
this
endeavor.
+L+
1.
INTRODUCTION

Parallelization
of
Linearized
+L+
Applications
in
Fortran
D
+L+
Lorie
M.
Liebrock
+L+
Ken
Kennedy
+L+
CRPC
TR93342-S
+L+
November,
1993
+L+
Center
for
Research
on
Parallel
Computation
+L+
Rice
University
+L+
P.O.
Box
1892
+L+
Houston,
TX
77251-1892
+L+
This
research
was
supported
by:
Center
for
Research
on
+L+
Parallel
Computation,
a
National
Science
Foundation
Science
and
Technology
Center,
through
Cooperative
Agreement
No.
CCR-9120008,
NSF/NASA
Agreement
No.
+L+
ASC-9213821,
and
ONR
Agreement
No.
N00014-93-1-0158.
Use
of
the
Intel
i860
was
provided
under
a
Texas
+L+
CER
Grant
No.
CISE
8619893.
+L+
+PAGE+

Deterministic
Parallel
Fortran
+L+
K.
Mani
Chandy
Ian
Foster
+L+
Abstract
+L+
We
describe
Fortran
M,
message-passing
extensions
to
Fortran
77
that
provide
+L+
deterministic
execution
and
information
hiding
while
preserving
desirable
properties
of
+L+
message
passing.
+L+
1
Introduction

RICE
UNIVERSITY
+L+
Optimizing
Fortran90D/HPF
for
+L+
Distributed-Memory
Computers
+L+
by
+L+
Gerald
H.
Roth
+L+
A
Thesis
Submitted
+L+
in
Partial
Fulfillment
of
the
+L+
Requirements
for
the
Degree
+L+
Doctor
of
Philosophy
+L+
Approved,
Thesis
Committee:
+L+
Ken
Kennedy,
Noah
Harding
Professor
+L+
Computer
Science
+L+
John
Mellor-Crummey,
Faculty
Fellow
+L+
Computer
Science
+L+
William
W.
Symes,
Professor
+L+
Computational
and
Applied
Mathematics
+L+
R.
Gregg
Brickner,
Technical
Staff
Member
+L+
Los
Alamos
National
Laboratory
+L+
Houston,
Texas
+L+
April,
1997
+L+
+PAGE+

Reference:
Proceedings
of
the
IASTED
International
Conference
on
Artificial
Intelligence,
Expert
Systems
and
Neural
Networks,
pp.
+L+
249-252,
1996.
+L+
Using
Multiple
Node
Types
to
Improve
the
+L+
Performance
of
DMP
(Dynamic
Multilayer
Perceptron)
+L+
Tim
L.
Andersen
and
Tony
R.
Martinez
+L+
Computer
Science
Department,
Brigham
Young
University,
Provo,
Utah
84602
+L+
email:
tim@axon.cs.byu.edu,
martinez@cs.byu.edu
+L+
ABSTRACT
+L+
This
paper
discusses
a
method
for
training
multilayer
+L+
perceptron
networks
called
DMP2
(Dynamic
Multilayer
+L+
Perceptron
2).
The
method
is
based
upon
a
divide
and
conquer
+L+
approach
which
builds
networks
in
the
form
of
binary
trees,
+L+
dynamically
allocating
nodes
and
layers
as
needed.
The
focus
+L+
of
this
paper
is
on
the
effects
of
using
multiple
node
types
+L+
within
the
DMP
framework.
Simulation
results
show
that
+L+
DMP2
performs
favorably
in
comparison
with
other
learning
+L+
algorithms,
and
that
using
multiple
node
types
can
be
+L+
beneficial
to
network
performance.
+L+
1
Introduction

The
weakest
reasonable
memory
model
+L+
by
+L+
Matteo
Frigo
+L+
Laurea,
Universit
a
di
Padova
(1992)
+L+
Dottorato
di
Ricerca,
Universit
a
di
Padova
(1996)
+L+
Submitted
to
the
Department
of
Electrical
Engineering
and
Computer
+L+
Science
+L+
in
partial
fulfillment
of
the
requirements
for
the
degree
of
+L+
Master
of
Science
+L+
at
the
+L+
MASSACHUSETTS
INSTITUTE
OF
TECHNOLOGY
+L+
October
1997
+L+
c
Matteo
Frigo,
MCMXCVII.
All
rights
reserved.
+L+
The
author
hereby
grants
to
MIT
permission
to
reproduce
and
distribute
+L+
publicly
paper
and
electronic
copies
of
this
thesis
document
in
whole
or
in
+L+
part,
and
to
grant
others
the
right
to
do
so.
+L+
Author
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+L+
Department
of
Electrical
Engineering
and
Computer
Science
+L+
January
28,
1998
+L+
Certified
by
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+L+
Charles
E.
Leiserson
+L+
Professor
of
Computer
Science
and
Engineering
+L+
Thesis
Supervisor
+L+
Accepted
by
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+L+
PUT
NAME
HERE
+L+
Chairman,
Departmental
Committee
on
Graduate
Students
+L+
+PAGE+

ON
THE
POWER
OF
TWO-POINTS
BASED
SAMPLING
+L+
Benny
Chor
Oded
Goldreich
flfl
+L+
MIT
Laboratory
for
Computer
Science
+L+
Cambridge,
Massachusetts
02139
+L+
Abstract
|
The
purpose
of
this
note
is
to
present
a
new
sampling
technique
and
to
demonstrate
+L+
some
of
its
properties.
The
new
technique
consists
of
picking
two
elements
at
random,
and
deterministically
generating
(from
them)
a
long
sequence
of
pairwise
independent
elements.
The
+L+
sequence
is
guarantees
to
intersect,
with
high
probability,
any
set
of
non-negligible
density.
+L+
1.
Introduction

Eventually-Serializable
Data
Services
+L+
Alan
Fekete
David
Gupta
Victor
Luchangco
Nancy
Lynch
Alex
Shvartsman
+L+
Abstract
+L+
We
present
a
new
specification
for
distributed
data
services
that
trade-off
immediate
consistency
guarantees
+L+
for
improved
system
availability
and
efficiency,
while
+L+
ensuring
the
long-term
consistency
of
the
data.
An
+L+
eventually-serializable
data
service
maintains
the
operations
requested
in
a
partial
order
that
gravitates
over
+L+
time
towards
a
total
order.
It
provides
clear
and
unambiguous
guarantees
about
the
immediate
and
long-term
+L+
behavior
of
the
system.
To
demonstrate
its
utility,
we
+L+
present
an
algorithm,
based
on
one
of
Ladin,
Liskov,
+L+
Shrira,
and
Ghemawat
[12],
that
implements
this
specification.
Our
algorithm
provides
the
interface
of
the
+L+
abstract
service,
and
generalizes
their
algorithm
by
allowing
general
operations
and
greater
flexibility
in
specifying
consistency
requirements.
We
also
describe
how
+L+
to
use
this
specification
as
a
building
block
for
applications
such
as
directory
services.
+L+
1
Introduction

Proactive
RSA
+L+
Yair
Frankel
Peter
Gemmell
Philip
D.
MacKenzie
Moti
Yung
x
+L+
August
4,
1996
+L+
Abstract
+L+
The
notion
of
"proactive
security"
of
basic
primitives
and
cryptosystems
that
are
distributed
+L+
amongst
various
servers,
was
introduced
in
order
to
tolerate
a
very
strong
"mobile
adversary."
This
+L+
adversary
may
corrupt
all
participants
throughout
the
lifetime
of
the
system
in
a
non-monotonic
+L+
fashion
(i.e.
recoveries
are
possible)
but
the
adversary
is
unable
to
corrupt
too
many
participants
+L+
during
any
short
time
period
[OstrovskyYung].
The
notion
assures
increased
security
and
availability
+L+
of
the
cryptographic
primitive.
+L+
We
present
a
proactive
RSA
system
in
which
a
threshold
of
servers
applies
the
RSA
signature
+L+
(or
decryption)
function
in
a
distributed
manner;
RSA
is
perhaps
the
most
important
trapdoor
+L+
function
in
use.
Employing
new
combinatorial
and
elementary
number
theoretic
techniques,
our
+L+
protocol
enables
the
dynamic
updating
of
the
servers
(which
hold
the
RSA
key
distributively);
+L+
it
is
secure
even
when
a
linear
number
of
the
servers
are
corrupted
during
any
time
period
(linear
+L+
redundancy);
it
efficiently
"self-maintains"
the
security
of
the
function
and
its
messages
(ciphertexts
+L+
or
signatures);
and
it
enables
continuous
availability,
namely,
correct
function
application
using
the
+L+
shared
key
is
possible
at
any
time.
+L+
We
present
an
efficient
way
in
which
l
servers
can
share
an
RSA
private
function
so
that,
given
+L+
0
&lt;
&lt;
t
&lt;
1:
+L+
*
Proactive
(Dynamic)
Robustness:
A
gateway
G
can
combine
information
from
any
set
of
lt
+L+
(honest)
servers
to
deduce
the
RSA
signature
for
any
authorized
message
at
any
period.
+L+
*
Proactive
Security
(against
mobile
adversary):
Our
protocol
is
secure
against
a
polynomial
+L+
time
adversary
who
controls
the
gateway
G
and
time-variant
sets
of
up
to
minfl(1
t
);
lg
+L+
servers,
and
can
obtain
the
shares
of
up
to
l
servers
(including
those
that
it
corrupts).
+L+
*
Uniform
Boundedness:
The
share-size
is
always
bounded
by
the
size
of
an
RSA
private
key
+L+
(i.e.,
logarithmically
in
N
).
+L+
We
also
present
special
practical
instances
based
on
designs;
some
of
these
instances
were
recently
+L+
implemented
as
part
of
a
highly
secure
application
testbed
at
Sandia
National
Laboratories.
+L+
A
major
technical
difficulty
in
"proactivizing"
RSA
was
the
fact
that
the
servers
have
to
update
+L+
the
"distributed
representation"
of
an
RSA
key,
while
not
learning
the
order
of
the
group
from
+L+
which
keys
are
drawn
(in
order
not
to
compromise
the
RSA
security).
+L+
Sandia
National
Labs,
P.O
Box
5800,
Albuquerque,
NM
87185-1110,
yair@cs.sandia.gov
+L+
Sandia
National
Labs,
P.O
Box
5800,
Albuquerque,
NM
87185-1110,
psgemme@cs.sandia.gov
+L+
Sandia
National
Labs,
P.O
Box
5800,
Albuquerque,
NM
87185-1110,
philmac@cs.sandia.gov.
+L+
x
IBM
T.
J.
Watson
Research
Center,
Yorktown
Heights,
NY,
moti@watson.ibm.com,
moti@cs.columbia.edu
+L+
+PAGE+

M.I.T
Media
Laboratory
Perceptual
Computing
Section
Technical
Report
No.
307
+L+
Appears:
International
Conference
on
Computer
Vision
'95,
Cambridge,
MA,
June
20-23,
1995
+L+
Facial
Expression
Recognition
using
a
Dynamic
Model
and
Motion
Energy
+L+
Irfan
A.
Essa
and
Alex
P.
Pentland
+L+
Perceptual
Computing
Group,
The
Media
Laboratory,
+L+
Massachusetts
Institute
of
Technology
+L+
Cambridge,
MA
02139,
U.S.A.
+L+
Abstract
+L+
Previous
efforts
at
facial
expression
recognition
have
+L+
been
based
on
the
Facial
Action
Coding
System
(FACS),
a
+L+
representation
developed
in
order
to
allow
human
psychologists
to
code
expression
from
static
facial
mugshots.
In
+L+
this
paper
we
develop
new,
more
accurate
representations
+L+
for
facial
expression
by
building
a
video
database
of
facial
+L+
expressions
and
then
probabilistically
characterizing
the
+L+
facial
muscle
activation
associated
with
each
expression
+L+
using
a
detailed
physical
model
of
the
skin
and
muscles.
+L+
This
produces
a
muscle-based
representation
of
facial
motion,
which
is
then
used
to
recognize
facial
expressions
in
+L+
two
different
ways.
The
first
method
uses
the
physics-based
+L+
model
directly,
by
recognizing
expressions
through
comparison
of
estimated
muscle
activations.
The
second
method
+L+
uses
the
physics-based
model
to
generate
spatio-temporal
+L+
motion-energy
templates
of
the
whole
face
for
each
different
+L+
expression.
These
simple,
biologically-plausible
motion
+L+
energy
templates
are
then
used
for
recognition.
Both
+L+
methods
show
substantially
greater
accuracy
at
expression
+L+
recognition
than
has
been
previously
achieved.
+L+
1
Introduction

Multimodal
Person
Recognition
using
+L+
Unconstrained
Audio
and
Video
+L+
Tanzeem
Choudhury
,
Brian
Clarkson
,
Tony
Jebara
,
Alex
Pentland
+L+
Perceptual
Computing
Group
+L+
MIT
Media
Laboratory
+L+
Cambridge,
MA
02139
+L+
ftanzeem,clarkson,jebara,sandyg@media.mit.edu
+L+
Abstract
+L+
We
propose
a
person
identification
technique
that
+L+
can
recognize
and
verify
people
from
unconstrained
+L+
video
and
audio.
We
do
not
expect
fully
frontal
face
+L+
image
or
clean
speech
as
our
input.
Our
recognition
algorithm
can
detect
and
compensate
for
pose
variation
+L+
and
changes
in
the
auditory
background
and
also
select
the
most
reliable
video
frame
and
audio
clip
to
use
+L+
for
recognition.
We
also
use
3D
depth
information
of
+L+
a
human
head
to
detect
the
presence
of
an
actual
person
as
opposed
to
an
image
of
that
person.
Our
system
achieves
100%
recognition
and
verification
rates
+L+
on
natural
real-time
input
with
26
registered
clients.
+L+
1
Introduction

PARTIAL
SHAPE
MATCHING
USING
GENETIC
+L+
ALGORITHMS
+L+
Ender
Ozcan
and
Chilukuri
K.
Mohan
+L+
eozcan/mohan@top.cis.syr.edu
+L+
2-120
Center
for
Science
and
Technology,
+L+
Department
of
Electrical
Engineering
and
Computer
Science,
+L+
Syracuse
University,
Syracuse,
NY
13244-4100,
U.S.A.
+L+
315-443-2322/(fax)
1122
+L+
Abstract
+L+
Shape
recognition
is
a
challenging
task
when
images
contain
overlapping,
noisy,
occluded,
partial
shapes.
+L+
This
paper
addresses
the
task
of
matching
input
shapes
with
model
shapes
described
in
terms
of
features
+L+
such
as
line
segments
and
angles.
The
quality
of
matching
is
gauged
using
a
measure
derived
from
attributed
+L+
shape
grammars.
We
apply
genetic
algorithms
to
the
partial
shape-matching
task.
Preliminary
results,
using
+L+
model
shapes
with
6
to
70
features
each,
are
extremely
encouraging.
+L+
Key
words
:
Partial
Shape
Matching,
Genetic
Algorithms,
Attributed
Strings,
Pattern
Recognition.
+L+
+PAGE+

Finding
pattern
matchings
for
permutations
+L+
Louis
Ibarra
+L+
Dept.
of
Computer
Science
+L+
Hill
Center,
Busch
Campus
+L+
Rutgers
University
+L+
Piscataway,
NJ
08855
+L+
ibarra@paul.rutgers.edu
+L+
January
19,
1995
+L+
Abstract
+L+
Given
a
permutation
P
of
f1;
:
:
:
;
kg
and
T
of
f1;
:
:
:
;
ng,
the
pattern
matching
problem
for
per
+L+
mutations
is
to
determine
whether
there
is
a
length
k
subsequence
of
T
whose
elements
are
ordered
+L+
in
the
same
way
as
the
elements
of
P
.
We
present
an
O(kn
4
)
time
and
O(kn
3
)
space
algorithm
+L+
for
finding
a
match
of
P
into
T
or
determining
that
no
match
exists,
given
that
P
is
separable,
i.e.
+L+
contains
neither
(2,
4,
1,
3)
nor
(3,
1,
4,
2)
as
a
subpattern.
+L+
1
Introduction

An
Algorithm
for
Bayesian
Belief
Network
Construction
from
Data
+L+
Jie
Cheng
,
David
A.
Bell
,
Weiru
Liu
+L+</author>
School
of
Information
and
Software
Engineering
+L+
University
of
Ulster
at
Jordanstown
+L+
Northern
Ireland,
UK,
BT37
0QB
+L+
email:
-j.cheng,
da.bell,
w.liu-@ulst.ac.uk
+L+
Abstract
+L+
This
paper
presents
an
efficient
algorithm
for
constructing
Bayesian
belief
networks
from
databases.
The
+L+
algorithm
takes
a
database
and
an
attributes
ordering
(i.e.,
the
causal
attributes
of
an
attribute
should
appear
earlier
+L+
in
the
order)
as
input
and
constructs
a
belief
network
structure
as
output.
The
construction
process
is
based
on
the
+L+
computation
of
mutual
information
of
attribute
pairs.
Given
a
data
set
which
is
large
enough
and
has
a
DAG-Isomorphic
probability
distribution,
this
algorithm
guarantees
that
the
perfect
map
[1]
of
the
underlying
dependency
+L+
model
is
generated,
and
at
the
same
time,
enjoys
the
time
complexity
of
O
N(
)
2
on
conditional
independence
(CI)
+L+
tests.
To
evaluate
this
algorithm,
we
present
the
experimental
results
on
three
versions
of
the
well-known
ALARM
+L+
network
database,
which
has
37
attributes
and
10,000
records.
The
correctness
proof
and
the
analysis
of
+L+
computational
complexity
are
also
presented.
We
also
discuss
the
features
of
our
work
and
relate
it
to
previous
+L+
works.
+L+
1
Introduction

Evaluation
of
Architectural
Support
for
Global
Address-Based
Communication
+L+
in
Large-Scale
Parallel
Machines
+L+
Arvind
Krishnamurthy
,
Klaus
E.
Schauser
,
Chris
J.
Scheiman
,
+L+
Randolph
Y.
Wang
,
David
E.
Culler
,
and
Katherine
Yelick
+L+
Abstract
+L+
Large-scale
parallel
machines
are
incorporating
increasingly
sophisticated
architectural
support
for
user-level
messaging
and
global
memory
access.
We
provide
a
systematic
+L+
evaluation
of
a
broad
spectrum
of
current
design
alternatives
+L+
based
on
our
implementations
of
a
global
address
language
+L+
on
the
Thinking
Machines
CM-5,
Intel
Paragon,
Meiko
CS-2,
Cray
T3D,
and
Berkeley
NOW.
This
evaluation
includes
+L+
a
range
of
compilation
strategies
that
make
varying
use
of
+L+
the
network
processor;
each
is
optimized
for
the
target
architecture
and
the
particular
strategy.
We
analyze
a
family
+L+
of
interacting
issues
that
determine
the
performance
tradeoffs
in
each
implementation,
quantify
the
resulting
latency,
+L+
overhead,
and
bandwidth
of
the
global
access
operations,
+L+
and
demonstrate
the
effects
on
application
performance.
+L+
1
Introduction

Extended
Capabilities
for
+L+
Visual
Cryptography
+L+
Giuseppe
Ateniese
1
,
Carlo
Blundo
2
,
Alfredo
De
Santis
2
,
and
Douglas
R.
Stinson
3
+L+
1
Dipartimento
di
Informatica
e
Scienze
dell'Informazione,
+L+
Universita
di
Genova,
via
Dodecaneso
35,
16146
Genova,
Italy
+L+
E-mail:
ateniese@disi.unige.it
+L+
URL:
http://www.disi.unige.it/phd/ateniese/ateniese.html
+L+
2
Dipartimento
di
Informatica
ed
Applicazioni,
+L+
Universita
di
Salerno,
84081
Baronissi
(SA),
Italy
+L+
E-mail:
fcarblu,adsg@dia.unisa.it
+L+
URL:
http://www.unisa.it/f~carblu,
~adsg
+L+
3
Department
of
Combinatorics
and
Optimization
+L+
University
of
Waterloo,
Waterloo
Ontario,
N2L
3G1,
Canada
+L+
Abstract
+L+
An
extended
visual
cryptography
scheme,
EVCS
for
short,
for
an
access
structure
+L+
(
Qual
;
Forb
)
on
a
set
of
n
participants,
is
a
technique
to
encode
n
images
in
such
a
+L+
way
that
when
we
stack
together
the
transparencies
associated
to
participants
in
any
+L+
set
X
2
Qual
we
get
the
secret
message
with
no
trace
of
the
original
images,
but
any
+L+
X
2
Forb
has
no
information
on
the
shared
image.
Moreover,
after
the
original
images
+L+
are
encoded
they
are
still
meaningful,
that
is,
any
user
will
recognize
the
image
on
his
+L+
transparency.
+L+
The
main
contributions
of
this
paper
are
the
following:
+L+
*
A
trade-off
between
the
contrast
of
the
reconstructed
image
and
the
contrast
of
the
+L+
image
on
each
transparency
for
(k;
k)-threshold
EVCS
(in
a
(k;
k)-threshold
EVCS
+L+
the
image
is
visible
if
and
only
if
k
transparencies
are
stacked
together).
This
yields
+L+
a
necessary
and
sufficient
condition
for
the
existence
of
(k;
k)-threshold
EVCS
for
+L+
the
values
of
such
contrasts.
In
case
a
scheme
exists
we
explicitly
construct
it.
+L+
*
A
general
technique
to
implement
extended
visual
cryptography
schemes,
which
+L+
uses
hypergraph
colourings.
This
technique
yields
(k;
k)-threshold
EVCS
which
+L+
are
optimal
with
respect
to
the
pixel
expansion.
Finally,
we
discuss
some
applications
of
this
technique
to
various
interesting
classes
of
access
structures
by
using
+L+
relevant
results
from
the
theory
of
hypergraph
colourings.
+L+
Keywords:
Visual
Cryptography,
Secret
Sharing
Schemes.
+L+
1
Introduction

Final
Version,
94-Mar-17,
1
of
9
+L+
Automation
Tools
for
+L+
NonDestructive
Inspection
of
Aircraft:
+L+
Promise
of
Technology
Transfer
from
the
+L+
Civilian
to
the
Military
Sector
+L+
Chris
Seher
1
,
Mel
Siegel
2
,
and
William
M.
Kaufman
3
+L+
1
Federal
Aviation
Administration,
Technical
Center,
Atlantic
City
NJ
08201
+L+
2
Carnegie
Mellon
University,
Robotics
Institute,
Pittsburgh
PA
15213
+L+
3
Carnegie
Mellon
University,
CMRI,
Pittsburgh
PA
15213
+L+
Abstract
+L+
The
FAA
Aging
Aircraft
Research
Program
is
+L+
supporting
the
development
of
a
robotic
mobile
+L+
nondestructive
inspection
(NDI)
instrument
+L+
deployment
tool
at
Carnegie
Mellon
University
+L+
(CMU)
with
the
active
participation
of
USAir.
The
+L+
program
has
spawned
several
new
relationships
+L+
and
entities:
an
alliance
with
an
ARPA-funded
+L+
research
program
at
CMU
having
the
capability
to
+L+
add
3D-stereoscopic
enhanced
visual
inspection
+L+
capability,
a
start-up
company
organized
to
+L+
commercialize
the
combined
technologies,
and
+L+
State
of
Pennsylvania
funding
to
foster
this
+L+
commercialization.
As
a
result
of
these
activities
+L+
and
connections
the
civilian
sector
appears
to
be
+L+
ahead
of
the
military
sector
in
important
aspects
of
+L+
automation
for
deployment
of
aircraft
inspection
+L+
equipment.
A
partnership
between
the
university
+L+
researchers,
the
airline
operator,
the
start-up
+L+
company,
and
the
state
government
is
thus
+L+
emerging
as
the
likely
agent
for
transfer
of
the
+L+
civilian-developed
technology
to
the
military
sector.
+L+
1.
Introduction

Feature
selection
for
classification
based
on
text
hierarchy
+L+
Dunja
Mladenic
and
Marko
Grobelnik
+L+
Department
of
Intelligent
Systems,
J.Stefan
Institute,
+L+
Jamova
39,
1111
Ljubljana,
Slovenia
+L+
Phone:
(+386)(61)
1773
272,
Fax:
(+386)(61)
1258-158
+L+
E-mail:
Dunja.Mladenic@ijs.si,
Marko.Grobelnik@ijs.si
+L+
Abstract
+L+
This
paper
describes
automatic
document
categorization
based
on
large
text
hierarchy.
We
+L+
handle
the
large
number
of
features
and
training
examples
by
taking
into
account
hierarchical
+L+
structure
of
examples
and
using
feature
selection
for
large
text
data.
We
experimentally
evaluate
+L+
feature
subset
selection
on
real-world
text
data
collected
from
the
existing
Web
hierarchy
named
+L+
Yahoo.
In
our
learning
experiments
naive
Bayesian
classifier
was
used
on
text
data
using
feature-vector
document
representation
that
includes
word
sequences
(n-grams)
instead
of
just
single
words
+L+
(unigrams).
Experimental
evaluation
on
real-world
data
collected
form
the
Web
shows
that
our
+L+
approach
gives
promising
results
and
can
potentially
be
used
for
document
categorization
on
the
+L+
Web.
Additionally
the
best
result
on
our
data
is
achieved
for
relatively
small
feature
subset,
while
for
+L+
larger
subset
the
performance
substantially
drops.
The
best
performance
among
six
tested
feature
+L+
scoring
measure
was
achieved
by
the
feature
scoring
measure
called
Odds
ratio
that
is
known
from
+L+
information
retrieval.
+L+
1
Introduction

An
Introduction
to
Software
Architecture
+L+
David
Garlan
and
Mary
Shaw
+L+
January
1994
+L+
CMU-CS-94-166
+L+
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213-3890
+L+
Also
published
as
An
Introduction
to
Software
Architecture,
Advances
in
Software
Engineering
+L+
and
Knowledge
Engineering,
Volume
I,
edited
by
V.Ambriola
and
G.Tortora,
World
Scientific
+L+
Publishing
Company,
New
Jersey,
1993.
+L+
Also
appears
as
CMU
Software
Engineering
Institute
Technical
Report
+L+
CMU/SEI-94-TR-21,
ESC-TR-94-21.
+L+
1994
by
David
Garlan
and
Mary
Shaw
+L+
This
work
was
funded
in
part
by
the
Department
of
Defense
Advanced
Research
Project
Agency
under
grant
+L+
MDA972-92-J-1002,
by
National
Science
Foundation
Grants
CCR-9109469
and
CCR-9112880,
and
by
a
grant
+L+
from
Siemens
Corporate
Research.
It
was
also
funded
in
part
by
the
Carnegie
Mellon
University
School
of
+L+
Computer
Science
and
Software
Engineering
Institute
(which
is
sponsored
by
the
U.S.
Department
of
Defense).
+L+
The
views
and
conclusions
contained
in
this
document
are
those
of
the
authors
and
should
not
be
interpreted
+L+
as
representing
the
official
policies,
either
expressed
or
implied,
of
the
U.S.
Government,
the
Department
of
+L+
Defense,
the
National
Science
Foundation,
Siemens
Corporation,
or
Carnegie
Mellon
University.
+L+
Keywords:
Software
architecture,
software
design,
software
engineering
+L+
+PAGE+

To
appear
in
the
Proceedings
of
the
16th
ACM
Symposium
on
Operating
System
Principles
+L+
Agile
Application-Aware
Adaptation
for
Mobility
+L+
Brian
D.
Noble
,
M.
Satyanarayanan
,
Dushyanth
Narayanan
,
James
Eric
Tilton
,
Jason
F
linn
,
Kevin
R.
Walker
+L+
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
Abstract
+L+
In
this
paper
we
show
that
application-aware
adaptation,
a
+L+
collaborative
partnership
between
the
operating
system
and
+L+
applications,
offers
the
most
general
and
effective
approach
+L+
to
mobile
information
access.
We
describe
the
design
of
+L+
Odyssey,
a
prototype
implementing
this
approach,
and
show
+L+
how
it
supports
concurrent
execution
of
diverse
mobile
applications.
We
identify
agility
as
a
key
attribute
of
adaptive
systems,
and
describe
how
to
quantify
and
measure
it.
+L+
We
present
the
results
of
our
evaluation
of
Odyssey,
indicating
performance
improvements
up
to
a
factor
of
5
on
a
+L+
benchmark
of
three
applications
concurrently
using
remote
+L+
services
over
a
network
with
highly
variable
bandwidth.
+L+
1
Introduction

RECENT
ADVANCES
IN
JANUS:
+L+
A
SPEECH
TRANSLATION
SYSTEM
+L+
M.Woszczyna
,
N.Coccaro
,
A.Eisele
,
A.Lavie
,
A.McNair
,
T.Polzin
,
I.Rogina,
+L+
C.P.Rose
,
T.Sloboda
,
M.Tomita
,
J.Tsutsumi
,
N.Aoki-Waibel
,
A.Waibel
,
W.
Ward
+L+
Carnegie
Mellon
University
+L+
University
of
Karlsruhe
+L+
ABSTRACT
+L+
We
present
recent
advances
from
our
efforts
in
increasing
coverage,
robustness,
generality
and
speed
of
JANUS,
CMU's
+L+
speech-to-speech
translation
system.
JANUS
is
a
speaker-independent
system
which
translates
spoken
utterances
in
+L+
English
and
also
in
German
into
one
of
German,
English
or
+L+
Japanese.
The
system
has
been
designed
around
the
task
+L+
of
conference
registration
(CR).
It
has
initially
been
built
+L+
based
on
a
speech
database
of
12
read
dialogs,
encompassing
a
vocabulary
of
around
500
words.
We
have
since
been
+L+
expanding
the
system
along
several
dimensions
to
improve
+L+
speed,
robustness
and
coverage
and
to
move
toward
spontaneous
input.
+L+
1.
INTRODUCTION

A
Combinatorial
Approach
to
Trajectory
Planning
for
Binary
+L+
Manipulators
+L+
David
S.
Lees
+L+
Gregory
S.
Chirikjian
+L+
Department
of
Mechanical
Engineering,
Johns
Hopkins
University,
Baltimore,
MD
21218
+L+
Abstract
+L+
Binary
manipulators
are
powered
by
actuators
+L+
which
have
only
two
stable
states.
Therefore,
they
+L+
can
reach
only
a
discrete
(but
possibly
large)
number
+L+
of
locations.
Compared
to
a
manipulator
built
with
+L+
continuous
actuators,
a
binary
manipulator
provides
+L+
reasonable
performance,
and
is
relatively
inexpensive
+L+
(up
to
an
order
of
magnitude
cheaper).
The
number
+L+
of
states
of
a
binary
manipulator
grows
exponentially
+L+
with
the
number
of
actuators.
This
makes
the
calculation
of
its
inverse
kinematics
quite
difficult.
This
+L+
paper
presents
a
combinatorial
method
for
computing
+L+
the
inverse
kinematics
of
a
binary
manipulator
that
+L+
reduces
the
search
space
to
a
manageable
size.
It
also
+L+
creates
extremely
smooth
motions
that
follow
a
specified
trajectory
very
accurately
(in
both
position
and
+L+
orientation),
despite
the
discrete
nature
of
binary
actuation.
+L+
1
Introduction

CC
+L+
++
:
A
Declarative
Concurrent
+L+
Object
Oriented
Programming
Notation
+L+
K.
Mani
Chandy
Carl
Kesselman
+L+
California
Institute
of
Technology
+L+
September
18,
1992
+L+
Abstract
+L+
CC
++
is
Compositional
C
++
,
a
parallel
object-oriented
notation
+L+
that
consists
of
C
++
with
six
extensions.
The
goals
of
the
CC
++
+L+
project
are
to
provide
a
theory,
notation
and
tools
for
developing
reliable
scalable
concurrent
program
libraries,
and
to
provide
a
framework
+L+
for
unifying:
+L+
1.
distributed
reactive
systems,
batch-oriented
numeric
and
sym
+L+
bolic
applications,
and
user-interface
systems,
+L+
2.
declarative
programs
and
object-oriented
imperative
programs,
+L+
and
+L+
3.
deterministic
and
nondeterministic
programs.
+L+
This
paper
is
a
brief
description
of
the
motivation
for
CC
++
,
the
+L+
extensions
to
C
++
,
a
few
examples
of
CC
++
programs
with
reasoning
+L+
about
their
correctness,
and
an
evaluation
of
CC
++
in
the
context
of
+L+
other
research
on
concurrent
computation.
A
short
description
of
+L+
C
++
is
provided.
+L+
1
Introduction

Building
Scalable
Parallel
Processors
Using
+L+
Networked
Computers
-
A
Tutorial
For
+L+
Synergy
V2.0
+L+
Yuan
Shi
+L+
January
1994
+L+
@1994
Temple
University
+L+
SYNERGY
+L+
+PAGE+

Projections:
A
Preliminary
Performance
Tool
for
Charm
+L+
Amitabh
B.
Sinha
Laxmikant
V.
Kale
+L+
Department
of
Computer
Science
Department
of
Computer
Science
+L+
University
of
Illinois
University
of
Illinois
+L+
Urbana,
IL
61801
Urbana,
IL
61801
+L+
email:
sinha@cs.uiuc.edu
email:
kale@cs.uiuc.edu
+L+
Abstract
+L+
The
advent
and
acceptance
of
massively
parallel
+L+
machines
has
made
it
increasingly
important
to
have
+L+
tools
to
analyze
the
performance
of
programs
running
on
these
machines.
Current
day
performance
+L+
tools
suffer
from
two
drawbacks:
they
are
not
scalable
+L+
and
they
lose
specific
information
about
the
user
program
in
their
attempt
for
generality.
In
this
paper,
+L+
we
present
Projections,
a
scalable
performance
tool,
+L+
for
Charm
that
can
provide
program-specific
information
to
help
the
users
better
understand
the
behavior
+L+
of
their
programs.
+L+
1
Introduction

Comparison
of
Distributed
Concurrency
Control
Protocols
+L+
on
a
Distributed
Database
Testbed
+L+
Chia-Shiang
Shih
and
Asit
Dan
+L+
ECE
Department,
University
of
Massachusetts
+L+
Amherst,
MA
01003
+L+
Walter
H.
Kohler
+L+
Digital
Equipment
Corporation
+L+
200
Forest
Street,
MRO1-1/A65
+L+
Marlboro,
MA
01752-9101
+L+
John
A.
Stankovic
and
Don
Towsley
+L+
COINS
Department,
University
of
Massachusetts
+L+
Amherst,
MA
01003
+L+
This
work
was
supported
by
the
National
Science
Foundation,
grant
number
SDB-8418216
and
by
a
grant
from
+L+
Digital
Equipment
Corporation.
+L+
Walter
H.
Kohler
is
manager
of
High
Performance
Transaction
Processing
System
group
at
Digital
Equipment
+L+
Corporation.
+L+
+PAGE+

On
computing
global
similarity
in
images
+L+
S.
Ravela
and
R.
Manmatha
+L+
Computer
Science
Department
+L+
University
of
Massachusetts,
Amherst,
MA
01003
+L+
Email:
fravela,manmathag@cs.umass.edu
+L+
Abstract
+L+
The
retrieval
of
images
based
on
their
visual
similarity
+L+
to
an
example
image
is
an
important
and
fascinating
area
of
+L+
research.
Here,
a
method
to
characterize
visual
appearance
+L+
for
determining
global
similarity
in
images
is
described.
+L+
Images
are
filtered
with
Gaussian
derivatives
and
geometric
features
are
computed
from
the
filtered
images.
+L+
The
geometric
features
used
here
are
curvature
and
phase.
+L+
Two
images
may
be
said
to
be
similar
if
they
have
similar
distributions
of
such
features.
Global
similarity
may,
+L+
therefore,
be
deduced
by
comparing
histograms
of
these
+L+
features.
This
allows
for
rapid
retrieval
and
examples
from
+L+
collection
of
gray-level
and
trademark
images
are
shown.
+L+
1
Introduction

Raising
Roofs,
Crashing
Cycles,
and
Playing
Pool:
+L+
Applications
of
a
Data
Structure
for
Finding
Pairwise
Interactions
+L+
David
Eppstein
Jeff
Erickson
+L+
Submitted
to
Discrete
&
Computational
Geometry
+L+
July
1,
1998
+L+
Abstract
+L+
The
straight
skeleton
of
a
polygon
is
a
variant
of
the
medial
axis,
introduced
by
+L+
Aichholzer
et
al.,
defined
by
a
shrinking
process
in
which
each
edge
of
the
polygon
+L+
moves
inward
at
a
fixed
rate.
We
construct
the
straight
skeleton
of
an
n-gon
with
r
+L+
reflex
vertices
in
time
O(n
1+"
+
n
8=11+"
r
9=11+"
),
for
any
fixed
"
&gt;
0,
improving
the
+L+
previous
best
upper
bound
of
O(nr
log
n).
Our
algorithm
simulates
the
sequence
of
+L+
collisions
between
edges
and
vertices
during
the
shrinking
process,
using
a
technique
of
+L+
Eppstein
for
maintaining
extrema
of
binary
functions
to
reduce
the
problem
of
finding
+L+
successive
interactions
to
two
dynamic
range
query
problems:
(1)
maintain
a
changing
+L+
set
of
triangles
in
IR
3
and
answer
queries
asking
which
triangle
would
be
first
hit
by
+L+
a
query
ray,
and
(2)
maintain
a
changing
set
of
rays
in
IR
3
and
answer
queries
asking
+L+
for
the
lowest
intersection
of
any
ray
with
a
query
triangle.
We
also
exploit
a
novel
+L+
characterization
of
the
straight
skeleton
as
a
lower
envelope
of
triangles
in
IR
3
.
The
same
+L+
time
bounds
apply
to
constructing
non-self-intersecting
offset
curves
with
mitered
or
+L+
beveled
corners,
and
similar
methods
extend
to
other
problems
of
simulating
collisions
+L+
and
other
pairwise
interactions
among
sets
of
moving
objects.
+L+
An
extended
abstract
of
this
paper
was
presented
at
the
14th
Annual
ACM
Symposium
on
Computational
+L+
Geometry
[29].
See
http://www:cs:duke:edu/
~
jeffe/pubs/cycles:html
for
the
most
recent
version
of
this
paper.
+L+
Department
of
Information
and
Computer
Science,
University
of
California,
Irvine,
CA
92697,
USA;
epp-stein@ics.uci.edu;
http://www:ics:uci:edu/
~
eppstein.
Research
partially
supported
by
NSF
grant
CCR-9258355
and
+L+
by
matching
funds
from
Xerox
Corporation.
+L+
Center
for
Geometric
Computing,
Department
of
Computer
Science,
Duke
University,
Box
90129,
Durham,
+L+
NC
27708-0129,
USA;
jeffe@cs.duke.edu;
http://www:cs:duke:edu/
~
jeffe.
Research
supported
by
NSF
grant
DMS-9627683
and
by
U.
S.
Army
Research
Office
MURI
grant
DAAH04-96-1-0013.
+L+
+PAGE+

From
Ordinal
to
Euclidean
Reconstruction
with
Partial
Scene
+L+
Calibration
+L+
Daphna
Weinshall
+L+
Inst.
of
Computer
Sci.
+L+
Hebrew
University
+L+
91904
Jerusalem,
Israel
+L+
daphna@cs.huji.ac.il
+L+
P.
Anandan
+L+
Microsoft
Research
+L+
One
Microsoft
Way
+L+
Redmond,
WA
98052
+L+
anandan@microsoft.com
+L+
Micahl
Irani
+L+
Dept.
of
Appl.
Math
and
CS
+L+
The
Weizmann
Inst.
of
Sci.
+L+
Rehovot,
Israel
+L+
irani@wisdom.weizmann.ac.il
+L+
Abstract
+L+
Since
uncalibrated
images
permit
only
projective
reconstruction,
metric
information
requires
either
+L+
camera
or
scene
calibration.
We
propose
a
stratified
approach
to
projective
reconstruction,
in
which
+L+
gradual
increase
in
domain
information
for
scene
calibration
leads
to
gradual
increase
in
3D
information.
+L+
Our
scheme
includes
the
following
steps:
(1)
Register
the
images
with
respect
to
a
reference
plane;
this
+L+
can
be
done
using
limited
scene
information,
e.g.,
the
knowledge
that
two
pairs
of
lines
on
the
plane
are
+L+
parallel.
We
show
that
this
calibration
is
sufficient
for
ordinal
reconstruction
sorting
the
points
by
their
+L+
height
over
the
reference
plane.
(2)
If
available,
use
the
relative
height
of
two
additional
out-of-plane
+L+
points
to
compute
the
height
of
the
remaining
points
up
to
constant
scaling.
Our
scheme
is
based
on
the
+L+
dual
epipolar
geometry
in
the
reference
frame,
which
we
develop
below.
We
show
good
results
with
five
+L+
sequences
of
real
images,
using
mostly
scene
calibration
that
can
be
inferred
directly
from
the
images
+L+
themselves.
+L+
Keywords:
projective
reconstruction,
affine
reconstruction,
partial
calibration,
qualitative
depth
+L+
1
Introduction

On
the
Analysis
of
Indexing
Schemes
+L+
Joseph
M.
Hellerstein
+L+
Division
of
Computer
Science
+L+
UC
Berkeley,
Berkeley,
CA
94720
+L+
jmh@cs.berkeley.edu
+L+
Elias
Koutsoupias
+L+
Department
of
Computer
Science
+L+
UCLA,
Los
Angeles,
CA
90095
+L+
elias@cs.ucla.edu
+L+
Christos
H.
Papadimitriou
+L+
Division
of
Computer
Science
+L+
UC
Berkeley,
Berkeley,
CA
94720
+L+
christos@cs.berkeley.edu
+L+
Abstract
+L+
We
consider
the
problem
of
indexing
general
database
+L+
workloads
(combinations
of
data
sets
and
sets
of
potential
queries).
We
define
a
framework
for
measuring
the
+L+
efficiency
of
an
indexing
scheme
for
a
workload
based
on
+L+
two
characterizations:
storage
redundancy
(how
many
+L+
times
each
item
in
the
data
set
is
stored),
and
access
+L+
overhead
(how
many
times
more
blocks
than
necessary
+L+
does
a
query
retrieve).
Using
this
framework
we
present
+L+
some
initial
results,
showing
upper
and
lower
bounds
+L+
and
trade-offs
between
them
in
the
case
of
multi-dimensional
range
queries
and
set
queries.
+L+
1
Introduction

DIMACS
Technical
Report
93-8
+L+
February
1993
+L+
Hilbert
Series
of
Group
Representations
and
+L+
Grobner
Bases
for
Generic
Modules
+L+
by
+L+
Shmuel
Onn
1
+L+
DIMACS
+L+
Rutgers
University
+L+
Piscataway,
New
Jersey
08855-1179
+L+
E-mail:
onn@dimacs.rutgers.edu
+L+
1
Research
was
supported
by
the
Mittag-Le*er
Institute,
by
the
Mathematical
Sciences
Institute
+L+
at
Cornell
University,
and
by
the
Center
for
Discrete
Mathematics
and
Theoretical
Computer
+L+
Science
at
Rutgers
University.
+L+
DIMACS
is
a
cooperative
project
of
Rutgers
University,
Princeton
University,
AT&T
Bell
+L+
Laboratories
and
Bellcore.
+L+
DIMACS
is
an
NSF
Science
and
Technology
Center,
funded
under
contract
STC-88-09648;
+L+
and
also
receives
support
from
the
New
Jersey
Commission
on
Science
and
Technology.
+L+
+PAGE+

A
Randomized
Linear-Time
Algorithm
for
Finding
Minimum
Spanning
+L+
Trees
+L+
Philip
N.
Klein
+L+
Robert
E.
Tarjan
+L+
October
12,
1993
+L+
Abstract
+L+
We
present
a
randomized
linear-time
algorithm
for
finding
a
minimum
spanning
tree
in
a
connected
+L+
graph
with
edge
weights.
The
algorithm
is
a
modification
of
one
proposed
by
Karger
and
uses
random
+L+
sampling
in
combination
with
a
recently
discovered
linear-time
algorithm
for
verifying
a
minimum
spanning
tree.
Our
computational
model
is
a
unit-cost
random-access
machine
with
the
restriction
that
the
+L+
only
operations
allowed
on
edge
weights
are
binary
comparisons.
+L+
1
Introduction

HIGH
PERFORMANCE
IMPLEMENTATION
+L+
OF
SERVER
DIRECTED
I/O
+L+
BY
+L+
MAHESH
SUBRAMANIAM
+L+
B.E,
Birla
Institute
of
Technology
&
Science,
1993
+L+
M.Sc,
Birla
Institute
of
Technology
&
Science,
1993
+L+
THESIS
+L+
Submitted
in
partial
fulfillment
of
the
requirements
+L+
for
the
degree
of
Master
of
Science
in
Computer
Science
+L+
in
the
Graduate
College
of
the
+L+
University
of
Illinois
at
Urbana-Champaign,
1996
+L+
Urbana,
Illinois
+L+
+PAGE+

Efficient
Scheduling
of
Branching
Computations
on
Rings
of
+L+
Processors:
An
Empirical
Study
+L+
Lixin
Gao
Dawn
E.
Gregory
Arnold
L.
Rosenberg
Paul
R.
Cohen
+L+
Department
of
Computer
Science
+L+
University
of
Massachusetts
+L+
Amherst,
Mass.
01003,
USA
+L+
fgao,gregory,rsnbrg,coheng@cs.umass.edu
+L+
Abstract
+L+
We
empirically
analyze
and
compare
two
simple,
deterministic
policies
for
scheduling
dynamically
evolving
tree-structured
computations
on
parallel
architectures
that
are
rings
of
identical
+L+
processing
elements
(PEs).
Our
computations
have
each
task
either
halt
or
spawn
two
independent
children;
they
abstract,
for
instance,
computations
generated
by
multigrid
methods.
Our
+L+
simpler
policy,
called
koso,
has
each
PE
keep
one
child
of
a
spawning
task
and
pass
the
other
+L+
to
its
clockwise
neighbor
in
the
ring;
our
more
sophisticated
policy,
called
koso
?
,
operates
similarly,
but
allows
child-passing
only
from
a
more
heavily
loaded
PE
to
a
more
lightly
loaded
one.
+L+
Both
policies
execute
waiting
tasks
in
increasing
order
of
their
depths
in
the
evolving
task-tree.
+L+
Based
on
partial
(mathematical)
analyses
of
our
policies'
behaviors,
we
conjectured
that
both
+L+
yield
good
parallel
speedup
on
large
classes
of
the
computations
we
study,
but
that
policy
+L+
koso
?
outperforms
policy
koso
in
many
important
situations.
Not
having
been
able
to
verify
+L+
these
conjectures
analytically,
we
study
them
in
this
paper
via
a
suite
of
carefully
designed
and
+L+
analyzed
experiments.
Our
experiments
largely
substantiate
both
of
our
conjectures.
We
find
+L+
that
both
policies
give
close
to
optimal
parallel
speedup
on
large
classes
of
computations,
and
+L+
that
koso
?
outperforms
koso
on
these
computations,
except
on
very
small
processor
rings.
+L+
We
believe
that
our
methodology
of
experimental
design
and
analysis
will
prove
useful
in
other
+L+
such
studies.
+L+
1
Introduction

The
Interaction
of
+L+
Architecture
and
Operating
System
Design
+L+
Thomas
E.
Anderson
,
Henry
M.
Levy
,
Brian
N.
Bershad
,
and
Edward
D.
Lazowska
+L+
Department
of
Computer
Science
and
Engineering
+L+
University
of
Washington
+L+
Seattle,
WA
98195
+L+
Abstract
+L+
Today's
high-performance
RISC
microprocessors
have
been
+L+
highly
tuned
for
integer
and
floating
point
application
performance.
These
architectures
have
paid
less
attention
to
+L+
operating
system
requirements.
At
the
same
time,
new
operating
system
designs
often
have
overlooked
modern
architectural
trends
which
may
unavoidably
change
the
relative
+L+
cost
of
certain
primitive
operations.
The
result
is
that
operating
system
performance
is
well
below
application
code
+L+
performance
on
contemporary
RISCs.
+L+
This
paper
examines
recent
directions
in
computer
architecture
and
operating
systems,
and
the
implications
of
+L+
changes
in
each
domain
for
the
other.
The
requirements
of
+L+
three
components
of
operating
system
design
are
discussed
+L+
in
detail:
interprocess
communication,
virtual
memory,
and
+L+
thread
management.
For
each
component,
we
relate
operating
system
functional
and
performance
needs
to
the
mechanisms
available
on
commercial
RISC
architectures
such
as
+L+
the
MIPS
R2000
and
R3000,
Sun
SPARC,
IBM
RS6000,
+L+
Motorola
88000,
and
Intel
i860.
+L+
Our
analysis
reveals
a
number
of
specific
reasons
why
+L+
the
performance
of
operating
system
primitives
on
RISCs
+L+
has
not
scaled
with
integer
performance.
In
addition,
+L+
we
identify
areas
in
which
architectures
could
better
(and
+L+
cost-effectively)
accommodate
operating
system
needs,
and
+L+
areas
in
which
operating
system
design
could
accommodate
certain
necessary
characteristics
of
cost-effective
high-performance
microprocessors.
+L+
This
work
was
supported
in
part
by
the
National
Science
+L+
Foundation
under
Grants
No.
CCR-8703049,
CCR-8619663,
and
+L+
CCR-8907666,
by
the
Washington
Technology
Center,
by
the
Digital
Equipment
Corporation
Systems
Research
Center
and
External
Research
Program,
and
by
IBM
and
AT&T
Fellowships.
+L+
Bershad
is
now
with
the
School
of
Computer
Science,
Carnegie
+L+
Mellon
University.
+L+
1
Introduction

Translucent
Sums:
A
Foundation
for
+L+
Higher-Order
Module
Systems
+L+
Mark
Lillibridge
+L+
May,
1997
+L+
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213
+L+
Submitted
in
partial
fulfillment
of
the
requirements
+L+
for
the
degree
of
Doctor
of
Philosophy.
+L+
Thesis
Committee:
+L+
Robert
Harper,
Chair
+L+
Peter
Lee
+L+
John
Reynolds
+L+
Luca
Cardelli,
DEC
SRC
+L+
Copyright
c
fl1997
Mark
Lillibridge
+L+
This
research
was
sponsored
by
the
Air
Force
Materiel
Command
(AFMC)
and
the
Defense
Advanced
Research
Projects
Agency
(DARPA)
under
contract
number,
F19628-95-C-0050.
The
U.S.
Government
is
authorized
to
reproduce
and
distribute
reprints
for
Government
purposes
notwithstanding
+L+
any
copyright
notation
thereon.
+L+
The
views
and
conclusions
contained
in
this
document
are
those
of
the
author
and
should
not
be
+L+
interpreted
as
representing
the
official
policies
or
endorsements,
either
expressed
or
implied,
of
the
U.S.
+L+
Government.
+L+
+PAGE+

A
Tutorial
on
Visual
Servo
Control
+L+
Seth
Hutchinson
+L+
Department
of
Electrical
and
Computer
Engineering
+L+
The
Beckman
Institute
for
Advanced
Science
and
Technology
+L+
University
of
Illinois
at
Urbana-Champaign
+L+
405
N.
Mathews
Avenue
+L+
Urbana,
IL
61801
+L+
Email:
seth@uiuc.edu
+L+
Greg
Hager
+L+
Department
of
Computer
Science
+L+
Yale
University
+L+
New
Haven,
CT
06520-8285
+L+
Phone:
203
432-6432
+L+
Email:
hager@cs.yale.edu
+L+
Peter
Corke
+L+
CSIRO
Division
of
Manufacturing
Technology
+L+
P.O.
Box
883,
+L+
Kenmore.
Australia,
4069.
+L+
pic@brb.dmt.csiro.au
+L+
May
14,
1996
+L+
Abstract
+L+
This
paper
provides
a
tutorial
introduction
to
visual
servo
control
of
robotic
manipulators.
+L+
Since
the
topic
spans
many
disciplines
our
goal
is
limited
to
providing
a
basic
conceptual
framework.
We
begin
by
reviewing
the
prerequisite
topics
from
robotics
and
computer
vision,
including
+L+
a
brief
review
of
coordinate
transformations,
velocity
representation,
and
a
description
of
the
+L+
geometric
aspects
of
the
image
formation
process.
We
then
present
a
taxonomy
of
visual
servo
+L+
control
systems.
The
two
major
classes
of
systems,
position-based
and
image-based
systems,
are
+L+
then
discussed.
Since
any
visual
servo
system
must
be
capable
of
tracking
image
features
in
a
+L+
sequence
of
images,
we
include
an
overview
of
feature-based
and
correlation-based
methods
for
+L+
tracking.
We
conclude
the
tutorial
with
a
number
of
observations
on
the
current
directions
of
+L+
the
research
field
of
visual
servo
control.
+L+
+PAGE+

Belief
Networks,
Hidden
Markov
Models,
and
Markov
+L+
Random
Fields:
a
Unifying
View
+L+
Padhraic
Smyth
+L+
Information
and
Computer
Science
Department
+L+
University
of
California,
Irvine
+L+
CA
92697-3425.
+L+
smyth@ics.uci.edu
+L+
March
20,
1998
+L+
Abstract
+L+
The
use
of
graphs
to
represent
independence
structure
in
multivariate
probability
+L+
models
has
been
pursued
in
a
relatively
independent
fashion
across
a
wide
variety
of
+L+
research
disciplines
since
the
beginning
of
this
century.
This
paper
provides
a
brief
+L+
overview
of
the
current
status
of
such
research
with
particular
attention
to
recent
developments
which
have
served
to
unify
such
seemingly
disparate
topics
as
probabilistic
+L+
expert
systems,
statistical
physics,
image
analysis,
genetics,
decoding
of
error-correcting
+L+
codes,
Kalman
filters,
and
speech
recognition
with
Markov
models.
+L+
1
Introduction

Once
Upon
an
Object...
+L+
Computationally-Augmented
Toys
for
Storytelling
+L+
Jennifer
W.
Glos
and
Marina
Umaschi
+L+
MIT
Media
Lab
+L+
20
Ames
Street,
E15-320R/N
+L+
Cambridge,
MA
02139
USA
+L+
+1
617
253
6096
+L+
-
jenglos,
marinau-@media.mit.edu
+L+
Abstract
+L+
We
are
developing
design
principles
applying
tangible
interfaces
to
storytelling.
This
paper
describes
an
underlying
+L+
philosophy
and
three
resultant
designs
for
computer-mediated
toys,
exploring
how
the
merging
of
physical
objects
+L+
with
computer
technology
can
enhance
childrens
storytelling.
Each
prototype
aims
to
develop
a
specific
set
of
both
+L+
oral
and
written
storytelling
skills,
as
well
as
collaboration,
sharing,
and
the
notion
of
revision.
By
creating
+L+
narratives,
children
learn
about
culture
and
identity,
and
develop
a
sense
of
self.
In
addition,
narrative
can
be
used
as
+L+
a
gateway
to
draw
girls
into
technology.
The
use
of
multi-sensory
interfaces
allows
for
richer
interaction.
+L+
Keywords
+L+
storytelling,
children,
computer-mediated
toys,
identity,
education,
gender,
tangible
interfaces.
+L+
Motivation

MPI-FM:
High
Performance
MPI
on
Workstation
+L+
Clusters
+L+
Mario
Lauria
+L+
Dipartimento
di
Informatica
e
Sistemistica
+L+
Universita
di
Napoli
"Federico
II"
+L+
via
Claudio
21
+L+
80125
Napoli,
Italy
+L+
lauria@nadis.dis.unina.it.
+L+
Andrew
Chien
+L+
Department
of
Computer
Science
+L+
University
of
Illinois
at
Urbana-Champaign
+L+
1304
W.
Springfield
Ave.
+L+
Urbana,
IL
61801,
USA
+L+
achien@cs.uiuc.edu
+L+
Abstract
+L+
Despite
the
emergence
of
high
speed
LANs,
the
communication
performance
available
to
applications
on
workstation
clusters
still
falls
short
of
that
available
on
MPPs.
+L+
A
new
generation
of
efficient
messaging
layers
is
needed
to
take
advantage
of
the
hardware
performance
and
to
deliver
it
to
the
application
level.
Communication
software
+L+
is
the
key
element
in
bridging
the
communication
performance
gap
separating
MPPs
+L+
and
workstation
clusters.
+L+
MPI-FM
is
a
high
performance
implementation
of
MPI
for
networks
of
workstations
connected
with
a
Myrinet
network,
built
on
top
of
the
Fast
Messages
(FM)
library.
+L+
Based
on
the
FM
version
1.1
released
in
Fall
1995,
MPI-FM
achieves
a
minimum
one-way
latency
of
19
s
and
a
peak
bandwidth
of
17.3
MByte/s
with
common
MPI
send
+L+
and
receive
function
calls.
A
direct
comparison
using
published
performance
figures
+L+
shows
that
MPI-FM
running
on
SPARCstation
20
workstations
connected
with
a
relatively
inexpensive
Myrinet
network
outperforms
the
MPI
implementations
available
+L+
on
the
IBM
SP2
and
the
Cray
T3D,
both
in
latency
and
in
bandwidth,
for
messages
+L+
up
to
2
KByte
in
size.
+L+
Visiting
at
time
of
writing
+L+
+PAGE+

104
+L+
New-Value
Logging
in
the
Echo
+L+
Replicated
File
System
+L+
Andy
Hisgen
,
Andrew
Birrell
,
Charles
Jerian,
+L+
Timothy
Mann
,
Garret
Swart
+L+
June
23,
1993
+L+
Systems
Research
Center
+L+
130
Lytton
Avenue
+L+
Palo
Alto,
California
94301
+L+
+PAGE+

Using
Global
Consistency
to
Recognise
Euclidean
Objects
with
an
+L+
Uncalibrated
Camera
+L+
D.A.
Forsyth
J.L.
Mundy
A.
Zisserman
C.A.
Rothwell
+L+
Computer
Science
General
Electric
Robotics
Research
Group
Robotics
Research
Group
+L+
University
of
Iowa
Center
for
Research
and
Development
Oxford
University
Oxford
University
+L+
Iowa
City,
IA
52242
Schenectady,
NY
12345
Oxford,
UK
Oxford,
UK
+L+
Abstract
+L+
A
recognition
strategy
consisting
of
a
mixture
of
indexing
on
invariants
and
search,
allows
objects
to
be
recog-nised
up
to
a
Euclidean
ambiguity
with
an
uncalibrated
+L+
camera.
The
approach
works
by
using
projective
invariants
to
determine
all
the
possible
projectively
equivalent
+L+
models
for
a
particular
imaged
object;
then
a
system
of
+L+
global
consistency
constraints
is
used
to
determine
which
of
+L+
these
projectively
equivalent,
but
Euclidean
distinct,
models
corresponds
to
the
objects
viewed.
These
constraints
+L+
follow
from
properties
of
the
imaging
geometry.
In
particular,
a
recognition
hypothesis
is
equivalent
to
an
assertion
about,
among
other
things,
viewing
conditions
and
geometric
relationships
between
objects,
and
these
assertions
+L+
must
be
consistent
for
hypotheses
to
be
correct.
The
approach
is
demonstrated
to
work
on
images
of
real
scenes
+L+
consisting
of
polygonal
objects
and
polyhedra.
Keywords:
+L+
Recognition,
Computer
Vision,
Invariant
Theory,
Indexing,
Model-based
Vision
+L+
1
Introduction

Importing
Pre-packaged
Software
into
Lisp:
Experience
with
+L+
Arbitrary-Precision
Floating-Point
Numbers
+L+
Richard
J.
Fateman
+L+
University
of
California
at
Berkeley
+L+
Abstract
+L+
We
advocate
the
use
of
Common
Lisp
as
a
powerful
glue
for
+L+
building
scientific
computing
environments.
Naturally
one
+L+
then
has
to
address
mixing
pre-existing
(non
Lisp)
code
into
+L+
this
system.
We
provide
a
specific
example
as
an
elaborate
+L+
FORTRAN
system
written
by
David
Bailey
for
arbitrary-precision
floating-point
numeric
calculation.
We
discuss
the
+L+
advantages
and
disadvantages
of
wholesale
importing
into
+L+
Lisp.
A
major
advantage
is
being
able
to
use
state-of-the
+L+
art
packaged
software
sooner,
while
overcoming
the
disadvantages
caused
by
FORTRAN's
traditional
batch
orientation
and
weak
storage
model.
In
this
paper
we
emphasize
in
+L+
particular
how
effective
use
of
imported
systems
may
require
+L+
one
to
address
the
contrast
between
the
functional
(Lisp-like)
versus
state-transition-based
(Fortran-like)
approaches
+L+
to
dealing
with
compound
objects.
While
our
example
is
+L+
high-precision
floats,
other
highly
useful
packages
including
those
for
simulation,
PDE
solutions,
signal
processing,
+L+
statistical
computation,
etc.
may
also
benefit
by
similar
+L+
consideration.
+L+
1
Introduction

Abstract
+L+
TCP
is
a
reliable
transport
protocol
tuned
to
perform
well
+L+
in
traditional
networks
made
up
of
wired
links
with
stationary
hosts.
Networks
with
wireless
links
and
mobile
+L+
hosts
violate
many
of
the
assumptions
made
by
TCP,
causing
degraded
performance.
In
this
paper,
we
describe
a
+L+
simple
protocol
that
improves
TCP
performance
by
modifying
network-layer
software
only
at
a
basestation
without
+L+
violating
end-to-end
TCP
semantics.
The
main
idea
is
to
+L+
cache
packets
at
the
basestation
and
perform
local
+L+
retransmissions.
Simulations
of
this
protocol
show
that
it
+L+
is
significantly
more
robust
in
the
presence
of
multiple
+L+
packet
losses
in
a
single
transmission
window
as
compared
to
TCP.
This
enables
our
protocol
to
tolerate
at
least
+L+
10
times
as
high
an
error
rate
without
any
performance
+L+
degradation.
+L+
1
Introduction

Empirical
Study
of
a
Dataflow
Language
on
the
CM-5
+L+
David
E.
Culler
+L+
Seth
Copen
Goldstein
+L+
Klaus
Erik
Schauser
+L+
Thorsten
von
Eicken
+L+
Computer
Science
Division
+L+
Department
of
Electrical
Engineering
and
Computer
Sciences
+L+
College
of
Engineering
+L+
University
of
California,
Berkeley
+L+
Abstract:
This
paper
presents
empirical
data
on
the
behavior
of
large
dataflow
programs
on
+L+
a
distributed
memory
multiprocessor.
The
programs,
written
in
the
dataflow
language
Id90,
are
+L+
compiled
via
a
Threaded
Abstract
Machine
(TAM)
for
the
CM-5.
TAM
refines
dataflow
execution
+L+
models
by
addressing
critical
constraints
that
modern
parallel
architectures
place
on
the
compilation
+L+
of
general-purpose
parallel
programming
languages.
It
exposes
synchronization,
scheduling,
and
+L+
network
access
so
that
the
compiler
can
optimize
against
the
cost
of
these
operations.
+L+
The
data
presented
in
this
paper
evaluates
the
TAM
approach
in
compiling
dataflow
languages
+L+
on
stock
hardware.
We
present
data
on
the
instruction
mix,
speedup,
scheduling
behavior,
and
+L+
locality
of
large
ID90
programs.
It
is
shown
that
the
TAM
scheduling
hierarchy
is
able
to
tolerate
+L+
long
communication
latencies,
especially
when
some
degree
of
I-structure
locality
is
present.
We
investigate
how
frame
allocation
strategies,
k-bounded
loops,
and
I-structure
caching
and
distribution
+L+
together
affect
the
overall
efficiency.
Finally
we
document
some
scheduling
anomalies.
+L+
1
Introduction

Comparing
Data
Forwarding
and
Prefetching
+L+
for
Communication-Induced
Misses
in
Shared-Memory
MPs
1
+L+
David
Koufaty
2
and
Josep
Torrellas
+L+
Department
of
Computer
Science
+L+
University
of
Illinois
at
Urbana-Champaign,
IL
61801
+L+
dkoufaty@ichips.intel.com
torrella@cs.uiuc.edu
+L+
http://iacoma.cs.uiuc.edu
+L+
Abstract
+L+
As
the
difference
in
speed
between
processor
and
memory
system
continues
to
increase,
it
is
becoming
crucial
to
develop
+L+
and
refine
techniques
that
enhance
the
effectiveness
of
cache
+L+
hierarchies.
Two
such
techniques
are
data
prefetching
and
+L+
data
forwarding.
With
prefetching,
a
processor
hides
the
latency
of
cache
misses
by
requesting
the
data
before
it
actually
+L+
needs
it.
With
forwarding,
a
producer
processor
hides
the
+L+
latency
of
communication-induced
cache
misses
in
the
consumer
processors
by
sending
the
data
to
the
caches
of
the
+L+
latter.
These
two
techniques
are
complementary
approaches
+L+
to
hiding
the
latency
of
communication-induced
misses.
+L+
This
paper
compares
the
effectiveness
of
data
forwarding
+L+
and
data
prefetching
to
hide
communication-induced
misses.
+L+
Although
both
techniques
require
comparable
hardware
support,
forwarding
usually
has
a
lower
instruction
overhead.
We
+L+
evaluate
prefetching
and
forwarding
algorithms
in
a
paralleliz-ing
compiler
using
execution-driven
simulations
of
a
shared-memory
multiprocessor.
Both
data
forwarding
and
prefetch-ing
reduce
the
execution
time
of
applications
significantly
(30-40%
on
average).
Forwarding
performs
better
on
average,
+L+
while
prefetching
is
more
robust
to
changes
in
cache
and
memory
parameters.
Finally,
we
propose
two
ways
of
integrating
+L+
the
two
techniques.
The
integration
of
the
two
techniques
reduces
the
execution
time
even
more
(43-48%
on
average)
and
+L+
is
very
robust.
+L+
1
Introduction

An
Analysis
of
Message
Sequence
Charts
+L+
Peter
B.
Ladkin
Stefan
Leue
+L+
University
of
Berne
+L+
Institute
for
Informatics
and
Applied
Mathematics
+L+
Langgassstrasse
51
+L+
CH-3012
Bern,
Switzerland
+L+
ladkin@iam.unibe.ch,
leue@iam.unibe.ch
+L+
IAM
92-013
+L+
June
1992
+L+
+PAGE+

SDL
and
MSC
Based
Test
Case
Generation
-
+L+
An
Overall
View
of
the
SAMSTAG
Method
+L+
Jens
Grabowski
+L+
IAM-94-005
+L+
+PAGE+

Generation
of
task-specific
segmentation
+L+
procedures
as
a
model
selection
task
+L+
Ralf
Herbrich
and
Tobias
Scheffer
+L+
Technische
Universitat
Berlin,
+L+
Artificial
Intelligence
Research
Group,
Sekr.
FR
5-8,
+L+
Franklinstr.
28/29.
D-10587
Berlin,
Germany
+L+
email:
ralfh|scheffer@cs.tu-berlin.de
+L+
December
1,
1997
+L+
Abstract
+L+
In
image
segmentation
problems,
there
is
usually
a
vast
amount
of
filter
+L+
operations
available,
a
subset
of
which
has
to
be
selected
and
instantiated
+L+
in
order
to
obtain
a
satisfactory
segmentation
procedure
for
a
particular
domain.
In
supervised
segmentation,
a
mapping
from
features,
such
as
filter
+L+
outputs
for
individual
pixels,
to
classes
is
induced
automatically.
However,
+L+
since
the
sample
size
required
for
supervised
learning
grows
exponentially
+L+
in
the
number
of
features
it
is
not
feasible
to
learn
a
segmentation
procedure
+L+
from
a
large
amount
of
possible
filters.
But
we
argue
that
automatic
model
+L+
selection
methods
are
able
to
select
a
region
model
in
terms
of
some
filters.
+L+
We
propose
a
wrapper
algorithm
that
performs
this
task.
We
present
results
+L+
on
artificial
textured
images
(Brodatz)
and
report
on
our
experiences
with
+L+
x-ray
images.
+L+
Keywords:
model
based
image
segmentation,
automatic
model
selection,
+L+
learning
pixel
classifier,
texture
segmentation
+L+
+PAGE+

ICOPS
97
+L+
ICOPS
97
+L+
Nonlinear
Poisson
Solve
for
Boltzmann
Electrons
+L+
K.
L.
Cartwright
,
J.
P.
Verboncoeur
,
and
C.
K.Birdsall
+L+
Electronics
Research
Laboratory
+L+
University
of
California,
Berkeley,
CA
94720
+L+
October
29,
1997
+L+
Abstract
+L+
Kinetic
simulation
of
plasmas
in
which
equilibrium
occurs
over
ion
timescales
poses
+L+
a
computational
challenge
due
to
the
disparate
timescales
of
the
electron
plasma
frequency
(~
10
9
),
the
ion
plasma
frequency
(~
10
7
),
ion
transit
frequency
(~
10
6
),
and
+L+
the
ionization
frequency
(~
10
7
).
Hybrid
electrostatic
PIC
algorithms
are
presented
+L+
in
which
the
electrons
reach
thermodynamic
equilibrium
with
the
ions
each
time
step.
+L+
There
are
two
different
approximations
for
the
electrons.
First,
the
nonlinear
Boltzmann
relationship
for
the
electrons
can
be
applied
to
the
bulk
of
a
plasma.
Second,
+L+
there
is
a
truncated
Maxwellian
which
is
used
in
sheaths;
this
approximation
truncates
+L+
the
electron
distribution
at
the
wall
potential.
The
error
associated
with
neglecting
+L+
this
second
approximation
in
the
sheath
is
small.
The
collision
cross
section,
(E),
can
+L+
be
a
tabulated
or
fitted
function;
the
method
is
implemented
with
He
cross
sections.
+L+
These
approximations
neglect
effects
faster
than
ion
time-scales,
decreasing
the
computer
time
used
by
over
an
order
of
magnitude;
however,
they
increase
the
complexity
+L+
of
the
boundary
conditions
and
the
simulation
is
no
longer
self-consistent.
Theoretical
+L+
ramifications
of
these
approximations
are
examined,
and
results
are
compared
with
full
+L+
Supported
by
ONR-AASERT
N100014-94-1-1033
+L+
Supported
by
the
Air
Force
Office
of
Scientific
Research-MURI
under
grant
F49620-95-1-0253
+L+
Supported
by
the
Air
Force
Office
of
Scientific
Research-under
grant
FDF49620-96-1-0154
+L+
+PAGE+

Survey
Paper
+L+
Update-in-place
Analysis
for
Sets
+L+
Chung
Yung
+L+
Computer
Science
Department
+L+
Courant
Institute
of
Mathematical
Sciences
+L+
New
York
University
+L+
yung@cs.nyu.edu
+L+
December
15,
1997
+L+
Abstract
+L+
This
survey
paper
describes
the
current
approaches
on
the
update-in-place
analysis
+L+
for
sets.
Pure
functional
languages
do
not
allow
mutations,
destructive
updates,
or
selective
updates
so
that
straightforward
implementations
of
functional
language
compilers
+L+
may
induce
large
amounts
of
copying
to
preserve
program
semantics.
The
unnecessary
+L+
copying
of
data
can
increase
both
the
execution
time
and
the
memory
requirements
of
+L+
an
application.
Introducing
sets
to
functional
languages
as
a
primitive
data
constructor
posts
a
new
problem
of
update-in-place
analysis
in
functional
languages.
Moreover,
most
of
the
compiler
optimization
techniques
depend
on
the
side-effects
and
the
+L+
update-in-place
analysis
serves
as
the
premise
of
applying
such
optimization
techniques.
+L+
Among
other
compiler
optimization
techniques,
finite
differencing
captures
common
yet
+L+
distinctive
program
constructions
of
costly
repeated
calculations
and
transforms
them
+L+
into
more
efficient
incremental
program
constructions.
This
dissertation
is
an
attempt
+L+
to
explore
the
update-in-place
analysis
for
sets
in
functional
languages
in
order
to
apply
finite
differencing
to
compiling
pure
functional
languages.
In
this
survey
paper,
+L+
we
will
describe
the
approaches
of
update-in-place
analysis
and
the
finite
differencing
+L+
techniques.
+L+
1
Motivation
and
Introduction

Improving
the
Performance
of
Radial
Basis
+L+
Function
Networks
by
Learning
Center
Locations
+L+
Dietrich
Wettschereck
and
Thomas
Dietterich
+L+
Department
of
Computer
Science
+L+
Oregon
State
University
+L+
Corvallis,
OR
97331-3202
+L+
Advances
in
Neural
Information
Processing
Systems
4
+L+
Edited
by
J.E.Moody,
S.J.Hanson,
and
R.P.Lippmann,
+L+
Morgan
Kaufmann
Publishers,
San
Mateo,
CA,
1992
+L+
+PAGE+

System
Administration:
Monitoring,
Diagnosing,
and
Repairing
+L+
Eric
Anderson
+L+
We
first
describe
the
general
goals
of
system
administration
explaining
the
+L+
relationship
to
monitoring,
diagnosing,
and
repairing
(MDR).
Then,
we
describe
the
+L+
functional
and
environmental
concerns
for
a
MDR
system,
and
use
these
concerns
to
+L+
explain
how
previous
approaches
have
failed
to
fully
address
the
problem.
Next,
we
+L+
describe
the
major
pieces
of
our
approach,
and
identify
the
research
questions
+L+
associated
with
each
piece.
Finally
we
present
a
method
for
testing
the
system
when
+L+
it
is
created.
+L+
Introduction

ACM
SIGPLAN
Workshop
on
Languages,
Compilers
and
Tools
for
Real-Time
Systems,
La
Jolla,
California,
June
1995.
+L+
RTsynchronizer:
Language
Support
for
+L+
Real-Time
Specifications
in
Distributed
Systems
+L+
Shangping
Ren
and
Gul
A.
Agha
+L+
Department
of
Computer
Science
+L+
1304
W.
Springfield
Avenue
+L+
University
of
Illinois
at
Urbana-Champaign
+L+
Urbana,
IL
61801,
USA
+L+
Email:
f
ren
j
agha
g@cs.uiuc.edu
+L+
Abstract
+L+
We
argue
that
the
specification
of
an
object's
functional
behavior
and
the
timing
constraints
imposed
+L+
on
it
may
be
separated.
Specifically,
we
describe
+L+
RTsynchronizer,
a
high-level
programming
language
+L+
construct
for
specifying
real-time
constraints
between
+L+
objects
in
a
distributed
concurrent
system.
During
program
execution,
RTsynchronizers
affect
the
+L+
scheduling
of
distributed
objects
to
enforce
real-time
+L+
relations
between
events.
Objects
in
our
system
are
+L+
defined
in
terms
of
the
actor
model
extended
with
+L+
timing
assumptions.
Separation
of
the
functional
+L+
behaviors
of
actors
and
the
timing
constraints
on
+L+
patterns
of
actor
invocation
provides
at
least
three
+L+
important
advantages.
First,
it
simplifies
code
development
by
separating
design
concerns.
Second,
+L+
multiple
timing
constraints
can
be
independently
+L+
specified
and
composed.
And
finally,
a
specification
+L+
of
timing
constraints
can
be
reused
even
if
the
+L+
representation
of
the
functional
behavior
of
actors
has
+L+
changed,
and
conversely.
+L+
A
number
of
examples
are
given
to
illustrate
the
+L+
use
of
RTsynchronizers.
These
examples
illustrate
+L+
how
real-time
constraints
for
periodic
events,
simultaneous
events,
exception
handling,
and
producer-consumer
may
be
specified.
+L+
The
research
described
has
been
made
possible
by
support
from
the
Office
of
Naval
Research
(ONR
contract
numbers
N00014-90-J-1899
and
N00014-93-1-0273),
by
an
Incentives
for
Excellence
Award
from
the
Digital
Equipment
Corporation
Faculty
Program,
and
by
joint
support
from
the
Defense
+L+
Advanced
Research
Projects
Agency
and
the
National
Science
+L+
Foundation
(NSF
CCR
90-07195).
The
authors
would
like
to
+L+
thank
Mark
Astley,
Brian
Nielsen,
Masahiko
Saitoh
and
Daniel
+L+
Sturman
for
helpful
discussions
concerning
the
manuscript.
+L+
1
Introduction

IEEE
JOURNAL
ON
SELECTED
AREAS
IN
COMMUNICATIONS,
VOL.
13,
NO.
7,
SEPTEMBER
1995
1
+L+
Billing
Users
and
Pricing
for
TCP
+L+
Richard
J.
Edell
,
Nick
McKeown
and
Pravin
P.
Varaiya
+L+
Abstract|
This
paper
presents
a
system
for
billing
users
+L+
for
their
TCP
traffic.
This
is
achieved
by
postponing
the
+L+
establishment
of
connections
while
the
user
is
contacted,
+L+
verifying
in
a
secure
way
that
they
are
prepared
to
pay.
By
+L+
presenting
the
user
with
cost
and
price
information,
the
system
can
be
used
for
cost
recovery
and
to
encourage
efficient
+L+
use
of
network
resources.
The
system
requires
no
changes
to
+L+
existing
protocols
or
applications
and
can
be
used
to
recover
+L+
costs
between
cooperating
sites.
Statistics
collected
from
a
+L+
four
day
trace
of
traffic
between
the
University
of
Califor-nia,
Berkeley
and
the
rest
of
the
Internet
demonstrate
that
+L+
such
a
billing
system
is
practical
and
introduces
acceptable
+L+
latency.
An
implementation
based
on
the
BayBridge
prototype
router
is
described.
Our
study
also
indicates
that
+L+
pricing
schemes
may
be
used
to
control
network
congestion
+L+
either
by
rescheduling
time-insensitive
traffic
to
a
less
expensive
time
of
the
day,
or
by
smoothing
packet
transfers
to
+L+
reduce
traffic
peaks.
+L+
Keywords|
Internet
economics,
network
tracing,
usage-accounting
+L+
I.
Introduction

A
Comparative
Study
of
Three
Paradigms
for
Object
+L+
Recognition
Bayesian
Statistics,
Neural
Networks
and
+L+
Expert
Systems.
+L+
J.
K.
Aggarwal
,
Joydeep
Ghosh
,
Dinesh
Nair
and
Ismail
Taha
+L+
Computer
and
Vision
Research
Center
+L+
The
University
of
Texas
at
Austin,
Austin,
TX,
USA
+L+
email:
aggarwaljk@mail.utexas.edu
+L+
Abstract
+L+
Object
recognition,
which
involves
the
classification
of
objects
into
one
of
many
a
priori
known
object
types,
and
determining
object
characteristics
such
as
pose,
is
a
difficult
+L+
problem.
A
wide
range
of
approaches
have
been
proposed
and
applied
to
this
problem
with
+L+
limited
success.
This
paper
presents
a
brief
comparative
study
of
methods
from
three
different
paradigms
for
object
recognition:
Bayesian,
Neural
Network
and
Expert
Systems.
+L+
1
Introduction

DART/HYESS
Users
Guide
+L+
Jerome
H.
Friedman
+L+
Department
of
Statistics
and
+L+
Stanford
Linear
Accelerator
Center
+L+
Stanford
University
+L+
jhf@stat.stanford.edu
+L+
December
20,
1996
+L+
Abstract
+L+
This
note
provides
information
for
using
the
Fortran
program
[Friedman
(1996b)]
that
imple
+L+
ments
the
recursive
covering
approach
to
local
learning
described
in
Friedman
(1996a).
+L+
1.
Introduction

DEPARTMENT
OF
ELECTRICAL
ENGINEERING
AND
COMPUTER
SCIENCE
+L+
UNIVERSITY
OF
CALIFORNIA
+L+
BERKELEY,
CALIFORNIA
94720
August
17,
1997
+L+
A
PRELIMINARY
STUDY
OF
+L+
HIERARCHICAL
FINITE
STATE
MACHINES
+L+
WITH
MULTIPLE
CONCURRENCY
MODELS
+L+
by
+L+
Alain
Girault
,
Bilung
Lee
,
and
Edward
A.
Lee
+L+
Memorandum
No.
UCB/ERL
M97/57
+L+
ELECTRONICS
RESEARCH
LABORATORY
+L+
College
of
Engineering
+L+
University
of
California,
Berkeley
+L+
94720
+L+
+PAGE+

Switching
through
Singularities
+L+
C.
J.
Tomlin
and
S.
S.
Sastry
+L+
Department
of
Electrical
Engineering
and
Computer
Sciences
+L+
University
of
California,
Berkeley
CA
94720
+L+
fclairet,
sastryg@eecs.berkeley.edu
+L+
Abstract
+L+
Asymptotic
tracking
is
studied
for
systems
in
which
the
relative
degree
is
not
well
defined,
+L+
meaning
that
the
control
law
derived
from
exact
input-output
linearization
has
singularities
in
+L+
the
state
space.
We
propose
a
tracking
control
law
which
switches
between
approximate
tracking
+L+
[1]
close
to
the
singularities,
and
exact
tracking
away
from
the
singularities,
and
we
study
the
+L+
applicability
of
this
law
based
on
the
behavior
of
the
system's
zero
dynamics
at
the
switching
+L+
boundary.
As
in
[1],
the
ball
and
beam
example
is
used
to
motivate
the
study.
+L+
Keywords:
Switching,
nonlinear
control,
zero
dynamics,
exact
and
asymptotic
tracking,
+L+
nonminimum
phase.
+L+
1
Introduction

Construction
of
Fuzzy
Linguistic
Model
+L+
Tak-Kuen
John
Koo
+L+
Robotics
and
Intelligent
Machines
Laboratory
+L+
211-85
Cory
Hall,
Department
of
EECS
+L+
University
of
California
at
Berkeley
+L+
Berkeley,
CA94720
+L+
koo@robotics.eecs.berkeley.edu
+L+
Abstract
+L+
Using
linguistic
variables
to
describe
the
behavior
of
a
+L+
hybrid
system,
which
consists
of
a
discrete
event
system
and
a
continuous
system,
could
make
the
design
+L+
of
the
controller
and
verification
of
the
system
perform
+L+
on
an
unified
framework.
In
this
paper,
we
show
the
+L+
construction
of
a
fuzzy
linguistic
model
from
a
given
+L+
mathematical
model
of
a
physical
system.
By
considering
the
state-space
realization
of
the
model,
the
system
+L+
construction
problem
can
be
transformed
into
a
function
approximation
problem.
We
propose
to
use
projection
theorem
in
obtaining
an
optimal
fuzzy
system
+L+
which
is
the
best
approximation
of
a
given
nonlinear
+L+
function
in
L
2
(U
)
space.
We
show
that
the
existence
+L+
and
uniqueness
of
the
optimal
solution
is
assured
when
+L+
the
Fuzzy
Basis
Functions(FBFs)
are
linearly
independent.
We
demonstrate
that
the
dependence
of
the
basis
+L+
functions
can
be
examined
by
checking
the
condition
of
+L+
the
Gram
determinant
associated
to
the
FBFs.
Finally,
+L+
a
method
is
proposed
in
converting
the
optimal
coefficients
into
fuzzy
sets
to
obtain
a
fuzzy
linguistic
model.
+L+
1
Introduction

A
Fault
Tolerant
Control
Architecture
+L+
for
Automated
Highway
Systems
+L+
John
Lygeros
,
Datta
N.
Godbole
,
Mireille
Broucke
+L+
Department
of
Electrical
Engineering
and
Computer
Sciences
+L+
University
of
California,
Berkeley
+L+
Berkeley,
CA
94720
+L+
lygeros,
godbole,
mire@robotics.eecs.berkeley.edu
+L+
Abstract
+L+
We
propose
a
hierarchical
control
architecture
for
dealing
with
faults
and
adverse
environmental
conditions
on
an
Automated
Highway
System
(AHS).
Our
design
extends
+L+
a
previous
control
architecture
that
works
under
normal
conditions
of
operation.
The
+L+
faults
that
are
considered
in
our
design
are
classified
according
to
the
capabilities
remaining
on
the
vehicle
or
roadside
after
the
fault
has
occurred.
Information
about
these
+L+
capabilities
is
used
by
supervisors
in
each
of
the
layers
of
the
architecture
to
select
appropriate
control
strategies.
We
outline
the
extended
control
strategies
that
are
needed
+L+
by
these
supervisors
and,
in
certain
cases,
give
examples
of
their
detailed
operation.
+L+
Keywords
+L+
Automated
Highway
System
Design,
Fault
Tolerant
Control,
Safety
+L+
Research
supported
by
the
California
PATH
program,
Institute
of
Transportation
Studies,
University
of
+L+
California,
Berkeley,
under
MOU-135
+L+
+PAGE+

SmartATMS
:
A
SIMULATOR
FOR
AIR
TRAFFIC
MANAGEMENT
SYSTEMS
+L+
Tak-Kuen
John
Koo
+L+
Yi
Ma
+L+
George
J.
Pappas
+L+
Claire
Tomlin
+L+
Robotics
and
Intelligent
Machines
Laboratory
+L+
Department
of
Electrical
Engineering
and
Computer
Sciences
+L+
University
of
California
at
Berkeley
+L+
Berkeley,
CA
94720
+L+
koo,mayi,gpappas,clairet@eecs.berkeley.edu
+L+
ABSTRACT
+L+
Air
Traffic
Management
Systems
(ATMS)
of
the
future
will
feature
Free
Flight,
in
which
aircraft
choose
+L+
their
own
routes,
altitude,
and
speed,
and
automated
conflict
resolution
methods
in
which
aircraft
+L+
will
coordinate
to
resolve
conflicts.
The
resulting
distributed
control
architecture
is
a
hybrid
system,
with
+L+
mixed
discrete
event
and
continuous
time
dynamics.
+L+
SmartATMS
is
an
object
oriented
modeling
and
simulation
facility
which
accounts
for
these
hybrid
issues
+L+
and
will
serve
as
a
uniform
modeling
framework
for
+L+
the
design
and
evaluation
of
various
ATMS
concepts.
+L+
1
INTRODUCTION

FROM
KNOWLEDGE
TO
BELIEF
+L+
a
dissertation
+L+
submitted
to
the
department
of
computer
science
+L+
and
the
committee
on
graduate
studies
+L+
of
stanford
university
+L+
in
partial
fulfillment
of
the
requirements
+L+
for
the
degree
of
+L+
doctor
of
philosophy
+L+
By
+L+
Daphne
Koller
+L+
October
1994
+L+
+PAGE+

Tioga:
Providing
Data
Management
Support
for
+L+
Scientific
Visualization
Applications
+L+
Michael
Stonebraker
,
Jolly
Chen
,
Nobuko
Nathan
,
Caroline
Paxson
,
Jiang
Wu
+L+
Computer
Science
Division,
EECS
Department
+L+
University
of
California
+L+
Berkeley,
CA
94720
+L+
Abstract
+L+
We
present
a
user
interface
paradigm
for
+L+
database
management
systems
that
is
motivated
+L+
by
scientific
visualization
applications.
Our
+L+
graphical
user
interface
includes
a
"boxes
and
arrows"
notation
for
database
access
and
a
flight
+L+
simulator
model
of
movement
through
information
space.
We
also
provide
means
to
specify
a
+L+
hierarchy
of
abstracts
of
data
of
different
types
+L+
and
resolutions,
so
that
a
"zoom"
capability
can
+L+
be
supported.
The
underlying
DBMS
support
for
+L+
this
system
is
described
and
includes
the
compilation
of
query
plans
into
megaplans,
new
algorithms
for
data
buffering,
and
provisions
for
+L+
a
guaranteed
rate
of
data
delivery.
The
current
state
of
the
Tioga
implementation
is
also
described.
+L+
This
research
was
sponsored
by
NSF
Grant
IRI-9107455,
ARO
Grant
DAAL03-91-G-0183,
and
DARPA
+L+
Contract
DABT63-92-C-0007.
Additional
support
was
+L+
provided
by
the
University
of
California
and
Digital
Equipment
Corporation
under
Research
Grant
#1243.
+L+
Other
industrial
and
government
partners
include
the
+L+
State
of
California
Department
of
Water
Resources,
United
+L+
States
Geological
Survey,
Construction
Engineering
Research
Laboratory
(CERL)
of
the
U.S.
Army
Corps
of
+L+
Engineers,
the
National
Aeronautics
and
Space
Administration
(NASA),
Epoch
Systems,
Inc.,
Hewlett-Packard
+L+
Corp.,
Hughes
Aircraft
Company,
MCI,
Metrum
Corporation,
PictureTel
Corporation,
Research
Systems
Inc.,
Science
Applications
International
Corporation,
Siemens
Inc.,
+L+
and
TRW
Space
and
Electronics.
+L+
1
Introduction

GEN++
|
an
analyzer
generator
for
C++
programs
+L+
Prem
Devanbu
+L+
Articifial
Intelligence
Principles
Research
Department
+L+
prem@research.att.com
+L+
Laura
Eaves
+L+
Object
Oriented
and
Artificial
Intelligence
Technologies
Group,
+L+
laurae@mozart.att.com
+L+
1
Introduction

The
following
paper
was
originally
published
in
the
+L+
Proceedings
of
the
USENIX
2nd
Symposium
on
+L+
Operating
Systems
Design
and
Implementation
+L+
Seattle,
Washington,
October
1996
+L+
For
more
information
about
USENIX
Association
contact:
+L+
1.
Phone:
510
528-8649
+L+
2.
FAX:
510
548-5738
+L+
3.
Email:
office@usenix.org
+L+
4.
WWW
URL:
http://www.usenix.org
+L+
Dealing
With
Disaster:
+L+
Surviving
Misbehaved
Kernel
Extensions
+L+
Margo
I.
Seltzer
,
Yasuhiro
Endo
,
Christopher
Small
,
Keith
A.
Smith
+L+
Harvard
University
+L+
+PAGE+

Efficient
Restoration
of
Multicolor
Images
with
+L+
Independent
Noise
+L+
Yuri
Boykov
Olga
Veksler
Ramin
Zabih
+L+
Cornell
University,
USA
+L+
Keywords:
Bayesian
image
restoration;
Markov
random
fields;
max
flow-min
cut
+L+
Abstract
+L+
We
consider
the
problem
of
maximum
a
posteriori
(MAP)
restoration
of
multicolor
images
+L+
where
each
pixel
has
been
degraded
by
independent
arbitrary
noise.
We
assume
that
the
+L+
prior
distribution
is
given
by
a
Markov
random
field
with
only
pairwise
site
interactions.
+L+
Two
classes
of
site
interactions
are
considered:
two-valued
site
interactions,
which
form
a
+L+
generalized
Potts
model;
and
linear
site
interactions.
We
give
efficient
algorithms
based
on
+L+
graph
cuts
for
both
classes.
The
MAP
estimate
for
a
generalized
Potts
model
can
be
computed
by
solving
a
multiway
minimum
cut
problem
on
a
graph.
While
this
graph
problem
is
+L+
computationally
intractable,
there
are
fast
algorithms
for
computing
provably
good
approximations.
The
MAP
estimate
with
linear
site
interactions
can
be
computed
exactly
by
solving
+L+
a
minimum
cut
problem
on
a
graph.
This
can
be
performed
in
nearly
linear
time.
+L+
1
Introduction

Client/Server
Architectures
for
Business
Information
Systems
Page
1
+L+
Client/Server
Architectures
for
Business
+L+
Information
Systems
+L+
A
Pattern
Language
+L+
Klaus
Renzel
+L+
sd&m
GmbH
&
Co.
KG
+L+
Project
ARCUS
1
+L+
Thomas-Dehler-Str.
27
+L+
D-81737
Mnchen,
Germany
+L+
email:
Klaus.Renzel@sdm.de
+L+
Phone:
+49-89-63812-251
+L+
http://www.sdm.de/g/arcus
+L+
Wolfgang
Keller
+L+
EA
Generali
+L+
Reumannplatz
7
+L+
A-1100
Vienna,
Austria
+L+
email:
WofgangWKeller@compuserve.com
+L+
Phone:
+43-1-53401-0
+L+
Copyright
1997,
Klaus
Renzel,
Wolfgang
Keller
+L+
Permission
granted
to
copy
for
PLoP97
Conference.
+L+
All
other
rights
reserved.
+L+
Abstract:
This
paper
presents
several
patterns
for
distributing
business
information
systems
that
are
structured
according
to
a
layered
architecture.
2
Each
distribution
pattern
cuts
the
architecture
into
different
client
and
server
components.
All
+L+
the
patterns
presented
give
an
answer
to
the
same
question:
How
do
I
distribute
a
+L+
business
information
system?
However,
the
consequences
of
applying
the
patterns
+L+
are
very
different
with
regards
to
the
forces
influencing
distributed
systems
design.
+L+
1
Introduction

Semantic
Query
Caching
for
Heterogeneous
Databases
+L+
Parke
Godfrey
+L+
U.S.
Army
Research
Laboratory
+L+
2800
Powder
Mill
Road
+L+
Adelphi,
Maryland
20783-1197
+L+
U.S.A.
+L+
godfrey@arl.mil
+L+
Jarek
Gryz
+L+
Department
of
Computer
Science
+L+
York
University
+L+
Toronto,
Ontario
M3J
1P3
+L+
Canada
+L+
jarek@cs.yorku.ca
+L+
Abstract
+L+
Query
caching
can
play
a
vital
role
in
heterogeneous,
multi-database
environments.
Answers
to
a
query
that
are
available
in
cache
+L+
at
the
local
client
can
be
returned
to
the
user
+L+
quickly,
while
the
rest
of
the
query
is
evaluated.
The
use
of
caches
can
optimize
query
+L+
evaluation.
By
caching
certain
sensitive
data
+L+
locally,
caches
can
be
used
to
answer
the
parts
+L+
of
queries
that
involve
the
sensitive
data,
so
it
+L+
need
not
be
shipped
across
the
network.
Most
+L+
prior
cache
schemes
have
been
tuple-based
or
+L+
page-based.
It
is
unclear,
however,
how
these
+L+
might
be
adapted
for
multi-databases.
We
explore
a
more
flexible
semantic
query
caching
+L+
(SQC)
approach.
In
SQC,
caches
are
the
answer
sets
of
previous
queries,
labeled
by
the
+L+
query
expressions
that
produced
them.
We
+L+
promote
developing
the
technology,
based
on
+L+
logic,
to
manipulate
semantic
caches,
to
determine
when
and
how
caches
can
be
used
to
answer
subsequent
queries,
and
to
optimize
via
+L+
cache
use.
+L+
The
copyright
of
this
paper
belongs
to
the
papers
authors.
Permission
to
copy
without
fee
all
or
part
of
this
material
is
granted
+L+
provided
that
the
copies
are
not
made
or
distributed
for
direct
+L+
commercial
advantage.
+L+
Proceedings
of
the
4th
KRDB
Workshop
+L+
Athens,
Greece,
30-August-1997
+L+
(F.
Baader,
M.A.
Jeusfeld,
W.
Nutt,
eds.)
+L+
http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-8/
+L+
1
Introduction

Finding
Maximum
Flows
in
Undirected
Graphs
Seems
Easier
than
+L+
Bipartite
Matching
+L
David
R.
Karger
and
Matthew
S.
Levine
+L+
Abstract
+L+
Consider
an
n-vertex,
m-edge,
undirected
graph
with
maximum
flow
value
v.
We
give
a
method
to
find
augmenting
+L+
paths
in
such
a
graph
in
amortized
sub-linear
(O(n
+L+
p
+L+
v))
time
+L+
per
path.
This
lets
us
improve
the
time
bound
of
the
classic
augmenting
path
algorithm
to
O(m
+
nv
3=2
)
on
simple
+L+
graphs.
The
addition
of
a
blocking
flow
subroutine
gives
a
+L+
simple,
deterministic
O(nm
2=3
v
1=6
)-time
algorithm.
We
also
+L+
use
our
technique
to
improve
known
randomized
algorithms,
+L+
giving
O(m+nv
5=4
)-time
and
O(m+n
11=9
v)-time
algorithms
+L+
for
capacitated
undirected
graphs.
For
simple
graphs,
in
+L+
which
v
n,
the
last
bound
is
O(n
2:2
),
improving
on
the
best
+L+
previous
bound
of
O(n
2:5
),
which
is
also
the
best
known
time
+L+
bound
for
bipartite
matching.
+L+
1
Introduction

Electronic
Lottery
Tickets
as
Micropayments
+L+
Ronald
L.
Rivest
+L+
MIT
Lab
for
Computer
Science
+L+
(RSA
/
Security
Dynamics)
+L+
rivest@theory.lcs.mit.edu
+L+
Abstract.
We
present
a
new
micropayment
scheme
based
on
the
use
of
+L+
"electronic
lottery
tickets."
This
scheme
is
exceptionally
efficient
since
+L+
the
bank
handles
only
winning
tickets,
instead
of
handling
each
micro
+L+
payment.
+L+
1
Introduction

Compiler
Technology
for
Portable
Checkpoints
+L+
Volker
Strumpen
+L+
Laboratory
for
Computer
Science
+L+
Massachusetts
Institute
of
Technology
+L+
Cambridge,
MA
02139
+L+
strumpen@theory.lcs.mit.edu
+L+
Abstract
+L+
We
have
implemented
a
prototype
compiler
called
porch
that
transforms
C
programs
into
C
programs
supporting
portable
checkpoints.
Portable
checkpoints
capture
the
state
of
a
computation
in
+L+
a
machine-independent
format
that
allows
the
transfer
of
computations
across
binary
incompatible
machines.
We
introduce
source-to-source
compilation
techniques
for
generating
code
to
save
and
+L+
recover
from
such
portable
checkpoints
automatically.
These
techniques
instrument
a
program
with
code
that
maps
the
state
of
a
computation
into
a
machine-independent
representation
and
vice
versa.
+L+
In
particular,
the
following
problems
are
addressed:
(1)
providing
+L+
stack
environment
portability,
(2)
enabling
conversion
of
complex
+L+
data
types,
and
(3)
rendering
pointers
portable.
Experimental
results
show
that
the
overhead
of
checkpointing
is
reasonably
small,
+L+
even
if
data
representation
conversion
is
required
for
portability.
+L+
1
Introduction

Task
Driven
Perceptual
Organization
for
Extraction
of
Rooftop
+L+
Polygons
+L+
Christopher
Jaynes
Frank
Stolle
Robert
Collins
+L+
Computer
Science
Department
+L+
University
of
Massachusetts
+L+
Amherst,
MA
01003
+L+
Email:
@cs.umass.edu
+L+
Abstract
+L+
A
new
method
for
extracting
planar
polygonal
rooftops
in
monocular
aerial
imagery
is
proposed.
Through
bottom-up
and
top-down
construction
of
perceptual
groups,
polygons
in
a
+L+
single
aerial
image
can
be
robustly
extracted.
+L+
Orthogonal
corners
and
lines
are
extracted
and
+L+
hierarchically
related
using
perceptual
grouping
techniques.
Top-down
feature
verification
is
+L+
used
so
that
features,
and
links
between
the
features,
are
verified
with
local
information
in
the
+L+
image
and
weighed
in
a
graph
structure
according
to
the
underlying
support
for
each
feature.
+L+
Cycles
in
the
graph
correspond
to
possible
+L+
building
rooftop
hypotheses.
Virtual
features
+L+
are
hypothesized
for
the
perceptual
completion
+L+
of
partial
rooftops.
Extraction
of
the
"best"
+L+
grouping
of
features
into
a
building
rooftop
hypothesis
is
posed
as
a
graph
search
problem.
+L+
The
maximally
weighted,
independent
set
of
cycles
in
the
graph
is
extracted
as
the
final
set
of
+L+
roof
boundaries.
+L+
1.
Introduction

Determining
3-D
Hand
Motion
+L+
James
Davis
Mubarak
Shah
+L+
Media
Lab
Computer
Vision
Lab
+L+
Massachusetts
Institute
of
Technology
University
of
Central
Florida
+L+
Cambridge,
MA
02139
Orlando,
FL
32826
+L+
Abstract
+L+
This
paper
presents
a
glove-free
method
for
tracking
+L+
hand
movements
using
a
set
of
3-D
models.
In
this
+L+
approach,
the
hand
is
represented
by
five
cylindrical
+L+
models
which
are
fit
to
the
third
phalangeal
segments
+L+
of
the
fingers.
Six
3-D
motion
parameters
for
each
+L+
model
are
calculated
that
correspond
to
the
movement
+L+
of
the
fingertips
in
the
image
plane.
Trajectories
of
+L+
the
moving
models
are
then
established
to
show
the
3-D
nature
of
hand
motion.
+L+
1
Introduction

Efficient
Gate
Delay
Modeling
for
Large
Interconnect
Loads
+L+
Andrew
B.
Kahng
and
Sudhakar
Muddu
+L+
UCLA
Computer
Science
Department,
Los
Angeles,
CA
90095-1596
USA
+L+
abk@cs.ucla.edu,
sudhakar@cs.ucla.edu
+L+
Abstract
+L+
With
fast
switching
speeds
and
large
interconnect
trees
(MCMs),
the
+L+
resistance
and
inductance
of
interconnect
has
a
dominant
impact
on
+L+
logic
gate
delay.
In
this
paper,
we
propose
a
new
P
model
for
distributed
RC
and
RLC
interconnects
to
estimate
the
driving
point
admittance
at
the
output
of
a
CMOS
gate.
Using
this
model
we
are
able
to
+L+
compute
the
gate
delay
efficiently,
within
25%
of
SPICE-computed
delays.
Our
parameters
depend
only
on
total
interconnect
tree
resistance
+L+
and
capacitance
at
the
output
of
the
gate.
Previous
effective
load
capacitance
methods
[7,
9],
applicable
only
for
distributed
RC
interconnects,
are
based
on
P
model
parameters
obtained
via
a
recursive
admittance
moment
computation.
Our
model
should
be
useful
for
iterative
+L+
optimization
of
performance-driven
routing
or
for
estimation
of
gate
+L+
delay
and
rise
times
in
high-level
synthesis.
+L+
Keywords:
gate
delay,
reduced-order
models,
driving
point
admittance,
+L+
effective
capacitance,
interconnect
modeling
+L+
1
Introduction

Robust
IP
Watermarking
Methodologies
for
Physical
Design
+L+
Andrew
B.
Kahng
,
Stefanus
Mantik
,
Igor
L.
Markov
,
Miodrag
Potkonjak,
+L+
Paul
Tucker
,
Huijuan
Wang
and
Gregory
Wolfe
+L+
UCLA
Computer
Science
Dept.,
Los
Angeles,
CA
90095-1596
+L+
UCSD
Computer
Science
&
Engineering
Dept.,
La
Jolla,
CA
92093-0114
+L+
Abstract
+L+
Increasingly
popular
reuse-based
design
paradigms
create
a
pressing
need
for
authorship
enforcement
techniques
that
protect
the
intellectual
property
rights
of
designers.
We
develop
the
first
intellectual
property
protection
protocols
for
embedding
design
watermarks
at
the
physical
design
level.
We
demonstrate
that
these
protocols
are
transparent
with
respect
to
existing
industrial
tools
and
+L+
design
flows,
and
that
they
can
embed
watermarks
into
real-world
+L+
industrial
designs
with
very
low
implementation
overhead
(as
measured
by
such
standard
metrics
as
wirelength,
layout
area,
number
+L+
of
vias,
routing
congestion
and
CPU
time).
On
several
industrial
+L+
test
cases,
we
obtain
extremely
strong,
tamper-resistant
proofs
of
+L+
authorship
for
placement
and
routing
solutions.
+L+
1
Introduction

Quick
Simulation
of
ATM
Buffers
with
+L+
On-off
Multiclass
Markov
Fluid
Sources
1
+L+
G.
Kesidis
+L+
E
&
CE
Dept,
University
of
Waterloo,
Waterloo,
Ontario,
N2L
3G1,
Canada.
+L+
J.
Walrand
+L+
EECS
Dept,
University
of
California,
Berkeley,
CA94720.
+L+
ACM
TOMACS,
Vol.
3,
No.
3,
pp.
269-276,
July,
1993.
+L+
Abstract
+L+
The
problem
we
address
is
how
to
quickly
estimate
by
simulation
the
loss
in
a
+L+
buffer
with
multiclass
on-off
Markov
fluid
sources.
We
generate
the
Markov
fluids
+L+
with
the
altered
rate
matrices
given
in
[11],
instead
of
the
originals,
to
speed
up
+L+
the
simulation.
Likelihood
ratios
are
used
to
recover
an
estimate
of
the
loss
for
the
+L+
original
traffic
parameters.
+L+
1
Introduction

Inferring
Reduced
Ordered
Decision
Graphs
of
Minimal
Description
+L+
Length
+L+
Arlindo
L.
Oliveira
+L+
Alberto
Sangiovanni-Vincentelli
+L+
Dept.
of
EECS,
UC
Berkeley,
Berkeley
CA
94720
+L+
May
17,
1994
+L+
1
Introduction

VIS
:
A
System
for
Verification
and
Synthesis
+L+
Robert
K.
Brayton
Gary
D.
Hachtel
Alberto
Sangiovanni-Vincentelli
+L+
Fabio
Somenzi
Adnan
Aziz
Szu-Tsung
Cheng
Stephen
Edwards
Sunil
Khatri
+L+
Yuji
Kukimoto
Abelardo
Pardo
Shaz
Qadeer
Rajeev
K.
Ranjan
+L+
Shaker
Sarwary
Thomas
R.
Shiple
Gitanjali
Swamy
Tiziano
Villa
+L+
1
Introduction

Using
Don't
Cares
in
Logic
Minimization
for
+L+
LUT-Based
FPGAs
+L+
Philip
Chong
13327872
+L+
May
25,
1997
+L+
Abstract
+L+
Don't
care
information
has
proven
to
be
useful
in
logic
minimization.
+L+
Here,
the
use
of
don't
care
information
in
network
collapsing
for
mapping
+L+
to
LUT-based
FPGAs
is
explored.
Results
are
shown
which
indicate
that
+L+
this
approach
does
not
result
in
appreciable
improvements
in
network
size.
+L+
1
Introduction

Submitted
to
ACM
SIGCOMM
'98
+L+
Modeling
TCP
Throughput:
A
Simple
Model
and
its
Empirical
Validation
+L+
Jitendra
Padhye
Victor
Firoiu
Don
Towsley
Jim
Kurose
+L+
jitu@cs.umass.edu
vfiroiu@cs.umass.edu
towsley@cs.umass.edu
kurose@cs.umass.edu
+L+
1-413-545-2447
1-413-545-3179
1-413-545-0207
1-413-545-1585
+L+
Department
of
Computer
Science
+L+
University
of
Massachusetts
+L+
LGRC,
Box
34610
+L+
Amherst,
MA
01003-4610
USA
+L+
May
29,
1998
+L+
Abstract
+L+
In
this
paper
we
develop
a
simple
analytic
characterization
of
the
steady
state
throughput,
as
a
function
of
loss
rate
and
round
trip
time
for
a
bulk
transfer
TCP
flow,
i.e.,
a
flow
with
an
unlimited
amount
+L+
of
data
to
send.
Unlike
the
models
in
[6,
7,
10],
our
model
captures
not
only
the
behavior
of
TCP's
fast
+L+
retransmit
mechanism
(which
is
also
considered
in
[6,
7,
10])
but
also
the
effect
of
TCP's
timeout
mechanism
on
throughput.
Our
measurements
suggest
that
this
latter
behavior
is
important
from
a
modeling
+L+
perspective,
as
almost
all
of
our
TCP
traces
contained
more
timeout
events
than
fast
retransmit
events.
+L+
Our
measurements
demonstrate
that
our
model
is
able
to
more
accurately
predict
TCP
throughput
and
is
+L+
accurate
over
a
wider
range
of
loss
rates.
+L+
This
material
is
based
upon
work
supported
by
the
National
Science
Foundation
under
grants
NCR-95-08274,
NCR-95-23807
+L+
and
CDA-95-02639.
Any
opinions,
findings,
and
conclusions
or
recommendations
expressed
in
this
material
are
those
of
the
authors
+L+
and
do
not
necessarily
reflect
the
views
of
the
National
Science
Foundation.
+L+
Corresponding
Author
+L+
http://www.cs.umass.edu/~vfiroiu/
+L+
+PAGE+

Use
of
Architecture-Altering
Operations
to
Dynamically
+L+
Adapt
a
Three-Way
Analog
Source
Identification
Circuit
to
+L+
Accommodate
a
New
Source
+L+
John
R.
Koza
+L+
Computer
Science
Dept.
+L+
Stanford
University
+L+
Stanford,
California
94305-9020
+L+
koza@cs.stanford.edu
+L+
http://www-cs
+L+
faculty.stanford.edu/~koza/
+L+
Forrest
H
Bennett
III
+L+
Visiting
Scholar
+L+
Computer
Science
Dept.
+L+
Stanford
University
+L+
Stanford,
California
94305
+L+
forrest@evolute.com
+L+
Jason
Lohn
+L+
Visiting
Scholar
+L+
Computer
Science
Dept.
+L+
Stanford
University
+L+
Stanford,
California
94305
+L+
jlohn7@leland.stanford.edu
+L+
Frank
Dunlap
+L+
Dunlap
Consulting
+L+
Palo
Alto,
California
+L+
Martin
A.
Keane
+L+
Martin
Keane
Inc.
+L+
5733
West
Grover
+L+
Chicago,
Illinois
60630
+L+
makeane@ix.netcom.com
+L+
David
Andre
+L+
Computer
Science
Division
+L+
University
of
California
+L+
Berkeley,
California
+L+
dandre@cs.berkeley.edu
+L+
ABSTRACT
+L+
The
problem
of
source
+L+
identification
involves
correctly
+L+
classifying
an
incoming
signal
into
+L+
a
category
that
identifies
the
+L+
signal's
source.
+L+
The
problem
is
difficult
because
+L+
information
is
not
provided
+L+
distinguishing
characteristics
and
+L+
because
successive
signals
from
the
+L+
same
source
differ.
The
source
+L+
identification
problem
can
be
made
+L+
more
difficult
by
dynamically
+L+
changing
the
repertoire
of
sources
+L+
while
the
problem
is
being
solved.
+L+
We
used
genetic
programming
to
+L+
evolve
both
the
topology
and
the
+L+
sizing
(numerical
values)
for
each
+L+
component
of
an
analog
electrical
+L+
circuit
that
can
correctly
classify
an
+L+
incoming
analog
electrical
signal
+L+
into
three
categories.
Then,
the
+L+
repertoire
of
sources
was
+L+
dynamically
changed
by
adding
a
+L+
new
source
during
the
run.
The
+L+
paper
describe
show
the
+L+
enabled
genetic
programming
to
+L+
adapt,
during
the
run,
to
the
+L+
changed
environment.
Specifically,
+L+
a
three-way
source
identification
+L+
circuit
was
evolved
and
then
+L+
adapted
into
a
four-way
classifier,
+L+
during
the
run,
thereby
successfully
+L+
handling
the
additional
new
source.
+L+
1.
Introduction

Learning
to
Retrieve
Information
+L+
Brian
Bartell
+L+
Encylopdia
Britannica
and
+L+
Institute
for
Neural
Computation
+L+
Computer
Science
&
Engineering
+L+
University
of
California,
San
Diego
+L+
La
Jolla,
California
92093
+L+
Garrison
W.
Cottrell
+L+
Institute
for
Neural
Computation
+L+
Computer
Science
&
Engineering
+L+
University
of
California,
San
Diego
+L+
La
Jolla,
California
92093
+L+
Rik
Belew
+L+
Institute
for
Neural
Computation
+L+
Computer
Science
&
Engineering
+L+
University
of
California,
San
Diego
+L+
La
Jolla,
California
92093
+L+
Abstract
+L+
Information
retrieval
differs
significantly
from
function
approximation
in
that
the
goal
is
for
the
system
to
achieve
the
same
ranking
+L+
function
of
documents
relative
to
queries
as
the
user:
the
outputs
of
+L+
the
system
relative
to
one
another
must
be
in
the
proper
order.
We
+L+
hypothesize
that
a
particular
rank-order
statistic,
Guttman's
point
+L+
alienation,
is
the
proper
objective
function
for
such
a
system,
and
+L+
demonstrate
its
efficacy
by
using
it
to
find
the
optimal
combination
+L+
of
retrieval
experts.
In
application
to
a
commercial
retrieval
system,
+L+
the
combination
performs
47%
better
than
any
single
expert.
+L+
1
Introduction

Higher
Bandwidth
X
+L+
(Extended
Abstract)
+L+
Submitted
to
ACM
MULTIMEDIA
'94
+L+
John
Danskin
+L+
Princeton
University
+L+
Computer
Science
Department
+L+
Princeton
NJ,
08540
+L+
jmd@cs.princeton.edu
+L+
Abstract
+L+
Network
bandwidth
has
always
been
a
key
issue
for
multimedia
protocols.
Many
potential
users
of
networked
multimedia
protocols
will
continue
to
have
low
bandwidth
network
+L+
connections
for
some
time:
copper
wire
ISDN,
infra-red,
cellular
modems,
etc..
Compression
provides
potential
relief
for
users
of
slow
networks
by
increasing
effective
bandwidth.
+L+
HBX
introduces
a
new
technique,
based
on
arithmetic
coding
and
statistical
modeling,
for
+L+
compressing
structured
data.
Applied
to
the
X
networked
graphics
protocol,
this
technique
+L+
yields
4.5:1
compression
across
a
representative
set
of
traces,
performing
twice
as
well
as
the
+L+
popular
LZW-based
Xremote
compression
protocol.
HBX's
coding
techniques
are
generally
+L+
applicable
to
the
graphics
and
imaging
subset
of
multimedia
protocols.
Future
work
will
+L+
determine
whether
HBX's
coding
techniques
can
be
applied
to
audio
and
video
streams
as
+L+
well.
+L+
1
Introduction

David
Laur
and
Pat
Hanrahan
+L+
Princeton
University
+L+
Princeton,
NJ
08544,
USA
+L+
Abstract
+L+
This
paper
presents
a
progressive
refinement
algorithm
for
+L+
volume
rendering
which
uses
a
pyramidal
volume
representation.
Besides
storing
average
values,
the
pyramid
stores
+L+
estimated
error,
so
an
oct-tree
can
be
fit
to
the
pyramid
+L+
given
a
user-supplied
precision.
This
oct-tree
is
then
drawn
+L+
using
a
set
of
splats,
or
footprints,
each
scaled
to
match
the
+L+
size
of
the
projection
of
a
cell.
The
splats
themselves
are
approximated
with
RGBA
Gouraud-shaded
polygons,
so
that
+L+
they
can
be
drawn
efficiently
on
modern
graphics
workstations.
The
result
is
a
real-time
rendering
algorithm
suitable
+L+
for
interactive
applications.
+L+
CR
Categories
and
Subject
Descriptors:
I.3.7
[Computer
Graphics]:
Three-Dimensional
Graphics
and
Realism.
+L+
Key
Words:
volume
rendering,
coherence,
progressive
refinement,
interactive
techniques.
+L+
1
Introduction

Robust
Meshes
from
Multiple
Range
Maps
+L+
Kari
Pulli
Tom
Duchamp
Hugues
Hoppe
+L+
John
McDonald
Linda
Shapiro
Werner
Stuetzle
+L+
University
of
Washington,
Seattle,
WA
+L+
Microsoft
Research,
Redmond,
WA
+L+
Abstract
+L+
This
paper
presents
a
method
for
modeling
the
surface
+L+
of
an
object
from
a
sequence
of
range
maps.
Our
method
+L+
is
based
on
a
volumetric
approach
that
produces
a
compact
+L+
surface
without
boundary.
It
provides
robustness
through
+L+
the
use
of
interval
analysis
techniques
and
computational
+L+
efficiency
through
hierarchical
processing
using
octrees.
+L+
1.
Introduction

Dis-equality
Constraints
in
Linear/Integer
+L+
Programming
+L+
Mozafar
T.
Hajian
+L+
IC-Parc,
William
Penny
Laboratory,
Imperial
College,
London,
SW7
2AZ.
+L+
mh10@doc.ic.ac.uk
+L+
June
21,
1996
+L+
Abstract
+L+
We
have
proposed
an
extension
to
the
definition
of
general
integer
linear
programs
(ILP)
to
accept
dis-equality
constraints
explicitly.
A
new
class
of
logical
+L+
variables
is
introduced
to
transform
the
extended
ILP
in
general
form
to
standard
+L+
form.
Branch
and
Bound
algorithm
is
modified
to
solve
this
new
class
of
ILP.
+L+
Keywords:
Mathematical
Modelling,
Linear/Integer
Programming,
Algorithms,
+L+
Branch
and
Bound,
Dis-equality
Constraints.
+L+
+PAGE+

A
Parallel
Implementation
of
an
MPEG1
Encoder:
+L+
Faster
Than
Real-Time!
+L+
Ke
Shen
,
Lawrence
A.
Rowe
and
Edward
J.
Delp
+L+
Computer
Vision
and
Image
Processing
Laboratory
+L+
School
of
Electrical
Engineering
+L+
Purdue
University
+L+
West
Lafayette,
Indiana
+L+
Computer
Science
Division
+L+
Department
of
Electrical
Engineering
and
Computer
Science
+L+
University
of
California
+L+
Berkeley,
California
+L+
ABSTRACT
+L+
In
this
paper
we
present
an
implementation
of
an
MPEG1
encoder
on
the
Intel
Touchstone
Delta
and
Intel
+L+
Paragon
parallel
computers.
We
describe
the
unique
aspects
of
mapping
the
algorithm
onto
the
parallel
+L+
machines
and
present
several
versions
of
the
algorithms.
We
will
show
that
I/O
contention
can
be
a
bottleneck
+L+
relative
to
performance.
We
will
also
describe
how
the
Touchstone
Delta
and
Paragon
can
be
used
to
compress
+L+
video
sequences
faster
than
real-time.
+L+
1.
INTRODUCTION

Structuring
Graphical
Paradigms
in
TkGofer
+L+
Koen
Claessen
+L+
OGI
and
Utrecht
University
+L+
koen@cse.ogi.edu
+L+
Ton
Vullinghs
+L+
Universitat
Ulm
+L+
ton@informatik.uni-ulm.de
+L+
Erik
Meijer
+L+
OGI
and
Utrecht
University
+L+
erik@cse.ogi.edu
+L+
Abstract
+L+
In
this
paper
we
describe
the
implementation
of
several
+L+
graphical
programming
paradigms
(Model
View
Controller,
+L+
Fudgets,
and
Functional
Animations)
using
the
GUI
library
+L+
TkGofer.
This
library
relies
on
a
combination
of
monads
+L+
and
multiple-parameter
type
classes
to
provide
an
abstract,
+L+
type
safe
interface
to
Tcl/Tk.
We
show
how
choosing
the
+L+
right
abstractions
makes
the
given
implementations
surprisingly
concise
and
easy
to
understand.
+L+
1
Introduction

Disk
Packings
and
Planar
Separators
+L+
Daniel
A.
Spielman
+L+
U.C.
Berkeley/MIT
+L+
Shang-Hua
Teng
+L+
University
of
Minnesota
+L+
Abstract
+L+
We
demonstrate
that
the
geometric
separator
algorithm
of
Miller,
Teng,
Thurston,
and
Vavasis
finds
a
+L+
3=4-separator
of
size
1:84
+L+
p
+L+
n
for
every
n
node
planar
+L+
graph.
Our
bound
is
derived
from
an
analysis
of
disk
+L+
packings
on
the
sphere.
+L+
1.
Introduction

Building
Interpreters
by
Composing
Monads
+L+
Guy
L.
Steele
Jr.
+L+
Thinking
Machines
Corporation
+L+
245
First
Street
+L+
Cambridge,
Massachusetts
02142
+L+
(617)
234-2860
+L+
gls@think.com
+L+
Abstract:
We
exhibit
a
set
of
functions
coded
in
+L+
Haskell
that
can
be
used
as
building
blocks
to
construct
+L+
a
variety
of
interpreters
for
Lisp-like
languages.
The
+L+
building
blocks
are
joined
merely
through
functional
+L+
composition.
Each
building
block
contributes
code
to
+L+
support
a
specific
feature,
such
as
numbers,
continuations,
functions
calls,
or
nondeterminism.
The
result
of
+L+
composing
some
number
of
building
blocks
is
a
parser,
+L+
an
interpreter,
and
a
printer
that
support
exactly
the
+L+
expression
forms
and
data
types
needed
for
the
combined
set
of
features,
and
no
more.
+L+
The
data
structures
are
organized
as
pseudomonads,
+L+
a
generalization
of
monads
that
allows
composition.
+L+
Functional
composition
of
the
building
blocks
implies
+L+
type
composition
of
the
relevant
pseudomonads.
+L+
Our
intent
was
that
the
Haskell
type
resolution
system
ought
to
be
able
to
deduce
the
approprate
data
+L+
types
automatically.
Unfortunately
there
is
a
deficiency
+L+
in
current
Haskell
implementations
related
to
recursive
+L+
data
types:
circularity
must
be
reflected
statically
in
the
+L+
type
definitions.
+L+
We
circumvent
this
restriction
by
applying
a
purpose-built
program
simplifier
that
performs
partial
evaluation
+L+
and
a
certain
amount
of
program
algebra.
We
construct
+L+
a
wide
variety
of
interpreters
in
the
style
of
Wadler
by
+L+
starting
with
the
building
blocks
and
a
page
of
boiler-plate
code,
writing
three
lines
of
code
(one
to
specify
the
+L+
building
blocks
and
two
to
(redundantly)
specify
type
+L+
compositions),
and
then
applying
the
simplifier.
The
+L+
resulting
code
is
acceptable
Haskell
code.
+L+
We
have
tested
a
dozen
different
interpreters
with
+L+
various
combinations
of
features.
In
this
paper
we
discuss
the
overall
code
structuring
strategy,
exhibit
several
building
blocks,
briefly
describe
the
partial
evaluator,
and
present
a
number
of
automatically
generated
+L+
interpreters.
+L+
This
is
a
preprint
of
a
paper
that
is
to
appear
+L+
in
the
Proceedings
of
the
Twenty-first
Annual
ACM
+L+
SIGPLAN-SIGACT
Symposium
on
Principles
of
Programming
Languages,
January
1994.
+L+
1
Introduction

Theory
and
Design
of
Multidimensional
QMF
Sub-Band
Filters
From
+L+
1-D
Filters
Using
Transforms
+L+
I.A.
Shah
A.A.C.
Kalker
+L+
Philips
Research
Laboratories,
+L+
P.O.
Box
80.000,
5600
JA
Eindhoven,
The
Netherlands
+L+
Net:
kalker@prl.philips.nl,
shah@prl.philips.nl
+L+
Abstract
+L+
The
paper
presents
the
general
theory
of
designing
+L+
multidimensional
Quadrature
Mirror
Filters
(QMF),
+L+
for
use
in
sub-band
coding
(SBC)
systems,
using
the
+L+
McClellan
transform
[1].
It
was
recently
shown
that
+L+
McClellan
transform
could
be
used
to
generate
2-D
+L+
diamond
shape
QMF
filters
[2].
In
this
paper
we
will
+L+
formalize
the
proofs
of
the
diamond
shape
case,
and
+L+
generalize
it
to
other
shapes,
sampling
rasters
and
+L+
dimensions.
Examples
are
given
of
two
dimensional
+L+
diamond
shape
filters
and
three
dimensional
tetrad
+L+
filters
designed
using
the
technique.
+L+
1
Introduction

16
IEEE
TRANSACTIONS
ON
INFORMATION
THEORY,
VOL.
44,
NO.
1,
JANUARY
1998
+L+
Quantized
Overcomplete
Expansions
in
R
+L+
N
+L+
Analysis,
Synthesis,
and
Algorithms
+L+
Vivek
K
Goyal
,Student
Member,
IEEE
,
Martin
Vetterli
,
Fellow,
IEEE,
+L+
and
Nguyen
T.
Thao
,
Member,
IEEE
+L+
Abstract|Coefficient
quantization
has
peculiar
qualitative
+L+
effects
on
representations
of
vectors
in
R
N
with
respect
to
+L+
overcomplete
sets
of
vectors.
These
effects
are
investigated
+L+
in
two
settings:
frame
expansions
(representations
obtained
+L+
by
forming
inner
products
with
each
element
of
the
set)
+L+
and
matching
pursuit
expansions
(approximations
obtained
+L+
by
greedily
forming
linear
combinations).
In
both
cases,
+L+
based
on
the
concept
of
consistency,
it
is
shown
that
traditional
linear
reconstruction
methods
are
suboptimal,
and
+L+
better
consistent
reconstruction
algorithms
are
given.
The
+L+
proposed
consistent
reconstruction
algorithms
were
in
each
+L+
case
implemented,
and
experimental
results
are
included.
+L+
For
frame
expansions,
results
are
proven
to
bound
distortion
as
a
function
of
frame
redundancy
r
and
quantization
+L+
step
size
for
linear,
consistent,
and
optimal
reconstruction
+L+
methods.
Taken
together,
these
suggest
that
optimal
reconstruction
methods
will
yield
O(1=r
2
)
MSE,
and
that
consistency
is
sufficient
to
insure
this
asymptotic
behavior.
A
+L+
result
on
the
asymptotic
tightness
of
random
frames
is
also
+L+
proven.
+L+
Applicability
of
quantized
matching
pursuit
to
lossy
vector
compression
is
explored.
Experiments
demonstrate
the
+L+
likelihood
that
a
linear
reconstruction
is
inconsistent,
the
+L+
MSE
reduction
obtained
with
a
nonlinear
(consistent)
reconstruction
algorithm,
and
generally
competitive
performance
+L+
at
low
bit
rates.
+L+
Keywords|
quantization,
source
coding,
frames,
matching
+L+
pursuit,
consistent
reconstruction,
optimal
reconstruction,
+L+
overcomplete
representations,
MSE
bounds
+L+
I.
Introduction

The
Role
of
Learning
in
Autonomous
Robots
+L+
Rodney
A.
Brooks
MIT
Artificial
Intelligence
Laboratory
545
Technology
Square
Cambridge,
MA
02139
brooks@ai.mit.edu
+L+
Abstract
+L+
Applications
of
learning
to
autonomous
+L+
agents
(simulated
or
real)
have
often
been
+L+
restricted
to
learning
a
mapping
from
perceived
state
of
the
world
to
the
next
action
+L+
to
take.
Often
this
is
couched
in
terms
of
+L+
learning
from
no
previous
knowledge.
This
+L+
general
case
for
real
autonomous
robots
is
+L+
very
difficult.
In
any
case,
when
building
a
+L+
real
robot
there
is
usually
a
lot
of
a
priori
+L+
knowledge
(e.g.,
from
the
engineering
that
+L+
went
into
its
design)
which
doesn't
need
to
+L+
be
learned.
We
describe
the
behavior-based
+L+
approach
to
autonomous
robots,
and
then
examine
four
classes
of
learning
problems
associated
with
such
robots.
+L+
1
INTRODUCTION

A
Perturbation
Scheme
for
Spherical
Arrangements
+L+
with
Application
to
Molecular
Modeling
+L+
Dan
Halperin
+L+
Tel
Aviv
University
+L+
Christian
R.
Shelton
+L+
Massachusetts
Institute
of
Technology
+L+
July
31,
1997
+L+
Abstract
+L+
We
describe
a
software
package
for
computing
and
manipulating
the
subdivision
of
a
sphere
+L+
by
a
collection
of
(not
necessarily
great)
circles
and
for
computing
the
boundary
surface
of
the
+L+
union
of
spheres.
We
present
problems
that
arise
in
the
implementation
of
the
software
and
the
+L+
solutions
that
we
have
found
for
them.
At
the
core
of
the
paper
is
a
novel
perturbation
scheme
to
+L+
overcome
degeneracies
and
precision
problems
in
computing
spherical
arrangements
while
using
+L+
floating
point
arithmetic.
The
scheme
is
relatively
simple,
it
balances
between
the
efficiency
+L+
of
computation
and
the
magnitude
of
the
perturbation,
and
it
performs
well
in
practice.
We
+L+
report
and
discuss
experimental
results.
Our
package
is
a
major
component
in
a
larger
package
+L+
aimed
to
support
geometric
queries
on
molecular
models;
it
is
currently
employed
by
chemists
+L+
working
in
`rational
drug
design.'
The
spherical
subdivisions
are
used
to
construct
a
geometric
+L+
model
of
a
molecule
where
each
sphere
represents
an
atom.
We
also
give
an
overview
of
the
+L+
molecular
modeling
package
and
detail
additional
features
and
implementation
issues.
+L+
This
work
has
been
supported
in
part
by
a
grant
from
Pfizer
Central
Research.
Dan
Halperin
has
also
been
+L+
supported
by
an
Alon
Fellowship,
by
ESPRIT
IV
LTR
Project
No.
21957
(CGAL),
and
by
the
Hermann
Minkowski
+L+
-
Minerva
Center
for
Geometry
at
Tel
Aviv
University.
+L+
Department
of
Computer
Science,
Tel
Aviv
University,
Tel
Aviv
69978,
Israel.
E-mail:
+L+
halperin@math.tau.ac.il.
+L+
Department
of
Computer
Science,
MIT,
Cambridge,
MA
02139.
E-mail:
cshelton@ai.mit.edu.
Part
of
the
+L+
work
on
this
paper
was
carried
out
while
Christian
Shelton
was
at
the
Department
of
Computer
Science,
Stanford
+L+
University
+L+
+PAGE+

Taxonomic
Syntax
for
First
Order
Inference
+L+
DAVID
MCALLESTER
and
ROBERT
GIVAN
+L+
Massachusetts
Institute
of
Technology,
Cambridge
Massachusetts
+L+
Abstract:
We
identify
a
new
polynomial
time
decidable
fragment
of
first
order
+L+
logic
and
present
a
general
method
for
using
polynomial
time
inference
procedures
+L+
in
knowledge
representation
systems.
Our
results
indicate
that
a
non-standard
+L+
"taxonomic"
syntax
is
essential
in
constructing
natural
and
powerful
polynomial
+L+
time
inference
procedures.
The
central
role
of
taxonomic
syntax
in
our
polynomial
time
inference
procedures
provides
technical
support
for
the
often
expressed
+L+
intuition
that
knowledge
is
better
represented
in
terms
of
taxonomic
relationships
+L+
than
classical
first
order
formulas.
To
use
our
procedures
in
a
knowledge
representation
system
we
define
a
"Socratic
proof
system"
which
is
complete
for
first
+L+
order
inference
and
which
can
be
used
as
a
semi-automated
interface
to
a
first
+L+
order
knowledge
base.
+L+
Categories
and
Subject
Descriptors:
F.4.1
[Mathematical
Logic
and
Formal
Languages]:
Mathematical
logic
|
computational
logic,
mechanical
theorem
+L+
proving;
I.2.3
[Artificial
Intelligence]:
Deduction
and
Theorem
Proving
|
deduction
+L+
General
Terms:
Deduction,
Algorithms
+L+
Additional
Keywords
and
Phrases:
Proof
Theory,
Machine
Inference,
Theorem
Proving,
Automated
Reasoning,
Polynomial
Time
Algorithms,
Inference
+L+
Rules,
Proof
Systems,
Mechanical
Verification.
+L+
This
research
was
supported
in
part
by
National
Science
Foundation
Grant
IRI-8819624
and
in
part
by
the
Advanced
Research
Projects
Agency
of
the
Department
+L+
of
Defense
under
Office
of
Naval
Research
contract
N00014-85-K-0124
and
N00014-89-J-3202.
+L+
Author's
Address:
MIT
Artificial
Intelligence
Laboratory,
545
Technology
Square,
+L+
Cambridge
Mass,
02139,
DAM@ai.mit.edu
+L+
This
paper
appeared
in
JACM,
vol.
40,
no.
2,
April
1993.
A
postscript
electronic
source
+L+
for
this
paper
can
be
found
in
ftp.ai.mit.edu:/pub/dam/jacm1.ps.
A
bibtex
reference
can
+L+
be
found
in
internet
file
ftp.ai.mit.edu:/pub/dam/dam.bib.
+L+
+PAGE+

Anatomical
origin
and
computational
role
of
diversity
+L+
in
the
response
properties
of
cortical
neurons
+L+
Kalanit
Grill
Spectory
Shimon
Edelmany
Rafael
Malachz
+L+
Departments
of
Applied
Mathematics
and
Computer
Science
and
Neurobiology
+L+
The
Weizmann
Institute
of
Science
+L+
Rehovot
76100,
Israel
+L+
fkalanit,edelmang@wisdom.weizmann.ac.il
bnmalach@weizmann.weizmann.ac.il
+L+
Abstract
+L+
The
maximization
of
diversity
of
neuronal
response
properties
has
been
recently
suggested
+L+
as
an
organizing
principle
for
the
formation
of
such
prominent
features
of
the
functional
+L+
architecture
of
the
brain
as
the
cortical
columns
and
the
associated
patchy
projection
patterns
+L+
(Malach,
1994).
We
report
a
computational
study
of
two
aspects
of
this
hypothesis.
First,
we
+L+
show
that
maximal
diversity
is
attained
when
the
ratio
of
dendritic
and
axonal
arbor
sizes
is
+L+
equal
to
one,
as
it
has
been
found
in
many
cortical
areas
and
across
species
(Lund
et
al.,
1993;
+L+
Malach,
1994).
Second,
we
show
that
maximization
of
diversity
leads
to
better
performance
in
+L+
two
case
studies:
in
systems
of
receptive
fields
implementing
steerable/shiftable
filters,
and
in
+L+
matching
spatially
distributed
signals,
a
problem
that
arises
in
visual
tasks
such
as
stereopsis,
+L+
motion
processing,
and
recognition.
+L+
1
Introduction

Parametric
Models
are
Versatile:
+L+
The
Case
of
Model
Based
Optimization
+L+
P.
Fua
+L+
Artificial
Intelligence
Center
+L+
SRI
International
+L+
333
Ravenswood
Avenue
+L+
Menlo
Park,
California
94025
+L+
Abstract
+L+
Model-Based
Optimization
(MBO)
is
a
paradigm
in
which
an
objective
function
is
used
to
express
+L+
both
geometric
and
photometric
constraints
on
features
of
interest.
A
parametric
model
of
a
feature
+L+
(such
as
a
road,
a
building,
or
coastline)
is
extracted
from
one
or
more
images
by
adjusting
the
model's
+L+
state
variables
until
a
minimum
value
of
the
objective
function
is
obtained.
The
optimization
procedure
+L+
yields
a
description
that
simultaneously
satisfies
(or
nearly
satisfies)
all
constraints,
and,
as
a
result,
is
+L+
likely
to
be
a
good
model
of
the
feature.
+L+
1
Introduction

Cooperative
Bayesian
and
Case-Based
Reasoning
+L+
for
Solving
Multiagent
Planning
Tasks
+L+
David
W.
Aha
&
Li
Wu
Chang
+L+
Navy
Center
for
Applied
Research
in
AI
+L+
Naval
Research
Laboratory,
Code
5510
+L+
Washington,
DC
20375
+L+
faha;
liwug@aic.nrl.navy.mil
+L+
(202)
767-2884
/
FAX:
767-3172
+L+
January
25,
1996
+L+
Abstract
+L+
We
describe
an
integrated
problem
solving
architecture
named
INBANCA
in
+L+
which
Bayesian
networks
and
case-based
reasoning
(CBR)
work
cooperatively
on
+L+
multiagent
planning
tasks.
This
includes
two-team
dynamic
tasks,
and
this
paper
+L+
concentrates
on
simulated
soccer
as
an
example.
Bayesian
networks
are
used
to
characterize
action
selection
whereas
a
case-based
approach
is
used
to
determine
how
to
+L+
implement
actions.
This
paper
has
two
contributions.
First,
we
survey
integrations
+L+
of
case-based
and
Bayesian
approaches
from
the
perspective
of
a
popular
CBR
task
+L+
decomposition
framework,
thus
explaining
what
types
of
integrations
have
been
attempted.
This
allows
us
to
explain
the
unique
aspects
of
our
proposed
integration.
+L+
Second,
we
demonstrate
how
Bayesian
nets
can
be
used
to
provide
environmental
+L+
context,
and
thus
feature
selection
information,
for
the
case-based
reasoner.
+L+
1
Introduction

THE
LOAD,
CAPACITY
AND
AVAILABILITY
OF
QUORUM
+L+
SYSTEMS
+L+
MONI
NAOR
AND
AVISHAI
WOOL
+L+
Abstract.
+L+
A
quorum
system
is
a
collection
of
sets
(quorums)
every
two
of
which
intersect.
Quorum
systems
+L+
have
been
used
for
many
applications
in
the
area
of
distributed
systems,
including
mutual
exclusion,
+L+
data
replication
and
dissemination
of
information
+L+
Given
a
strategy
to
pick
quorums,
the
load
L(S)
is
the
minimal
access
probability
of
the
busiest
+L+
element,
minimizing
over
the
strategies.
The
capacity
Cap(S)
is
the
highest
quorum
accesses
rate
+L+
that
S
can
handle,
so
Cap(S)
=
1=L(S).
+L+
The
availability
of
a
quorum
system
S
is
the
probability
that
at
least
one
quorum
survives,
+L+
assuming
that
each
element
fails
independently
with
probability
p.
A
tradeoff
between
L(S)
and
the
+L+
availability
of
S
is
shown.
+L+
We
present
four
novel
constructions
of
quorum
system,
all
featuring
optimal
or
near
optimal
+L+
load,
and
high
availability.
The
best
construction,
based
on
paths
in
a
grid,
has
a
load
of
O(1=
+L+
p
+L+
and
a
failure
probability
of
exp((
+L+
p
+L+
n))
when
the
elements
fail
with
probability
p
&lt;
1
+L+
2
.
Moreover,
+L+
even
in
the
presence
of
faults,
with
exponentially
high
probability
the
load
of
this
system
is
still
+L+
O(1=
+L+
n).
The
analysis
of
this
scheme
is
based
on
Percolation
Theory.
+L+
Key
words.
quorum
systems,
load,
fault
tolerance,
distributed
computing,
percolation
theory,
+L+
linear
programming.
+L+
AMS
subject
classifications.
60K35,
62N05,
68M10,
68Q22,
68R05,
90A28,
90C05.
+L+
1.
Introduction.

URL:
http://www.cam.sri.com/tr/crc033/paper.ps.Z
Eurospeech,
Berlin,
1993
+L+
A
SPEECH-BASED
ROUTE
ENQUIRY
SYSTEM
BUILT
FROM
+L+
GENERAL-PURPOSE
COMPONENTS
1
+L+
Ian
Lewin
,
Martin
Russell
,
David
Carter
,
Sue
Browning
,
Keith
Ponting
and
Stephen
Pulman
+L+
SRI
International,
23
Millers
Yard,
Cambridge,
CB2
1RQ,
UK
+L+
Speech
Research
Unit,
DRA
Malvern,
St
Andrews
Road,
Malvern,
Worcs,
WR14
3PS,
UK
+L+
ABSTRACT
+L+
The
adaptation
of
existing
general-purpose
speech
recognition
and
language
understanding
systems
can
greatly
+L+
reduce
the
cost
of
developing
applications.
However,
the
+L+
components
must
have
appropriate
characteristics
for
this
+L+
to
be
possible.
+L+
Work
is
in
progress
to
adapt
two
task-independent
+L+
components,
the
AURIX
speech
recognizer
and
the
CLARE
+L+
language
processor
to
create
a
system
allowing
spoken
+L+
queries
of
the
PC-based
Autoroute
route
planning
package.
+L+
Keywords:
adaptability,
general
purpose,
speech
recognition,
language
understanding,
AURIX,
CLARE
+L+
1.
INTRODUCTION

AAAI-98,
Madison,
WI,
to
appear
+L+
Needles
in
a
Haystack
:
Plan
Recognition
in
Large
Spatial
+L+
Domains
Involving
Multiple
Agents
+L+
Mark
Devaney
and
Ashwin
Ram
+L+
College
of
Computing
+L+
Georgia
Institute
of
Technology
+L+
Atlanta,
GA
30332-0280
+L+
markd@cc.gatech.edu
+L+
ashwin@cc.gatech.edu
+L+
Abstract
+L+
While
plan
recognition
research
has
been
applied
to
a
+L+
wide
variety
of
problems,
it
has
largely
made
identical
assumptions
about
the
number
of
agents
participating
in
the
plan,
the
observability
of
the
plan
execution
process,
and
the
scale
of
the
domain.
We
describe
a
method
for
plan
recognition
in
a
real-world
+L+
domain
involving
large
numbers
of
agents
performing
+L+
spatial
maneuvers
in
concert
under
conditions
of
limited
observability.
These
assumptions
differ
radically
+L+
from
those
traditionally
made
in
plan
recognition
and
+L+
produce
a
problem
which
combines
aspects
of
the
fields
+L+
of
plan
recognition,
pattern
recognition,
and
object
+L+
tracking.
We
describe
our
initial
solution
which
borrows
and
builds
upon
research
from
each
of
these
areas,
+L+
employing
a
pattern-directed
approach
to
recognize
individual
movements
and
generalizing
these
to
produce
+L+
inferences
of
large-scale
behavior.
+L+
Introduction

The
State
of
the
Art
in
Ontology
Design:
+L+
A
Survey
and
Comparative
Review
+L+
Natalya
Fridman
Noy
+L+
Carole
D.
Hafner
+L+
College
of
Computer
Science
+L+
Northeastern
University
+L+
Boston,
MA
02115
+L+
-natasha,
hafner-@ccs.neu.edu
+L+
Abstract
+L+
In
this
paper
we
develop
a
framework
for
comparing
+L+
ontologies,
and
place
a
number
of
the
more
+L+
prominent
ontologies
into
it.
We
have
selected
10
+L+
specific
projects
for
this
study,
including
general
+L+
ontologies,
domain
specific
ones,
and
one
knowledge
+L+
representation
system.
The
comparison
framework
+L+
includes
general
characteristics
such
as
the
purpose
of
+L+
an
ontology,
its
coverage
(general
or
domain-specific),
its
size,
and
the
formalism
used.
It
also
+L+
includes
the
design
process
used
in
creating
an
+L+
ontology
and
the
methods
used
to
evaluate
it.
+L+
Characteristics
that
describe
the
content
of
an
+L+
ontology
include
taxonomic
organization,
types
of
+L+
concepts
covered,
top-level
divisions,
internal
+L+
structure
of
concepts,
representation
of
part-whole
+L+
relations,
and
the
presence
and
nature
of
additional
+L+
axioms.
Finally
we
consider
what
experiments
or
+L+
applications
have
used
the
ontologies.
Knowledge
+L+
sharing
and
reuse
will
require
a
common
framework
+L+
to
support
interoperability
of
independently
created
+L+
ontologies.
Our
study
shows
there
is
great
diversity
+L+
in
the
way
ontologies
are
designed
and
the
way
they
+L+
represent
the
world.
By
identifying
the
similarities
+L+
and
differences
among
existing
ontologies,
we
clarify
+L+
the
range
of
alternatives
in
creating
a
standard
+L+
framework
for
ontology
design.
+L+
1
Introduction

RESONANCE
AND
THE
PERCEPTION
OF
+L+
MUSICAL
METER
+L+
Edward
W.
Large
+L+
John
F.
Kolen
+L+
The
Ohio
State
University
+L+
Abstract
+L+
Many
connectionist
approaches
to
musical
expectancy
and
music
composition
let
the
+L+
question
of
What
next?
overshadow
the
equally
important
question
of
When
next?.
One
cannot
+L+
escape
the
latter
question,
one
of
temporal
structure,
when
considering
the
perception
of
musical
+L+
meter.
We
view
the
perception
of
metrical
structure
as
a
dynamic
process
where
the
temporal
+L+
organization
of
external
musical
events
synchronizes,
or
entrains,
a
listeners
internal
processing
+L+
mechanisms.
This
article
introduces
a
novel
connectionist
unit,
based
upon
a
mathematical
model
+L+
of
entrainment,
capable
of
phase
and
frequency-locking
to
periodic
components
of
incoming
+L+
rhythmic
patterns.
Networks
of
these
units
can
self-organize
temporally
structured
responses
to
+L+
rhythmic
patterns.
The
resulting
network
behavior
embodies
the
perception
of
metrical
structure.
+L+
The
article
concludes
with
a
discussion
of
the
implications
of
our
approach
for
theories
of
metrical
+L+
structure
and
musical
expectancy.
+L+
Connection
Science,
6
(1),
177
-
208.
+L+
+PAGE+

Refining
Interactions
in
a
Distributed
System
+L+
Neelam
Soundarajan
+L+
Computer
and
Information
Science
+L+
The
Ohio
State
University
+L+
2015
Neil
Avenue
+L+
Columbus,
OH
43210
+L+
USA
+L+
e-mail:
neelam@cis.ohio-state.edu
+L+
+PAGE+

AUCTION-DRIVEN
COORDINATION
FOR
+L+
PLANTWIDE
OPTIMIZATION
+L+
Rinaldo
A.
Jose
and
Lyle
H.
Ungar
+L+
University
of
Pennsylvania
+L+
Philadelphia,
PA
19104
+L+
Abstract
+L+
Model
predictive
control
strategies
generally
focus
on
controlling
plant
outputs
to
setpoints;
in
+L+
industry,
however,
a
more
desirable
goal
is
maximizing
a
plants
profitability.
In
principle,
this
can
be
+L+
done
by
creating
a
plant
model
and
maximizing
profit
with
respect
to
the
market
prices
of
the
plants
+L+
inputs
and
outputs,
but
in
practice,
such
centralized
approaches
often
cannot
effectively
be
applied
at
+L+
the
operations
time
scale
due
to
the
size
and
complexity
of
the
problem.
One
solution
is
to
use
+L+
decentralized
optimization
at
the
unit
operations
level
by
tearing
process
streams
and
coordinating
the
+L+
resulting
pieces.
Such
optimization,
however,
requires
that
unit
inputs
and
outputs
be
priced.
We
show
+L+
that
a
traditional
Lagrangean-based
approach
to
this
pricing
fails
for
simple
systems.
Instead,
we
define
+L+
slack
resources
over
the
torn
process
streams
and
price
them
using
auctions.
Unlike
Lagrange
+L+
multipliers,
slack
resource
prices
contain
useful
information
and
can
be
used
to
make
decisions
+L+
regarding
capital
improvements,
thus
providing
a
strong
tie
between
the
operations
and
management
+L+
layers
in
chemical
plants.
+L+
Keywords
+L+
auctions,
distributed
optimization,
resource
prices,
process
decomposition,
optimal
coordination,
+L+
penalty-based
methods,
Lagrangean
decomposition
+L+
Introduction

To
appear
in
Proceedings
of
Virtual
Reality
Vienna
'93
+L+
Constructing
Cyberspace:
+L+
Virtual
Reality
and
Hypermedia
+L+
Keith
Andrews
+L+
Institute
for
Information
Processing
and
Computer
Supported
New
Media
(IICM)
+L+
Graz
University
of
Technology,
+L+
A-8010
Graz,
Austria.
+L+
Abstract
+L+
Large-scale,
distributed
hypermedia
information
systems
allow
fast,
structured
access
to
very
large,
dynamic
information
bases.
The
highly
perceptual
nature
of
a
virtual
reality
interface
has
the
power
to
take
users
both
inside
information
and
inside
its
+L+
structure.
Combining
the
two
takes
us
a
step
towards
cyberspace,
William
Gibson's
+L+
vision
of
a
virtual
model
of
all
the
world's
interconnected
data.
This
paper
reviews
+L+
current
work
on
the
boundary
of
virtual
reality
and
hypermedia.
+L+
1
Introduction

Type
classes
in
Haskell
+L+
Cordelia
Hall
,
Kevin
Hammond
,
Simon
Peyton
Jones
+L+
and
Philip
Wadler
+L+
Glasgow
University
+L+
Abstract
+L+
This
paper
defines
a
set
of
type
inference
rules
for
resolving
overloading
introduced
by
type
classes.
Programs
including
type
classes
+L+
are
transformed
into
ones
which
may
be
typed
by
the
Hindley-Milner
inference
rules.
In
contrast
to
other
work
on
type
classes,
the
+L+
rules
presented
here
relate
directly
to
user
programs.
An
innovative
+L+
aspect
of
this
work
is
the
use
of
second-order
lambda
calculus
to
+L+
record
type
information
in
the
program.
+L+
1.
Introduction

Crooked
Functions,
Bent
Functions,
and
Distance
+L+
Regular
Graphs
+L+
T.D.
Bending
D.
Fon-Der-Flaass
+L+
School
of
Mathematical
Sciences,
+L+
Queen
Mary
and
Westfield
College,
London
E1
4NS,
U.K.
+L+
T.Bending@mdx.ac.uk
d.g.flaass@writeme.com
+L+
Submitted:
March
25,
1998;
Accepted:
June
30,
1998.
+L+
1991
Mathematical
Subject
Classification:
05E30,
05B20
+L+
Abstract
+L+
Let
V
and
W
be
n-dimensional
vector
spaces
over
GF
(2).
A
mapping
+L+
Q
:
V
!
W
is
called
crooked
if
it
satisfies
the
following
three
properties:
+L+
Q(0)
=
0;
+L+
Q(x)
+
Q()
+
Q()
+
Q(x
+
+
)
6=
0
for
any
three
distinct
x;
;
;
+L+
Q(x)
+
Q()
+
Q()
+
Q(x
+
a)
+
Q(
+
a)
+
Q(
+
a)
6=
0
if
a
6=
0
(x;
;
+L+
arbitrary).
+L+
We
show
that
every
crooked
function
gives
rise
to
a
distance
regular
graph
+L+
of
diameter
3
having
=
0
and
=
2
which
is
a
cover
of
the
complete
+L+
graph.
Our
approach
is
a
generalization
of
a
recent
construction
found
by
+L+
de
Caen,
Mathon,
and
Moorhouse.
We
study
graph-theoretical
properties
of
+L+
the
resulting
graphs,
including
their
automorphisms.
Also
we
demonstrate
a
+L+
connection
between
crooked
functions
and
bent
functions.
+L+
1
Crooked
functions
and
bent
functions

On
the
Maximum
Tolerable
Noise
for
+L+
Reliable
Computation
by
Formulas
+L+
William
Evans*
+L+
will@cs.arizona.edu
+L+
Department
of
Computer
Science
+L+
The
University
of
Arizona
+L+
Tucson,
AZ
85721-0077,
USA
+L+
Nicholas
Pippenger**
+L+
nicholas@cs.ubc.ca
+L+
Department
of
Computer
Science
+L+
The
University
of
British
Columbia
+L+
Vancouver,
BC
V6T
1Z4,
Canada
+L+
Abstract:
It
is
shown
that
if
a
formula
is
constructed
from
noisy
2-input
NAND
gates,
+L+
with
each
gate
failing
independently
with
probability
",
then
reliable
computation
can
or
+L+
cannot
take
place
according
as
"
is
less
than
or
greater
than
"
0
=
(3
+L+
p
+L+
*
This
research
was
supported
by
an
NSERC
Canada
International
Fellowship.
+L+
**
This
research
was
supported
by
an
NSERC
Research
Grant.
+L+
+PAGE+

Virtual
Radios
+L+
Vanu
Bose
,
Mike
Ismert
,
Matt
Welborn
,
John
Guttag
+L+
Software
Devices
and
Systems
Group
+L+
Laboratory
for
Computer
Science
+L+
Massachusetts
Institute
of
Technology
+L+
Abstract
+L+
Conventional
software
radios
take
advantage
of
vastly
improved
A/D
converters
and
DSP
hardware.
Our
+L+
approach,
which
we
refer
to
as
virtual
radios,
also
depends
upon
high
performance
A/D
converters.
However,
+L+
rather
than
use
DSPs,
we
have
chosen
to
ride
the
curve
of
rapidly
improving
workstation
hardware.
We
use
+L+
wideband
digitization
and
then
perform
all
of
the
digital
signal
processing
in
user
space
on
a
general
purpose
+L+
workstation.
This
approach
allows
us
to
experiment
with
new
approaches
to
signal
processing
that
exploit
the
+L+
hardware
and
software
resources
of
the
workstation.
Furthermore,
it
allows
us
to
experiment
with
different
+L+
ways
of
structuring
systems
in
which
the
radio
component
of
communication
devices
are
integrated
with
+L+
higher-level
applications.
+L+
This
paper
describes
the
design
and
performance
of
an
environment
we
have
constructed
that
facilitates
building
virtual
radios
and
of
two
applications
built
using
that
environment.
The
environment
consists
of
an
I/O
+L+
subsystem
that
provides
high
bandwidth
low
latency
user-level
access
to
digitized
signals
and
a
programming
+L+
environment
that
provides
an
infrastructure
for
building
applications.
The
applications,
which
exemplify
+L+
some
of
the
benefits
of
virtual
radios,
are
a
software
cellular
receiver
and
a
novel
wireless
network
interface.
+L+
1
Introduction

Draft
16
Nov
94
-
1
-
To
appear
IEEE
MICRO
Feb
1995
+L+
Myrinet
-
A
Gigabit-per-Second
Local-Area
Network
+L+
(Based
on
a
keynote
talk
presented
by
Charles
L.
Seitz)
+L+
Nanette
J.
Boden
,
Danny
Cohen
,
Robert
E.
Felderman,
+L+
Alan
E.
Kulawik
,
Charles
L.
Seitz
,
Jakov
N.
Seizovic
,
and
Wen-King
Su
+L+
Myricom,
Inc.
+L+
325
N.
Santa
Anita
Ave.
+L+
Arcadia,
CA
91006
+L+
(http://www.myri.com)
+L+
Abstract.
Myrinet
is
a
new
type
of
local-area
network
(LAN)
based
on
the
+L+
technology
used
for
packet
communication
and
switching
within
"massively-parallel
processors"
(MPPs).
Think
of
Myrinet
as
an
MPP
message-passing
+L+
network
that
can
span
campus
dimensions,
rather
than
as
a
wide-area
+L+
telecommunications
network
that
is
operating
in
close
quarters.
The
+L+
technical
steps
toward
making
Myrinet
a
reality
included
the
development
+L+
of
(1)
robust,
25m
communication
channels
with
flow
control,
packet
+L+
framing,
and
error
control;
(2)
self-initializing,
low-latency,
cut-through
+L+
switches;
(3)
host
interfaces
that
can
map
the
network,
select
routes,
and
+L+
translate
from
network
addresses
to
routes,
as
well
as
handle
packet
traffic;
+L+
and
(4)
streamlined
host
software
that
allows
direct
communication
+L+
between
user
processes
and
the
network.
+L+
+PAGE+

Virtual
Memory
Architecture
in
SunOS
+L+
Robert
A.
Gingell
+L+
Joseph
P.
Moran
+L+
William
A.
Shannon
+L+
Sun
Microsystems,
Inc.
+L+
2550
Garcia
Ave.
+L+
Mountain
View,
CA
94043
+L+
ABSTRACT
+L+
A
new
virtual
memory
architecture
for
the
Sun
implementation
of
the
UNIX
+L+
operating
system
is
described.
Our
goals
included
unifying
and
simplifying
the
concepts
+L+
the
system
used
to
manage
memory,
as
well
as
providing
an
implementation
that
fit
well
+L+
with
the
rest
of
the
system.
We
discuss
an
architecture
suitable
for
environments
that
+L+
(potentially)
consist
of
systems
of
heterogeneous
hardware
and
software
architectures.
+L+
The
result
is
a
page-based
system
in
which
the
fundamental
notion
is
that
of
mapping
+L+
process
addresses
to
files.
+L+
1.
Introduction
and
Motivation

A
Survey
of
Collective
Communication
in
+L+
Wormhole-Routed
Massively
Parallel
Computers
+L+
Philip
K.
McKinley
,
Yih-jia
Tsai
,
and
David
F.
Robinson
+L+
Technical
Report
+L+
MSU-CPS-94-35
+L+
June
1994
+L+
Submitted
for
publication,
June
1994.
+L+
+PAGE+

Feature
Correspondence
by
Interleaving
Shape
and
Texture
+L+
Computations
+L+
David
Beymer
+L+
Artificial
Intelligence
Laboratory,
and
+L+
Center
for
Biological
and
Computational
Learning
+L+
Massachusetts
Institute
of
Technology
+L+
Cambridge,
MA
02139,
USA
+L+
email:
beymer@ai.mit.edu
+L+
Abstract
+L+
The
correspondence
problem
in
computer
vision
is
basically
a
matching
task
between
two
or
more
sets
of
features.
In
this
paper,
we
introduce
a
vectorized
image
+L+
representation,
which
is
a
feature-based
representation
+L+
where
correspondence
has
been
established
with
respect
+L+
to
a
reference
image.
The
representation
consists
of
two
+L+
image
measurements
made
at
the
feature
points:
shape
+L+
and
texture.
Feature
geometry,
or
shape,
is
represented
+L+
using
the
(x;
)
locations
of
features
relative
to
the
some
+L+
standard
reference
shape.
Image
grey
levels,
or
texture,
+L+
are
represented
by
mapping
image
grey
levels
onto
the
+L+
standard
reference
shape.
Computing
this
representation
+L+
is
essentially
a
correspondence
task,
and
in
this
paper
+L+
we
explore
an
automatic
technique
for
"vectorizing"
face
+L+
images.
Our
face
vectorizer
alternates
back
and
forth
+L+
between
computation
steps
for
shape
and
texture,
and
a
+L+
key
idea
is
to
structure
the
two
computations
so
that
each
+L+
one
uses
the
output
of
the
other.
In
addition
to
describing
the
vectorizer,
an
application
to
the
problem
of
facial
+L+
feature
detection
will
be
presented.
+L+
1
Introduction

Joseph
D.
Darcy
+L+
CS
270
Project
Report,
spring
1998
+L+
Narrowing
Interval
Bounds
+L+
1.
Abstract
+L+
Interval
arithmetic
is
an
automated
attempt
to
give
guaranteed
upper
and
lower
bounds
of
a
numerical
+L+
computation
in
the
face
of
uncertainly
in
the
input
data
and
floating
point
roundoff
during
the
calculation.
+L+
While
a
simple
interval
equivalent
of
a
rational
function
can
be
readily
synthesized,
the
bounds
from
this
+L+
construction
may
be
too
pessimistically
large
to
be
useful.
This
paper
surveys
a
variety
of
techniques
for
+L+
refining
the
interval
bounds.
An
appendix
identifies
issues
with
realizing
floating
point
based
interval
+L+
arithmetic
on
current
IEEE
754
compliant
processors.
+L+
2.
Introduction

A
New
O(n
+L+
2
)
Algorithm
for
the
Symmetric
Tridiagonal
+L+
Eigenvalue/Eigenvector
Problem
+L+
by
+L+
Inderjit
Singh
Dhillon
+L+
B.Tech.
(Indian
Institute
of
Technology,
Bombay)
1989
+L+
A
dissertation
submitted
in
partial
satisfaction
of
the
+L+
requirements
for
the
degree
of
+L+
Doctor
of
Philosophy
+L+
in
+L+
Computer
Science
+L+
in
the
+L+
GRADUATE
DIVISION
+L+
of
the
+L+
UNIVERSITY
of
CALIFORNIA,
BERKELEY
+L+
Committee
in
charge:
+L+
Professor
James
W.
Demmel,
Chair
+L+
Professor
Beresford
N.
Parlett
+L+
Professor
Phil
Colella
+L+
1997
+L+
+PAGE+

Using
Skeletons
for
Nonholonomic
Path
Planning
among
Obstacles
+L+
Brian
Mirtich
+L+
John
Canny
+L+
Computer
Science
Division
+L+
University
of
California
+L+
Berkeley,
CA
94720
+L+
Abstract
+L+
This
paper
describes
a
practical
path
planner
for
+L+
nonholonomic
robots
in
environments
with
obstacles.
+L+
The
planner
is
based
on
building
a
one-dimensional,
+L+
maximal
clearance
skeleton
through
the
configuration
+L+
space
of
the
robot.
However
rather
than
using
the
Eu-clidean
metric
to
determine
clearance,
a
special
metric
+L+
which
captures
information
about
the
nonholonomy
of
+L+
the
robot
is
used.
The
robot
navigates
from
start
to
+L+
goal
states
by
loosely
following
the
skeleton;
the
resulting
paths
taken
by
the
robot
are
of
low
"complexity."
+L+
We
describe
how
much
of
the
computation
can
be
done
+L+
off-line
once
and
for
all
for
a
given
robot,
making
for
+L+
an
efficient
planner.
The
focus
is
on
path
planning
+L+
for
mobile
robots,
particularly
the
planar
two-axle
car,
+L+
but
the
underlying
ideas
are
quite
general
and
may
be
+L+
applied
to
planners
for
other
nonholonomic
robots.
+L+
1
Introduction

Belief
Revision:
A
Critique
+L+
Nir
Friedman
+L+
Computer
Science
Department
+L+
Stanford
University
+L+
Gates
Building
1A
+L+
Stanford,
CA
94305-9010
+L+
nir@cs.stanford.edu
+L+
Joseph
Y.
Halpern
+L+
IBM
Research
Division
+L+
Almaden
Research
Center,
Dept.
K53-B2
+L+
650
Harry
Road
+L+
San
Jose,
CA
95120-6099
+L+
halpern@almaden.ibm.com
+L+
May
6,
1996
+L+
Abstract
+L+
The
problem
of
belief
changehow
an
agent
should
revise
her
beliefs
upon
learning
new
+L+
informationhas
been
an
active
area
of
research
in
both
philosophy
and
artificial
intelligence.
+L+
Many
approaches
to
belief
change
have
been
proposed
in
the
literature.
Our
goal
is
not
to
+L+
introduce
yet
another
approach,
but
to
examine
carefully
the
rationale
underlying
the
approaches
+L+
already
taken
in
the
literature,
and
to
highlight
what
we
view
as
methodological
problems
in
the
+L+
literature.
The
main
message
is
that
to
study
belief
change
carefully,
we
must
be
quite
explicit
+L+
about
the
ontology
or
scenario
underlying
the
belief
change
process.
This
is
something
that
+L+
has
been
missing
in
previous
work,
with
its
focus
on
postulates.
Our
analysis
shows
that
we
+L+
must
pay
particular
attention
to
two
issues
which
have
often
been
taken
for
granted:
The
first
+L+
is
how
we
model
the
agent's
epistemic
state.
(Do
we
use
a
set
of
beliefs,
or
a
richer
structure,
+L+
such
as
an
ordering
on
worlds?
And
if
we
use
a
set
of
beliefs,
in
what
language
are
these
+L+
beliefs
are
expressed?)
The
second
is
the
status
of
observations.
(Are
observations
known
to
+L+
be
true,
or
just
believed?
In
the
latter
case,
how
firm
is
the
belief?)
For
example,
we
argue
that
+L+
even
postulates
that
have
been
called
beyond
controversy
are
unreasonable
when
the
agent's
+L+
beliefs
include
beliefs
about
her
own
epistemic
state
as
well
as
the
external
world.
Issues
of
the
+L+
status
of
observations
arise
particularly
when
we
consider
iterated
belief
revision,
and
we
must
+L+
confront
the
possibility
of
revising
by
'
and
then
by
:'.
+L+
Keyword:
Belief
revision
+L+
+PAGE+

A
Fast
Algorithm
for
Incremental
Distance
Calculation
+L+
Ming
C.
Lin
and
John
F.
Canny
+L+
University
of
California,
Berkeley
+L+
Berkeley,
CA
94720
+L+
Abstract
+L+
A
simple
and
efficient
algorithm
for
finding
the
closest
points
between
two
convex
polyhedra
is
described
+L+
here.
Data
from
numerous
experiments
tested
on
a
+L+
broad
set
of
convex
polyhedra
on
&lt;
3
show
that
the
+L+
running
time
is
roughly
constant
for
finding
closest
+L+
points
when
nearest
points
are
approximately
known
+L+
and
is
linear
in
total
number
of
vertices
if
no
special
+L+
initialization
is
done.
This
algorithm
can
be
used
for
+L+
collision
detection,
computation
of
the
distance
between
two
polyhedra
in
three-dimensional
space,
and
+L+
other
robotics
problems.
It
forms
the
heart
of
the
+L+
motion
planning
algorithm
of
[1].
+L+
1
Introduction

High-Performance
Sorting
on
Networks
of
Workstations
+L+
Andrea
C.
Arpaci-Dusseau
+L+
Computer
Science
Division
+L+
University
of
California,
Berkeley
+L+
dusseau@cs.berkeley.edu
+L+
Remzi
H.
Arpaci-Dusseau
+L+
Computer
Science
Division
+L+
University
of
California,
Berkeley
+L+
remzi@cs.berkeley.edu
+L+
David
E.
Culler
+L+
Computer
Science
Division
+L+
University
of
California,
Berkeley
+L+
culler@cs.berkeley.edu
+L+
Joseph
M.
Hellerstein
+L+
Computer
Science
Division
+L+
University
of
California,
Berkeley
+L+
jmh@cs.berkeley.edu
+L+
David
A.
Patterson
+L+
Computer
Science
Division
+L+
University
of
California,
Berkeley
+L+
patterson@cs.berkeley.edu
+L+
Abstract
+L+
We
report
the
performance
of
NOW-Sort,
a
collection
of
sorting
implementations
on
a
Network
of
Workstations
(NOW).
+L+
We
find
that
parallel
sorting
on
a
NOW
is
competitive
to
sorting
on
the
large-scale
SMPs
that
have
traditionally
held
the
+L+
performance
records.
On
a
64-node
cluster,
we
sort
6.0
GB
+L+
in
just
under
one
minute,
while
a
32-node
cluster
finishes
the
+L+
Datamation
benchmark
in
2.41
seconds.
+L+
Our
implementations
can
be
applied
to
a
variety
of
disk,
+L+
memory,
and
processor
configurations;
we
highlight
salient
+L+
issues
for
tuning
each
component
of
the
system.
We
evaluate
the
use
of
commodity
operating
systems
and
hardware
for
+L+
parallel
sorting.
We
find
existing
OS
primitives
for
memory
+L+
management
and
file
access
adequate.
Due
to
aggregate
communication
and
disk
bandwidth
requirements,
the
bottleneck
+L+
of
our
system
is
the
workstation
I/O
bus.
+L+
1
Introduction

The
Interaction
of
Parallel
and
Sequential
Workloads
on
a
+L+
Network
of
Workstations
+L+
Remzi
H.
Arpaci
,
Andrea
C.
Dusseau
,
Amin
M.
Vahdat,
+L+
Lok
T.
Liu
,
Thomas
E.
Anderson
,
and
David
A.
Patterson
+L+
Computer
Science
Division
+L+
University
of
California,
Berkeley
+L+
Berkeley,
CA
94720
+L+
Abstract
+L+
This
paper
examines
the
plausibility
of
using
a
network
of
+L+
workstations
(NOW)
for
a
mixture
of
parallel
and
sequential
+L+
jobs.
Through
simulations,
our
study
examines
issues
that
arise
+L+
when
combining
these
two
workloads
on
a
single
platform.
Starting
from
a
dedicated
NOW
just
for
parallel
programs,
we
incrementally
relax
uniprogramming
restrictions
until
we
have
a
+L+
multi-programmed,
multi-user
NOW
for
both
interactive
sequential
users
and
parallel
programs.
We
show
that
a
number
of
issues
+L+
associated
with
the
distributed
NOW
environment
(e.g.,
daemon
+L+
activity,
coscheduling
skew)
can
have
a
small
but
noticeable
effect
on
parallel
program
performance.
We
also
find
that
efficient
+L+
migration
to
idle
workstations
is
necessary
to
maintain
acceptable
parallel
application
performance.
Furthermore,
we
present
a
+L+
methodology
for
deriving
an
optimal
delay
time
for
recruiting
idle
+L+
machines
for
use
by
parallel
programs;
this
recruitment
threshold
+L+
was
just
3
minutes
for
the
research
cluster
we
measured.
Finally,
+L+
we
quantify
the
effects
of
the
additional
parallel
load
upon
interactive
users
by
keeping
track
of
the
potential
number
of
user
+L+
delays
in
our
simulations.
When
we
limit
the
maximum
number
+L+
of
delays
per
user,
we
can
still
maintain
acceptable
parallel
program
performance.
In
summary,
we
find
that
for
our
workloads
a
+L+
2:1
rule
applies:
a
NOW
cluster
of
approximately
60
machines
can
+L+
sustain
a
32-node
parallel
workload
in
addition
to
the
sequential
+L+
load
placed
upon
it
by
interactive
users.
+L+
1
Introduction

"Go
With
the
Winners"
Algorithms
+L+
David
Aldous
+L+
Department
of
Statistics
+L+
University
of
California
+L+
Berkeley
CA
94720
+L+
Umesh
Vazirani
+L+
Department
of
Computer
Science
+L+
University
of
California
+L+
Berkeley
CA
94720
+L+
1
Introduction

Stochastic
Interaction
and
Linear
Logic
+L+
Patrick
D.
Lincoln
John
C.
Mitchell
Andre
Scedrov
x
+L+
Abstract
+L+
We
present
stochastic
interactive
semantics
for
propositional
linear
+L+
logic
without
modalities.
The
framework
is
based
on
interactive
+L+
protocols
considered
in
computational
complexity
theory,
in
which
+L+
a
prover
with
unlimited
power
interacts
with
a
verifier
that
can
+L+
only
toss
fair
coins
or
perform
simple
tasks
when
presented
with
+L+
the
given
formula
or
with
subsequent
messages
from
the
prover.
+L+
The
additive
conjunction
&
is
described
as
random
choice,
which
+L+
reflects
the
intuitive
idea
that
the
verifier
can
perform
only
"random
spot
checks".
This
stochastic
interactive
semantic
framework
+L+
is
shown
to
be
sound
and
complete.
Furthermore,
the
prover's
+L+
winning
strategies
are
basically
proofs
of
the
given
formula.
In
+L+
this
framework
the
multiplicative
and
additive
connectives
of
linear
logic
are
described
by
means
of
probabilistic
operators,
giving
a
+L+
new
basis
for
intuitive
reasoning
about
linear
logic
and
a
potential
+L+
new
tool
in
automated
deduction.
+L+
A
revised
version
appears
in
:
"Advances
in
Linear
Logic",
ed.
by
J.-Y.
Girard
et
+L+
al.,
London
Mathematical
Society
Lecture
Notes
Series,
Volume
222,
Cambridge
University
+L+
Press,
1995,
pp.
147-166.
+L+
lincoln@csl.sri.com
SRI
International
Computer
Science
Laboratory,
Menlo
Park
+L+
CA
94025
USA.
Work
supported
under
NSF
Grant
CCR-9224858.
+L+
jcm@cs.stanford.edu
http://theory.stanford.edu/people/jcm/home.html
Department
of
Computer
Science,
Stanford
University,
Stanford,
CA
94305.
Supported
in
part
+L+
by
an
NSF
PYI
Award,
matching
funds
from
Digital
Equipment
Corporation,
the
Pow-ell
Foundation,
and
Xerox
Corporation;
and
the
Wallace
F.
and
Lucille
M.
Davis
Faculty
+L+
Scholarship.
+L+
andre@cis.upenn.edu
http://www.cis.upenn.edu/~andre
Department
of
Mathematics,
University
of
Pennsylvania,
Philadelphia,
PA
19104-6395.
Partially
supported
by
+L+
NSF
Grants
CCR-91-02753
and
CCR-94-00907
and
by
ONR
Grant
N00014-92-J-1916.
Sce-drov
is
an
American
Mathematical
Society
Centennial
Research
Fellow.
+L+
+PAGE+

Form(ers)
Over
Function(s):
The
KOLA
Reference
Manual
+L+
Mitch
Cherniack
+L+
June
18,
1996
+L+
0.1
Still
To
Do
+L+
*
6
more
fold
proofs,
precondition
proofs
+L+
*
Adjust
primitives
for
Int,
Str
and
Char
to
be
synchronized
with
the
Theta
operators
for
these
types
+L+
*
Add
floats
+L+
*
Add
section
describing
preconditions
+L+
*
Add
section
on
semantic
optimizations,
nested
query
optimization
+L+
*
Fix
awk
script
to
generate
Latex
version
of
Larch
scripts
without
typeface
glitches
+L+
1
Introduction

To
appear
in
Proc.
11th
Intl.
Conf.
on
Data
Engg.,
1995
+L+
The
AQUA
Approach
to
Querying
Lists
and
+L+
Trees
in
Object-Oriented
Databases
+L+
Bharathi
Subramanian
Theodore
W.
Leung
+L+
Brown
University
Brown
University
+L+
Scott
L.
Vandenberg
Stanley
B.
Zdonik
+L+
Siena
College
Brown
University
+L+
Abstract
+L+
Relational
database
systems
and
most
object-oriented
database
systems
provide
support
for
queries.
+L+
Usually
these
queries
represent
retrievals
over
sets
or
+L+
multisets.
Many
new
applications
for
databases,
such
+L+
as
multimedia
systems
and
digital
libraries,
need
support
for
queries
on
complex
bulk
types
such
as
lists
+L+
and
trees.
In
this
paper
we
describe
an
object-oriented
+L+
query
algebra
for
lists
and
trees.
The
operators
in
the
+L+
algebra
preserve
the
ordering
between
the
elements
of
a
+L+
list
or
tree,
even
when
the
result
list
or
tree
contains
an
+L+
arbitrary
set
of
nodes
from
the
original
tree.
We
also
+L+
present
predicate
languages
for
lists
and
trees
which
+L+
allow
order-sensitive
queries
because
they
use
pattern
+L+
matching
to
examine
groups
of
list
or
tree
nodes
rather
+L+
than
individual
nodes.
The
ability
to
decompose
predicate
patterns
enables
optimizations
that
make
use
of
+L+
indices.
+L+
1
Introduction

Computational
Intelligence,
Volume
12,
Number
3,
1996
+L+
LOCALIZED
TEMPORAL
REASONING
USING
SUBGOALS
+L+
AND
ABSTRACT
EVENTS
+L+
Shieu-Hong
Lin
,Thomas
Dean
1
+L+
Department
of
Computer
Science,
Brown
University,
Providence,
RI
02912
+L+
We
are
concerned
with
temporal
reasoning
problems
where
there
is
uncertainty
about
the
order
+L+
in
which
events
occur.
The
task
of
temporal
reasoning
is
to
derive
an
event
sequence
consistent
with
+L+
a
given
set
of
ordering
constraints
to
achieve
a
goal.
Previous
research
shows
that
the
associated
+L+
decision
problems
are
hard
even
for
very
restricted
cases.
In
this
paper,
we
investigate
locality
in
+L+
event
ordering
and
causal
dependencies.
We
present
a
localized
temporal
reasoning
algorithm
that
+L+
uses
subgoals
and
abstract
events
to
exploit
locality.
The
computational
efficiency
of
our
algorithm
+L+
for
a
problem
instance
is
quantified
by
the
inherent
locality
in
the
instance.
We
theoretically
+L+
demonstrate
the
substantial
improvement
in
performance
gained
by
exploiting
locality.
This
work
+L+
provides
a
solid
evidence
of
the
usefulness
of
localized
reasoning
in
exploiting
locality.
+L+
Key
words:
Dynamical
Systems,
Temporal
Reasoning,
Planning,
Locality,
Localized
Reasoning,
Subgoals,
Abstract
Events
+L+
1.
INTRODUCTION

How
good
are
genetic
algorithms
at
finding
+L+
large
cliques:
an
experimental
study
+L+
Bob
Carter
+L+
carter@cs.bu.edu
+L+
Kihong
Park
+L+
park@cs.bu.edu
+L+
BU-CS-93-015
+L+
October
10,
1993
+L+
Boston
University
+L+
Computer
Science
Department
+L+
Boston,
MA
02215
+L+
Phone:
617
353-8919
+L+
Fax:
617
353-6457
+L+
Abstract
+L+
This
paper
investigates
the
power
of
genetic
algorithms
at
solving
the
MAX-CLIQUE
problem.
We
measure
the
performance
of
a
standard
genetic
algorithm
on
an
elementary
set
of
+L+
problem
instances
consisting
of
embedded
cliques
in
random
graphs.
We
indicate
the
need
+L+
for
improvement,
and
introduce
a
new
genetic
algorithm,
the
multi-phase
annealed
GA,
which
+L+
exhibits
superior
performance
on
the
same
problem
set.
+L+
As
we
scale
up
the
problem
size
and
test
on
"hard"
benchmark
instances,
we
notice
a
+L+
degraded
performance
in
the
algorithm
caused
by
premature
convergence
to
local
minima.
To
+L+
alleviate
this
problem,
a
sequence
of
modifications
are
implemented
ranging
from
changes
in
+L+
input
representation
to
systematic
local
search.
The
most
recent
version,
called
union
GA,
+L+
incorporates
the
features
of
union
cross-over,
greedy
replacement,
and
diversity
enhancement.
+L+
It
shows
a
marked
speed-up
in
the
number
of
iterations
required
to
find
a
given
solution,
as
well
+L+
as
some
improvement
in
the
clique
size
found.
+L+
We
discuss
issues
related
to
the
SIMD
implementation
of
the
genetic
algorithms
on
a
Thinking
Machines
CM-5,
which
was
necessitated
by
the
intrinsically
high
time
complexity
(O(n
3
))
+L+
of
the
serial
algorithm
for
computing
one
iteration.
+L+
Our
preliminary
conclusions
are:
(1)
a
genetic
algorithm
needs
to
be
heavily
customized
to
+L+
work
"well"
for
the
clique
problem;
(2)
a
GA
is
computationally
very
expensive,
and
its
use
is
+L+
only
recommended
if
it
is
known
to
find
larger
cliques
than
other
algorithms;
(3)
although
our
+L+
customization
effort
is
bringing
forth
continued
improvements,
there
is
no
clear
evidence,
at
this
+L+
time,
that
a
GA
will
have
better
success
in
circumventing
local
minima.
+L+
Part
of
this
research
was
presented
at
the
2nd
DIMACS
Implementation
Challenge
on
Combinatorial
Optimiza
+L+
tion,
DIMACS,
October,
1993.
+L+
Supported
in
part
by
NSF
grant
CCR-9204284
+L+
+PAGE+

In
Proceedings
of
ICC'97:
The
IEEE
International
Conference
onCommunications,
Montreal,
Canada,
June
1997.
+L+
Implementation
and
Performance
Evaluation
of
TCP
Boston
+L+
A
Fragmentation-tolerant
TCP
Protocol
for
ATM
Networks
+L+
Azer
Bestavros
+L+
best@cs.bu.edu
+L+
Gitae
Kim
+L+
kgtjan@cs.bu.edu
+L+
Computer
Science
Department
+L+
Boston
University
+L+
Boston,
MA
02215
+L+
ABSTRACT:
+L+
In
this
paper,
we
overview
the
implementation
of
TCP
Boston
+L+
a
novel
fragmentation-tolerant
transport
protocol,
especially
+L+
suited
for
ATM's
53-byte
cell-oriented
switching
architecture.
+L+
TCP
Boston
integrates
a
standard
TCP/IP
protocol,
such
as
+L+
Reno
or
Vegas,
with
a
powerful
redundancy
control
mechanism
based
on
AIDAan
adaptive
version
of
Rabin's
IDA
dispersal
and
reconstruction
algorithms.
Our
results
show
that
+L+
TCP
Boston
improves
TCP/IP's
performance
over
ATMs
for
+L+
both
network-centric
metrics
(e.g.,
effective
throughput)
and
+L+
application-centric
metrics
(e.g.,
response
time).
+L+
1
Introduction

Generating
CAD-models
of
teeth
+L+
Peter
Johannes
Neugebauer
+L+
Fraunhofer-Institute
for
Computer
Graphics,
+L+
Wilhelminenstrasse
7,
64283
Darmstadt,
Germany,
+L+
email:
neugeb@igd.fhg.de
+L+
In
this
paper,
we
present
an
approach
for
the
reconstruction
of
teeth
model.
Therefore,
+L+
a
tooth
model
has
to
be
scanned
from
several
directions
with
a
3D-laser
scanner.
Several
+L+
views
are
necessary
because
of
shadows
and
occluded
areas
in
the
range
images.
Then,
+L+
all
acquired
range
views
are
combined
to
build
a
CAD-model
of
the
tooth.
The
idea
is
+L+
that
every
part
of
the
surface
should
be
visible
in
at
least
one
view.
The
reconstruction
+L+
process
is
divided
into
the
steps
registration,
volume
sculpturing
and
generation
of
an
+L+
accurate
polygonal
representation.
+L+
1.
Introduction

Journal
of
Artificial
Intelligence
Research
1
(1994)
159-208
Submitted
8/93;
published
2/94
+L+
Bias-Driven
Revision
of
Logical
Domain
Theories
+L+
Moshe
Koppel
KOPPEL@BIMACS.CS.BIU.AC.IL
+L+
Ronen
Feldman
FELDMAN@BIMACS.CS.BIU.AC.IL
+L+
Department
of
Mathematics
and
Computer
Science,
Bar-Ilan
University,
+L+
Ramat-Gan,
Israel
+L+
Alberto
Maria
Segre
SEGRE@CS.CORNELL.EDU
+L+
Department
of
Computer
Science,
Cornell
University,
+L+
Ithaca,
NY
14853,
USA
+L+
Abstract
+L+
The
theory
revision
problem
is
the
problem
of
how
best
to
go
about
revising
a
deficient
+L+
domain
theory
using
information
contained
in
examples
that
expose
inaccuracies.
In
this
paper
we
+L+
present
our
approach
to
the
theory
revision
problem
for
propositional
domain
theories.
The
+L+
approach
described
here,
called
PTR,
uses
probabilities
associated
with
domain
theory
elements
to
+L+
numerically
track
the
``ow''
of
proof
through
the
theory.
This
allows
us
to
measure
the
precise
+L+
role
of
a
clause
or
literal
in
allowing
or
preventing
a
(desired
or
undesired)
derivation
for
a
given
+L+
example.
This
information
is
used
to
efficiently
locate
and
repair
awed
elements
of
the
theory.
+L+
PTR
is
proved
to
converge
to
a
theory
which
correctly
classifies
all
examples,
and
shown
+L+
experimentally
to
be
fast
and
accurate
even
for
deep
theories.
+L+
1.
Introduction

A
Middleware
Service
for
Real-Time
Push-Pull
+L+
Communications
+L+
Kanaka
Juvva
and
Raj
Rajkumar
+L+
Real-Time
and
Multimedia
Laboratory
+L+
Carnegie
Mellon
University
+L+
Pittsburgh
PA
15213
+L+
fkjuvva,
raj+g@cs.cmu.edu
+L+
Abstract:
+L+
Current
and
emerging
real-time
and
multimedia
applications
like
multi-party
collaboration,
internet
telephony
and
+L+
distributed
command
control
systems
require
the
exchange
of
information
over
distributed
and
heterogeneous
nodes.
+L+
Multiple
data
types
including
voice,
video,
sensor
data,
real-time
intelligence
data
and
text
are
being
transported
+L+
widely
across
today's
information,
control
and
surveillance
networks.
All
such
applications
can
benefit
enormously
+L+
from
middleware,
operating
system
and
networking
services
that
can
support
QoS
guarantees,
high
availability,
+L+
dynamic
reconfigurability
and
scalability.
+L+
In
this
paper,
we
propose
a
middleware
layer
called
a
"Real-Time
Push-Pull
Communications
Service"
to
easily
and
quickly
disseminate
information
across
heterogeneous
nodes
with
an
underlying
architecture
to
satisfy
the
+L+
above-mentioned
requirements.
Push-Pull
Communications
is
an
extension
of
the
real-time
publisher/subscriber
+L+
model
[4],
and
represents
both
"push"
(data
transfer
initiated
by
a
sender)
and
"pull"
(data
transfer
initiated
by
a
+L+
receiver)
communications.
Nodes
with
widely
differing
processing
power
and
networking
bandwidth
can
coordinate
+L+
and
co-exist
by
the
provision
of
appropriate
and
automatic
support
for
transformation
on
data
and
supports
scaling.
+L+
Different
information
sources
and
sinks
can
operate
at
different
frequencies
and
also
can
choose
another
(intermedi-ate)
node
to
act
as
their
proxy
and
and
deliver
data
at
the
desired
frequency.
This
service
has
been
implemented
+L+
on
RT-Mach,
a
resource-centric
kernel
using
resource
kernel
primitives
[7].
This
paper
presents
an
overview
of
the
+L+
design,
implementation
and
preliminary
performance
evaluation
of
the
model.
+L+
Keywords:
Push
communications,
Pull
communications,
Proxy,
Scaling,
Middleware
service,
QoS
+L+
+PAGE+

Journal
of
Artificial
Intelligence
Research
3
(1995)
53-118
Submitted
3/95;
published
7/95
+L+
Building
and
Refining
Abstract
Planning
Cases
+L+
by
Change
of
Representation
Language
+L+
Ralph
Bergmann
bergmann@informatik.uni-kl.de
+L+
Wolfgang
Wilke
wilke@informatik.uni-kl.de
+L+
Centre
for
Learning
Systems
and
Applications
(LSA)
+L+
University
of
Kaiserslautern,
P.O.-Box
3049,
D-67653
Kaiserslautern,
Germany
+L+
Abstract
+L+
Abstraction
is
one
of
the
most
promising
approaches
to
improve
the
performance
of
problem
+L+
solvers.
In
several
domains
abstraction
by
dropping
sentences
of
a
domain
description
as
+L+
used
in
most
hierarchical
planners
has
proven
useful.
In
this
paper
we
present
examples
+L+
which
illustrate
significant
drawbacks
of
abstraction
by
dropping
sentences.
To
overcome
+L+
these
drawbacks,
we
propose
a
more
general
view
of
abstraction
involving
the
change
of
+L+
representation
language.
We
have
developed
a
new
abstraction
methodology
and
a
related
+L+
sound
and
complete
learning
algorithm
that
allows
the
complete
change
of
representation
+L+
language
of
planning
cases
from
concrete
to
abstract.
However,
to
achieve
a
powerful
+L+
change
of
the
representation
language,
the
abstract
language
itself
as
well
as
rules
which
+L+
describe
admissible
ways
of
abstracting
states
must
be
provided
in
the
domain
model.
+L+
This
new
abstraction
approach
is
the
core
of
Paris
(Plan
Abstraction
and
Refinement
+L+
in
an
Integrated
System),
a
system
in
which
abstract
planning
cases
are
automatically
+L+
learned
from
given
concrete
cases.
An
empirical
study
in
the
domain
of
process
planning
+L+
in
mechanical
engineering
shows
significant
advantages
of
the
proposed
reasoning
from
+L+
abstract
cases
over
classical
hierarchical
planning.
+L+
1.
Introduction

Final
Version,
94-Mar-17,1of
9
+L+
Automation
Tools
for
+L+
NonDestructive
Inspection
of
Aircraft:
+L+
Promise
of
Technology
Transfer
from
the
+L+
Civilian
to
the
Military
Sector
+L+
Chris
Seher
1
,
Mel
Siegel
2
,
and
William
M.
Kaufman
3
+L+
1
Federal
Aviation
Administration,
Technical
Center,
Atlantic
City
NJ
08201
+L+
2
Carnegie
Mellon
University,
Robotics
Institute,
Pittsburgh
PA
15213
+L+
3
Carnegie
Mellon
University,
CMRI,
Pittsburgh
PA
15213
+L+
Abstract
+L+
The
FAA
Aging
Aircraft
Research
Program
is
+L+
supporting
the
development
of
a
robotic
mobile
+L+
nondestructive
inspection
(NDI)
instrument
+L+
deployment
tool
at
Carnegie
Mellon
University
+L+
(CMU)
with
the
active
participation
of
USAir.
The
+L+
program
has
spawned
several
new
relationships
+L+
and
entities:
an
alliance
with
an
ARPA-funded
+L+
research
program
at
CMU
having
the
capability
to
+L+
add
3D-stereoscopic
enhanced
visual
inspection
+L+
capability,
a
start-up
company
organized
to
+L+
commercialize
the
combined
technologies,
and
+L+
State
of
Pennsylvania
funding
to
foster
this
+L+
commercialization.
As
a
result
of
these
activities
+L+
and
connections
the
civilian
sector
appears
to
be
+L+
ahead
of
the
military
sector
in
important
aspects
of
+L+
automation
for
deployment
of
aircraft
inspection
+L+
equipment.
A
partnership
between
the
university
+L+
researchers,
the
airline
operator,
the
start-up
+L+
company,
and
the
state
government
is
thus
+L+
emerging
as
the
likely
agent
for
transfer
of
the
+L+
civilian-developed
technology
to
the
military
sector.
+L+
1.
Introduction

The
Candide
System
for
Machine
Translation
+L+
Adam
L.
Berger
,
Peter
F.
Brown
,
Stephen
A.
Della
Pietra
,
Vincent
J.
Della
Pietra,
+L+
John
R.
Gillett
,
John
D.
Lafferty
,
Robert
L.
Mercer
,
Harry
Printz
,
Lubos
Ures
+L+
IBM
Thomas
J.
Watson
Research
Center
+L+
P.O.
Box
704
+L+
Yorktown
Heights,
NY
10598
+L+
ABSTRACT
+L+
We
present
an
overview
of
Candide,
a
system
for
automatic
+L+
translation
of
French
text
to
English
text.
Candide
uses
+L+
methods
of
information
theory
and
statistics
to
develop
a
+L+
probability
model
of
the
translation
process.
This
model,
+L+
which
is
made
to
accord
as
closely
as
possible
with
a
large
+L+
body
of
French
and
English
sentence
pairs,
is
then
used
to
+L+
generate
English
translations
of
previously
unseen
French
+L+
sentences.
This
paper
provides
a
tutorial
in
these
methods,
+L+
discussions
of
the
training
and
operation
of
the
system,
and
+L+
a
summary
of
test
results.
+L+
1.
Introduction

A
Simplified
Account
of
Polymorphic
References
+L+
Robert
Harper
+L+
June,
1993
+L+
CMU-CS-93-169
+L+
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213
+L+
Abstract
+L+
A
proof
of
the
soundness
of
Tofte's
imperative
type
discipline
with
respect
to
a
structured
operational
+L+
semantics
is
given.
The
presentation
is
based
on
a
semantic
formalism
that
combines
the
benefits
of
the
+L+
approaches
considered
by
Wright
and
Felleisen,
and
by
Tofte,
leading
to
a
particularly
simple
proof
of
+L+
soundness
of
Tofte's
type
discipline.
+L+
This
research
was
sponsored
by
the
Defense
Advanced
Research
Projects
Agency,
CSTO,
under
the
title
"The
Fox
+L+
Project:
Advanced
Development
of
Systems
Software",
ARPA
Order
No.
8313,
issued
by
ESD/AVS
under
Contract
+L+
No.
F19628-91-C-0168.
+L+
The
views
and
conclusions
contained
in
this
document
are
those
of
the
author
and
should
not
be
interpreted
as
+L+
representing
official
policies,
either
expressed
or
implied,
of
the
Defense
Advanced
Research
Projects
Agency
or
the
+L+
U.S.
Government.
+L+
+PAGE+

New
Approximation
Techniques
for
Some
Ordering
Problems
+L+
Satish
Rao
Andrea
W.
Richa
+L+
Abstract
+L+
We
describe
logarithmic
times
optimal
approximation
+L+
algorithms
for
the
NP-hard
graph
optimization
problems
of
minimum
linear
arrangement,
minimum
containing
interval
graph,
and
minimum
storage-time
+L+
product.
This
improves
on
the
best
previous
approximation
bounds
of
Even,
Naor,
Rao,
and
Schieber
for
+L+
these
problems
by
an
(log
log
n)
factor.
+L+
Even,
Naor,
Rao,
and
Schieber
defined
"spreading
+L+
metrics"
for
each
of
the
ordering
problems
above
(and
+L+
to
other
problems);
for
each
of
these
problems,
they
+L+
provided
a
spreading
metric
of
volume
W
,
such
that
W
+L+
is
a
lower
bound
on
the
cost
of
a
solution
to
the
problem.
+L+
They
used
this
spreading
metric
to
find
a
solution
of
+L+
cost
O(W
log
n
log
log
n)
(for
simplicity,
assume
that
+L+
all
tasks
have
unit
processing
time
in
the
minimum
+L+
storage-time
product
problem).
In
this
paper,
we
show
+L+
how
to
find
a
solution
within
a
logarithmic
factor
times
+L+
W
for
these
problems.
+L+
We
develop
a
recursion
where
at
each
level
we
+L+
identify
cost
which,
if
incurred,
yields
subproblems
+L+
with
reduced
spreading
metric
volume.
Specifically,
we
+L+
present
a
divide-and-conquer
strategy
where
the
cost
of
+L+
a
solution
to
a
problem
at
a
recursive
level
is
C
plus
the
+L+
cost
of
a
solution
to
the
subproblems
at
this
level,
and
+L+
where
the
spreading
metric
volume
on
the
subproblems
+L+
is
less
than
the
original
volume
by
(C=
log
n).
This
+L+
ensures
that
the
resulting
solution
has
cost
O(log
n)
+L+
times
the
original
spreading
metric
volume.
+L+
We
note
that
this
is
an
existentially
tight
bound
on
+L+
the
relationship
between
the
spreading
metric
volume
+L+
W
and
the
true
optimal
values
for
these
problems.
+L+
For
planar
graphs,
we
combine
a
structural
theorem
+L+
of
Klein,
Plotkin,
and
Rao
with
our
new
recursion
technique
to
show
that
the
spreading
metric
cost
volumes
+L+
are
within
an
O(log
log
n)
factor
of
the
cost
of
an
optimal
solution
for
the
minimum
linear
arrangement,
and
+L+
the
minimum
containing
interval
graph
problems.
+L+
To
appear
in
Proceedings
of
Ninth
Annual
ACM-SIAM
Symposium
on
Discrete
Algorithms
(SODA),
January
1998.
+L+
Research
supported
by
NEC
Research
Institute,
4
Independence
Way,
Princeton,
NJ
08540;
satish@research.nj.nec.com.
+L+
Research
supported
in
part
by
NSF
NYI
Award
No.
CCR-9457766,
ARPA
Contract
F33615-93-1-1330,
NEC
Research
Institute,
and
DIMACS.
School
of
Computer
Science,
Carnegie
Mellon
+L+
University,
Pittsburgh,
PA
15213,
aricha@cs.cmu.edu.
+L+
1
Introduction.

Implementing
Distributed
Server
Groups
for
the
+L+
World
Wide
Web
+L+
Michael
Garland
,
Sebastian
Grassia
,
Robert
Monroe
,
Siddhartha
Puri
+L+
25
January
1995
+L+
CMU-CS-95-114
+L+
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
Pennsylvania
15213-3890
+L+
Abstract
+L+
The
World
Wide
Web
(WWW)
has
recently
become
a
very
popular
facility
for
the
dissemination
of
+L+
information.
As
a
result
of
this
popularity,
it
is
experiencing
rapidly
increasing
traffic
load.
Single
+L+
machine
servers
cannot
keep
pace
with
the
ever
greater
load
being
placed
upon
them.
To
alleviate
this
+L+
problem,
we
have
implemented
a
distributed
Web
server
group.
The
server
group
can
effectively
balance
request
load
amongst
its
members
(within
about
10%
of
optimal),
and
client
response
time
is
no
+L+
worse
than
in
the
single
server
case.
Client
response
time
was
not
improved
because
the
measured
client
traffic
consumed
all
available
network
throughput.
The
distributed
operation
of
the
server
groups
is
+L+
completely
transparent
to
standard
Web
clients.
+L+
This
research
is
sponsored
in
part
by
the
Wright
Laboratory,
Aeronautical
SystemsCenter,
Air
Force
Materiel
Command,
USAF,
+L+
and
the
Advanced
Research
Projects
Agency
(ARPA)
under
grant
F33615-93-1-1330.
The
US
Government
is
authorized
to
reproduce
and
distribute
reprints
for
Government
purposes,
notwithstanding
any
copyright
notation
thereon.
Support
also
came
from
the
+L+
Air
Force
Materiel
Command
under
contract
number
F19628-93-C-0171.
Views
and
conclusions
contained
in
this
document
are
+L+
those
of
the
authors
and
should
not
be
interpreted
as
representing
the
official
policies,
either
expressed
or
implied,
of
Wright
Laboratory
or
the
United
States
Government.
+L+
+PAGE+

Performance
Measurements
of
the
Multimedia
+L+
Testbed
on
Real-Time
Mach
+L+
Roger
B.
Dannenberg
,David
B.
Anderson,
+L+
Tom
Neuendorffer
,
Dean
Rubine
+L+
April
1994
+L+
CMU-CS-94-141
+L+
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213-3890
+L+
Abstract
+L+
Multimedia
has
generated
widespread
interest
in
real-time
support
within
general
purpose
+L+
operating
systems.
Multimedia
also
places
new
demands
on
operating
systems
for
interprocess
+L+
communication.
The
Multimedia
Testbed
is
a
set
of
applications
that
stress
consistent
low-latency
response
and
efficient
interprocess
communication
for
large
blocks
of
data.
The
+L+
Multimedia
Testbed
was
ported
to
Real-Time
Mach
in
the
hopes
of
providing
predictable
low-latency
response
and,
consequently,
good
synchronization
and
low
jitter
as
required
for
+L+
multimedia
applications.
Our
work
compares
the
performance
of
Real-Time
Mach
with
that
of
+L+
Mach
3.0.
Although
the
fixed-priority
scheduling
of
Real-Time
Mach
is
a
substantial
+L+
improvement,
user
threads
are
still
preempted
by
device
drivers,
and
the
overall
real-time
+L+
performance
is
not
suitable
for
multimedia
applications.
We
discuss
areas
where
Real-Time
+L+
Mach
needs
improvement.
+L+
This
research
was
performed
by
the
Carnegie
Mellon
Information
Technology
Center
and
+L+
supported
by
the
IBM
Corporation.
+L+
+PAGE+

Moving
target
classification
and
tracking
from
real-time
video
+L+
Alan
J.
Lipton
Hironobu
Fujiyoshi
Raju
S.
Patil
+L+
The
Robotics
Institute.
Carnegie
Mellon
University
+L+
5000
Forbes
Avenue,
Pittsburgh,
PA,
15213
+L+
email:
fajljhironobujrajug@cs.cmu.edu
+L+
URL:
http://www.cs.cmu.edu/~
vsam
+L+
Abstract
+L+
This
paper
describes
an
end-to-end
method
for
extracting
moving
targets
from
a
real-time
video
stream,
+L+
classifying
them
into
predefined
categories
according
+L+
to
image-based
properties,
and
then
robustly
tracking
+L+
them.
Moving
targets
are
detected
using
the
pixelwise
+L+
difference
between
consecutive
image
frames.
A
classi-ficatoin
metric
is
applied
these
targets
with
a
temporal
+L+
consistency
constraint
to
classify
them
into
three
categories:
human,
vehicle
or
background
clutter.
Once
+L+
classified,
targets
are
tracked
by
a
combination
of
temporal
differencing
and
template
matching.
+L+
The
resulting
system
robustly
identifies
targets
of
+L+
interest,
rejects
background
clutter,
and
continually
+L+
tracks
over
large
distances
and
periods
of
time
despite
+L+
occlusions,
appearance
changes
and
cessation
of
target
+L+
motion.
+L+
1
Introduction

TIGHT
ANALYSES
OF
TWO
LOCAL
LOAD
BALANCING
ALGORITHMS
+L+
BHASKAR
GHOSH
1
F.
T.
LEIGHTON
2
BRUCE
M.
MAGGS
3;4
+L+
S.
MUTHUKRISHNAN
5
C.
GREG
PLAXTON
6;7
R.
RAJARAMAN
6;7
+L+
ANDR
EA
W.
RICHA
3;4
ROBERT
E.
TARJAN
8
DAVID
ZUCKERMAN
6;9
+L+
Abstract.
+L+
This
paper
presents
an
analysis
of
the
following
load
balancing
algorithm.
At
each
step,
each
node
in
a
network
examines
the
number
+L+
of
tokens
at
each
of
its
neighbors
and
sends
a
token
to
each
neighbor
with
at
least
2d
+
1
fewer
tokens,
where
d
is
the
maximum
degree
+L+
of
any
node
in
the
network.
We
show
that
within
O(=ff)
steps,
the
algorithm
reduces
the
maximum
difference
in
tokens
between
any
+L+
two
nodes
to
at
most
O((d
2
log
n)=ff),
where
is
the
global
imbalance
in
tokens
(i.e.,
the
maximum
difference
between
the
number
of
+L+
tokens
at
any
node
initially
and
the
average
number
of
tokens),
n
is
the
number
of
nodes
in
the
network,
and
ff
is
the
edge
expansion
of
+L+
the
network.
The
time
bound
is
tight
in
the
sense
that
for
any
graph
with
edge
expansion
ff,
and
for
any
value
,
there
exists
an
initial
+L+
distribution
of
tokens
with
imbalance
for
which
the
time
to
reduce
the
imbalance
to
even
=2
is
at
least
(=ff).
The
bound
on
the
+L+
final
imbalance
is
tight
in
the
sense
that
there
exists
a
class
of
networks
that
can
be
locally
balanced
everywhere
(i.e.,
the
maximum
+L+
difference
in
tokens
between
any
two
neighbors
is
at
most
2d),
while
the
global
imbalance
remains
((d
2
log
n)=ff).
Furthermore,
we
show
+L+
that
upon
reaching
a
state
with
a
global
imbalance
of
O((d
2
log
n)=ff),
the
time
for
this
algorithm
to
locally
balance
the
network
can
be
+L+
as
large
as
(n
1=2
).
We
extend
our
analysis
to
a
variant
of
this
algorithm
for
dynamic
and
asynchronous
networks.
We
also
present
tight
+L+
bounds
for
a
randomized
algorithm
in
which
each
node
sends
at
most
one
token
in
each
step.
+L+
Keywords:
load
balancing,
distributed
network
algorithms.
+L+
AMS
subject
classification:
68Q22
+L+
1.
Introduction.
A
natural
way
to
balance
the
workload
in
a
distributed
system
is
to
have
each
work
station

,
,
1-8
()
+L+
c
Kluwer
Academic
Publishers,
Boston.
Manufactured
in
The
Netherlands.
+L+
A
Note
on
Learning
from
Multiple-Instance
+L+
Examples
+L+
AVRIM
BLUM
avrim+@cs.cmu.edu
+L+
ADAM
KALAI
akalai+@cs.cmu.edu
+L+
School
of
Computer
Science,
Carnegie
Mellon
University,
Pittsburgh,
PA
15213
+L+
Abstract.
+L+
We
describe
a
simple
reduction
from
the
problem
of
PAC-learning
from
multiple-instance
examples
to
that
of
PAC-learning
with
one-sided
random
classification
noise.
Thus,
all
concept
classes
+L+
learnable
with
one-sided
noise,
which
includes
all
concepts
learnable
in
the
usual
2-sided
random
+L+
noise
model
plus
others
such
as
the
parity
function,
are
learnable
from
multiple-instance
examples.
We
also
describe
a
more
efficient
(and
somewhat
technically
more
involved)
reduction
to
+L+
the
Statistical-Query
model
that
results
in
a
polynomial-time
algorithm
for
learning
axis-parallel
+L+
rectangles
with
sample
complexity
~
O(d
2
r=*
2
),
saving
roughly
a
factor
of
r
over
the
results
of
Auer
+L+
et
al.
(1997).
+L+
Keywords:
Multiple-instance
examples,
classification
noise,
statistical
queries
+L+
1.
Introduction
and
Definitions

SIGGRAPH
95,
Los
Angeles,
August
6-11
COMPUTER
GRAPHICS
Proceedings,
Annual
Conference
Series,
1995
+L+
Interactive
Physically-Based
Manipulation
+L+
of
Discrete/Continuous
Models
+L+
Mikako
Harada
+L+
Department
of
Architecture
+L+
Andrew
Witkin
+L+
Department
of
Computer
Science
+L+
David
Baraff
+L+
Robotics
Institute
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213
+L+
Abstract
+L+
Physically-based
modeling
has
been
used
in
the
past
to
support
a
variety
of
interactive
modeling
tasks
including
free-form
surface
design,
mechanism
design,
constrained
drawing,
and
interactive
camera
control.
In
these
systems,
the
user
interacts
with
the
model
+L+
by
exerting
virtual
forces,
to
which
the
system
responds
subject
+L+
to
the
active
constraints.
In
the
past,
this
kind
of
interaction
has
+L+
been
applicable
only
to
models
that
are
governed
by
continuous
+L+
parameters.
In
this
paper
we
present
an
extension
to
mixed
con-tinuous/discrete
models,
emphasizing
constrained
layout
problems
+L+
that
arise
in
architecture
and
other
domains.
When
the
object
being
+L+
dragged
is
blocked
from
further
motion
by
geometric
constraints,
a
+L+
local
discrete
search
is
triggered,
during
which
transformations
such
+L+
as
swapping
of
adjacent
objects
may
be
performed.
The
result
of
the
+L+
search
is
a
nearby
state
in
which
the
target
object
has
been
moved
+L+
in
the
indicated
direction
and
in
which
all
constraints
are
satisfied.
+L+
The
transition
to
this
state
is
portrayed
using
simple
but
effective
animated
visual
effects.
Following
the
transition,
continuous
dragging
+L+
is
resumed.
The
resulting
seamless
transitions
between
discrete
and
+L+
continuous
manipulation
allow
the
user
to
easily
explore
the
mixed
+L+
design
space
just
by
dragging
objects.
We
demonstrate
the
method
+L+
in
application
to
architectural
floor
plan
design,
circuit
board
layout,
+L+
art
analysis,
and
page
layout.
+L+
KeywordsInteractive
techniques,
physically-based
modeling,
+L+
grammars.
+L+
1
Introduction

Mixed-Initiative
Management
of
Integrated
Process-Planning
and
+L+
Production-Scheduling
Solutions
+L+
David
W.
Hildum
,
Norman
M.
Sadeh
,
Thomas
J.
Laliberty,
+L+
Stephen
F.
Smith
,
John
McA'Nulty
and
Dag
Kjenstad
+L+
Intelligent
Coordination
and
Logistics
Laboratory
+L+
Center
for
Integrated
Manufacturing
Decision
Systems
+L+
The
Robotics
Institute
+L+
Carnegie
Mellon
University
+L+
Pittsburgh
PA
15213-3890
+L+
412.268.7598
fax:
412.268.5569
+L+
fHILDUM,SADEH,SFS,DAGg@ISL1.RI.CMU.EDU
+L+
Software
Engineering
Laboratory
+L+
Raytheon
Electronic
Systems
+L+
Raytheon
Company
+L+
Tewksbury
MA
01876-0901
+L+
508.858.5756
fax:
508.858.5976
+L+
LALIBERTY
THOMAS@CAEMAC.MSD.RAY.COM
+L+
MCANULTY@CAESUN.MSD.RAY.COM
+L+
Abstract
+L+
Increased
reliance
on
agile
manufacturing
techniques
has
created
a
demand
for
systems
to
solve
integrated
process-planning
+L+
and
production-scheduling
problems
in
large-scale
dynamic
environments.
To
be
effective,
these
systems
should
provide
user-oriented
interactive
functionality
for
managing
the
various
user
+L+
tasks
and
objectives
and
reacting
to
unexpected
events.
This
paper
describes
the
mixed-initiative
problem-solving
features
of
+L+
IP3S,
an
Integrated
Process-Planning/Production-Scheduling
+L+
shell
for
agile
manufacturing.
IP3S
is
a
blackboard
-based
system
+L+
that
supports
the
concurrent
development
and
dynamic
revision
+L+
of
integrated
process-planning
and
production-scheduling
solutions
and
the
maintenance
of
multiple
problem
instances
and
+L+
solutions,
as
well
as
other
flexible
user-oriented
decision-making
+L+
capabilities,
allowing
the
user
to
control
the
scope
of
the
problem
and
explore
alternate
tradeoffs
(what-if
scenarios)
interactively.
The
system
is
scheduled
for
initial
deployment
+L+
and
evaluation
in
a
large
and
highly
dynamic
machine
shop
at
+L+
Raytheon's
Andover
manufacturing
facility.
+L+
Introduction

Operant
Conditioning
in
Skinnerbots
+L+
David
S.
Touretzky
+L+
Computer
Science
Department
&
+L+
Center
for
the
Neural
Basis
of
Cognition
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213-3891
+L+
dst@cs.cmu.edu
+L+
Lisa
M.
Saksida
+L+
Robotics
Institute
&
+L+
Center
for
the
Neural
Basis
of
Cognition
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213-3891
+L+
saksida@ri.cmu.edu
+L+
To
appear
in
Adaptive
Behavior
5(3/4),
1997.
+L+
Copyright
c
1997
The
MIT
Press.
+L+
Abstract
+L+
Instrumental
(or
operant)
conditioning,
a
form
of
animal
learning,
is
similar
to
reinforcement
learning
+L+
(Watkins,
1989)
in
that
it
allows
an
agent
to
adapt
its
actions
to
gain
maximally
from
the
environment
+L+
while
only
being
rewarded
for
correct
performance.
But
animals
learn
much
more
complicated
behaviors
+L+
through
instrumental
conditioning
than
robots
presently
acquire
through
reinforcement
learning.
We
+L+
describe
a
new
computational
model
of
the
conditioning
process
that
attempts
to
capture
some
of
the
+L+
aspects
that
are
missing
from
simple
reinforcement
learning:
conditioned
reinforcers,
shifting
reinforcement
contingencies,
explicit
action
sequencing,
and
state
space
refinement.
We
apply
our
model
to
a
task
+L+
commonly
used
to
study
working
memory
in
rats
and
monkeys:
the
DMTS
(Delayed
Match
to
Sample)
+L+
task.
Animals
learn
this
task
in
stages.
In
simulation,
our
model
also
acquires
the
task
in
stages,
in
a
+L+
similar
manner.
We
have
used
the
model
to
train
an
RWI
B21
robot.
+L+
Keywords:
operant
conditioning,
instrumental
learning,
shaping,
chaining,
learning
robots
+L+
Running
Head:
Operant
Conditioning
in
Skinnerbots
+L+
+PAGE+

Improving
Text
Classification
by
Shrinkage
in
a
Hierarchy
of
Classes
+L+
Andrew
McCallum
+L+
mccallum@justresearch.com
+L+
Ronald
Rosenfeld
+L+
roni@cs.cmu.edu
+L+
Tom
Mitchell
+L+
mitchell+@cs.cmu.edu
+L+
Andrew
Y.
Ng
+L+
ayn@ai.mit.edu
+L+
Just
Research
+L+
4616
Henry
Street
+L+
Pittsburgh,
PA
15213
+L+
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213
+L+
MIT
AI
Lab
+L+
545
Technology
Square
+L+
Cambridge,
MA
02139
+L+
Abstract
+L+
When
documents
are
organized
in
a
large
+L+
number
of
topic
categories,
the
categories
+L+
are
often
arranged
in
a
hierarchy.
The
U.S.
+L+
patent
database
and
Yahoo
are
two
examples.
+L+
This
paper
shows
that
the
accuracy
of
a
naive
+L+
Bayes
text
classifier
can
be
significantly
improved
by
taking
advantage
of
a
hierarchy
of
+L+
classes.
We
adopt
an
established
statistical
+L+
technique
called
shrinkage
that
smoothes
parameter
estimates
of
a
data-sparse
child
with
+L+
its
parent
in
order
to
obtain
more
robust
parameter
estimates.
The
approach
is
also
employed
in
deleted
interpolation,
a
technique
+L+
for
smoothing
n-grams
in
language
modeling
+L+
for
speech
recognition.
+L+
Our
method
scales
well
to
large
data
sets,
+L+
with
numerous
categories
in
large
hierarchies.
+L+
Experimental
results
on
three
real-world
data
+L+
sets
from
UseNet,
Yahoo,
and
corporate
web
+L+
pages
show
improved
performance,
with
a
reduction
in
error
up
to
29%
over
the
tradi
+L+
tional
flat
classifier.
+L+
1
Introduction

Remote
Access
to
Interactive
Media
+L+
Roger
B.
Dannenberg
+L+
Carnegie
Mellon
University,
School
of
Computer
Science
+L+
Pittsburgh,
PA
15213
USA
+L+
Email:
dannenberg@cs.cmu.edu
+L+
ABSTRACT
+L+
Digital
interactive
media
augments
interactive
computing
with
video,
audio,
computer
graphics
and
text,
+L+
allowing
multimedia
presentations
to
be
individually
and
dynamically
tailored
to
the
user.
Multimedia,
and
+L+
particularly
continuous
media
pose
interesting
problems
for
system
designers,
including
those
of
latency
+L+
and
synchronization.
These
problems
are
especially
evident
when
multimedia
data
is
remote
and
must
be
+L+
accessed
via
networks.
Latency
and
synchronization
issues
are
discussed,
and
an
integrated
system,
+L+
Tactus,
is
described.
Tactus
facilitates
the
implementation
of
interactive
multimedia
computer
programs
by
+L+
managing
latency
and
synchronization
in
the
framework
of
an
object-oriented
graphical
user
interface
+L+
toolkit.
+L+
1.
Introduction

Informing
Memory
Operations:
Providing
Memory
+L+
Performance
Feedback
in
Modern
Processors
+L+
Mark
Horowitz
Margaret
Martonosi
Todd
C.
Mowry
Michael
D.
Smith
+L+
Computer
Systems
Department
of
Department
of
Electrical
Division
of
+L+
Laboratory
Electrical
Engineering
and
Computer
Engineering
Applied
Sciences
+L+
Stanford
University
Princeton
University
University
of
Toronto
Harvard
University
+L+
horowitz@ee.stanford.edu
martonosi@princeton.edu
tcm@eecg.toronto.edu
smith@eecs.harvard.edu
+L+
Abstract
+L+
Memory
latency
is
an
important
bottleneck
in
system
performance
+L+
that
cannot
be
adequately
solved
by
hardware
alone.
Several
promising
software
techniques
have
been
shown
to
address
this
problem
+L+
successfully
in
specific
situations.
However,
the
generality
of
these
+L+
software
approaches
has
been
limited
because
current
architectures
+L+
do
not
provide
a
fine-grained,
low-overhead
mechanism
for
+L+
observing
and
reacting
to
memory
behavior
directly.
To
fill
this
+L+
need,
we
propose
a
new
class
of
memory
operations
called
informing
memory
operations,
which
essentially
consist
of
a
memory
+L+
operation
combined
(either
implicitly
or
explicitly)
with
a
conditional
branch-and-link
operation
that
is
taken
only
if
the
reference
+L+
suffers
a
cache
miss.
We
describe
two
different
implementations
of
+L+
informing
memory
operationsone
based
on
a
cache-outcome
+L+
condition
code
and
another
based
on
low-overhead
trapsand
find
+L+
that
modern
in-order-issue
and
out-of-order-issue
superscalar
processors
already
contain
the
bulk
of
the
necessary
hardware
support.
+L+
We
describe
how
a
number
of
software-based
memory
optimizations
can
exploit
informing
memory
operations
to
enhance
performance,
and
look
at
cache
coherence
with
fine-grained
access
+L+
control
as
a
case
study.
Our
performance
results
demonstrate
that
+L+
the
runtime
overhead
of
invoking
the
informing
mechanism
on
the
+L+
Alpha
21164
and
MIPS
R10000
processors
is
generally
small
+L+
enough
to
provide
considerable
exibility
to
hardware
and
software
designers,
and
that
the
cache
coherence
application
has
+L+
improved
performance
compared
to
other
current
solutions.
We
+L+
believe
that
the
inclusion
of
informing
memory
operations
in
+L+
future
processors
may
spur
even
more
innovative
performance
+L+
optimizations.
+L+
1
Introduction

A
Simple
Algorithm
for
Finding
the
+L+
Maximum
Recoverable
System
State
in
+L+
Optimistic
Rollback
Recovery
Methods
+L+
David
B.
Johnson
+L+
Peter
J.
Keleher
+L+
Willy
Zwaenepoel
+L+
Rice
COMP
TR90-125
+L+
July
1990
+L+
Department
of
Computer
Science
+L+
Rice
University
+L+
P.O.
Box
1892
+L+
Houston,
Texas
77251-1892
+L+
(713)
527-4834
+L+
This
research
was
supported
in
part
by
the
National
Science
Foundation
under
grants
CCR-8716914
and
+L+
CDA-8619893,
and
by
the
Office
of
Naval
Research
under
contract
ONR
N00014-88-K-0140.
+L+
+PAGE+

Towards
a
Principled
Representation
+L+
of
Discourse
Plans
+L+
R.
Michael
Young
+L+
Johanna
D.
Moore
+L+
Martha
E.
Pollack
+L+
ISP
Technical
Report
94-2
+L+
May
1994
+L+
Abstract
+L+
We
argue
that
discourse
plans
must
capture
the
intended
causal
and
decompositional
relations
between
communicative
actions.
We
present
a
planning
algorithm,
DPOCL,
that
builds
plan
structures
that
properly
capture
these
relations,
and
show
+L+
how
these
structures
are
used
to
solve
the
problems
that
plagued
previous
discourse
planners,
and
allow
a
system
to
participate
+L+
effectively
and
flexibly
in
an
ongoing
dialogue.
+L+
A
version
of
this
paper
appears
in
the
Proceedings
of
the
Sixteenth
Annual
Meeting
+L+
of
the
Cognitive
Science
Society,
Atlanta,
GA,
1994.
+L+
+PAGE+

NIFDY:
A
Low
Overhead,
High
Throughput
Network
Interface
+L+
Timothy
Callahan
and
Seth
Copen
Goldstein
+L+
ftimothyc,sethgg@cs.berkeley.edu
+L+
Computer
Science
Division
+L+
University
of
California-Berkeley
+L+
Abstract
+L+
In
this
paper
we
present
NIFDY,
a
network
interface
that
uses
admission
control
to
reduce
congestion
and
ensures
that
packets
are
+L+
received
by
a
processor
in
the
order
in
which
they
were
sent,
even
+L+
if
the
underlying
network
delivers
the
packets
out
of
order.
The
+L+
basic
idea
behind
NIFDY
is
that
each
processor
is
allowed
to
have
at
+L+
most
one
outstanding
packet
to
any
other
processor
unless
the
destination
processor
has
granted
the
sender
the
right
to
send
multiple
+L+
unacknowledged
packets.
Further,
there
is
a
low
upper
limit
on
the
+L+
number
of
outstanding
packets
to
all
processors.
+L+
We
present
results
from
simulations
of
a
variety
of
networks
+L+
(meshes,
tori,
butterflies,
and
fat
trees)
and
traffic
patterns
to
verify
NIFDY's
efficacy.
Our
simulations
show
that
NIFDY
increases
+L+
throughput
and
decreases
overhead.
The
utility
of
NIFDY
increases
+L+
as
a
network's
bisection
bandwidth
decreases.
When
combined
+L+
with
the
increased
payload
allowed
by
in-order
delivery
NIFDY
increases
total
bandwidth
delivered
for
all
networks.
The
resources
+L+
needed
to
implement
NIFDY
are
small
and
constant
with
respect
to
+L+
network
size.
+L+
1
Introduction

Exploiting
Global
Input/Output
Access
Pattern
Classification
+L+
Tara
M.
Madhyastha
Daniel
A.
Reed
+L+
ftara,reedg@cs.uiuc.edu
+L+
Department
of
Computer
Science
+L+
University
of
Illinois
+L+
Urbana,
Illinois
61801
+L+
1
Introduction

SUPRENUM:
Perspectives
and
Performance
+L+
Oliver
A.
McBryan
+L+
Dept.
of
Computer
Science
+L+
University
of
Colorado
+L+
Boulder,
CO
80309.
+L+
Email:
mcbryan@cs.colorado.edu
+L+
Abstract
+L+
We
describe
our
impressions
of
the
SUPRENUM
project
and
of
its
primary
+L+
supercomputer
result,
the
Suprenum-1
prototype.
We
comment
on
the
significance
+L+
of
the
architecture,
its
role
among
contemporary
systems
and
its
relevance
to
+L+
current
systems.
We
similarly
discuss
the
SUPRENUM
software
and
its
impact
on
+L+
distributed
systems.
Finally
we
discuss
the
successes
and
failures
observed
+L+
throughout
this
exciting
project
and
relate
these
to
the
organizational
decisions
on
+L+
which
SUPRENUM
was
based.
+L+
As
an
illustration
of
Suprenum-1
capabilities,
we
describe
the
+L+
implementation
of
a
fluid
dynamical
benchmark
on
the
256
node
Suprenum-1
+L+
parallel
computer.
The
benchmark,
the
Shallow
Water
Equations,
is
frequently
used
+L+
as
a
model
for
both
oceanographic
and
atmospheric
circulation.
We
describe
the
+L+
steps
involved
in
implementing
the
algorithm
on
the
Suprenum-1
and
we
provide
+L+
details
of
performance
obtained.
For
such
regular
grid-based
algorithms
the
system
+L+
delivers
a
very
impressive
fraction
(25%)
of
its
theoretical
peak
rate
of
5
Gflops.
+L+
Keywords:
SUPRENUM,
MPP,
MIMD,
parallel,
supercomputer,
performance,
+L+
atmospheric,
shallow
water,
architecture,
software,
message
passing.
+L+
*
Research
supported
in
part
by
NSF
Grand
Challenges
Applications
Group
grant
ASC-9217394
and
by
+L+
NASA
HPCC
Group
Grant
NAG5-2218.
+L+
To
appear
in
Parallel
Computing,
October
1994.
+L+
+PAGE+

The
Sybil
Database
Integration
and
Evolution
Environment:
An
Overview
+L+
Roger
King
,
Michael
Novak
,
Christian
Och
,
Richard
Osborne
+L+
Department
of
Computer
Science
+L+
University
of
Colorado
+L+
Campus
Box
430
+L+
Boulder,
CO
80309-0430
+L+
roger@cs.colorado.edu,
rick@cs.colorado.edu
+L+
Fernando
Velez
+L+
Unidata
Inc.
+L+
1099
18th
Street
+L+
Suite
2200
+L+
Denver,
CO
80202-1925
+L+
Abstract
+L+
Sybil
is
a
database
integration
and
evolution
environment
for
supporting
large,
heterogeneous
applications.
+L+
We
are
interested
in
using
Sybil
to
support
the
data
integration
and
evolution
needs
of
applications
that
are
using
legacy
databases
and
are
looking
to
integrate
with
+L+
more
modern
database
systems.
The
Sybil
approach
is
+L+
based
on
loosely
coupling
databases
or
other
persistent
+L+
tools
into
lightweight
alliances
tailored
for
specific
applications.
Such
alliances
are
built
via
four
sorts
of
constructs:
heterogeneous
views,
inter-database
constraints,
+L+
inter-database
propagations,
and
integration
supported
+L+
by
domain
specific
information.
+L+
Sybil
Overview

Goal-Directed
Classification
using
Linear
Machine
+L+
Decision
Trees
+L+
Bruce
A.
Draper
Carla
E.
Brodley
Paul
E.
Utgoff
+L+
Department
of
Computer
Science
+L+
University
of
Massachusetts
+L+
Amherst,
MA.,
USA.
01003
+L+
bdraper@cs.umass.edu
+L+
September
17,
1993
+L+
Abstract
+L+
Recent
work
in
feature-based
classification
has
focused
on
non-parametric
techniques
that
can
classify
instances
even
when
the
underlying
feature
distributions
are
+L+
unknown.
The
inference
algorithms
for
training
these
techniques,
however,
are
designed
+L+
to
maximize
the
accuracy
of
the
classifier,
with
all
errors
weighted
equally.
In
many
+L+
applications,
certain
errors
are
far
more
costly
than
others,
and
the
need
arises
for
+L+
non-parametric
classification
techniques
that
can
be
trained
to
optimize
task-specific
+L+
cost
functions.
This
paper
reviews
the
Linear
Machine
Decision
Tree
(LMDT)
algorithm
for
inducing
multivariate
decision
trees,
and
shows
how
LMDT
can
be
altered
to
+L+
induce
decision
trees
that
minimize
arbitrary
misclassification
cost
functions
(MCFs).
+L+
Demonstrations
of
pixel
classification
in
outdoor
scenes
show
how
MCFs
can
optimize
+L+
the
performance
of
embedded
classifiers
within
the
context
of
larger
image
understand
+L+
ing
systems.
+L+
Keywords:
Decision
Trees,
Non-Parametric
Classification,
Pattern
Recognition,
Object
Recognition,
Computer
Vision,
Machine
Learning.
+L+
1
Introduction

Efficient
Indexing
for
Object
Recognition
Using
Large
Networks
+L+
Mark
R.
Stevens
Charles
W.
Anderson
J.
Ross
Beveridge
+L+
Department
of
Computer
Science
+L+
Colorado
State
University
+L+
Fort
Collins,
CO
80523
+L+
fstevensm,anderson,rossg@cs.colostate.edu
+L+
Abstract
+L+
Template
matching
is
an
effective
means
of
locating
vehicles
in
outdoor
scenes,
but
it
tends
to
be
+L+
a
computationally
expensive.
To
reduce
processing
time,
we
use
large
neural
networks
to
predict,
or
+L+
index,
a
small
subset
of
templates
that
are
likely
to
match
each
window
in
an
image.
Results
on
actual
+L+
LADAR
range
images
show
that
limiting
the
templates
to
those
selected
by
the
neural
networks
reduces
+L+
the
computation
time
by
a
factor
of
5
without
sacrificing
the
accuracy
of
the
results.
+L+
1
Introduction

Internet
Telephony:
A
(Partial)
Research
Agenda
+L+
Jonathan
Rosenberg
+L+
Bell
Laboratories
and
Columbia
U.
+L+
Rm.
4C-526
+L+
101
Crawfords
Corner
Rd.
+L+
Holmdel,
NJ
07733
+L+
jdrosen@bell-labs.com
+L+
TEL:
+1
908
949-6418
+L+
October
17,
1997
+L+
1
Introduction

Mining
Audit
Data
to
Build
Intrusion
Detection
Models
+L+
Wenke
Lee
Salvatore
J.
Stolfo
+L+
Kui
W.
Mok
+L+
Computer
Science
Department
+L+
Columbia
University
+L+
500
West
120th
Street,
New
York,
NY
10027
+L+
fwenke,sal,mokg@cs.columbia.edu
+L+
Abstract
+L+
In
this
paper
we
discuss
a
data
mining
framework
for
constructing
intrusion
detection
models.
+L+
The
key
ideas
are
to
mine
system
audit
data
for
consistent
and
useful
patterns
of
program
and
+L+
user
behavior,
and
use
the
set
of
relevant
system
features
presented
in
the
patterns
to
compute
+L+
(inductively
learned)
classifiers
that
can
recognize
anomalies
and
known
intrusions.
Our
past
experiments
showed
that
classifiers
can
be
used
to
detect
intrusions,
provided
that
sufficient
audit
+L+
data
is
available
for
training
and
the
right
set
of
system
features
are
selected.
We
propose
to
use
+L+
the
association
rules
and
frequent
episodes
computed
from
audit
data
as
the
basis
for
guiding
the
+L+
audit
data
gathering
and
feature
selection
processes.
We
modify
these
two
basic
algorithms
to
use
+L+
axis
attribute(s)
as
a
form
of
item
constraints
to
compute
only
the
relevant
(useful)
patterns,
and
+L+
an
iterative
level-wise
approximate
mining
procedure
to
uncover
the
low
frequency
(but
important)
+L+
patterns.
We
report
our
experiments
in
using
these
algorithms
on
real-world
audit
data.
+L+
Keywords:
Intrusion
detection,
audit
data,
classification,
association
rules,
frequent
episodes.
+L+
Contact
Author:
Wenke
Lee,
wenke@cs.columbia.edu,
(212)
939-7078.
+L+
This
research
is
supported
in
part
by
grants
from
DARPA
(F30602-96-1-0311)
and
NSF
(IRI-96-32225
and
CDA-96-25374).
+L+
+PAGE+

Learning
Distributions
from
Random
Walks
+L+
Funda
Ergun
S
Ravi
Kumar
+L+
Department
of
Computer
Science
+L+
Cornell
University
+L+
Ithaca,
NY
14853.
+L+
Ronitt
Rubinfeld
+L+
Abstract
+L+
We
introduce
a
new
model
of
distributions
generated
by
random
walks
on
graphs.
This
model
+L+
suggests
a
variety
of
learning
problems,
using
the
definitions
and
models
of
distribution
+L+
learning
defined
in
[6].
Our
framework
is
general
enough
to
model
previously
studied
distribution
learning
problems,
as
well
as
to
suggest
+L+
new
applications.
We
describe
special
cases
of
+L+
the
general
problem,
and
investigate
their
relative
difficulty.
We
present
algorithms
to
solve
+L+
the
learning
problem
under
various
conditions.
+L+
1
INTRODUCTION

Embedded
Machine
Learning
Systems
for
+L+
Natural
Language
Processing:
A
General
+L+
Framework
+L+
Claire
Cardie
+L+
Department
of
Computer
Science,
Cornell
University,
Ithaca
NY
14853,
USA
+L+
In
Wermter,
S.
and
Riloff,
E.
and
Scheler,
Gabriele
(eds.),
Connectionist,
Statistical
and
+L+
Symbolic
Approaches
to
Learning
for
Natural
Language
Processing,
Lecture
Notes
in
+L+
Artificial
Intelligence,
315-328,
Springer,
1996.
+L+
Abstract.
This
paper
presents
Kenmore,
a
general
framework
for
knowledge
acquisition
for
natural
language
processing
(NLP)
systems.
To
ease
+L+
the
acquisition
of
knowledge
in
new
domains,
Kenmore
exploits
an
online
corpus
using
robust
sentence
analysis
and
embedded
symbolic
machine
learning
techniques
while
requiring
only
minimal
human
intervention.
By
treating
all
problems
in
ambiguity
resolution
as
classification
+L+
tasks,
the
framework
uniformly
addresses
a
range
of
subproblems
in
sentence
analysis,
each
of
which
traditionally
had
required
a
separate
computational
mechanism.
In
a
series
of
experiments,
we
demonstrate
the
+L+
successful
use
of
Kenmore
for
learning
solutions
to
several
problems
in
+L+
lexical
and
structural
ambiguity
resolution.
We
argue
that
the
learning
+L+
and
knowledge
acquisition
components
should
be
embedded
components
+L+
of
the
NLP
system
in
that
(1)
learning
should
take
place
within
the
+L+
larger
natural
language
understanding
system
as
it
processes
text,
and
+L+
(2)
the
learning
components
should
be
evaluated
in
the
context
of
prac
+L+
tical
language-processing
tasks.
+L+
1
Introduction

On-Line
Search
in
a
Simple
Polygon
+L+
Jon
M.
Kleinberg
+L+
Abstract
+L+
We
consider
a
number
of
search
and
exploration
problems,
from
the
perspective
of
+L+
robot
navigation
in
a
simple
polygon.
These
problems
are
"on-line"
in
the
sense
that
+L+
the
robot
does
not
have
access
to
the
map
of
the
polygon;
it
must
make
decisions
as
it
+L+
proceeds,
based
only
on
what
it
has
seen
so
far.
For
the
problem
of
exploring
a
simple
+L+
rectilinear
polygon
(under
the
L
1
norm),
Deng,
Kameda,
and
Papadimitriou
give
a
+L+
2-competitive
deterministic
algorithm;
we
present
a
randomized
exploration
algorithm
+L+
which
is
5=4-competitive.
Using
similar
techniques,
we
are
able
to
give
an
algorithm
+L+
for
searching
an
arbitrary,
unknown
rectilinear
polygon.
No
constant
competitive
ratio
+L+
is
attainable
in
this
case,
but
our
algorithm
is
within
a
constant
factor
of
optimal
in
+L+
the
worst
case;
in
a
sense,
it
is
a
generalization
of
some
of
the
strategies
of
Baeza-Yates,
Culberson,
and
Rawlins
to
a
much
more
general
class
of
search
spaces.
Finally,
+L+
we
examine
a
type
of
polygon
for
which
competitive
search
is
possible
|
the
class
of
+L+
"streets"
considered
by
Klein,
who
gave
a
1
+
3
+L+
2
-competitive
algorithm
for
the
search
+L+
problem
in
this
case.
We
present
a
simple
algorithm
with
a
competitive
ratio
of
at
+L+
most
+L+
q
+L+
p
+L+
8
(~
2:61);
in
rectilinear
streets
it
achieves
the
optimal
competitive
ratio
+L+
of
+L+
2.
+L+
This
work
was
supported
in
part
by
the
Sloan
Fellowship
and
NSF
PYI
award
of
Eva
Tardos.
Author
+L+
is
supported
by
an
ONR
Graduate
Fellowship
+L+
Laboratory
for
Computer
Science,
MIT,
Cambridge
MA
02139
USA.
+L+
+PAGE+

Experimental
Study
of
Minimum
Cut
Algorithms
+L+
Chandra
S.
Chekuri
+L+
Computer
Science
Department
+L+
Stanford
University
+L+
Stanford,
CA
94305
+L+
chekuri@theory.stanford.edu
+L+
Andrew
V.
Goldberg
+L+
NEC
Research
Institute
+L+
Princeton,
NJ
08540
+L+
avg@research.nj.nec.com
+L+
David
R.
Karger
+L+
Laboratory
for
Computer
Science
+L+
MIT
+L+
Cambridge,
MA
02139
+L+
karger@theory.lcs.mit.edu
+L+
Matthew
S.
Levine
+L+
Laboratory
for
Computer
Science
+L+
MIT
+L+
Cambridge,
MA
02139
+L+
mslevine@theory.lcs.mit.edu
+L+
Cliff
Stein
x
+L+
Department
of
Computer
Science
+L+
Dartmouth
College
+L+
Hanover,
NH,
03755
+L+
cliff@cs.dartmouth.edu.
+L+
October
1996
+L+
Abstract
+L+
Recently,
several
new
algorithms
have
been
developed
for
the
minimum
cut
problem.
+L+
These
algorithms
are
very
different
from
the
earlier
ones
and
from
each
other
and
substantially
improve
worst-case
time
bounds
for
the
problem.
We
conduct
experimental
evaluation
+L+
the
relative
performance
of
these
algorithms.
In
the
process,
we
develop
heuristics
and
data
+L+
structures
that
substantially
improve
practical
performance
of
the
algorithms.
We
also
develop
problem
families
for
testing
minimum
cut
algorithms.
Our
work
leads
to
a
better
+L+
understanding
of
practical
performance
of
the
minimum
cut
algorithms
and
produces
very
+L+
efficient
codes
for
the
problem.
+L+
Supported
by
NSF
Award
CCR-9357849,
with
matching
funds
from
IBM,
Mitsubishi,
Schlumberger
Foun
+L+
dation,
Shell
Foundation,
and
Xerox
Corporation.
+L+
Research
partly
supported
by
ARPA
contract
N00014-95-1-1246.
+L+
Research
partly
supported
by
a
grant
from
the
World
Wide
Web
Consortium.
Some
of
this
work
was
done
+L+
while
visiting
the
second
author
at
NEC.
+L+
x
Research
partly
supported
by
NSF
Award
CCR-9308701,
a
Walter
Burke
Research
Initiation
Award
and
a
+L+
Dartmouth
College
Research
Initiation
Award.
Some
of
this
work
was
done
while
visiting
the
second
author
at
+L+
NEC,
and
while
visiting
Stanford
University.
+L+
+PAGE+

Agent
Tcl:
Alpha
Release
1.1
+L+
Robert
S.
Gray
+L+
Department
of
Computer
Science
+L+
Dartmouth
College
+L+
Hanover,
NH
03755
+L+
E-mail:
robert.s.gray@dartmouth.edu
+L+
December
1,
1995
+L+
Abstract
+L+
Agent
Tcl
is
a
transportable
agent
system.
The
agents
are
written
in
an
extended
version
of
the
+L+
Tool
Command
Lanuage
(Tcl).
Each
agent
can
suspend
its
execution
at
an
arbitrary
point,
transport
+L+
to
another
machine
and
resume
execution
on
the
new
machine.
This
migration
is
accomplished
with
+L+
the
agent
jump
command.
agent
jump
captures
the
current
state
of
the
Tcl
script
and
transfers
this
+L+
state
to
the
destination
machine.
The
state
is
restored
on
the
new
machine
and
the
Tcl
script
continues
+L+
its
execution
from
the
command
immediately
after
the
agent
jump.
In
addition
to
migration,
agents
+L+
can
send
messages
to
each
other
and
can
establish
direct
connections.
A
direct
connection
is
more
+L+
efficient
than
message
passing
for
bulk
data
transfer.
Finally,
agents
can
use
the
Tk
toolkit
to
create
+L+
graphical
user
interfaces
on
their
current
machine.
Agent
Tcl
is
implemented
as
two
components.
The
+L+
first
component
is
an
extended
Tcl
interpreter.
The
second
component
is
a
server
which
runs
on
each
+L+
machine.
The
server
accepts
incoming
agents,
messages
and
connection
requests
and
keeps
track
of
the
+L+
agents
that
are
running
on
its
machine.
An
alpha
release
of
Agent
Tcl
is
available
for
public
use.
This
+L+
documentation
describes
how
to
obtain
and
compile
the
source
code,
how
to
run
the
server
and
how
to
+L+
write
transportable
agents.
+L+
Supported
by
AFOSR
contract
F49620-93-1-0266
and
ONR
contract
N00014-95-1-1204
+L+
+PAGE+

A
Hyperconcentrator
Switch
+L+
for
Routing
Bit-Serial
Messages
+L+
Thomas
H.
Cormen
+L+
Charles
E.
Leiserson
+L+
Laboratory
for
Computer
Science
+L+
Massachusetts
Institute
of
Technology
+L+
Cambridge,
Massachusetts
02139
+L+
Supported
in
part
by
the
Defense
Advanced
Research
Projects
Agency
under
Contracts
N00014-80-C-0622
and
N00014-87-K-0825
and
by
a
National
Science
Foundation
+L+
Fellowship.
+L+
Supported
in
part
by
the
Defense
Advanced
Research
Projects
Agency
under
Contracts
+L+
N00014-80-C-0622
and
N00014-87-K-0825
and
by
an
NSF
Presidential
Young
Investigator
Award
with
matching
funds
provided
by
AT&T
Bell
Laboratories,
IBM
Corporation,
+L+
and
Xerox
Corporation.
+L+
+PAGE+

Probabilistic
Analysis
for
Combinatorial
+L+
Functions
of
Moving
Points
+L+
Li
Zhang
Harish
Devarajan
Julien
Basch
Piotr
Indyk
+L+
Computer
Science
Department
+L+
Stanford
University
+L+
Stanford,
CA94305
+L+
flizhang,harish,jbasch,indykg@cs.stanford.edu
+L+
September
9,
1997
+L+
Abstract
+L+
Given
a
set
of
n
points,
what
is
the
description
complexity
of
their
convex
hull?
In
our
world,
this
question
+L+
is
understood
with
an
implicit
"in
the
worst
case",
and
the
answer
is
n
bd=2c
where
d
is
the
dimension
of
+L+
the
underlying
space.
This
is
not
entirely
satisfactory,
as
this
description
complexity
can
vary
tremendously
+L+
depending
on
the
positions
of
the
points.
Another
approach
is
to
look
at
the
expected
description
complexity
+L+
when
the
points
are
drawn
from
a
given
distribution.
This
type
of
analysis,
initiated
by
Renyi
and
Sulanke
+L+
[RS63]
and
pursued
by
others
gets
its
value
from
the
fact
that
this
expectation
is
in
general
much
smaller
+L+
than
in
the
worst
case,
and,
more
importantly,
in
that
it
often
allows
one
to
design
algorithms
that
have
+L+
expected
running
times
against
which
worst
case
aware
algorithms
cannot
compete.
For
instance,
the
convex
+L+
hull
of
n
points
drawn
independently
uniformly
at
random
from
a
d-dimensional
hypercube
has
expected
+L+
complexity
O(log
d1
n),
and
can
be
computed
in
expected
linear
time.
+L+
In
parallel,
in
the
past
decade,
a
number
of
papers
have
considered
a
setting
where
points
are
allowed
+L+
to
move
along
low
degree
algebraic
trajectories.
Different
questions
have
been
asked
in
this
context.
In
+L+
particular,
Atallah
[Ata85],
studied
the
number
of
times
the
combinatorial
description
of
the
convex
hull
+L+
or
closest
pair
can
change,
in
the
worst
case
("dynamic
computational
geometry").
More
recently,
Basch,
+L+
Guibas,
and
Hershberger
[BGH97]
have
designed
kinetic
data
structures
to
maintain
these
attributes
in
an
+L+
online
setting,
measuring
the
quality
of
a
kinetic
data
structure
by
the
ratio
of
the
worst
case
number
of
+L+
changes
to
the
configuration
of
interest,
to
the
worst
case
number
of
changes
to
the
data
structure
itself,
for
+L+
low
degree
algebraic
motions.
However,
an
experimental
study
undertaken
in
[BGSZ97]
to
assess
the
quality
+L+
of
these
data
structures
in
practice
shows
that
the
worst
case
analysis
can
hide
vastly
different
results
in
+L+
terms
of
expectation
when
the
point
positions
and
speeds
are
drawn
at
random
from
some
distributions.
It
+L+
is
this
study
that
motivated
the
present
paper.
+L+
In
this
communication,
we
report
several
results
on
the
expected
number
of
changes
to
various
combinatorial
structures
for
the
case
when
points
are
drawn
from
the
uniform
distribution
on
the
unit
square.
These
+L+
results
can
be
generalized
for
any
dimension
d
&gt;
2.
+L+
This
paper
appeared
in
the
proceedings
of
the
13th
Symposium
of
Computational
Geometry
(1997)
as
a
communication
+L+
Supported
in
part
by
National
Science
Foundation
grant
CCR-9623851
and
by
US
Army
MURI
grant
DAAH04-96-1-0007.
+L+
Supported
in
part
by
the
Okawa
Foundation.
+L+
x
Supported
in
part
by
ARO
MURI
Grant
DAAH04-96-1-0007
and
by
NSF
Award
CCR-9357849,
with
matching
funds
+L+
from
IBM,
Mitsubishi,
Schlumberger
Foundation,
Shell
Foundation,
and
Xerox
Corporation.
+L+
+PAGE+

Lexicographic
Bit
Allocation
for
MPEG
Video
+L+
Dzung
T.
Hoang
+L
Sony
Semiconductor
of
America
+L+
3300
Zanker
Road,
MS
SJ3C3
+L+
San
Jose,
CA
95134
+L+
dth@ricochet.net
+L+
Elliot
L.
Linzer
+L+
C-Cube
Microsystems
+L+
One
Water
Street,
2nd
Floor
+L+
White
Plains,
NY
10601
+L+
elliot.linzer@c-cube.com
+L+
Jeffrey
Scott
Vitter
+L+
Duke
University
+L+
Box
90129
+L+
Durham,
NC
27708-0129
+L+
jsv@cs.duke.edu
+L+
+PAGE+

A
Unified
Analysis
of
Value-Function-Based
+L+
Reinforcement-Learning
Algorithms
+L+
Csaba
Szepesvari
+L+
Research
Group
on
Artificial
Intelligence
+L+
"Jozsef
Attila"
University
+L+
Szeged
6720,
Aradi
vrt
tere
1.
+L+
Hungary
+L+
szepes@sol.cc.u-szeged.hu
+L+
Michael
L.
Littman
+L+
Department
of
Computer
Science
+L+
Duke
University
+L+
Durham,
NC
27708-0129
+L+
mlittman@cs.duke.edu
+L+
December
3,
1997
+L+
Abstract
+L+
Reinforcement
learning
is
the
problem
of
generating
optimal
behavior
in
a
sequential
decision-making
environment
given
the
opportunity
of
+L+
interacting
with
it.
Many
algorithms
for
solving
reinforcement-learning
+L+
problems
work
by
computing
improved
estimates
of
the
optimal
value
+L+
function.
We
extend
prior
analyses
of
reinforcement-learning
algorithms
+L+
and
present
a
powerful
new
theorem
that
can
provide
a
unified
analysis
of
+L+
value-function-based
reinforcement-learning
algorithms.
The
usefulness
+L+
of
the
theorem
lies
in
how
it
allows
the
asynchronous
convergence
of
a
+L+
complex
reinforcement-learning
algorithm
to
be
proven
by
verifying
that
+L+
a
simpler
synchronous
algorithm
converges.
We
illustrate
the
application
+L+
of
the
theorem
by
analyzing
the
convergence
of
Q-learning,
model-based
+L+
reinforcement
learning,
Q-learning
with
multi-state
updates,
Q-learning
+L+
for
Markov
games,
and
risk-sensitive
reinforcement
learning.
+L+
1
Introduction

An
Extensible
End-to-End
Protocol
and
Framework
+L+
K.
L.
Calvert
+L+
R.
H.
Kravets
+L+
R.
D.
Krupczak
+L+
College
of
Computing
+L+
Georgia
Institute
of
Technology
+L+
Atlanta,
Georgia,
USA
+L+
fcalvert,robink,rdkg@cc.gatech.edu
+L+
Abstract
+L+
We
describe
a
framework
for
composing
end-to-end
protocol
functions.
The
framework
comprises:
+L+
a
generic
model
of
protocol
processing;
a
metaheader
protocol
supporting
per-packet
configuration
+L+
of
protocol
function
and
efficient
demultiplexing
of
incoming
data
units;
and
an
extensible
set
of
+L+
modular
protocol
functions.
This
paper
describes
the
pieces
of
the
framework
and
motivates
some
+L+
of
the
design
decisions.
+L+
1
Introduction
and
Motivations

PARALLEL
AND
DISTRIBUTED
SIMULATION
+L+
Richard
M.
Fujimoto
+L+
College
of
Computing
+L+
Georgia
Institute
of
Technology
+L+
Atlanta,
Georgia
30332-0280,
U.S.A.
+L+
ABSTRACT
+L+
Research
and
development
efforts
in
the
parallel
and
+L+
distributed
simulation
field
over
the
last
15
years
+L+
has
progressed,
largely
independently,
in
two
separate
camps:
the
largely
academic
high
performance
+L+
Parallel
And
Distributed
(discrete
event)
Simulation
(PADS)
community,
and
the
DoD-centered
Distributed
Interactive
Simulation
(DIS)
community.
+L+
This
tutorial
gives
an
overview
and
comparison
of
+L+
work
in
these
two
areas,
emphasizing
issues
related
to
+L+
distributed
execution
where
these
fields
have
the
most
+L+
overlap.
Differences
in
the
fundamental
assumptions
+L+
routinely
used
within
each
community
are
contrasted,
+L+
followed
by
overviews
of
work
in
each
community.
+L+
1
INTRODUCTION

Evaluating
Blocking
Probability
in
Generalized
+L+
Connectors
+L+
Ellen
Witte
Zegura
+L+
Abstract|
Generalized
connectors
provide
the
capability
to
connect
a
single
input
to
one
or
more
outputs.
Such
networks
play
an
important
role
in
supporting
any
application
that
involves
the
distribution
of
information
from
one
source
to
many
destinations
or
many
sources
to
many
destinations.
We
present
the
first
analytic
model
for
evaluating
blocking
probability
in
generalized
connectors.
The
model
allows
flexibility
in
specifying
traffic
fanout
characteristics
and
network
routing
algorithms.
Equations
are
derived
for
computing
blocking
probability
for
the
important
class
of
series-parallel
networks.
We
investigate
the
accuracy
of
the
equations
by
comparing
the
blocking
probability
computed
using
the
equations
to
results
from
simulation.
+L+
1
Introduction

Department
of
Computer
Science
+L+
Series
of
Publications
C
+L+
Report
C-1997-15
+L+
Discovery
of
frequent
episodes
in
event
sequences
+L+
Heikki
Mannila,
Hannu
Toivonen,
and
A.
Inkeri
Verkamo
+L+
University
of
Helsinki
+L+
Finland
+L+
+PAGE+

A
structured
document
database
system
+L+
Pekka
Kilpelainen
Greger
Linden
Heikki
Mannila
+L+
Erja
Nikunen
+L+
University
of
Helsinki
+L+
Abstract
+L+
We
describe
a
database
system
for
writing,
editing,
and
querying
structured
documents.
The
structure
of
text
is
described
using
a
context-free
+L+
grammar.
The
operations
are
implemented
using
a
powerful
query
language.
The
system
supports
the
use
of
user-defined
multiple
views
of
the
+L+
documents:
one
view
can
contain
all
the
structure
explicitly,
while
another
+L+
can
contain
only
part
of
the
document
and
have
only
part
of
the
structure
visible.
This
makes
the
system
flexible
for
different
editing
tasks.
+L+
The
system
is
implemented
in
C
using
a
relational
database
system.
+L+
1
Introduction

??,
??,
1-10
(??)
+L+
c
??
Kluwer
Academic
Publishers,
Boston.
Manufactured
in
The
Netherlands.
+L+
Unifying
Two-View
and
Three-View
Geometry
+L+
SHAI
AVIDAN,
AMNON
SHASHUA
+L+
favidan,shashuag@cs.huji.ac.il
+L+
Institute
of
Computer
Science,
The
Hebrew
University,
Jerusalem
91904,
Israel
+L+
Received
??.
Revised
??.
+L+
Abstract.
The
core
of
multiple-view
geometry
is
governed
by
the
fundamental
matrix
and
the
trilinear
+L+
tensor.
In
this
paper
we
unify
both
representations
by
first
re-deriving
the
fundamental
matrix
as
a
rank
+L+
deficient
tensor,
and
secondly
by
deriving
a
unified
set
of
operators
that
are
transparent
to
the
number
of
+L+
views.
As
a
result,
we
show
that
the
basic
building
block
of
the
geometry
of
multiple
views
is
the
trilinear
+L+
tensor
of
three
views
and
that
this
tensor
specializes
to
the
fundamental
matrix
(in
it's
tensor
form)
in
+L+
the
case
of
two
views.
The
properties
of
the
tensor
(geometric
interpretation,
contraction
properties,
+L+
etc.)
are
independent
of
the
number
of
views
(two
or
three).
As
a
byproduct,
every
two-view
algorithm
+L+
can
be
considered
as
a
degenerate
three-view
algorithm
and
three-view
algorithms
can
work
with
either
+L+
two
or
three
images,
all
using
one
standard
set
of
tensor
operations.
To
highlight
the
usefulness
of
this
+L+
paradigm
we
provide
two
practical
applications.
First
we
present
a
novel
view
synthesis
algorithm
that
+L+
starts
with
the
fundamental
matrix
(in
its
tensor
form)
and
seamlessly
move
to
the
general
trilinear
+L+
tensor,
all
using
one
set
of
tensor
operations.
The
second
application
is
a
camera
stabilization
algorithm,
+L+
originally
introduced
for
three
views,
now
working
with
two
views
without
modification.
+L+
1.
Introduction

Measure,
Stochasticity,
and
the
Density
+L+
of
Hard
Languages
+L+
TR
92-13
+L+
Jack
H.
Lutz
and
Elvira
Mayordomo
+L+
May
1992
+L+
Iowa
State
University
of
Science
and
Technology
+L+
Department
of
Computer
Science
+L+
226
Atanasoff
+L+
Ames,
IA
50011
+L+
+PAGE+

Coordination
and
Control
Structures
and
Processes:
+L+
Possibilities
for
Connectionist
Networks
(CN)
+L+
Vasant
Honavar
&
Leonard
Uhr
+L+
Computer
Sciences
Department
+L+
University
of
Wisconsin-Madison
+L+
Abstract
+L+
The
absence
of
powerful
control
structures
and
processes
that
synchronize,
coordinate,
switch
between,
choose
among,
regulate,
direct,
modulate
interactions
between,
and
+L+
combine
distinct
yet
interdependent
modules
of
large
connectionist
networks
(CN)
is
+L+
probably
one
of
the
most
important
reasons
why
such
networks
have
not
yet
succeeded
at
+L+
handling
difficult
tasks
(e.g.
complex
object
recognition
and
description,
complex
+L+
problem-solving,
planning).
+L+
In
this
paper
we
examine
how
CN
built
from
large
numbers
of
relatively
simple
+L+
neuron-like
units
can
be
given
the
ability
to
handle
problems
that
in
typical
multi-computer
networks
and
artificial
intelligence
programs
along
with
all
other
types
of
+L+
programs
are
always
handled
using
extremely
elaborate
and
precisely
worked
out
central
control
(coordination,
synchronization,
switching,
etc.).
We
point
out
the
several
+L+
mechanisms
for
central
control
of
this
un-brain-like
sort
that
CN
already
have
built
into
+L+
them
albeit
in
hidden,
often
overlooked,
ways.
+L+
We
examine
the
kinds
of
control
mechanisms
found
in
computers,
programs,
fetal
+L+
development,
cellular
function
and
the
immune
system,
evolution,
social
organizations,
+L+
and
especially
brains,
that
might
be
of
use
in
CN.
Particularly
intriguing
suggestions
are
+L+
found
in
the
pacemakers,
oscillators,
and
other
local
sources
of
the
brain's
complex
partial
synchronies;
the
diffuse,
global
effects
of
slow
electrical
waves
and
neurohormones;
+L+
the
developmental
program
that
guides
fetal
development;
communication
and
coordination
within
and
among
living
cells;
the
working
of
the
immune
system;
the
evolutionary
+L+
processes
that
operate
on
large
populations
of
organisms;
and
the
great
variety
of
partially
competing
partially
cooperating
controls
found
in
small
groups,
organizations,
and
+L+
larger
societies.
All
these
systems
are
rich
in
control
but
typically
control
that
emerges
+L+
from
complex
interactions
of
many
local
and
diffuse
sources.
We
explore
how
several
+L+
different
kinds
of
plausible
control
mechanisms
might
be
incorporated
into
CN,
and
+L+
assess
their
potential
benefits
with
respect
to
their
cost.
+L+
Introduction

EFFICIENT
COMPILATION
AND
PROFILE-DRIVEN
+L+
DYNAMIC
RECOMPILATION
IN
SCHEME
+L+
Robert
G.
Burger
+L+
Submitted
to
the
faculty
of
the
University
Graduate
School
+L+
in
partial
fulfillment
of
the
requirements
+L+
for
the
degree
+L+
Doctor
of
Philosophy
+L+
in
the
Department
of
Computer
Science,
+L+
Indiana
University
+L+
March
1997
+L+
+PAGE+

Using
Goals
and
Experience
to
Guide
Abduction
+L+
David
B.
Leake
+L+
leake@cs.indiana.edu
+L+
Technical
Report
#359
+L+
Department
of
Computer
Science,
Indiana
University
+L+
Lindley
Hall
215,
Bloomington,
IN
47405
+L+
Abstract
+L+
Standard
methods
for
abductive
understanding
are
neutral
to
prior
experience
and
current
goals.
+L+
Candidate
explanations
are
built
from
scratch
by
backwards
chaining,
without
considering
how
+L+
similar
situations
were
previously
explained,
and
selection
of
the
candidate
to
accept
is
based
on
its
+L+
likelihood,
without
considering
the
information
needs
beyond
routine
understanding.
Problems
arise
+L+
when
applying
these
methods
to
everyday
understanding:
The
vast
range
of
possible
explanations
+L+
makes
it
difficult
to
control
the
cost
of
explanation
construction
and
to
assure
that
the
explanations
+L+
generated
will
actually
be
useful.
+L+
We
argue
that
these
problems
can
be
overcome
by
using
goals
and
experience
to
guide
both
+L+
explanation
generation
and
evaluation.
Our
work
is
within
the
framework
of
case-based
explanation,
which
builds
explanations
by
retrieving
and
adapting
prior
explanations
stored
in
memory.
+L+
We
substantiate
our
model
by
describing
mechanisms
that
enable
it
to
effectively
generate
good
+L+
explanations.
First,
we
demonstrate
that
there
exists
a
theory
of
anomaly
and
explanation
that
can
+L+
guide
retrieval
of
relevant
explanations.
Second,
we
present
a
plausibility
evaluation
process
that
+L+
efficiently
detects
conflicts
and
confirmations
of
an
explanation's
assumptions
by
prior
patterns,
+L+
making
it
possible
to
focus
explanation
adaptation
when
retrieved
explanations
are
implausible.
+L+
Third,
we
present
methods
for
judging
whether
explanations
provide
the
information
needed
to
satisfy
explainer
goals
beyond
routine
understanding.
By
reflecting
experience
and
goals
in
the
search
+L+
for
explanations,
case-based
explanation
provides
a
practical
mechanism
for
guiding
search
towards
+L+
explanations
that
are
both
plausible
and
useful.
+L+
1
The
work
described
here
was
supported
in
part
by
the
Defense
Advanced
Research
Projects
Agency,

Proceedings
of
the
Fifteenth
International
Joint
Conference
on
Artificial
Intelligence,
Morgan
Kaufmann,
San
Francisco,
1997.
+L+
Learning
to
Integrate
Multiple
Knowledge
Sources
+L+
for
Case-Based
Reasoning
+L+
David
B.
Leake,
Andrew
Kinley,
and
David
Wilson
+L+
Computer
Science
Department
+L+
Lindley
Hall
215,
Indiana
University
+L+
Bloomington,
IN
47405,
U.S.A.
+L+
fleake,
akinley,
davwilsg@cs.indiana.edu
+L+
Abstract
+L+
The
case-based
reasoning
process
depends
on
+L+
multiple
overlapping
knowledge
sources,
each
+L+
of
which
provides
an
opportunity
for
learning.
Exploiting
these
opportunities
requires
+L+
not
only
determining
the
learning
mechanisms
+L+
to
use
for
each
individual
knowledge
source,
+L+
but
also
how
the
different
learning
mechanisms
interact
and
their
combined
utility.
This
+L+
paper
presents
a
case
study
examining
the
+L+
relative
contributions
and
costs
involved
in
+L+
learning
processes
for
three
different
knowledge
sources|cases,
case
adaptation
knowledge,
and
similarity
information|in
a
case-based
planner.
It
demonstrates
the
importance
+L+
of
interactions
between
different
learning
processes
and
identifies
a
promising
method
for
integrating
multiple
learning
methods
to
improve
+L+
case-based
reasoning.
+L+
1
Introduction

3-D
Stereo
Using
Photometric
Ratios
+L+
Lawrence
B.
Wolff
+L+
Elli
Angelopoulou
+L+
Computer
Vision
Laboratory
+L+
Department
of
Computer
Science
+L+
The
Johns
Hopkins
University
+L+
Baltimore,
MD
21218
+L+
ABSTRACT
+L+
We
present
a
novel
robust
methodology
for
corresponding
a
dense
set
of
points
on
an
+L+
object
surface
from
photometric
values,
for
3-D
stereo
computation
of
depth.
The
methodology
utilizes
multiple
stereo
pairs
of
images,
each
stereo
pair
taken
of
exactly
the
same
+L+
scene
but
under
different
illumination.
With
just
2
stereo
pairs
of
images
taken
respectively
for
2
different
illumination
conditions,
a
stereo
pair
of
ratio
images
can
be
produced;
one
for
the
ratio
of
left
images,
and
one
for
the
ratio
of
right
images.
We
+L+
demonstrate
how
the
photometric
ratios
composing
these
images
can
be
used
for
accurate
+L+
correspondence
of
object
points.
Object
points
having
the
same
photometric
ratio
with
+L+
respect
to
2
different
illumination
conditions
comprise
a
well-defined
equivalence
class
of
+L+
physical
constraints
defined
by
local
surface
orientation
relative
to
illumination
conditions.
We
formally
show
that
for
diffuse
reection
the
photometric
ratio
is
invariant
to
+L+
varying
camera
characteristics,
surface
albedo,
and
viewpoint
and
that
therefore
the
same
+L+
photometric
ratio
in
both
images
of
a
stereo
pair
implies
the
same
equivalence
class
of
+L+
physical
constraints.
Corresponding
photometric
ratios
along
epipolar
lines
in
a
stereo
pair
+L+
of
images
under
different
illumination
conditions
is
therefore
a
robust
correspondence
of
+L+
equivalent
physical
constraints,
and
determination
of
depth
from
stereo
can
be
performed
+L+
without
explicitly
knowing
what
these
physical
constraints
being
corresponded
actually
+L+
are.
This
implies
a
very
practical
shape-from-stereo
methodology
applicable
to
perspective
views
and
not
requiring
any
knowledge
whatsoever
of
illumination
conditions.
This
is
+L+
particularly
practical
for
determination
of
3-D
shape
on
smooth
featureless
surfaces
which
+L+
has
previously
been
hard
to
perform
using
stereo.
We
demonstrate
experimental
3-D
shape
+L+
determination
from
a
dense
set
of
points
using
our
stereo
technique
on
smooth
objects
of
+L+
known
ground
truth
shape
that
are
accurate
to
well
within
1%
depth
accuracy.
+L+
+PAGE+

Deterministic
Sorting
+L+
in
Nearly
Logarithmic
Time
+L+
on
the
Hypercube
+L+
and
Related
Computers
+L+
Robert
Cypher
+L+
IBM
Almaden
Research
Center
+L+
650
Harry
Rd.
+L+
San
Jose,
CA
95120
+L+
C.
Greg
Plaxton
+L+
MIT
Laboratory
for
Computer
Science
+L+
545
Technology
Square
+L+
Cambridge,
MA
02139
+L+
November
29,
1995
+L+
Abstract
+L+
This
paper
presents
a
deterministic
sorting
algorithm,
called
Sharesort,
that
sorts
n
+L+
records
on
an
n-processor
hypercube,
shu*e-exchange,
or
cube-connected
cycles
in
+L+
O(log
n
(log
log
n)
2
)
time
in
the
worst
case.
The
algorithm
requires
only
a
constant
+L+
amount
of
storage
at
each
processor.
The
fastest
previous
deterministic
algorithm
for
+L+
this
problem
was
Batcher's
bitonic
sort,
which
runs
in
O(log
2
n)
time.
+L+
Supported
by
an
NSERC
postdoctoral
fellowship,
and
DARPA
contracts
N00014-87-K-825
and
N00014
+L+
89-J-1988.
+L+
+PAGE+

The
Royal
Tree
Problem,
a
Benchmark
for
Single
and
+L+
Multi-population
Genetic
Programming
+L+
appears
in
"Advances
in
Genetic
Programming
II",
MIT
Press,
Pete
Angeline
and
+L+
Kim
Kinnear,
editors
+L+
Bill
Punch,
Doug
Zongker,
and
Erik
Goodman
+L+
We
report
on
work
done
to
develop
a
benchmark
problem
for
genetic
programming,
both
+L+
as
a
difficult
problem
to
test
GP
abilities
and
as
a
platform
for
tuning
GP
parameters.
+L+
This
benchmark,
the
royal
tree,
is
a
function
that
accounts
for
tree
shape
as
part
of
its
+L+
evaluation
function,
thus
it
controls
for
a
parameter
not
often
found
in
the
GP
literature.
+L+
It
also
is
a
progressive
function,
allowing
the
user
to
set
the
difficulty
of
the
problem
+L+
attempted.
We
not
only
describe
the
function,
but
also
report
on
results
of
using
island
+L+
parallelism
for
solving
GP
problems.
The
results
obtained
are
somewhat
surprising,
as
it
+L+
appears
that
a
single
large
population
outperforms
a
group
of
smaller
populations
under
+L+
all
the
conditions
tested.
+L+
15.1
Introduction

Inductive
Constraint
Logic
+L+
and
the
Mutagenesis
Problem
+L+
Wim
Van
Laer
Hendrik
Blockeel
+L+
Luc
De
Raedt
+L+
Department
of
Computer
Science,
Katholieke
Universiteit
Leuven
+L+
Celestijnenlaan
200A,
B-3001
Heverlee,
Belgium
+L+
Email:fWimV,Hendrik,LucDRg@cs.kuleuven.ac.be
+L+
Abstract
+L+
A
novel
approach
to
learning
first
order
logic
formulae
from
positive
and
negative
examples
is
incorporated
in
a
system
named
ICL
(Inductive
Constraint
+L+
Logic).
In
ICL,
examples
are
viewed
as
interpretations
which
are
true
or
false
+L+
for
the
target
theory,
whereas
in
present
inductive
logic
programming
systems,
+L+
examples
are
true
and
false
ground
facts
(or
clauses).
Furthermore,
ICL
uses
a
+L+
clausal
representation,
which
corresponds
to
a
conjunctive
normal
form
where
+L+
each
conjunct
forms
a
constraint
on
positive
examples,
whereas
classical
learning
+L+
techniques
have
concentrated
on
concept
representations
in
disjunctive
normal
+L+
form.
+L+
We
present
some
experiments
with
this
new
system
on
the
mutagenesis
problem.
These
experiments
illustrate
some
of
the
differences
with
other
systems,
+L+
and
indicate
that
our
approach
should
work
at
least
as
well
as
the
more
classical
+L+
approaches.
+L+
1
Introduction

Statistical
Characteristics
and
Multiplexing
of
MPEG
Streams
+L+
Marwan
Krunz
,
Ron
Sass
,
and
Herman
Hughes
+L+
Department
of
Electrical
Engineering
+L+
Department
of
Computer
Science
+L+
Michigan
State
University
+L+
East
Lansing,
MI
48824
+L+
Abstract
+L+
This
paper
presents
a
study
of
the
statistical
characteristics
and
multiplexing
of
Variable-Bit-Rate
(VBR)
+L+
MPEG-coded
video
streams.
Our
results
are
based
on
+L+
23
minutes
of
video
obtained
from
the
entertainment
+L+
movie,
The
Wizard
of
Oz.
The
experimental
setup
+L+
which
was
used
to
capture,
digitize,
and
compress
the
+L+
video
stream
is
described.
Although
the
study
is
conducted
at
the
frame
level
(as
opposed
to
the
slice
level),
+L+
it
is
observed
that
the
inter-frame
correlation
structure
for
the
frame-size
sequence
involves
complicated
+L+
forms
of
pseudo-periodicity
that
are
mainly
affected
+L+
by
the
compression
pattern
of
the
sequence.
A
simple
model
for
an
MPEG
traffic
source
is
developed
in
+L+
which
frames
are
generated
according
to
the
compression
pattern
of
the
original
captured
video
stream.
The
+L+
number
of
cells
per
frame
is
fitted
by
a
lognormal
distribution.
Simulations
are
used
to
study
the
performance
of
an
ATM
multiplexer
for
MPEG
streams.
+L+
1
Introduction

An
Agent-Based
Approach
+L+
for
Robot
Vision
System
+L+
Technical
Report
95/34
+L+
Tak
Keung
CHENG
+L+
Leslie
KITCHEN
+L+
Zhi-Qiang
LIU
+L+
James
COOPER
+L+
+PAGE+

Termination
Analysis
for
Mercury
+L+
Chris
Speirs,
Zoltan
Somogyi
and
Harald
Stndergaard
+L+
Department
of
Computer
Science
+L+
The
University
of
Melbourne
+L+
Parkville,
Victoria
3052,
Australia
+L+
Abstract
+L+
Since
the
late
eighties,
much
progress
has
been
made
in
the
theory
of
termination
analysis
for
+L+
logic
programs.
However,
from
a
practical
point
of
view,
the
significance
of
much
of
the
work
+L+
on
termination
is
hard
to
judge,
since
experimental
evaluations
rarely
get
published.
Here
we
+L+
describe
and
evaluate
a
termination
analyzer
for
Mercury,
a
strongly
typed
and
moded
logic-
+L+
functional
programming
language.
Mercury's
high
degree
of
referential
transparency
and
the
+L+
guaranteed
availability
of
reliable
mode
information
simplify
the
termination
analysis
of
Mer-
+L+
cury
compared
with
that
of
other
logic
programming
languages.
We
describe
our
termination
+L+
analyzer,
which
uses
a
variant
of
a
method
developed
by
Plumer.
It
deals
with
full
Mercury,
+L+
including
modules,
declarative
input/output,
the
foreign
language
interface,
and
higher-order
+L+
features.
In
spite
of
these
obstacles,
it
produces
high-quality
termination
information,
comparable
to
the
results
recently
obtained
by
Lindenstrauss
and
Sagiv.
Most
important,
in
stark
+L+
contrast
with
Lindenstrauss
and
Sagiv's
experimental
results,
our
analyzer
has
a
negligible
+L+
impact
on
the
running
time
of
the
compiler
of
which
it
is
part,
even
for
large
programs.
This
+L+
means
that
the
Mercury
compiler
can
produce
valuable
termination
information
at
no
real
+L+
cost
to
the
programmer.
+L+
1
Introduction

Methods
for
Handling
Faults
and
Asynchrony
+L+
in
Parallel
Computation
+L+
Z.
M.
Kedem
+L+
1.
Introduction
and
Motivation

The
Development
of
the
C
Language
+L+
Dennis
M.
Ritchie
+L+
AT&T
Bell
Laboratories
+L+
Murray
Hill,
NJ
07974
USA
+L+
dmr@research.att.com
+L+
ABSTRACT
+L+
The
C
programming
language
was
devised
in
the
early
1970s
as
a
system
+L+
implementation
language
for
the
nascent
Unix
operating
system.
Derived
from
+L+
the
typeless
language
BCPL,
it
evolved
a
type
structure;
created
on
a
tiny
+L+
machine
as
a
tool
to
improve
a
meager
programming
environment,
it
has
become
+L+
one
of
the
dominant
languages
of
today.
This
paper
studies
its
evolution.
+L+
Introduction

DISTRIBUTED
CONTROL
IN
OPTICAL
WDM
NETWORKS
+L+
X.
Yuan
R.
Gupta
R.
Melhem
+L+
Department
of
Computer
Science
+L+
University
of
Pittsburgh
+L+
Pittsburgh,
PA
15260
+L+
ABSTRACT
+L+
This
paper
describes
and
evaluates
distributed
+L+
wavelength
reservation
protocols
for
all-optical
WDM
+L+
networks.
These
protocols
are
essential
for
applying
+L+
WDM
techniques
to
large
scale
all-optical
networks.
+L+
The
protocols
ensure
that
the
wavelengths
on
the
links
+L+
along
a
path
are
reserved
before
communication
takes
+L+
place.
A
message
is
transmitted
using
the
reserved
+L+
wavelengths
and
remains
in
the
optical
domain
utill
+L+
it
reaches
the
destination.
Based
upon
the
timing
at
+L+
which
the
reservation
is
performed,
the
protocols
are
+L+
classified
into
two
categories:
forward
reservation
protocols
and
backward
reservation
protocols.
Although
+L+
forward
reservation
protocols
are
simpler,
our
performance
study
shows
that
backward
reservation
protocols
provide
better
performance.
+L+
INTRODUCTION

Working
Sets,
Cache
Sizes,
and
Node
Granularity
Issues
+L+
for
Large-Scale
Multiprocessors
+L+
Edward
Rothberg
Jaswinder
Pal
Singh
and
Anoop
Gupta
+L+
Intel
Supercomputer
Systems
Division
Computer
Systems
Laboratory
+L+
14924
N.W.
Greenbrier
Parkway
Stanford
University
+L+
Beaverton,
OR
97006
Stanford,
CA
94305
+L+
Abstract
+L+
The
distribution
of
resources
among
processors,
memory
and
+L+
caches
is
a
crucial
question
faced
by
designers
of
large-scale
+L+
parallel
machines.
If
a
machine
is
to
solve
problems
with
a
+L+
certain
data
set
size,
should
it
be
built
with
a
large
number
of
+L+
processors
each
with
a
small
amount
of
memory,
or
a
smaller
+L+
number
of
processors
each
with
a
large
amount
of
memory?
+L+
How
much
cache
memory
should
be
provided
per
processor
for
+L+
cost-effectiveness?
And
how
do
these
decisions
change
as
larger
+L+
problems
are
run
on
larger
machines?
+L+
In
this
paper,
we
explore
the
above
questions
based
on
the
+L+
characteristics
of
five
important
classes
of
large-scale
parallel
scientific
applications.
We
first
show
that
all
the
applications
have
a
hierarchy
of
well-defined
per-processor
working
+L+
sets,
whose
size,
performance
impact
and
scaling
characteristics
+L+
can
help
determine
how
large
different
levels
of
a
multiprocessor's
cache
hierarchy
should
be.
Then,
we
use
these
working
sets
together
with
certain
other
important
characteristics
of
+L+
the
applications|such
as
communication
to
computation
ratios,
+L+
concurrency,
and
load
balancing
behavior|to
reflect
upon
the
+L+
broader
question
of
the
granularity
of
processing
nodes
in
high-performance
multiprocessors.
+L+
We
find
that
very
small
caches
whose
sizes
do
not
increase
+L+
with
the
problem
or
machine
size
are
adequate
for
all
but
two
of
+L+
the
application
classes.
Even
in
the
two
exceptions,
the
working
+L+
sets
scale
quite
slowly
with
problem
size,
and
the
cache
sizes
+L+
needed
for
problems
that
will
be
run
in
the
foreseeable
future
+L+
are
small.
We
also
find
that
relatively
fine-grained
machines,
+L+
with
large
numbers
of
processors
and
quite
small
amounts
of
+L+
memory
per
processor,
are
appropriate
for
all
the
applications.
+L+
1
Introduction

Technical
Report
TR-514-96.
+L+
Irregular
Applications
under
Software
Shared
Memory
+L+
Liviu
Iftode,
Jaswinder
Pal
Singh
and
Kai
Li
+L+
Department
of
Computer
Science
+L+
Princeton
University
+L+
Princeton,
NJ
08544
+L+
liv,jps,li@cs.princeton.edu
+L+
Abstract
+L+
Shared
Virtual
Memory
(SVM)
provides
an
inexpensive
way
to
support
the
popular
shared
address
+L+
space
programming
model
on
networks
of
workstations
or
personal
computers.
Despite
recent
advances
+L+
in
SVM
systems,
their
performance
for
all
but
coarse-grained
or
regular
applications
is
not
well
understood.
+L+
Nor
is
there
an
understanding
of
whether
and
how
+L+
fine-grained,
irregular
programs
should
be
written
differently
for
SVM,
with
its
large
granularities
of
communication
and
coherence,
than
for
the
more
familiar
+L+
hardware
coherent
at
cache
line
granularity.
In
this
+L+
paper
we
try
to
understand
the
performance
and
programming
issues
for
emerging,
irregular
applications
+L+
on
SVM
systems.
We
examine
performance
on
both
+L+
an
aggressive
all-software
system
as
well
as
one
with
a
+L+
little
hardware
support
in
the
network
interface.
We
+L+
also
present
approaches
to
improve
the
performance
+L+
of
irregular
applications
at
both
the
programming
and
+L+
the
system
level.
As
a
result
of
our
experiences,
we
+L+
identify
a
set
of
guidelines
and
techniques
that
pertain
+L+
specifically
to
programming
SVM
systems,
beyond
the
+L+
guidelines
commonly
used
for
programming
hardware-coherent
systems
as
well.
We
also
present
a
further
+L+
relaxation
of
the
memory
consistency
model,
called
+L+
scope
consistency,
which
is
particularly
effective
for
+L+
such
applications.
+L+
1
Introduction

Database
and
Display
Algorithms
for
+L+
Interactive
Visualization
of
Architectural
Models
+L+
by
+L+
Thomas
Allen
Funkhouser
+L+
B.S.
(Stanford
University)
1983
+L+
M.S.
(University
of
California
at
Los
Angeles)
1989
+L+
A
dissertation
submitted
in
partial
satisfaction
of
the
+L+
requirements
for
the
degree
of
+L+
Doctor
of
Philosophy
+L+
in
+L+
Computer
Science
+L+
in
the
+L+
GRADUATE
DIVISION
+L+
of
the
+L+
UNIVERSITY
of
CALIFORNIA
at
BERKELEY
+L+
Committee
in
charge:
+L+
Professor
Carlo
H.
Sequin
,
Chair
+L+
Professor
Lawrence
Rowe
+L+
Professor
Jean
Pierre
Protzen
+L+
1993
+L+
+PAGE+

DIMACS
Technical
Report
98-10
+L+
February
1998
+L+
Patience
is
a
Virtue:
The
Effect
of
Delay
on
+L+
Competitiveness
for
Admission
Control
+L+
by
+L+
Michael
H.
Goldwasser
1
+L+
Department
of
Computer
Science
+L+
Princeton
University
+L+
Princeton,
NJ
08544
+L+
wass@cs.princeton.edu
+L+
1
Permanent
Member
+L+
DIMACS
is
a
partnership
of
Rutgers
University,
Princeton
University,
AT&T
Labs-Research,
+L+
Bell
Labs
and
Bellcore.
+L+
DIMACS
is
an
NSF
Science
and
Technology
Center,
funded
under
contract
STC-91-19999;
+L+
and
also
receives
support
from
the
New
Jersey
Commission
on
Science
and
Technology.
+L+
+PAGE+

Investigating
+L+
Genetic
Algorithms
+L+
for
Scheduling
+L+
Hsiao-Lan
Fang
+L+
MSc
Dissertation
+L+
Department
of
Artificial
Intelligence
+L+
University
of
Edinburgh
+L+
1992
+L+
+PAGE+

A
Color-based
Technique
for
Measuring
Visible
Loss
for
Use
+L+
in
Image
Data
Communication
1
+L+
Melliyal
Annamalai,
Aurobindo
Sundaram
and
Bharat
Bhargava
+L+
Department
of
Computer
Sciences
+L+
Purdue
University
+L+
W.
Lafayette,
IN
47906,
USA
+L+
fmelli,auro,bbg@cs.purdue.edu
+L+
Abstract
+L+
The
concept
of
the
global
information
infrastructure
and
specifically
that
of
the
World
+L+
Wide
Web
(WWW)
has
led
to
users
accessing
data
of
different
media
including
images
+L+
and
video
data
over
a
wide
area
network.
These
data
objects
have
sizes
the
order
of
+L+
megabytes
and
communication
time
is
very
large.
The
data
size
can
be
reduced
without
+L+
losing
information
by
applying
loss-inducing
techniques
and
this
will
lead
to
reduction
in
+L+
communication
time.
Several
loss-inducing
techniques
have
been
developed
and
each
image
+L+
is
treated
differently
by
each
technique.
In
some
cases
an
acceptable
quality
of
the
image
+L+
is
obtained
and
in
some
cases
it
is
not.
In
this
paper
we
develop
a
color-based
technique
+L+
to
quantify
the
data
loss
when
a
loss-inducing
technique
is
applied
to
an
image.
This
will
+L+
result
in
estimating
whether
the
resulting
image
is
indistinguishable
from
the
original
with
+L+
respect
to
the
human
eye.
We
illustrate
its
use
to
classify
images
according
to
the
loss
they
+L+
can
tolerate.
This
avoids
redundant
communication
of
a
high
quality
image
when
a
lower
+L+
quality
image
can
satisfy
the
application
resulting
in
the
conservation
and
better
usage
+L+
of
network
resources.
We
present
the
technique,
the
communication
time
saved,
and
an
+L+
experimental
evaluation
to
prove
the
validity
of
the
technique.
+L+
1
Introduction

PYTHIA:
A
Knowledge
Based
System
+L+
for
Intelligent
Scientific
Computing
+L+
Sanjiva
Weerawarana,
Elias
N.
Houstis,
John
R.
Rice,
Anupam
Joshi
+L+
Purdue
University
+L+
and
+L+
Catherine
E.
Houstis
+L+
University
of
Crete
+L+
Categories
and
Subject
Descriptors:
I.2.1
[Artificial
Intelligence]:
Applications
and
Expert
+L+
Systems;
G.1.8
[Numerical
Analysis]:
Partial
Differential
Equations
+L+
General
Terms:
Knowledge
Based
Systems
+L+
Additional
Key
Words
and
Phrases:
Computational
Intelligence,
Knowledge
Based
Systems,
Partial
Differential
Equations,
Performance
Evaluation,
Problem
Solving
Environments
+L+
1.
ABSTRACT
+L+
Domain
specific
Problem
Solving
Environments
(PSEs)
are
the
key
new
ingredients
that
will
aid
in
the
widespread
use
of
Computational
Science
&
Engineering
+L+
(CS&E)
systems.
Each
PSE
consists
of
a
well
defined
library
that
supports
the
+L+
numerical
and
symbolic
solution
of
certain
mathematical
model(s)
characterizing
a
+L+
specific
discipline,
together
with
an
easy
to
use
software
environment.
This
environment
should
ideally
interact
with
the
user
in
a
language
"natural"
to
the
associated
+L+
discipline,
and
provide
a
high
level
abstraction
of
the
underlying,
computationally
+L+
complex,
model.
However,
it
appears
that
almost
all
extant
PSEs
assume
that
+L+
the
user
is
familiar
with
the
specific
functionality/applicability
of
the
PSE.
Their
+L+
primary
design
objective
is
to
support
some
form
of
high
level
programming
with
+L+
predefined
state-of-the-art
algorithmic
infrastructure.
As
the
functionality
of
these
+L+
systems
increases,
the
user
is
expected
to
make
complex
decisions
in
the
parametric
space
of
the
algorithmic
infrastructure
supported
by
the
PSE.
In
this
paper
+L+
we
describe
a
knowledge
based
system,
PYTHIA,
to
automate
this
decision
making
process
and
aid
in
providing
a
high
level
abstraction
to
the
user.
Specifically,
+L+
PYTHIA
addresses
the
problem
of
(parameter,
algorithm)
pair
selection
within
a
+L+
scientific
computing
domain
assuming
some
minimum
user
specified
computational
+L+
objectives
and
some
characteristics
of
the
given
problem.
PYTHIA's
framework
+L+
and
methodology
is
general
and
applicable
to
any
class
of
scientific
problems
and
+L+
Work
supported
in
part
by
AFOSR
award
91-F49620,
NSF
awards
CCR
86-19817,
CCR
92-02536,
+L+
and
ASC
9404859.
+L+
Authors'
addresses:
S.
Weerawarana,
E.N.
Houstis,
J.R.
Rice
and
A.
Joshi:
Department
of
Computer
Sciences,
Purdue
University,
West
Lafayette,
IN
47907,
USA;
C.E.
Houstis:
Department
of
+L+
Computer
Science,
University
of
Crete,
Heraklion,
Greece.
+L+
+PAGE+

Authorship
Analysis:
Identifying
The
+L+
Author
of
a
Program
1
+L+
Ivan
Krsul
+L+
The
COAST
Project
+L+
Department
of
Computer
Sciences
+L+
Purdue
University
+L+
West
Lafayette,
IN
47907-1398
+L+
krsul@cs.purdue.edu
+L+
May
3,
1994
+L+
Technical
Report
CSD-TR-94-030
+L+
1
This
paper
was
originally
written
as
a
Master's
thesis
at
Purdue
University.
+L+
+PAGE+

A
Coarse-Grained
Parallel
+L+
QR-Factorization
Algorithm
for
+L+
Sparse
Least
Squares
Problems
+L+
Tz.
Ostromsky
P.
C.
Hansen
Z.
Zlatev
+L+
Abstract
+L+
A
sparse
QR-factorization
algorithm
SPARQR
for
coarse-grained
parallel
+L+
computations
is
described.
The
coefficient
matrix,
which
is
assumed
to
be
+L+
general
sparse,
is
reordered
in
an
attempt
to
bring
as
many
zero
elements
in
+L+
the
lower
left
corner
as
possible.
The
reordered
matrix
is
then
partitioned
into
+L+
block
rows,
and
Givens
plane
rotations
are
applied
in
each
block-row.
These
are
+L+
independent
tasks
and
can
be
done
in
parallel.
Row
and
column
permutations
+L+
are
carried
out
within
the
diagonal
blocks
in
an
attempt
to
preserve
better
the
+L+
sparsity
of
the
matrix.
+L+
The
algorithm
can
be
used
for
solving
least
squares
problems
either
directly
+L+
or
combined
with
an
iterative
method
(preconditioned
conjugate
gradients
are
+L+
used).
Small
non-zero
elements
can
optionally
be
dropped
in
the
latter
case.
+L+
This
leads
to
a
better
preservation
of
the
sparsity
and,
therefore,
to
a
faster
+L+
factorization.
The
price
which
has
to
be
paid
is
some
loss
of
accuracy.
The
+L+
iterative
method
is
used
to
regain
the
accuracy
lost
during
the
factorization.
+L+
Numerical
results
from
several
experiments
with
matrices
from
the
well-known
Harwell-Boeing
collection
as
well
as
with
some
larger
sparse
matrices
are
+L+
presented
in
this
work.
An
SGI
Power
Challenge
computer
with
16
processors
+L+
has
been
used
in
the
experiments.
+L+
Keywords:
coarse-grained
parallelism,
least
squares
problem,
QR-factorization,
+L+
general
sparse
matrix,
drop-tolerance,
reordering,
partitioning,
block
algorithm.
+L+
Purdue
University,
Department
of
Computer
Science,
+L+
West
Lafayette,
IN
47907,
USA
+L+
e-mail:
tto@cs.purdue.edu
+L+
Institute
of
Mathematical
Modelling,
Technical
University
of
Denmark,
+L+
Bldg.
305,
DK-2800
Lyngby,
Denmark
+L+
e-mail:
pch@imm.dtu.dk
+L+
National
Environmental
Research
Institute,
Frederiksborgvej
399,
DK-4000
Roskilde,
Denmark
+L+
e-mail:
luzz@sun2.dmu.dk
+L+
+PAGE+

Evaluating
the
Performance
of
Software
Distributed
Shared
Memory
+L+
as
a
Target
for
Parallelizing
Compilers
+L+
Alan
L.
Cox
,
Sandhya
Dwarkadas
,
Honghui
Lu
and
Willy
Zwaenepoel
+L+
Rice
University
+L+
Houston,
TX
77005-1892
+L+
falc,
hhl,
willyg@cs.rice.edu
+L+
University
of
Rochester
+L+
Rochester,
NY14627-0226
+L+
sandhya@cs.rochester.edu
+L+
Abstract
+L+
In
this
paper,
we
evaluate
the
use
of
software
distributed
+L+
shared
memory
(DSM)
on
a
message
passing
machine
as
+L+
the
target
for
a
parallelizing
compiler.
We
compare
this
approach
to
compiler-generated
message
passing,
hand-coded
+L+
software
DSM,
and
hand-coded
message
passing.
For
this
+L+
comparison,
we
use
six
applications:
four
that
are
regular
+L+
and
two
that
are
irregular.
+L+
Our
results
are
gathered
on
an
8-node
IBM
SP/2
using
the
TreadMarks
software
DSM
system.
We
use
the
APR
+L+
shared-memory
(SPF)
compiler
to
generate
the
shared
memory
programs,
and
the
APR
XHPF
compiler
to
generate
message
passing
programs.
The
hand-coded
message
passing
+L+
programs
run
with
the
IBM
PVMe
optimized
message
passing
library.
On
the
regular
programs,
both
the
compiler-generated
and
the
hand-coded
message
passing
outperform
+L+
the
SPF/TreadMarks
combination:
the
compiler-generated
+L+
message
passing
by
5.5%
to
40%,
and
the
hand-coded
+L+
message
passing
by
7.5%
to
49%.
On
the
irregular
programs,
the
SPF/TreadMarks
combination
outperforms
the
+L+
compiler-generated
message
passing
by
38%
and
89%,
and
+L+
only
slightly
underperforms
the
hand-coded
message
passing,
differing
by
4.4%
and
16%.
We
also
identify
the
factors
+L+
that
account
for
the
performance
differences,
estimate
their
+L+
relative
importance,
and
describe
methods
to
improve
the
+L+
performance.
+L+
1.
Introduction

8th
SIAM
Conference
on
Parallel
Processing
for
Scientific
Computing,
Minneapolis,
MN,
March
1997.
+L+
On
the
Accuracy
of
Anderson's
Fast
N-body
Algorithm
+L+
Yu
Charlie
Hu
S.
Lennart
Johnsson
+L+
Abstract
+L+
We
present
an
empirical
study
of
the
accuracy-cost
tradeoffs
of
Anderson's
method.
+L+
The
various
parameters
that
control
the
degree
of
approximation
of
the
computational
+L+
elements
and
the
separateness
of
interacting
computational
elements
govern
both
the
+L+
arithmetic
complexity
and
the
accuracy
of
the
method.
Our
experiment
shows
that
for
+L+
a
given
error
requirement,
using
a
near-field
containing
only
nearest
neighbor
boxes
+L+
and
a
hierarchy
depth
that
minimizes
the
number
of
arithmetic
operations
minimizes
+L+
the
total
number
of
arithmetic
operations.
+L+
1
Introduction

RICE
UNIVERSITY
+L+
Synchronization,
Coherence,
and
Consistency
for
+L+
High
Performance
Shared-Memory
+L+
Multiprocessing
+L+
by
+L+
Sandhya
Dwarkadas
+L+
A
Thesis
Submitted
+L+
in
Partial
Fulfillment
of
the
+L+
Requirements
for
the
Degree
+L+
Doctor
of
Philosophy
+L+
Approved,
Thesis
Committee:
+L+
J
Robert
Jump,
Co-Chairman
+L+
Professor
+L+
Electrical
and
Computer
Engineering
+L+
James
B.
Sinclair,
Co-Chairman
+L+
Associate
Professor
+L+
Electrical
and
Computer
Engineering
+L+
John
K.
Bennett
+L+
Assistant
Professor
+L+
Electrical
and
Computer
Engineering
+L+
Ken
Kennedy
+L+
Professor
+L+
Computer
Science
+L+
Houston,
Texas
+L+
September,
1992
+L+
+PAGE+

On
GDM's:
Geometrically
Deformed
Models
for
the
Extraction
of
Closed
+L+
Shapes
from
Volume
Data
+L+
By
+L+
James
Vradenburg
Miller
+L+
A
Thesis
Submitted
to
the
Graduate
+L+
Faculty
of
Rensselaer
Polytechnic
Institute
+L+
in
Partial
Fulfillment
of
the
+L+
Requirements
for
the
Degree
of
+L+
Master
of
Science
+L+
Approved:
+L+
Dr.
Michael
J.
Wozny
+L+
Thesis
Adviser
+L+
Rensselaer
Polytechnic
Institute
+L+
Troy,
New
York
+L+
December
1990
+L+
+PAGE+

Automated
Design
Optimization
for
the
P2
and
P8
+L+
Hypersonic
Inlets
+L+
Vijay
Shukla
,
Andrew
Gelsey
,
Mark
Schwabacher
+L+
Donald
Smith
,
Doyle
D.
Knight
+L+
Rutgers,
the
State
University
of
New
Jersey
+L+
New
Brunswick,
NJ
08903
+L+
February
20,
1997
+L+
To
appear
in
Journal
of
Aircraft,
Vol
34,
No.
2,
March-April
1997
+L+
Copyright
c
by
Vijay
Shukla,
Andrew
Gelsey,
Mark
Schwabacher,
Donald
Smith
and
Doyle
D.
Knight.
+L+
Abstract
+L+
An
automated
design
methodology
incorporating
industry-standard
Navier-Stokes
+L+
codes
and
a
gradient-based
optimizer
has
been
developed.
This
system
is
used
to
redesign
the
well-known
NASA
P2
and
P8
hypersonic
inlets.
First,
the
Navier-Stokes
+L+
simulations
of
the
original
P2
and
P8
inlet
designs
are
validated
using
numerical
convergence
studies
and
comparison
with
wind-tunnel
experimental
data
for
the
original
+L+
inlets
published
by
NASA
in
the
early
1970s.
Second,
the
P2
and
P8
inlets
are
redesigned
with
the
objective
of
canceling
the
cowl
shock
(and,
in
the
case
of
the
P8
+L+
inlet,
the
additional
cowl-generated
compression)
at
the
centerbody
by
appropriate
+L+
contouring
of
the
centerbody
boundary.
The
original
inlets
were
intended
to
achieve
+L+
these
same
objectives,
but
detailed
experimental
measurements
indicated
that
a
substantial
reflected
shock
system
was
present.
The
choice
of
the
objective
function,
which
+L+
is
used
to
drive
the
optimization,
has
a
significant
impact
on
the
final
design.
Several
+L+
different
formulations
for
the
objective
function
have
been
employed,
and
improvements
of
60%
to
90%
in
the
objective
function
have
been
achieved.
This
automated
+L+
design
system
represents
one
of
the
first
successful
combinations
of
numerical
optimization
methods
with
Reynolds-averaged
Navier-Stokes
fluid
dynamics
simulation
for
high
+L+
speed
inlets,
and
demonstrates
a
new
area
in
which
High
Performance
Computing
may
+L+
have
considerable
impact
on
problems
of
military
and
industrial
significance.
+L+
Postdoctoral
Research
Associate,
Dept.
of
Mechanical
and
Aerospace
Engineering.
AIAA
Member.
+L+
Assistant
Professor,
Computer
Science
Dept.
AIAA
Member.
+L+
Graduate
Student,
Computer
Science
Dept.
+L+
Assistant
Professor,
Computer
Science
Dept.
+L+
Professor,
Dept.
of
Mechanical
and
Aerospace
Engineering.
AIAA
Associate
Fellow.
+L+
+PAGE+

Towards
a
Cost-Effective
Parallel
Data
Mining
Approach
+L+
Zoltan
Jarai,
Aashu
Virmani,
Liviu
Iftode
+L+
Department
of
Computer
Science
+L+
Rutgers
University
+L+
Piscataway,
NJ
08854
+L+
fjarai,avirmanig@paul.rutgers.edu,
iftode@cs.rutgers.edu
+L+
Abstract
+L+
Massive
rule
induction
has
recently
emerged
as
one
of
the
+L+
powerful
data
mining
techniques.
The
problem
is
known
to
+L+
be
exponential
in
the
size
of
the
attributes,
and
given
its
ever
+L+
increasing
use,
can
greatly
benefit
from
parallelization.
+L+
In
this
paper,
we
study
cost-effective
approaches
to
paral-lelize
rule
generation
algorithms.
In
particular,
we
consider
+L+
the
propositional
rule
generation
algorithm
of
the
Discovery
+L+
Board
system,
and
present
our
design
and
implementation
of
+L+
a
parallel
algorithm
for
the
same
task.
We
then
present
some
+L+
early
performance
results
of
our
parallelization
scheme
on
+L+
hardware
and
software
distributed
shared
memory
multiprocessors.
+L+
1
Introduction

Why
Should
Architectural
Principles
be
+L+
Enforced?
+L+
Naftaly
H.
Minsky
+L+
minsky@cs.rutgers.edu
+L+
Department
of
Computer
Science
+L+
Rutgers
University
+L+
New
Brunswick,
NJ,
08903
USA
+L+
August
12,
1998
+L+
Abstract
+L+
There
is
an
emerging
consensus
that
an
explicit
architectural
model
+L+
would
be
invaluable
for
large
evolving
software
systems,
providing
them
+L+
with
a
framework
within
which
such
a
system
can
be
reasoned
about
and
+L+
maintained.
But
the
great
promise
of
architectural
models
has
not
been
+L+
fulfilled
so
far,
due
to
a
gap
between
the
model
and
the
system
it
purports
+L+
to
describe.
It
is
our
contention
that
this
gap
is
best
bridged
if
the
model
+L+
is
not
just
stated,
but
is
enforced.
+L+
This
gives
rise
to
a
concept
enforced
architectural
model
|or,
a
law
|
+L+
which
is
explored
in
this
paper.
We
argue
that
this
model
has
two
major
beneficial
consequences:
First,
by
bridging
the
above
mentioned
gap
+L+
between
an
architectural
model
and
the
actual
system,
an
enforced
architectural
model
provides
a
truly
reliable
framework
within
which
a
system
+L+
can
be
reasoned
about
and
maintained.
Second,
our
model
provides
software
developers
with
a
carefully
circumscribed
flexibility
in
molding
the
+L+
law
of
a
project,
during
its
evolutionary
lifetime|while
maintaining
certain
architectural
principles
as
invariant
of
evolution.
+L+
Keywords:
architectural
model,
law-governed
software,
evolution,
in
+L+
variants
of
evolution,
firewalls,
protection.
+L+
Work
supported
in
part
by
NSF
grants
No.
CCR-9308773
+L+
+PAGE+

Learning
Novel
Domains
Through
Curiosity
and
Conjecture
+L+
Paul
D.
Scott
&
Shaul
Markovitch
+L+
Center
for
Machine
Intelligence
+L+
2001,Commonwealth
Blvd.,
+L+
Ann
Arbor,
Michigan
48105
+L+
Abstract
+L+
This
paper
describes
DIDO,
a
system
we
have
+L+
developed
to
carry
out
exploratory
learning
of
+L+
unfamiliar
domains
without
assistance
from
an
+L+
external
teacher.
The
program
incorporates
novel
+L+
approaches
to
experience
generation
and
representation
+L+
generation.
The
experience
generator
uses
a
heuristic
+L+
based
on
Shannon's
uncertainty
function
to
find
+L+
informative
examples.
The
representation
generator
+L+
makes
conjectures
on
the
basis
of
small
amounts
of
+L+
evidence
and
retracts
them
if
they
prove
to
be
wrong
+L+
or
useless.
A
number
of
experiments
are
described
+L+
which
demonstrate
that
the
system
can
distribute
its
+L+
learning
resources
to
steadily
acquire
a
good
+L+
representation
of
the
whole
of
a
domain,
and
that
the
+L+
system
can
readily
acquire
both
disjunctive
and
+L+
conjunctive
concepts
even
in
the
presence
of
noise.
+L+
1.
Introduction

Practical
PAC
Learning
+L+
Dale
Schuurmans
+L+
Department
of
Computer
Science
+L+
University
of
Toronto
+L+
Toronto,
Ontario
M5S
1A4,
Canada
+L+
dale@cs.toronto.edu
+L+
Russell
Greiner
+L+
Siemens
Corporate
Research
+L+
Princeton,
NJ
08540,
USA
+L+
greiner@scr.siemens.com
+L+
Appears
in
+L+
Proceedings
of
the
Fourteenth
International
Conference
on
Artificial
Intelligence
(IJCAI-95),
+L+
Montreal,
August
1995.
+L+
Abstract
+L+
We
present
new
strategies
for
"probably
approximately
correct"
(pac)
learning
that
use
+L+
fewer
training
examples
than
previous
approaches.
The
idea
is
to
observe
training
examples
one-at-a-time
and
decide
"on-line"
when
to
+L+
return
a
hypothesis,
rather
than
collect
a
large
+L+
fixed-size
training
sample.
This
yields
sequential
learning
procedures
that
pac-learn
by
observing
a
small
random
number
of
examples.
+L+
We
provide
theoretical
bounds
on
the
expected
+L+
training
sample
size
of
our
procedure
|
but
establish
its
efficiency
primarily
by
a
series
of
experiments
which
show
sequential
learning
actually
uses
many
times
fewer
training
examples
in
+L+
practice.
These
results
demonstrate
that
pac-learning
can
be
far
more
efficiently
achieved
in
+L+
practice
than
previously
thought.
+L+
1
Introduction

Learning
Useful
Horn
Approximations
+L+
Russell
Greiner
+L+
755
College
Road
East
+L+
Siemens
Corporate
Research
+L+
Princeton,
NJ
08540
+L+
greiner@learning.siemens.com
+L+
Dale
Schuurmans
+L+
Department
of
Computer
Science
+L+
University
of
Toronto
+L+
Toronto,
Ontario
M5S
1A4
+L+
dale@cs.toronto.edu
+L+
Abstract
+L+
While
the
task
of
answering
queries
from
an
+L+
arbitrary
propositional
theory
is
intractable
in
+L+
general,
it
can
typically
be
performed
efficiently
+L+
if
the
theory
is
Horn.
This
suggests
that
it
+L+
may
be
more
efficient
to
answer
queries
using
a
"Horn
approximation";
i.e.,
a
horn
theory
that
is
semantically
similar
to
the
original
+L+
theory.
The
utility
of
any
such
approximation
+L+
depends
on
how
often
it
produces
answers
to
+L+
the
queries
that
the
system
actually
encounters;
+L+
we
therefore
seek
an
approximation
whose
expected
"coverage"
is
maximal.
Unfortunately,
+L+
there
are
several
obstacles
to
achieving
this
goal
+L+
in
practice:
(i)
The
optimal
approximation
depends
on
the
query
distribution,
which
is
typically
not
known
a
priori;
(ii)
identifying
the
optimal
approximation
is
intractable,
even
given
+L+
the
query
distribution;
and
(iii)
the
optimal
approximation
might
be
too
large
to
guarantee
+L+
tractable
inference.
This
paper
presents
an
approach
that
overcomes
(or
side-steps)
each
of
+L+
these
obstacles.
We
define
a
learning
process,
+L+
AdComp,
that
uses
observed
queries
to
estimate
the
query
distribution
"online",
and
then
+L+
uses
these
estimates
to
hill-climb,
efficiently,
+L+
in
the
space
of
size-bounded
Horn
approximations,
until
reaching
one
that
is,
with
provably
+L+
high
probability,
effectively
at
a
local
optimum.
+L+
Appears
in
the
+L+
Proceedings
of
the
Third
International
Conference
on
Knowledge
Representation
and
Reasoning,
+L+
25-29
October
1992,
Cambridge,
MA.
+L+
1
Introduction

Coding
Segments
Inside
and
Outside
the
Chromosome
+L+
Thomas
Haynes
+L+
Department
of
Computer
Science
+L+
Wichita
State
University
+L+
Wichita,
KS
67260
+L+
E-mail:
haynes@cs.twsu.edu
+L+
Phone:
(316)
978-3925
+L+
Abstract
+L+
Coding
segments
are
those
sub-segments
of
the
chromosome
which
contribute
either
positively
or
+L+
negatively
to
the
fitness
evaluation
of
the
chromosome.
We
extract
coding
segments
from
chromosomes
+L+
and
we
investigate
the
sharing
of
coding
segments
both
inside
and
outside
of
the
chromosome.
We
+L+
find
duplication
of
coding
segments
inside
the
chromosomes
provides
a
back-up
mechanism
for
the
+L+
search
heuristics.
We
further
find
local
search
in
a
collective
memory
of
coding
segments
outside
of
the
+L+
chromosome,
collective
adaptation,
enables
the
search
heuristic
to
represent
partial
solutions
which
are
+L+
larger
than
realistic
chromosomes
lengths
and
to
express
the
solution
outside
of
the
chromosome.
+L+
+PAGE+

Support
for
Implementation
of
Evolutionary
+L+
Concurrent
Systems
in
Concurrent
+L+
Programming
Languages
+L+
Raju
Pandey
1
and
J.
C.
Browne
2
+L+
1
Computer
Science
Department,
University
of
California,
Davis,
CA
95616
2
Department
of
Computer
Sciences,
The
University
of
Texas,
Austin,
TX
78712
+L+
Abstract.
In
many
concurrent
programming
languages,
concurrent
programs
are
difficult
to
extend
and
modify:
small
changes
in
a
concurrent
+L+
program
may
require
re-implementations
of
a
large
number
of
its
components.
In
this
paper
a
novel
concurrent
program
composition
mechanism
+L+
is
presented
in
which
implementations
of
computations
and
synchronizations
are
completely
separated.
Separation
of
implementations
facilitates
+L+
extensions
and
modifications
of
programs
by
allowing
one
to
change
implementations
of
both
computations
and
synchronizations.
The
paper
+L+
also
describes
a
concurrent
programming
model
and
a
programming
lan
+L+
guage
that
support
the
proposed
approach.
+L+
1
Introduction

ISSN
1360-1725
+L+
The
Test
Matrix
Toolbox
for
Matlab
(Version
3.0)
+L+
N.
J.
Higham
+L+
Numerical
Analysis
Report
No.
276
+L+
September
1995
+L+
Manchester
Centre
for
Computational
Mathematics
+L+
Numerical
Analysis
Reports
+L+
DEPARTMENTS
OF
MATHEMATICS
+L+
Reports
available
from:
+L+
Department
of
Mathematics
+L+
University
of
Manchester
+L+
Manchester
M13
9PL
+L+
England
+L+
And
over
the
World-Wide
Web
from
URLs
+L+
http://www.ma.man.ac.uk/MCCM/MCCM.html
+L+
ftp://ftp.ma.man.ac.uk/pub/narep
+L+
+PAGE+

Greedy
Algorithms
in
+L+
Greedy
with
Choice
and
Negation
+L+
Sergio
Greco
Carlo
Zaniolo
+L+
Dip:
Elettr:
Informatica
Sist:
Computer
Science
Dept:
+L+
Universita
della
Calabria
Univ:
of
California
at
Los
Angeles
+L+
87030
Rende;
Italy
LosAngeles;
CA
90024
+L+
greco@si:deis:unical:it
zaniolo@cs:ucla:edu
+L+
Abstract
+L+
In
the
design
of
algorithms,
the
greedy
paradigm
provides
a
powerful
tool
for
solving
+L+
efficiently
classical
computational
problems,
within
the
framework
of
procedural
languages.
However,
expressing
these
algorithms
within
the
declarative
framework
of
logic-based
languages
has
proved
to
be
a
difficult
research
challenge.
In
this
paper,
we
extend
the
framework
of
Datalog-like
languages
to
obtain
simple
declarative
formulations
+L+
for
such
problems,
and
propose
effective
implementation
techniques
to
ensure
computational
complexities
comparable
to
those
of
procedural
formulations.
These
advances
are
+L+
achieved
through
the
use
of
the
choice
construct,
that
has
semantics
reducible
to
that
of
+L+
programs
with
negation
under
stable
model
semantics.
Then
we
extend
the
fixpoint-based
+L+
semantics
of
choice
programs
with
preference
annotations
to
guide
search
strategies
and
+L+
simple
logic-based
formulations
of
classical
greedy
algorithms.
+L+
1
Introduction

Run-time
Compilation
for
Parallel
Sparse
Matrix
Computations
+L+
Cong
Fu
and
Tao
Yang
+L+
Department
of
Computer
Science
+L+
University
of
California,
+L+
Santa
Barbara,
CA
93106.
+L+
http://www.cs.ucsb.edu/f~cfu,~tyangg
+L+
Abstract
+L+
Run-time
compilation
techniques
have
been
shown
effective
+L+
for
automating
the
parallelization
of
loops
with
unstructured
+L+
indirect
data
accessing
patterns.
However,
it
is
still
an
open
+L+
problem
to
efficiently
parallelize
sparse
matrix
factorizations
+L+
commonly
used
in
iterative
numerical
problems.
The
difficulty
is
that
a
factorization
process
contains
irregularly-interleaved
communication
and
computation
with
varying
+L+
granularities
and
it
is
hard
to
obtain
scalable
performance
+L+
on
distributed
memory
machines.
In
this
paper,
we
present
+L+
an
inspector/executor
approach
for
parallelizing
such
applications
by
embodying
automatic
graph
scheduling
techniques
to
optimize
interleaved
communication
and
computation.
We
describe
a
run-time
system
called
RAPID
that
+L+
provides
a
set
of
library
functions
for
specifying
irregular
+L+
data
objects
and
tasks
that
access
these
objects.
The
system
+L+
extracts
a
task
dependence
graph
from
data
access
patterns,
+L+
and
executes
tasks
efficiently
on
a
distributed
memory
machine.
We
discuss
a
set
of
optimization
strategies
used
in
+L+
this
system
and
demonstrate
the
application
of
this
system
+L+
in
parallelizing
sparse
Cholesky
and
LU
factorizations.
+L+
1
Introduction

A
SUIF
Java
Compiler
+L+
Holger
M.
Kienle
+L+
kienle@cs.uscb.edu
+L+
Technical
Report
TRCS98-18
+L+
August,
1998
+L+
+PAGE+

EXPLOITING
COMMUTING
OPERATIONS
IN
PARALLELIZING
+L+
SERIAL
PROGRAMS
+L+
PEDRO
DINIZ
AND
MARTIN
RINARD
+L+
DEPARTMENT
OF
COMPUTER
SCIENCE
+L+
UNIVERSITY
OF
CALIFORNIA,
SANTA
BARBARA
+L+
SANTA
BARBARA,
CA
93106
+L+
Abstract.
Two
operations
commute
if
the
result
of
their
execution
is
independent
of
the
order
in
which
they
execute.
Commuting
operations
can
be
executed
+L+
concurrently
provided
they
execute
atomically
on
the
objects
they
access.
Statically
+L+
recognizing
commuting
operations
is
of
great
interest
because
they
increase
the
amount
+L+
of
concurrency
a
compiler
can
exploit.
In
this
document
we
introduce
commutativity
+L+
analysis
anew
technique
for
automatically
parallelizing
serial
programs.
We
then
+L+
conduct
a
feasibility
study
of
existing
scientific
applications
as
to
the
existence
and
+L+
exploitability
of
commuting
operations.
We
study
the
commuting
operations
present
+L+
in
one
such
application
the
Barnes-Hut
hierarchical
N-body
algorithm.
We
then
+L+
parallelize
this
application
using
knowledge
of
commuting
operations
and
present
performance
results
of
the
parallel
code
for
a
shared-memory
multiprocessor.
+L+
1.
Introduction.

Scintilla:
Cluster
Computing
with
SCI
+L+
Max
Ibel,
Klaus
E.
Schauser,
Chris
J.
Scheiman,
and
Michael
Schmitt
+L+
Department
of
Computer
Science
+L+
University
of
California,
Santa
Barbara
+L+
Santa
Barbara,
CA
93106
+L+
fibel,schauser,chriss,schmittmg@cs.ucsb.edu
+L+
http://www.cs.ucsb.edu/research/scintilla
+L+
Abstract
+L+
The
Scintilla
project
at
UCSB
studies
SCI-based
cluster
+L+
computing.
The
Scalable
Coherent
Interface
(SCI)
is
a
+L+
recent
communication
standard
for
cluster
interconnects.
+L+
We
focus
on
non-coherent
SCI,
using
our
cluster
setup
+L+
of
SBus-based
and
PCI-based
workstations
connected
via
+L+
Dolphin
SCI
adapters.
Our
motivation
for
choosing
SCI
as
+L+
network
fabric
is
the
very
low
latency
and
high
bandwidth.
+L+
We
study
how
to
map
a
variety
of
programming
models
efficiently
onto
the
SCI
hardware,
focusing
on
message
+L+
passing
and
global
address
space
support,
implementing
+L+
Active
Messages
and
Split-C.
We
present
implementation
+L+
trade-offs,
present
performance
measurements
and
compare
the
PCI
and
SBus
adapters.
+L+
We
found
that
the
user-level
load/store
programming
interface
of
SCI
is
very
convenient
to
use,
achieves
low
latencies,
and
is
fully
virtualized,
simultaneously
supporting
+L+
multiple
parallel
programs
and
communication
channels.
+L+
On
the
other
hand,
neither
of
the
programming
models
+L+
studied
maps
directly
to
SCI.
Issues
such
as
notification,
+L+
atomic
operations,
and
virtual
address
space
limitations
+L+
represent
major
implementation
challenges,
which
we
address
with
a
combination
of
compiler
and
run-time
support.
Overall,
we
found
the
SCI
network
a
good
substrate
+L+
for
high-performance
cluster
computing.
+L+
1
Introduction

UNIVERSITY
of
CALIFORNIA
+L+
Santa
Barbara
+L+
Design
and
Implementation
of
a
System
to
Support
Integration
of
+L+
Autonomous
Database
Systems
+L+
A
thesis
submitted
in
partial
satisfaction
of
the
+L+
requirements
for
the
degree
of
+L+
Master
of
Science
+L+
in
+L+
Computer
Science
+L+
by
+L+
Tze
Kwan
Lau
+L+
Committee
in
charge:
+L+
Professor
Jianwen
Su,
Chair
+L+
Professor
Oscar
Ibarra
+L+
Professor
Amr
El
Abbadi
+L+
December
1998
+L+
+PAGE+

Appears
in
Journal
of
the
ACM,
Vol.
39,
No.
1,
January
1992,
pp.
214-233.
Prelimanary
version
+L+
in
Proceedings
of
the
20th
Annual
Symposium
on
Theory
of
Computing,
ACM,
1988.
+L+
How
to
Sign
Given
Any
Trapdoor
Permutation
+L+
Mihir
Bellare
Silvio
Micali
+L+
January
1992
+L+
Abstract
+L+
We
present
a
digital
signature
scheme
which
is
based
on
the
existence
of
any
trapdoor
+L+
permutation.
Our
scheme
is
secure
in
the
strongest
possible
natural
sense:
namely,
it
is
secure
+L+
against
existential
forgery
under
adaptive
chosen
message
attack.
+L+
Department
of
Computer
Science
&
Engineering,
Mail
Code
0114,
University
of
California
at
San
Diego,
9500
+L+
Gilman
Drive,
La
Jolla,
CA
92093.
E-mail:
mihir@cs.ucsd.edu.
Work
done
while
author
was
at
MIT,
supported
in
+L+
part
by
NSF
grant
CCR-87-19689.
+L+
MIT
Laboratory
for
Computer
Science,
545
Technology
Square,
Cambridge,
MA
02139.
Supported
in
part
by
+L+
NSF
grant
DCR-84-13577
and
ARO
grant
DAALO3-86-K-0171.
+L+
+PAGE+

TR
-
UCSD
-
CS96-484
+L+
Mapping
Parallel
Applications
to
+L+
Distributed
Heterogeneous
Systems
+L+
Silvia
M.
Figueira
1
and
Francine
Berman
2
+L+
Department
of
Computer
Science
and
Engineering
+L+
University
of
California,
San
Diego
+L+
-silvia,berman-@cs.ucsd.edu
+L+
Abstract
+L+
Fast
networks
have
made
it
possible
to
coordinate
distributed
heterogeneous
CPU,
+L+
memory
and
storage
resources
to
provide
a
powerful
platform
for
executing
high-performance
applications.
However,
the
performance
of
parallel
applications
on
such
+L+
systems
is
highly
dependent
on
the
mapping
of
application
tasks
to
machines.
In
this
+L+
paper,
we
propose
a
mapping
strategy
for
applications
formed
by
multiple
tasks
targeted
+L+
to
heterogeneous
platforms.
We
first
define
a
mapping
model,
the
match-tree,
which
+L+
reects
the
data
movement
and
conversion
costs
of
distributed
algorithms
and
allows
for
+L+
alternative
implementations
of
individual
tasks
on
different
machines.
We
then
define
the
+L+
find-mapping
and
split-partition
algorithms,
based
on
the
match-tree
model,
to
+L+
determine
the
best
allocation
of
tasks
to
resources
in
heterogeneous
systems.
We
+L+
illustrate
the
use
of
these
algorithms
with
a
sample
distributed
application.
+L+
1
Introduction

Learning
to
Schedule
Straight-Line
Code
+L+
J.
Eliot
B.
Moss
+L+
Dept.
of
Comp.
Sci.
+L+
Univ.
of
Mass.
+L+
Amherst,
MA
01003
+L+
Paul
E.
Utgoff
+L+
Dept.
of
Comp.
Sci.
+L+
Univ.
of
Mass.
+L+
Amherst,
MA
01003
+L+
John
Cavazos
+L+
Dept.
of
Comp.
Sci.
+L+
Univ.
of
Mass.
+L+
Amherst,
MA
01003
+L+
Doina
Precup
+L+
Dept.
of
Comp.
Sci.
+L+
Univ.
of
Mass.
+L+
Amherst,
MA
01003
+L+
Darko
Stefanovi
c
+L+
Dept.
of
Comp.
Sci.
+L+
Univ.
of
Mass.
+L+
Amherst,
MA
01003
+L+
Carla
Brodley
+L+
Sch.
of
Elec.
and
Comp.
Eng.
+L+
Purdue
University
+L+
W.
Lafayette,
IN
47907
+L+
David
Scheeff
+L+
Sch.
of
Elec.
and
Comp.
Eng.
+L+
Purdue
University
+L+
W.
Lafayette,
IN
47907
+L+
Abstract
+L+
Execution
speed
of
programs
on
modern
computer
architectures
is
sensitive,
by
a
factor
of
two
or
more,
to
the
order
in
which
instructions
+L+
are
presented
to
the
processor.
To
realize
potential
execution
efficiency,
+L+
it
is
now
customary
for
an
optimizing
compiler
to
employ
a
heuristic
+L+
algorithm
for
instruction
scheduling.
These
algorithms
are
painstakingly
+L+
hand-crafted,
which
is
expenseive
and
time-consuming.
We
show
how
+L+
to
cast
the
instruction
scheduling
problem
as
a
learning
task,
so
that
one
+L+
obtains
the
heuristic
scheduling
algorithm
automatically.
Our
focus
is
the
+L+
narrower
problem
of
scheduling
straight-line
code,
also
known
as
a
basic
+L+
block
of
instructions.
Our
empirical
results
show
that
just
a
few
features
+L+
are
adequate
for
quite
good
performance
at
this
task
for
a
real
modern
+L+
processor,
and
that
any
of
several
supervised
learning
methods
perform
+L+
nearly
optimally
with
respect
to
the
features
used.
+L+
Category:
Applications
(compiler
optimization)
+L+
Original:
This
work
has
not
been
submitted
elsewhere.
+L+
Presentation:
We
prefer
oral
presentation.
+L+
Contact
author:
Eliot
Moss
+L+
+PAGE+

Intra-Option
Learning
about
Temporally
Abstract
Actions
+L+
Richard
S.
Sutton
+L+
Department
of
Computer
Science
+L+
University
of
Massachusetts
+L+
Amherst,
MA
01003-4610
+L+
rich@cs.umass.edu
+L+
Doina
Precup
+L+
Department
of
Computer
Science
+L+
University
of
Massachusetts
+L+
Amherst,
MA
01003-4610
+L+
dprecup@cs.umass.edu
+L+
Satinder
Singh
+L+
Department
of
Computer
Science
+L+
University
of
Colorado
+L+
Boulder,
CO
80309-0430
+L+
baveja@cs.colorado.edu
+L+
Abstract
+L+
Several
researchers
have
proposed
modeling
+L+
temporally
abstract
actions
in
reinforcement
+L+
learning
by
the
combination
of
a
policy
and
a
termination
condition,
which
we
refer
to
as
an
option.
Value
functions
over
options
and
models
of
+L+
options
can
be
learned
using
methods
designed
+L+
for
semi-Markov
decision
processes
(SMDPs).
+L+
However,
all
these
methods
require
an
option
to
+L+
be
executed
to
termination.
In
this
paper
we
explore
methods
that
learn
about
an
option
from
+L+
small
fragments
of
experience
consistent
with
+L+
that
option,
even
if
the
option
itself
is
not
executed.
We
call
these
methods
intra-option
learning
methods
because
they
learn
from
experience
+L+
within
an
option.
Intra-option
methods
are
sometimes
much
more
efficient
than
SMDP
methods
because
they
can
use
off-policy
temporal-difference
mechanisms
to
learn
simultaneously
+L+
about
all
the
options
consistent
with
an
experience,
not
just
the
few
that
were
actually
executed.
In
this
paper
we
present
intra-option
learning
methods
for
learning
value
functions
over
options
and
for
learning
multi-time
models
of
the
+L+
consequences
of
options.
We
present
computational
examples
in
which
these
new
methods
+L+
learn
much
faster
than
SMDP
methods
and
learn
+L+
effectively
when
SMDP
methods
cannot
learn
at
+L+
all.
We
also
sketch
a
convergence
proof
for
intra
+L+
option
value
learning.
+L+
1
Introduction

Composite
Model
Checking
with
Type
Specific
+L+
Symbolic
Encodings
+L+
Tevfik
Bultan
Richard
Gerber
+L+
Department
of
Computer
Science
+L+
University
of
Maryland,
College
Park,
MD
20742,
USA
+L+
Abstract
+L+
We
present
a
new
symbolic
model
checking
technique,
which
analyzes
temporal
properties
in
multi-typed
transition
systems.
Specifically,
the
method
uses
multiple
type-specific
data
encodings
to
represent
+L+
system
states,
and
it
carries
out
fixpoint
computations
via
the
corresponding
type-specific
symbolic
+L+
operations.
In
essence,
different
symbolic
encodings
are
unified
into
one
composite
model
checker.
Any
+L+
type-specific
language
can
be
included
in
this
framework
provided
that
the
language
is
closed
under
+L+
Boolean
connectives,
propositions
can
be
checked
for
satisfiability,
and
relational
images
can
be
computed.
+L+
Our
technique
relies
on
conjunctive
partitioning
of
transition
relations
of
atomic
events
based
on
variable
+L+
types
involved,
which
allows
independent
computation
of
one-step
pre-
and
post-conditions
for
each
+L+
variable
type.
+L+
In
this
paper
we
demonstrate
the
effectiveness
of
our
method
on
a
nontrivial
data-transfer
protocol,
+L+
which
contains
a
mixture
of
integer
and
Boolean-valued
variables.
The
protocol
operates
over
an
unreliable
channel
that
can
lose,
duplicate
or
reorder
messages.
Moreover,
the
protocol's
send
and
receive
+L+
window
sizes
are
not
specified
in
advance;
rather,
they
are
represented
as
symbolic
constants.
The
resulting
system
was
automatically
verified
using
our
composite
model
checking
approach,
in
concert
with
+L+
a
conservative
approximation
technique.
+L+
This
research
is
supported
in
part
by
ONR
grant
N00014-94-10228
and
NSF
CCR-9619808.
+L+
+PAGE+

Ratio
Rules:
+L+
A
New
Paradigm
for
Fast,
Quantifiable
Data
Mining
+L+
Flip
Korn,
Alexandros
Labrinidis,
Yannis
Kotidis
+L+
Department
of
Computer
Science
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
fflip,labrinid,kotidisg@cs.umd.edu
+L+
Christos
Faloutsos
+L+
Computer
Science
Department
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213
+L+
christos@cs.cmu.edu
+L+
Abstract
+L+
Association
Rule
Mining
algorithms
operate
+L+
on
a
data
matrix
(e.g.,
customers
fi
products)
+L+
to
derive
association
rules
[2,
23].
We
propose
a
new
paradigm,
namely,
Ratio
Rules,
+L+
which
are
quantifiable
in
that
we
can
measure
+L+
the
"goodness"
of
a
set
of
discovered
rules.
+L+
We
propose
to
use
the
"guessing
error"
as
a
+L+
measure
of
the
"goodness",
that
is,
the
root-mean-square
error
of
the
reconstructed
values
+L+
of
the
cells
of
the
given
matrix,
when
we
pretend
that
they
are
unknown.
Another
contribution
is
a
novel
method
to
guess
missing/hidden
values
from
the
Ratio
Rules
that
+L+
our
method
derives.
For
example,
if
somebody
bought
$10
of
milk
and
$3
of
bread,
our
+L+
rules
can
"guess"
the
amount
spent
on,
say,
+L+
butter.
Thus,
we
can
perform
a
variety
of
important
tasks
such
as
forecasting,
answering
+L+
"what-if"
scenarios,
detecting
outliers,
and
visualizing
the
data.
Moreover,
we
show
how
to
+L+
compute
Ratio
Rules
in
a
single
pass
over
the
+L+
dataset
with
small
memory
requirements
(a
+L+
few
small
matrices),
in
contrast
to
traditional
+L+
association
rule
mining
methods
that
require
+L+
multiple
passes
and/or
large
memory.
Experiments
+L+
Work
performed
while
at
the
University
of
Maryland.
This
+L+
research
was
partially
funded
by
the
Institute
for
Systems
Research
(ISR),
and
by
the
National
Science
Foundation
under
+L+
Grants
No.
EEC-94-02384,
IRI-9205273
and
IRI-9625428.
+L+
Permission
to
copy
without
fee
all
or
part
of
this
material
is
+L+
granted
provided
that
the
copies
are
not
made
or
distributed
for
+L+
direct
commercial
advantage,
the
VLDB
copyright
notice
and
+L+
the
title
of
the
publication
and
its
date
appear,
and
notice
is
+L+
given
that
copying
is
by
permission
of
the
Very
Large
Data
Base
+L+
Endowment.
To
copy
otherwise,
or
to
republish,
requires
a
fee
+L+
and/or
special
permission
from
the
Endowment.
+L+
Proceedings
of
the
24th
VLDB
Conference
+L+
New
York,
USA,
1998
+L+
on
several
real
datasets
(e.g.,
basketball
and
baseball
statistics,
biological
data)
+L+
demonstrate
that
the
proposed
method
consistently
achieves
a
"guessing
error"
of
up
to
+L+
5
times
less
than
the
straightforward
competi
+L+
tor.
+L+
1
Introduction

Appears
in
Working
Notes
of
the
NASA
Workshop
on
Planning
and
Scheduling
for
Space
+L+
Oxnard,
CA,
October
1997
+L+
CIRCA
and
the
Cassini
Saturn
Orbit
Insertion:
+L+
Solving
a
Prepositioning
Problem
+L+
David
J.
Musliner
and
Robert
P.
Goldman
+L+
Automated
Reasoning
Group
+L+
Honeywell
Technology
Center
+L+
3660
Technology
Drive
+L+
Minneapolis,
MN
55418
+L+
fmusliner,goldmang@htc.honeywell.com
+L+
Introduction

Within
the
Letter
of
the
Law:
open-textured
planning
+L+
Kathryn
E.
Sanders
+L+
Department
of
Computer
Science
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
sanders@cs.umd.edu
+L+
Abstract
+L+
Most
case-based
reasoning
systems
have
used
a
+L+
single
"best"
or
"most
similar"
case
as
the
basis
+L+
for
a
solution.
For
many
problems,
however,
+L+
there
is
no
single
exact
solution.
Rather,
there
+L+
is
a
range
of
acceptable
answers.
We
use
cases
+L+
not
only
as
a
basis
for
a
solution,
but
also
to
+L+
indicate
the
boundaries
within
which
a
solution
+L+
can
be
found.
We
solve
problems
by
choosing
+L+
some
point
within
those
boundaries.
+L+
In
this
paper,
I
discuss
this
use
of
cases
with
+L+
illustrations
from
chiron,
a
system
I
have
implemented
in
the
domain
of
personal
income
+L+
tax
planning.
+L+
1
Introduction

Efficiently
Supporting
Ad
Hoc
Queries
in
Large
Datasets
of
Time
+L+
Sequences
+L+
Flip
Korn
+L+
Dept.
of
Computer
Science
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
flip@cs.umd.edu
+L+
H.
V.
Jagadish
+L+
AT&T
Laboratories
+L+
Florham
Park,
NJ
07932
+L+
jag@research.att.com
+L+
Christos
Faloutsos
+L+
Dept.
of
Computer
Science
and
+L+
Inst.
for
Systems
Research
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
christos@cs.umd.edu
+L+
Abstract
+L+
Ad
hoc
querying
is
difficult
on
very
large
datasets,
since
it
+L+
is
usually
not
possible
to
have
the
entire
dataset
on
disk.
+L+
While
compression
can
be
used
to
decrease
the
size
of
the
+L+
dataset,
compressed
data
is
notoriously
difficult
to
index
+L+
or
access.
+L+
In
this
paper
we
consider
a
very
large
dataset
comprising
multiple
distinct
time
sequences.
Each
point
in
the
+L+
sequence
is
a
numerical
value.
We
show
how
to
compress
+L+
such
a
dataset
into
a
format
that
supports
ad
hoc
querying,
provided
that
a
small
error
can
be
tolerated
when
the
+L+
data
is
uncompressed.
Experiments
on
large,
real
world
+L+
datasets
(AT&T
customer
calling
patterns)
show
that
the
+L+
proposed
method
achieves
an
average
of
less
than
5%
error
+L+
in
any
data
value
after
compressing
to
a
mere
2.5%
of
the
+L+
original
space
(i.e.,
a
40:1
compression
ratio),
with
these
+L+
numbers
not
very
sensitive
to
dataset
size.
Experiments
+L+
on
aggregate
queries
achieved
a
0.5%
reconstruction
error
+L+
with
a
space
requirement
under
2%.
+L+
1
Introduction

To
appear,
Computer
Aided
Design,
1995
or
1996
+L+
GENERATING
REDESIGN
SUGGESTIONS
TO
REDUCE
SETUP
COST:
+L+
A
STEP
TOWARDS
AUTOMATED
REDESIGN
+L+
Diganta
Das
+L+
Mechanical
Engr.
Dept.
and
+L+
Institute
for
Systems
Research,
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
diganta@cs.umd.edu
+L+
Satyandra
K.
Gupta
+L+
The
Robotics
Institute
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213
+L+
skgupta@isl1.ri.cmu.edu
+L+
Dana
S.
Nau
+L+
Dept.
of
Computer
Science
and
+L+
Institute
for
Systems
Research,
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
nau@cs.umd.edu
+L+
Abstract
+L+
All
mechanical
designs
pass
through
a
series
of
formal
and
informal
redesign
steps,
involving
the
analysis
of
functionality,
manufacturability,
cost
and
other
life-cycle
factors.
The
+L+
speed
and
efficacy
of
these
steps
has
a
major
influence
on
the
lead
time
of
the
product
from
+L+
conceptualization
to
launching.
In
this
paper
we
propose
a
methodology
for
automatically
+L+
generating
redesign
suggestions
for
reducing
setup
costs
for
machined
parts.
+L+
Given
an
interpretation
of
the
design
as
a
collection
of
machinable
features,
our
approach
+L+
is
to
generate
alternate
machining
features
by
making
geometric
changes
to
the
original
+L+
features,
and
add
them
to
the
feature
set
of
the
original
part
to
create
an
extended
feature
+L+
set.
The
designer
may
provide
restrictions
on
the
design
indicating
the
type
and
extent
of
+L+
modifications
allowed
on
certain
faces
and
volumes,
in
which
case
all
redesign
suggestions
+L+
generated
by
our
approach
honor
those
restrictions.
+L+
By
taking
combinations
of
features
from
the
extended
feature
set
generated
above,
we
can
+L+
generate
modified
versions
of
the
original
design
that
still
satisfy
the
designer's
intent.
By
+L+
considering
precedence
constraints
and
approach
directions
for
the
machining
operations
as
+L+
well
as
simple
fixturability
constraints,
we
can
estimate
the
setup
time
that
will
be
required
+L+
for
each
design.
Any
modified
design
whose
setup
time
is
less
than
that
of
the
original
+L+
design
can
be
presented
to
the
designer
as
a
possible
way
to
modify
the
original
design.
+L+
+PAGE+

To
appear
in
1996
ACM
SIGMETRICS
(Extended
Version:
UMass
TR
95-98,
Nov.
1995)
+L+
Supporting
Stored
Video:
Reducing
Rate
Variability
and
End-to-End
Resource
+L+
Requirements
through
Optimal
Smoothing
+L+
James
D.
Salehi,
Zhi-Li
Zhang,
James
F.
Kurose,
and
Don
Towsley
+L+
Department
of
Computer
Science,
University
of
Massachusetts,
Amherst
MA
01003,
USA
+L+
Abstract
+L+
VBR
compressed
video
is
known
to
exhibit
significant,
multiple-
+L+
time-scale
bit
rate
variability.
In
this
paper,
we
consider
the
transmission
of
stored
video
from
a
server
to
a
client
across
a
high
speed
+L+
network,
and
explore
how
the
client
buffer
space
can
be
used
most
+L+
effectively
toward
reducing
the
variability
of
the
transmitted
bit
+L+
rate.
+L+
We
present
two
basic
results.
First,
we
present
an
optimal
+L+
smoothing
algorithm
for
achieving
the
greatest
possible
reduction
+L+
in
rate
variability
when
transmitting
stored
video
to
a
client
with
+L+
given
buffer
size.
We
provide
a
formal
proof
of
optimality,
and
+L+
demonstrate
the
performance
of
the
algorithm
on
a
set
of
long
+L+
MPEG-1
encoded
video
traces.
Second,
we
evaluate
the
impact
+L+
of
optimal
smoothing
on
the
network
resources
needed
for
video
+L+
transport,
under
two
network
service
models:
Deterministic
Guar-
+L+
anteed
service
[1,
11]
and
Renegotiated
CBR
(RCBR)
service
[9,
8].
+L+
Under
both
models,
we
find
the
impact
of
optimal
smoothing
to
be
+L+
dramatic.
+L+
1
Introduction

Reducing
False
Sharing
on
Shared
Memory
Multiprocessors
+L+
through
Compile
Time
Data
Transformations
+L+
Tor
E.
Jeremiassen
+L+
AT&T
Bell
Laboratories
+L+
600
Mountain
Ave.
+L+
Murray
Hill,
New
Jersey
07974
+L+
tor@research.att.com
+L+
Susan
J.
Eggers
+L+
Department
of
Computer
Science
and
Engineering
+L+
University
of
Washington
+L+
Seattle,
Washington
98195
+L+
eggers@cs.washington.edu
+L+
Abstract
+L+
We
have
developed
compiler
algorithms
that
analyze
explicitly
parallel
programs
and
restructure
their
shared
data
to
+L+
reduce
the
number
of
false
sharing
misses.
The
algorithms
+L+
analyze
per-process
shared
data
accesses,
pinpoint
the
data
+L+
structures
that
are
susceptible
to
false
sharing
and
choose
+L+
an
appropriate
transformation
to
reduce
it.
The
transformations
either
group
data
that
is
accessed
by
the
same
processor
or
separate
individual
data
items
that
are
shared.
+L+
This
paper
evaluates
that
technique.
We
show
through
+L+
simulation
that
our
analysis
successfully
identifies
the
data
+L+
structures
that
are
responsible
for
most
false
sharing
misses,
+L+
and
then
transforms
them
without
unduly
decreasing
spatial
+L+
locality.
The
reduction
in
false
sharing
positively
impacts
+L+
both
execution
time
and
program
scalability
when
executed
+L+
on
a
KSR2.
Both
factors
combine
to
increase
the
maximum
+L+
achievable
speedup
for
all
programs,
more
than
doubling
+L+
it
for
several.
Despite
being
able
to
only
approximate
actual
inter-processor
memory
accesses,
the
compiler-directed
+L+
transformations
always
outperform
programmer
efforts
to
+L+
eliminate
false
sharing.
+L+
1
Introduction

Compiler
Support
for
Maintaining
Cache
+L+
Coherence
Using
Data
Prefetching
?
+L+
(Extended
Abstract)
+L+
Hock-Beng
Lim
1
,
Lynn
Choi
2
and
Pen-Chung
Yew
3
+L+
1
Center
for
Supercomputing
R
&
D,
Univ.
of
Illinois,
Urbana,
IL
61801
+L+
2
Microprocessor
Group,
Intel
Corporation,
Santa
Clara,
CA
95095
+L+
3
Dept.
of
Computer
Science,
Univ.
of
Minnesota,
Minneapolis,
MN
55455
+L+
1
Introduction
and
Motivation

Constraint-Based
Animations
+L+
Allan
Heydon
Greg
Nelson
+L+
Digital
Systems
Research
Center
+L+
130
Lytton
Ave.,
Palo
Alto,
CA
94301
+L+
fheydon,gnelsong@pa.dec.com
+L+
Copyright
c
fl1995,
Digital
Equipment
Corporation.
All
right
reserved.
+L+
1.
Introduction

A
short
version
of
this
paper
appears
in
Supercomputing
1996
+L+
The
serial
algorithms
described
in
this
paper
are
implemented
by
the
+L+
`METIS:
Unstructured
Graph
Partitioning
and
Sparse
Matrix
Ordering
System'.
+L+
METIS
is
available
on
WWW
at
URL:
http://www.cs.umn.edu/karypis/metis/metis.html
+L+
Parallel
Multilevel
k
-way
Partitioning
Scheme
for
+L+
Irregular
Graphs
+L+
George
Karypis
and
Vipin
Kumar
+L+
University
of
Minnesota,
Department
of
Computer
Science
+L+
Minneapolis,
MN
55455,
Technical
Report:
96-036
+L+
fkarypis,
kumarg@cs.umn.edu
+L+
Last
updated
on
October
24,
1996
at
9:02am
+L+
Abstract
+L+
In
this
paper
we
present
a
parallel
formulation
of
a
multilevel
k-way
graph
partitioning
algorithm.
The
multilevel
+L+
k-way
partitioning
algorithm
reduces
the
size
of
the
graph
by
collapsing
vertices
and
edges
(coarsening
phase),
finds
+L+
a
k-way
partitioning
of
the
smaller
graph,
and
then
it
constructs
a
k-way
partitioning
for
the
original
graph
by
projecting
and
refining
the
partition
to
successively
finer
graphs
(uncoarsening
phase).
A
key
innovative
feature
of
our
+L+
parallel
formulation
is
that
it
utilizes
graph
coloring
to
effectively
parallelize
both
the
coarsening
and
the
refinement
+L+
during
the
uncoarsening
phase.
Our
algorithm
is
able
to
achieve
a
high
degree
of
concurrency,
while
maintaining
the
+L+
high
quality
partitions
produced
by
the
serial
algorithm.
We
test
our
scheme
on
a
large
number
of
graphs
from
finite
+L+
element
methods,
and
transportation
domains.
For
graphs
with
a
million
vertices,
our
parallel
formulation
produces
+L+
high
quality
128-way
partitions
on
128
processors
in
a
little
over
two
seconds,
on
Cray
T3D.
Thus
our
parallel
algorithm
makes
it
feasible
to
perform
frequent
dynamic
graph
partition
in
adaptive
computations
without
compromising
+L+
quality.
+L+
Keywords:
Parallel
Graph
Partitioning,
Multilevel
Partitioning
Methods,
Spectral
Partitioning
Methods,
+L+
Kernighan-Lin
Heuristic,
Parallel
Sparse
Matrix
Algorithms.
+L+
This
work
was
supported
by
NSF
CCR-9423082
and
by
Army
Research
Office
contract
DA/DAAH04-95-1-0538,
and
by
Army
High
Performance
Computing
Research
Center
under
the
auspices
of
the
Department
of
the
Army,
Army
Research
Laboratory
cooperative
agreement
+L+
number
DAAH04-95-2-0003/contract
number
DAAH04-95-C-0008,
the
content
of
which
does
not
necessarily
reflect
the
position
or
the
policy
of
the
government,
and
no
official
endorsement
should
be
inferred.
Access
to
computing
facilities
was
provided
by
AHPCRC,
Minnesota
+L+
Supercomputer
Institute,
Cray
Research
Inc,
and
by
the
Pittsburgh
Supercomputing
Center.
Related
papers
are
available
via
WWW
at
URL:
+L+
http://www.cs.umn.edu/karypis
+L+
+PAGE+

CS/TR-92-80
+L+
The
COOL
architecture
and
abstractions
for
object-oriented
+L+
distributed
operating
systems
+L+
Rodger
Lea,
Christian
Jacquemot
+L+
approved
by:
+L+
distribution:
COOL
+L+
abstract:
Building
distributed
operating
systems
benefits
from
the
micro-kernel
approach
by
allowing
better
support
for
modularization.
However,
we
believe
+L+
that
we
need
to
take
this
support
a
step
further.
A
more
modular,
or
object
+L+
oriented
approach
is
needed
if
we
wish
to
cross
the
barrier
of
complexity
that
+L+
is
holding
back
distributed
operating
system
development.
The
Chorus
Ob
+L+
ject
Oriented
Layer
(COOL)
is
a
layer
built
above
the
Chorus
micro-kernel
+L+
designed
to
extend
the
micro-kernel
abstractions
with
support
for
object
oriented
systems.
COOL
v2,
the
second
iteration
of
this
layer
provides
generic
+L+
support
for
clusters
of
objects,
in
a
distributed
virtual
memory
model.
It
is
+L+
built
as
a
layered
system
where
the
lowest
layer
support
only
clusters
and
the
+L+
upper
layers
support
objects.
+L+
c
Chorus
systemes,
1992
+L+
c
Chorus
systemes,
1992
September
21,
1992
+L+
+PAGE+

AN
ALGORITHM
FOR
GENERATING
EXECUTABLE
+L+
ASSERTIONS
FOR
FAULT
TOLERANCE
+L+
Martina
Schollmeyer,
Hanan
Lutfiyya
and
Bruce
McMillin
+L+
CSC-92-01
+L+
March
21,
1992
+L+
Department
of
Computer
Science
+L+
University
of
Missouri
at
Rolla
+L+
Rolla,
Missouri
65401
+L+
This
work
was
supported
in
part
by
the
National
Science
Foundation
under
Grant
Numbers
MIP-8909749
and
CDA-8820714,
and
in
part
by
the
AMOCO
Faculty
Development
+L+
Program.
+L+
+PAGE+

Universal
Constructions
for
Large
Objects
+L+
James
H.
Anderson
and
Mark
Moir
+L+
Dept.
of
Computer
Science,
University
of
North
Carolina
at
Chapel
Hill
+L+
Abstract
+L+
We
present
lock-free
and
wait-free
universal
constructions
for
implementing
large
+L+
shared
objects.
Most
previous
universal
constructions
require
processes
to
copy
the
+L+
entire
object
state,
which
is
impractical
for
large
objects.
Previous
attempts
to
+L+
address
this
problem
require
programmers
to
explicitly
fragment
large
objects
into
+L+
smaller,
more
manageable
pieces,
paying
particular
attention
to
how
such
pieces
are
+L+
copied.
In
contrast,
our
constructions
are
designed
to
largely
shield
programmers
+L+
from
this
fragmentation.
Furthermore,
for
many
objects,
our
constructions
result
+L+
in
lower
copying
overhead
than
previous
ones.
+L+
Fragmentation
is
achieved
in
our
constructions
through
the
use
of
load-linked,
+L+
store-conditional,
and
validate
operations
on
a
"large"
multi-word
shared
variable.
+L+
Before
presenting
our
constructions,
we
show
that
these
operations
can
be
efficiently
+L+
implemented
from
similar
one-word
primitives.
+L+
1
Introduction

Modeling
and
Parameter
Estimation
of
the
Human
Index
Finger
+L+
Robert
N.
Rohling
and
John
M.
Hollerbach
+L+
Biorobotics
Laboratory,
McGill
University
+L+
3775
University
St.,
Montreal,
Quebec
H3A
2B4
+L+
Abstract
+L+
Precise
teleoperation
of
dextrous
robotic
hands
by
+L+
hand
masters
requires
an
accurate
human
hand
model.
+L+
A
kinematic
model
of
a
human
index
finger
is
developed
as
an
example
for
human
hand
modeling.
The
+L+
parameters
of
the
model
are
determined
by
open-loop
+L+
kinematic
calibration.
Singular
value
decomposition
is
+L+
used
as
a
tool
for
analyzing
the
kinematic
model
and
+L+
the
identification
process.
Accurate
and
reliable
results
are
obtained
only
when
the
numerical
condition
+L+
is
minimized
through
parameter
scaling,
model
reduction
and
pose
set
selection.
The
identified
kinematic
+L+
parameters
show
the
kinematic
model
and
calibration
+L+
procedure
have
an
accuracy
on
the
order
of
a
few
millimeters.
+L+
1
Introduction

The
Next
Frontier:
Interactive
and
Closed
Loop
Performance
+L+
Steering
+L+
Daniel
A.
Reed
Christopher
L.
Elford
+L+
Tara
M.
Madhyastha
Evgenia
Smirni
+L+
Stephen
E.
Lamm
+L+
freed,elford,tara,esmirni,slammg@cs.uiuc.edu
+L+
Department
of
Computer
Science
+L+
University
of
Illinois
+L+
Urbana,
Illinois
61801
+L+
Abstract
+L+
Software
for
a
growing
number
of
problem
domains
+L+
has
complex,
time
varying
behavior
and
unpredictable
+L+
resource
demands
(e.g.,
WWW
servers
and
parallel
input/output
systems).
While
current
performance
analysis
tools
provide
insights
into
application
dynamics
+L+
and
the
causes
of
poor
performance,
with
a
posteriori
analysis
one
cannot
adapt
to
temporally
varying
+L+
application
resource
demands
and
system
responses.
+L+
We
believe
that
the
solution
to
this
performance
optimization
conundrum
is
integration
of
dynamic
performance
instrumentation
and
on-the-fly
performance
+L+
data
reduction
with
real-time
adaptive
control
mechanisms
that
select
and
configure
resource
management
+L+
algorithms
automatically,
based
on
observed
application
behavior,
or
interactively,
through
high-modality
+L+
virtual
environments.
We
motivate
this
belief
by
first
+L+
describing
our
experiences
with
performance
analysis
tools,
input/output
characterization,
and
WWW
+L+
server
analysis,
and
then
sketching
the
design
of
interactive
and
closed
loop
adaptive
control
systems.
+L+
1
Introduction

Optimal
Wire-Sizing
Formula
Under
the
Elmore
Delay
Model
+L+
Chung-Ping
Chen
,
Yao-Ping
Chen
,
and
D.
F.
Wong
+L+
Department
of
Computer
Sciences,
University
of
Texas,
Austin,
Texas
78712
+L+
Abstract
+L+
In
this
paper,
we
consider
non-uniform
wire-sizing.
Given
a
+L+
wire
segment
of
length
L,
let
f(x)
be
the
width
of
the
wire
+L+
at
position
x,
0
x
L.
We
show
that
the
optimal
wire-sizing
function
that
minimizes
the
Elmore
delay
through
the
+L+
wire
is
f(x)
=
ae
bx
,
where
a
&gt;
0
and
b
&gt;
0
are
constants
+L+
that
can
be
computed
in
O(1)
time.
In
the
case
where
lower
+L+
bound
(L
&gt;
0)
and
upper
bound
(U
&gt;
0)
on
the
wire
widths
+L+
are
given,
we
show
that
the
optimal
wire-sizing
function
f(x)
+L+
is
a
truncated
version
of
ae
bx
that
can
also
be
determined
in
+L+
O(1)
time.
Our
wire-sizing
formula
can
be
iteratively
applied
+L+
to
optimally
size
the
wire
segments
in
a
routing
tree.
+L+
1
Introduction

Programming
the
Web:
+L+
An
Application-oriented
Language
for
Hypermedia
Services
+L+
David
A.
Ladd
J.
Christopher
Ramming
+L+
ladd@research.att.com
jcr@research.att.com
+L+
AT&T
Bell
Laboratories
+L+
October
9,
1995
+L+
Abstract
+L+
MAWL
is
an
application
language
for
programming
interactive
services
in
the
context
of
the
Worldwide
+L+
Web.
The
language
is
small,
because
no
construct
was
introduced
without
compelling
justification;
as
with
+L+
yacc
[8],
general-purpose
computation
is
done
in
a
host
language.
MAWL
offers
conveniences
such
as
control
+L+
abstraction,
persistent
state
management,
synchronization,
and
shared
memory.
In
addition,
the
MAWL
+L+
compiler
performs
static
checking
designed
to
prevent
common
Web
programming
errors.
In
this
paper
we
+L+
discuss
the
design
and
engineering
of
MAWL.
+L+
We
describe
the
problems
MAWL
is
intended
to
solve,
and
then
discuss
our
design
choices
in
the
context
+L+
of
our
general
language
design
philosophy,
We
also
include
an
appendix
of
commentary
on
several
short
+L+
MAWL
programs.
+L+
1
Introduction

ON
UNAPPROXIMABLE
VERSIONS
OF
NP-COMPLETE
+L+
PROBLEMS
+L+
DAVID
ZUCKERMAN
+L+
Abstract.
+L+
We
prove
that
all
of
Karp's
21
original
NP
-complete
problems
have
a
version
that's
hard
to
+L+
approximate.
These
versions
are
obtained
from
the
original
problems
by
adding
essentially
the
same,
+L+
simple
constraint.
We
further
show
that
these
problems
are
absurdly
hard
to
approximate.
In
fact,
no
+L+
polynomial-time
algorithm
can
even
approximate
log
(k)
of
the
magnitude
of
these
problems
to
within
+L+
any
constant
factor,
where
log
(k)
denotes
the
logarithm
iterated
k
times,
unless
N
P
is
recognized
by
+L+
slightly
superpolynomial
randomized
machines.
We
use
the
same
technique
to
improve
the
constant
+L+
*
such
that
MAX
CLIQUE
is
hard
to
approximate
to
within
a
factor
of
n
*
.
Finally,
we
show
that
it
+L+
is
even
harder
to
approximate
two
counting
problems:
counting
the
number
of
satisfying
assignments
+L+
to
a
monotone
2-SAT
formula
and
computing
the
permanent
of
-1,0,1
matrices.
+L+
Key
words.
NP-complete,
unapproximable,
randomized
reduction,
clique,
counting
problems,
+L+
permanent,
2SAT
+L+
AMS
subject
classifications.
68Q15,
68Q25,
68Q99
+L+
1.
Introduction.

Frangipani:
A
Scalable
Distributed
File
System
+L+
Chandramohan
A.
Thekkath
+L+
Timothy
Mann
+L+
Edward
K.
Lee
+L+
Systems
Research
Center
+L+
Digital
Equipment
Corporation
+L+
130
Lytton
Ave,
Palo
Alto,
CA
94301
+L+
Abstract
+L+
The
ideal
distributed
file
system
would
provide
all
its
users
with
coherent,
shared
access
to
the
same
set
of
files,yet
would
be
arbitrarily
+L+
scalable
to
provide
more
storage
space
and
higher
performance
to
+L+
a
growing
user
community.
It
would
be
highly
available
in
spite
of
+L+
component
failures.
It
would
require
minimal
human
administration,
and
administration
would
not
become
more
complex
as
more
+L+
components
were
added.
+L+
Frangipani
is
a
new
file
system
that
approximates
this
ideal,
yet
+L+
was
relatively
easy
to
build
because
of
its
two-layer
structure.
The
+L+
lower
layer
is
Petal
(described
in
an
earlier
paper),
a
distributed
+L+
storage
service
that
provides
incrementally
scalable,
highly
available,
automatically
managed
virtual
disks.
In
the
upper
layer,
+L+
multiple
machines
run
the
same
Frangipani
file
system
code
on
top
+L+
of
a
shared
Petal
virtual
disk,
using
a
distributed
lock
service
to
+L+
ensure
coherence.
+L+
Frangipani
is
meant
to
run
in
a
cluster
of
machines
that
are
under
+L+
a
common
administration
and
can
communicate
securely.
Thus
the
+L+
machines
trust
one
another
and
the
shared
virtual
disk
approach
is
+L+
practical.
Of
course,
a
Frangipani
file
system
can
be
exported
to
+L+
untrusted
machines
using
ordinary
network
file
access
protocols.
+L+
We
have
implemented
Frangipani
on
a
collection
of
Alphas
+L+
running
DIGITAL
Unix
4.0.
Initial
measurements
indicate
that
+L+
Frangipani
has
excellent
single-server
performance
and
scales
well
+L+
as
servers
are
added.
+L+
1
Introduction

Constructing
Scripts
from
Components:
+L+
Working
Note
6
+L+
Peter
Clark
and
Bruce
Porter
+L+
Dept.
CS,
UT
Austin
+L+
fpclark,porterg@cs.utexas.edu
+L+
1
Introduction

Broadcasting
on
Meshes
with
Worm-Hole
Routing
+L+
Michael
Barnett
+L+
Department
of
Computer
Science
+L+
University
of
Idaho
+L+
Moscow,
Idaho
83844-1010
+L+
mbarnett@cs.uidaho.edu
+L+
David
G.
Payne
+L+
Supercomputer
Systems
Division
+L+
Intel
Corporation
+L+
15201
N.W.
Greenbrier
Pkwy
+L+
Beaverton,
Oregon
97006
+L+
payne@ssd.intel.com
+L+
Robert
A.
van
de
Geijn
+L+
Department
of
Computer
Sciences
+L+
The
University
of
Texas
at
Austin
+L+
Austin,
Texas
78712-1188
+L+
rvdg@cs.utexas.edu
+L+
Jerrell
Watts
+L+
Scalable
Concurrent
Programming
Laboratory
+L+
California
Institute
of
Technology
+L+
Pasadena,
California
91125
+L+
jwatts@scp.caltech.edu
+L+
Original
Version:
November
2,
1993
+L+
Revised
Version:
September
21,
1994
+L+
Abstract
+L+
We
address
the
problem
of
broadcasting
on
mesh
architectures
with
arbitrary
(non-power-two)
dimensions.
It
is
assumed
that
such
mesh
architectures
employ
cut-through
or
worm-hole
routing.
The
+L+
primary
focus
is
on
avoiding
network
conflicts
in
the
various
proposed
algorithms.
We
give
algorithms
+L+
for
performing
a
conflict-free
minimum-spanning
tree
broadcast,
a
pipelined
algorithm
that
is
similar
to
+L+
Ho
and
Johnsson's
EDST
algorithm
for
hypercubes,
and
a
novel
scatter-collect
approach
that
is
a
natural
+L+
choice
for
communication
libraries
due
to
its
simplicity.
Results
obtained
on
the
Intel
Paragon
system
+L+
are
included.
+L+
1
Introduction

Anatomy
of
a
Parallel
Out-of-Core
Dense
Linear
Solver
+L+
Kenneth
Klimkowski
+L+
Texas
Institute
for
Computational
+L+
and
Applied
Mathematics
+L+
The
University
of
Texas
at
Austin
+L+
Austin,
Texas
78712
+L+
ken@ticam.utexas.edu
+L+
Robert
A.
van
de
Geijn
+L+
Department
of
Computer
Sciences
+L+
The
University
of
Texas
at
Austin
+L+
Austin,
Texas
78712
+L+
rvdg@cs.utexas.edu
+L+
Abstract
In
this
paper,
we
describe
the
design
+L+
and
implementation
of
the
Platform
Independent
+L+
Parallel
Solver
(PIPSolver)
package
for
the
out-of-core
(OOC)
solution
of
complex
dense
linear
systems.
Our
approach
is
unique
in
that
it
allows
essentially
all
of
RAM
to
be
filled
with
the
current
+L+
portion
of
the
matrix
(slab)
to
be
updated
and
factored,
thereby
greatly
improving
the
computation
to
+L+
I/O
ratio
over
previous
approaches.
Experiences
+L+
and
performance
are
reported
for
the
Cray
T3D
+L+
system.
+L+
INTRODUCTION

NetSolve:
A
Network
Server
+L+
for
Solving
Computational
Science
Problems
+L+
Henri
Casanova
+L+
University
of
Tennessee
+L+
UTK,
Dept.
of
Computer
Science
104,
Ayres
Hall.
KNOXVILLE,
TN
37996-1301.
+L+
casanova@cs.utk.edu
+L+
http://www.cs.utk.edu/~casanova
+L+
Jack
Dongarra
+L+
University
of
Tennessee,
Oak
Ridge
National
Laboratory
+L+
UTK,
Dept.
of
Computer
Science
104,
Ayres
Hall.
KNOXVILLE,
TN
37996-1301.
+L+
dongarra@cs.utk.edu
+L+
http://www.netlib.org/utk/people/JackDongarra.html
+L+
November
12,
1996
+L+
Abstract
+L+
This
paper
presents
a
new
system,
called
NetSolve,
that
allows
users
to
access
computational
resources,
such
as
hardware
and
software,
distributed
across
the
network.
The
development
of
NetSolve
+L+
was
motivated
by
the
need
for
an
easy-to-use,
efficient
mechanism
for
using
computational
resources
remotely.
Ease
of
use
is
obtained
as
a
result
of
different
interfaces,
some
of
which
require
no
programming
+L+
effort
from
the
user.
Good
performance
is
ensured
by
a
load-balancing
policy
that
enables
NetSolve
to
+L+
use
the
computational
resources
available
as
efficiently
as
possible.
NetSolve
offers
the
ability
to
look
+L+
for
computational
resources
on
a
network,
choose
the
best
one
available,
solve
a
problem
(with
retry
for
+L+
fault-tolerance),
and
return
the
answer
to
the
user.
+L+
Keywords
+L+
Networking,
Heterogeneity,
Load
Balancing,
+L+
Client-Server,
Fault
Tolerance,
Numerical
Computing,
Virtual
Library.
+L+
+PAGE+

An
Ordering
on
Subgoals
for
Planning
+L+
Fangzhen
Lin
+L+
D<affiliation>
epartment
of
Computer
Science
+L+
The
Hong
Kong
University
of
Science
and
Technology
+L+
Clear
Water
Bay,
Kowloon,
Hong
Kong
+L+
@cs.ust.hk
+L+
Abstract
+L+
Subgoal
ordering
is
a
type
of
control
information
that
has
received
much
attention
+L+
in
AI
planning
community.
In
this
paper
we
formulate
precisely
a
subgoal
ordering
+L+
in
the
situation
calculus.
We
show
how
information
about
this
subgoal
ordering
can
+L+
be
deduced
from
the
background
action
theory.
We
also
show
for
both
linear
and
+L+
nonlinear
planners
how
knowledge
about
this
ordering
can
be
used
in
a
provably
+L+
correct
way
to
avoid
unnecessary
backtracking.
+L+
1
Introduction

Specifying
Instructions'
Semantics
Using
CSDL
+L+
(Preliminary
Report)
+L+
Norman
Ramsey
and
Jack
W.
Davidson
+L+
Department
of
Computer
Science
+L+
University
of
Virginia
+L+
Charlottesville,
VA
22903
+L+
June
10,
1998
+L+
+PAGE+

A
Distributed
Protocol
for
Channel-Based
+L+
Communication
with
Choice
+L+
Frederick
Knabe
+L+
European
Computer-Industry
Research
Centre
GmbH
+L+
Munich,
Germany
+L+
knabe@ecrc.de
+L+
Abstract
+L+
Recent
attempts
at
incorporating
concurrency
into
functional
languages
have
+L+
identified
synchronous
communication
via
shared
channels
as
a
promising
primitive.
An
additional
useful
feature
found
in
many
proposals
is
a
nondeterministic
+L+
choice
operator.
Similar
in
nature
to
the
CSP
alternative
command,
this
operator
+L+
allows
different
possible
actions
to
be
guarded
by
sends
or
receives.
Choice
is
+L+
difficult
to
implement
in
a
distributed
environment
because
it
requires
offering
+L+
many
potential
communications
but
closing
only
one.
In
this
paper
we
present
+L+
the
first
distributed,
deadlock-free
algorithm
for
choice.
+L+
Keywords:
Distributed
protocols,
channels,
synchronous
communication,
+L+
choice
operator,
CSP
alternative
command.
+L+
1.
Introduction

Non-Tree
Routing
+L+
Bernard
A.
McCoy
and
Gabriel
Robins
+L+
Department
of
Computer
Science,
University
of
Virginia,
Charlottesville,
VA
22903-2442
+L+
Appeared
in:
IEEE
Transactions
on
Computer-Aided
Design
of
Integrated
Circuits
and
Systems,
+L+
Vol.
14,
No.
6,
June
1995,
pp.
780-784.
+L+
Abstract
+L+
An
implicit
premise
of
existing
routing
methods
is
that
the
routing
topology
must
correspond
to
a
+L+
tree
(i.e.,
it
does
not
contain
cycles).
In
this
paper
we
investigate
the
consequences
of
abandoning
this
+L+
basic
axiom,
and
instead
we
allow
routing
topologies
that
correspond
to
arbitrary
graphs
(i.e.,
where
+L+
cycles
are
allowed).
We
show
that
non-tree
routing
can
significantly
improve
signal
propagation
delay,
+L+
reduce
signal
skew,
and
afford
increased
reliability
with
respect
to
open
faults
that
may
be
caused
by
+L+
manufacturing
defects
and
electro-migration.
Simulations
on
uniformly-distributed
nets
indicate
that
+L+
depending
on
net
size
and
technology
parameters,
our
non-tree
routing
construction
reduces
maximum
+L+
sourse-sink
SPICE
delay
by
an
average
of
up
to
62%,
and
reduces
signal
skew
by
an
average
of
up
to
+L+
63%,
as
compared
with
Steiner
routing.
Moreover,
up
to
77%
of
the
total
wirelength
in
non-trees
can
+L+
tolerate
an
open
fault
without
disconnecting
the
circuit.
+L+
1
Introduction

Using
Runtime
Measured
Workload
Characteristics
in
+L+
Parallel
Processor
Scheduling
+L+
Thu
D.
Nguyen
,
Raj
Vaswani
,
and
John
Zahorjan
+L+
Department
of
Computer
Science
and
Engineering,
Box
352350
+L+
University
of
Washington
+L+
Seattle,
WA
98195-2350
USA
+L+
Technical
Report
UW-CSE-95-10-01
+L+
October
15,
1995
+L+
+PAGE+

PCp
+L+
3
:
A
C
Front
End
for
+L+
Preprocessor
Analysis
and
Transformation
+L+
Greg
J.
Badros
+L+
gjb@cs.washington.edu
+L+
16
October
1997
+L+
Abstract
+L+
Though
the
C
preprocessor
provides
necessary
language
features,
it
does
so
in
an
unstructured
way.
The
lexical
nature
of
cpp
creates
numerous
problems
for
software
engineers
+L+
and
their
tools,
all
stemming
from
the
chasm
between
the
engineer's
view
of
the
source
code
+L+
and
the
compiler's
view.
The
simplest
way
to
reduce
this
problem
is
to
minimize
use
of
the
+L+
preprocessor.
In
light
of
the
data
collected
in
a
prior
empirical
analysis,
this
paper
describes
+L+
a
tool
to
aid
the
software
engineer
in
analyses
targeted
at
replacing
preprocessor
constructs
+L+
with
language
features.
Existing
tools
for
analyzing
C
source
in
the
context
of
the
preprocessor
are
unsuitable
for
such
transformations.
This
work
introduces
a
new
approach:
tightly
+L+
integrating
the
preprocessor
with
a
C
language
parser,
permitting
the
code
to
be
analyzed
at
+L+
both
the
preprocessor
and
syntactic
levels
simultaneously.
The
front-end
framework,
called
+L+
PCp
3
,
combines
a
preprocessor,
a
parser,
and
arbitrary
Perl
subroutine
"hooks"
invoked
upon
+L+
various
preprocessor
and
parser
events.
PCp
3
's
strengths
and
weaknesses
are
discussed
in
the
+L+
context
of
several
program
understanding
and
transformation
tools,
including
a
conservative
+L+
analysis
to
support
replacing
cpp's
#define
directives
with
C++
language
features.
+L+
1
Introduction

Effective
Cache
Prefetching
on
Bus-Based
Multiprocessors
+L+
Dean
M.
Tullsen
and
Susan
J.
Eggers
+L+
University
of
Washington
+L+
Abstract
+L+
Compiler-directed
cache
prefetching
has
the
potential
to
hide
much
of
the
high
memory
latency
seen
by
+L+
current
and
future
high-performance
processors.
However,
prefetching
is
not
without
costs,
particularly
on
a
+L+
multiprocessor.
Prefetching
can
negatively
affect
bus
utilization,
overall
cache
miss
rates,
memory
latencies
and
+L+
data
sharing.
+L+
We
simulate
the
effects
of
a
compiler-directed
prefetching
algorithm,
running
on
a
range
of
bus-based
multiprocessors.
We
show
that,
despite
a
high
memory
latency,
this
architecture
does
not
necessarily
support
prefetching
+L+
well,
in
some
cases
actually
causing
performance
degradations.
We
pinpoint
several
problems
with
prefetching
on
+L+
a
shared
memory
architecture
(additional
conflict
misses,
no
reduction
in
the
data
sharing
traffic
and
associated
+L+
latencies,
a
multiprocessor's
greater
sensitivity
to
memory
utilization
and
the
sensitivity
of
the
cache
hit
rate
to
+L+
prefetch
distance)
and
measure
their
effect
on
performance.
We
then
solve
those
problems
through
architectural
+L+
techniques
and
heuristics
for
prefetching
that
could
be
easily
incorporated
into
a
compiler:
1)
victim
caching,
+L+
which
eliminates
most
of
the
cache
conflict
misses
caused
by
prefetching
in
a
direct-mapped
cache,
2)
special
+L+
prefetch
algorithms
for
shared
data,
which
significantly
improve
the
ability
of
our
basic
prefetching
algorithm
+L+
to
prefetch
invalidation
misses,
and
3)
compiler-based
shared
data
restructuring,
which
eliminates
many
of
the
+L+
invalidation
misses
the
basic
prefetching
algorithm
doesn't
predict.
The
combined
effect
of
these
improvements
+L+
is
to
make
prefetching
effective
over
a
much
wider
range
of
memory
architectures.
+L+
keywords:
cache
prefetching,
bus-based
multiprocessor,
cache
misses,
prefetching
strategies,
parallel
programs,
false
sharing,
memory
latency
+L+
1
Introduction

Journal
of
Artificial
Intelligence
Research
8
(1998)
165-222
Submitted
8/97;
published
6/98
+L+
Model-Based
Diagnosis
using
Structured
System
+L+
Descriptions
+L+
Adnan
Darwiche
darwiche@aub.edu.lb
+L+
Department
of
Mathematics
+L+
American
University
of
Beirut
+L+
PO
Box
11-236
+L+
Beirut,
Lebanon
+L+
Abstract
+L+
This
paper
presents
a
comprehensive
approach
for
model-based
diagnosis
which
includes
proposals
for
characterizing
and
computing
preferred
diagnoses,
assuming
that
the
+L+
system
description
is
augmented
with
a
system
structure
(a
directed
graph
explicating
the
+L+
interconnections
between
system
components).
Specifically,
we
first
introduce
the
notion
of
+L+
a
consequence,
which
is
a
syntactically
unconstrained
propositional
sentence
that
characterizes
all
consistency-based
diagnoses
and
show
that
standard
characterizations
of
diagnoses,
+L+
such
as
minimal
conflicts,
correspond
to
syntactic
variations
on
a
consequence.
Second,
+L+
we
propose
a
new
syntactic
variation
on
the
consequence
known
as
negation
normal
form
+L+
(NNF)
and
discuss
its
merits
compared
to
standard
variations.
Third,
we
introduce
a
basic
+L+
algorithm
for
computing
consequences
in
NNF
given
a
structured
system
description.
We
+L+
show
that
if
the
system
structure
does
not
contain
cycles,
then
there
is
always
a
linear-size
+L+
consequence
in
NNF
which
can
be
computed
in
linear
time.
For
arbitrary
system
structures,
we
show
a
precise
connection
between
the
complexity
of
computing
consequences
+L+
and
the
topology
of
the
underlying
system
structure.
Finally,
we
present
an
algorithm
+L+
that
enumerates
the
preferred
diagnoses
characterized
by
a
consequence.
The
algorithm
is
+L+
shown
to
take
linear
time
in
the
size
of
the
consequence
if
the
preference
criterion
satisfies
+L+
some
general
conditions.
+L+
1.
Introduction

Partition
Based
Spatial-Merge
Join
+L+
Jignesh
M.
Patel
David
J.
DeWitt
+L+
Computer
Sciences
Department,
+L+
University
of
Wisconsin,
Madison
+L+
fjignesh,
dewittg@cs.wisc.edu
+L+
Abstract
+L+
This
paper
describes
PBSM
(Partition
Based
Spatial-Merge),
a
new
algorithm
for
performing
spatial
join
operation.
This
algorithm
is
especially
effective
when
neither
of
the
inputs
to
the
join
have
an
index
on
the
joining
+L+
attribute.
Such
a
situation
could
arise
if
both
inputs
to
the
join
are
intermediate
results
in
a
complex
query,
or
in
+L+
a
parallel
environment
where
the
inputs
must
be
dynamically
redistributed.
The
PBSM
algorithm
partitions
the
+L+
inputs
into
manageable
chunks,
and
joins
them
using
a
computational
geometry
based
plane-sweeping
technique.
This
paper
also
presents
a
performance
study
comparing
the
the
traditional
indexed
nested
loops
join
+L+
algorithm,
a
spatial
join
algorithm
based
on
joining
spatial
indices,
and
the
PBSM
algorithm.
These
comparisons
+L+
are
based
on
complete
implementations
of
these
algorithms
in
Paradise,
a
database
system
for
handling
GIS
applications.
Using
real
data
sets,
the
performance
study
examines
the
behavior
of
these
spatial
join
algorithms
in
a
+L+
variety
of
situations,
including
the
cases
when
both,
one,
or
none
of
the
inputs
to
the
join
have
an
suitable
index.
+L+
The
study
also
examines
the
effect
of
clustering
the
join
inputs
on
the
performance
of
these
join
algorithms.
The
+L+
performance
comparisons
demonstrates
the
feasibility,
and
applicability
of
the
PBSM
join
algorithm.
+L+
1
Introduction

Maximal-Munch
Tokenization
in
Linear
Time
+L+
THOMAS
REPS
+L+
University
of
Wisconsin
+L+
The
lexical-analysis
(or
scanning)
phase
of
a
compiler
attempts
to
partition
the
input
stream
into
a
sequence
of
tokens.
+L+
The
convention
in
most
languages
is
that
the
input
is
scanned
left
to
right,
and
each
token
identified
is
a
maximal
+L+
munch
of
the
remaining
inputthe
longest
prefix
of
the
remaining
input
that
is
a
token
of
the
language.
Most
textbooks
on
compiling
have
extensive
discussions
of
lexical
analysis
in
terms
of
finite-state
automata
and
regular
expressions:
Token
classes
are
defined
by
a
set
of
regular
expressions
R
i
,
1
i
k,
and
the
lexical
analyzer
is
based
on
some
+L+
form
of
finite-state
automaton
for
recognizing
the
language
L
(R
1
+
R
2
+
.
.
.
+
R
k
).
However,
the
treatment
is
unsatisfactory
in
one
respect:
The
theory
of
finite-state
automata
assumes
that
the
end
of
the
input
stringi.e.,
the
right-hand-side
boundary
of
the
candidate
for
recognitionis
known
a
priori,
whereas
a
scanner
must
identify
the
next
token
+L+
without
knowing
a
definite
bound
on
the
extent
of
the
token.
+L+
Although
most
of
the
standard
compiler
textbooks
discuss
this
issue,
the
solution
they
sketch
out
is
one
thatfor
+L+
certain
sets
of
token
definitionscan
cause
the
scanner
to
exhibit
quadratic
behavior
in
the
worst
case.
This
property
is
+L+
not
only
dissatisfying,
it
blemishes
an
otherwise
elegant
treatment
of
lexical
analysis.
+L+
In
this
paper,
we
rectify
this
defect:
We
show
that,
given
a
deterministic
finite-state
automaton
that
recognizes
the
+L+
tokens
of
a
language,
maximal-munch
tokenization
can
always
be
performed
in
time
linear
in
the
size
of
the
input.
+L+
CR
Categories
and
Subject
Descriptors:
D.3.1
[Programming
Languages]:
Formal
Definitions
and
Theory
syntax;
+L+
D.3.4
[Programming
Languages]:
Processors
compilers;
F.1.1
[Computation
by
Abstract
Devices]:
Models
of
+L+
Computation
automata;
F.2.2
[Analysis
of
Algorithms
and
Problem
Complexity]:
Nonnumerical
Algorithms
and
+L+
Problems
pattern
matching;
I.2.8
[Artificial
Intelligence]:
Problem
Solving,
Control
Methods,
and
Search
backtracking,
dynamic
programming;
I.5.4
[Pattern
Recognition]:
Applications
text
processing
+L+
General
Terms:
Algorithms,
Theory
+L+
Additional
Key
Words
and
Phrases:
memoization,
tabulation,
tokenization
+L+
1.
INTRODUCTION

Cost-Aware
WWW
Proxy
Caching
Algorithms
+L+
Pei
Cao
Sandy
Irani
+L+
Department
of
Computer
Science,
Information
and
Computer
Science
Department,
+L+
University
of
Wisconsin-Madison.
University
of
California-Irvine.
+L+
cao@cs.wisc.edu
irani@ics.uci.edu
+L+
Abstract
+L+
Web
caches
can
not
only
reduce
network
traffic
and
+L+
downloading
latency,
but
can
also
affect
the
distribution
of
web
traffic
over
the
network
through
cost-aware
caching.
This
paper
introduces
GreedyDual-Size,
which
incorporates
locality
with
cost
and
size
+L+
concerns
in
a
simple
and
non-parameterized
fashion
+L+
for
high
performance.
Trace-driven
simulations
show
+L+
that
with
the
appropriate
cost
definition,
GreedyDual-Size
outperforms
existing
web
cache
replacement
algorithms
in
many
aspects,
including
hit
ratios,
latency
reduction
and
network
cost
reduction.
In
addition,
GreedyDual-Size
can
potentially
improve
the
+L+
performance
of
main-memory
caching
of
Web
documents.
+L+
1
Introduction

Aart
J.C.
Bik
+L+
High
Performance
Computing
Division
+L+
Department
of
Computer
Science
+L+
Leiden
University
+L+
P.O.
Box
9512,
2300
RA
Leiden
+L+
The
Netherlands
+L+
ajcbik@cs.leidenuniv.nl
+L+
Tel.
+31
71
5277037
+L+
Submission
to
PLDI'96
+L+
+PAGE+

Kerberos:
An
Authentication
Service
for
Open
Network
Systems
+L+
Jennifer
G.
Steiner
+L+
Project
Athena
+L+
Massachusetts
Institute
of
Technology
+L+
Cambridge,
MA
02139
+L+
steiner@ATHENA.MIT.EDU
+L+
Clifford
Neuman
+L+
Department
of
Computer
Science,
FR-35
+L+
University
of
Washington
+L+
Seattle,
WA
98195
+L+
bcn@CS.WASHINGTON.EDU
+L+
Jeffrey
I.
Schiller
+L+
Project
Athena
+L+
Massachusetts
Institute
of
Technology
+L+
Cambridge,
MA
02139
+L+
jis@ATHENA.MIT.EDU
+L+
ABSTRACT
+L+
In
an
open
network
computing
environment,
a
workstation
cannot
be
trusted
to
+L+
identify
its
users
correctly
to
network
services.
Kerberos
provides
an
alternative
+L+
approach
whereby
a
trusted
third-party
authentication
service
is
used
to
verify
users'
+L+
identities.
This
paper
gives
an
overview
of
the
Kerberos
authentication
model
as
implemented
for
MIT's
Project
Athena.
It
describes
the
protocols
used
by
clients,
servers,
and
+L+
Kerberos
to
achieve
authentication.
It
also
describes
the
management
and
replication
of
+L+
the
database
required.
The
views
of
Kerberos
as
seen
by
the
user,
programmer,
and
+L+
administrator
are
described.
Finally,
the
role
of
Kerberos
in
the
larger
Athena
picture
is
+L+
given,
along
with
a
list
of
applications
that
presently
use
Kerberos
for
user
authentication.
We
describe
the
addition
of
Kerberos
authentication
to
the
Sun
Network
File
System
as
a
case
study
for
integrating
Kerberos
with
an
existing
application.
+L+
Introduction

Approximate
Analysis
of
Parallel
Processor
Allocation
Policies
+L+
Rajesh
K.
Mansharamani
and
Mary
K.
Vernon
+L+
(mansha@cs.wisc.edu)
(vernon@cs.wisc.edu)
+L+
Computer
Sciences
Department
+L+
University
of
Wisconsin
+L+
1210
West
Dayton
Street
+L+
Madison,
WI
53706.
+L+
November
29,
1993
+L+
Abstract
+L+
The
complexity
of
parallel
applications
and
parallel
processor
scheduling
policies
makes
both
exact
analysis
and
simulation
difficult,
if
not
intractable,
for
large
systems.
In
this
paper
we
propose
a
new
approach
+L+
to
performance
modeling
of
multiprogrammed
processor
scheduling
policies,
that
of
interpolation
approximations.
We
first
define
a
workload
model
that
contains
parameters
for
the
essential
properties
of
parallel
+L+
applications
with
respect
to
scheduling
discipline
performance,
yet
lends
itself
to
mathematical
analysis.
Key
+L+
features
of
the
workload
model
include
general
distribution
of
total
job
processing
time,
general
distribution
+L+
of
available
job
parallelism,
and
a
simple
characterization
of
parallelism
overheads.
We
then
show
that
one
+L+
can
find
specific
values
of
the
system
parameters
for
which
the
parallel
system
under
a
given
scheduling
+L+
policy
reduces
to
a
queueing
system
with
a
known
(closed-form)
solution.
Finally,
interpolation
between
+L+
the
points
with
known
solutions
is
used
to
arrive
at
mean
response
time
estimates
that
hold
over
the
entire
+L+
system
parameter
space.
The
interpolation
approximations
readily
yield
insight
into
policy
behavior
and
are
+L+
easy
to
evaluate
for
systems
with
hundreds
of
processors.
+L+
We
illustrate
the
approach
by
developing
and
validating
models
of
three
scheduling
policies,
under
the
+L+
assumptions
of
linear
job
execution
rates
and
independence
between
job
parallelism
and
processing
time.
+L+
We
discuss
several
insights
and
results
obtained
from
the
analysis
of
the
three
policies
under
the
assumed
+L+
workloads.
One
result
clarifies
and
generalizes
observations
in
two
previous
simulation
studies
of
how
policy
+L+
performance
varies
with
the
coefficient
of
variation
in
job
processing
requirement.
Another
result
of
the
+L+
interpolation
models
yields
new
insight
into
how
policy
performance
varies
with
job
parallelism.
We
also
+L+
comment
on
the
generalizations
of
these
insights
for
workloads
with
less
restrictive
assumptions.
+L+
This
research
was
partially
supported
by
the
National
Science
Foundation
under
grants
CCR-9024144
and
CDA-9024618.
+L+
+PAGE+

Use
of
Application
Characteristics
and
Limited
Preemption
for
+L+
Run-To-Completion
Parallel
Processor
Scheduling
Policies
*
+L+
Su-Hui
Chiang
,
Rajesh
K.
Mansharamani
,
and
Mary
K.
Vernon
+L+
Computer
Sciences
Department
+L+
University
of
Wisconsin-Madison
+L+
Madison,
WI
53706.
+L+
email:
-suhui,
vernon-@cs.wisc.edu
+L+
TRDDC
+L+
1
Mangaldas
Road
+L+
Pune
411
050,
India.
+L+
email:
mansha@research.trddc.ernet.in
+L+
Abstract
+L+
The
performance
potential
of
run-to-completion
(RTC)
parallel
+L+
processor
scheduling
policies
is
investigated
by
examining
+L+
whether
(1)
application
execution
rate
characteristics
such
as
average
parallelism
(avg)
and
processor
working
set
(pws)
and/or
(2)
+L+
limited
preemption
can
be
used
to
improve
the
performance
of
+L+
these
policies.
We
address
the
first
question
by
comparing
policies
+L+
(previous
as
well
as
new)
that
differ
only
in
whether
or
not
they
+L+
use
execution
rate
characteristics
and
by
examining
a
wider
range
+L+
of
the
workload
parameter
space
than
previous
studies.
We
address
+L+
the
second
question
by
comparing
a
simple
two-level
queueing
+L+
policy
with
RTC
scheduling
in
the
second
level
queue
against
+L+
RTC
policies
that
don't
allow
any
preemption
and
against
dynamic
+L+
equiallocation
(EQ).
+L+
Using
simulation
to
estimate
mean
response
times
we
find
that
for
+L+
promising
RTC
policies
such
as
adaptive
static
partitioning
(ASP)
+L+
and
shortest
demand
first
(SDF),
a
maximum
allocation
constraint
+L+
that
is
for
all
practical
purposes
independent
of
avg
and
pws
provides
greater
and
more
consistent
improvement
in
policy
performance
than
using
avg
or
pws.
Also,
under
the
assumption
that
job
+L+
demand
information
is
unavailable
to
the
scheduler
we
show
that
+L+
the
ASP-max
policy
outperforms
all
previous
high
performance
+L+
RTC
policies
for
workloads
with
coefficient
of
variation
in
processing
requirement
greater
than
one.
Furthermore,
a
two-level
+L+
queue
that
allows
at
most
one
preemption
per
job
outperforms
+L+
ASP-max
but
is
not
competitive
with
EQ.
+L+
1.
Introduction

Program
Generalization
for
Software
Reuse:
+L+
From
C
to
C++
+L+
Michael
Siff
+L+
siff@cs.wisc.edu
+L+
Thomas
Reps
+L+
reps@cs.wisc.edu
+L+
University
of
Wisconsin-Madison
+L+
1210
West
Dayton
Street
+L+
Madison,
WI
53706
+L+
Abstract
+L+
We
consider
the
problem
of
software
generalization:
Given
a
program
component
C,
create
+L+
a
parameterized
program
component
C
0
such
that
C
0
is
usable
in
a
wider
variety
of
syntactic
+L+
contexts
than
C.
Furthermore,
C
0
should
be
a
semantically
meaningful
generalization
of
C;
+L+
namely,
there
must
exist
an
instantiation
of
C
0
that
is
equivalent
in
functionality
to
C.
+L+
In
this
paper,
we
present
an
algorithm
that
generalizes
C
functions
via
type
inference.
The
+L+
original
functions
operate
on
specific
data
types;
the
result
of
generalization
is
a
collection
of
+L+
C++
function
templates
that
operate
on
parameterized
types.
This
version
of
the
generalization
+L+
problem
is
useful
in
the
context
of
converting
existing
C
programs
to
C++.
+L+
1
Introduction

MULTI-COORDINATION
METHODS
FOR
+L+
PARALLEL
SOLUTION
OF
+L+
BLOCK-ANGULAR
PROGRAMS
+L+
By
+L+
Golbon
Zakeri
+L+
A
thesis
submitted
in
partial
fulfillment
of
the
+L+
requirements
for
the
degree
of
+L+
Doctor
of
Philosophy
+L+
(Computer
Sciences
and
Mathematics)
+L+
at
the
+L+
UNIVERSITY
OF
WISCONSIN
-
MADISON
+L+
1995
+L+
+PAGE+

SYNCHRONOUS
AND
ASYNCHRONOUS
MULTI-COORDINATION
+L+
METHODS
FOR
THE
SOLUTION
OF
BLOCK-ANGULAR
+L+
PROGRAMS
+L+
R.R.
MEYER
AND
G.
ZAKERI
+L+
Abstract.
Several
types
of
multi-coordination
methods
for
block-angular
programs
are
considered.
We
present
a
computational
comparison
of
synchronous
multi-coordination
methods.
The
most
+L+
efficient
of
these
approaches
is
shown
to
involve
an
intermediate
number
of
blocks
in
the
coordination
+L+
phase.
We
also
develop
a
new
stabilization
algorithm
and
present
asynchronous
multi-coordination
+L+
schemes,
which
are
particularly
useful
when
the
number
of
blocks
exceeds
the
number
of
available
+L+
processors
or
when
the
block
sizes
vary
significantly.
+L+
1.
Introduction.
In
this
paper
we
present
multi-coordinator
synchronous
and

An
Evaluation
of
Object
Management
System
+L+
Architectures
for
Software
Engineering
Applications
+L+
Jayavel
Shanmugasundaram
,
Barbara
Staudt
Lerner
,
Lori
Clarke
+L+
Department
of
Computer
Science
+L+
University
of
Massachusetts
+L+
Amherst,
MA
01003
USA
+L+
+1
413
545
3787
+L+
fshan,
lerner,
clarkeg@cs.umass.edu
+L+
ABSTRACT
+L+
Software
engineering
applications
require
sophisticated
+L+
object
management
system
support
for
creating
and
+L+
manipulating
software
objects.
One
of
the
key
issues
for
+L+
object
management
systems
is
distribution.
Addressing
this
issue
in
the
context
of
software
engineering
applications
is
particularly
challenging
because
they
have
+L+
widely
varying
object
access
profiles.
Two
fundamental
+L+
approaches
to
dealing
with
distribution
are
the
object
+L+
server
architecture,
where
objects
are
shipped
to
the
+L+
application
program,
and
the
operation
server
architecture,
where
operation
requests
are
shipped
to
where
the
+L+
objects
reside.
We
compare
these
architectures
experimentally
to
determine
the
conditions
under
which
each
+L+
performs
better.
+L+
KEYWORDS
+L+
Distributed
object
management,
experimental
evaluation
+L+
1
INTRODUCTION

Fast
and
Accurate
Flow-Insensitive
Points-To
Analysis
+L+
Marc
Shapiro
and
Susan
Horwitz
+L+
Computer
Sciences
Department,
University
of
Wisconsin-Madison
+L+
1210
West
Dayton
Street,
Madison,
WI
53706
USA
+L+
Electronic
mail:
fmds,
horwitzg@cs.wisc.edu
+L+
Abstract
+L+
In
order
to
analyze
a
program
that
involves
pointers,
it
+L+
is
necessary
to
have
(safe)
information
about
what
each
+L+
pointer
points
to.
There
are
many
different
approaches
+L+
to
computing
points-to
information.
This
paper
addresses
techniques
for
flow-
and
context-insensitive
in-terprocedural
analysis
of
stack-based
storage.
+L+
The
paper
makes
two
contributions
to
work
in
this
+L+
area:
+L+
*
The
first
contribution
is
a
set
of
experiments
that
+L+
explore
the
trade-offs
between
techniques
previously
defined
by
Lars
Andersen
and
Bjarne
Steens-gaard.
The
former
has
a
cubic
worst-case
running
+L+
time,
while
the
latter
is
essentially
linear.
However,
the
former
may
be
much
more
precise
than
+L+
the
latter.
We
have
found
that
in
practice,
Ander-sen's
algorithm
is
consistently
more
precise
than
+L+
Steensgaard's.
For
small
programs,
there
is
very
+L+
little
difference
in
the
times
required
by
the
two
+L+
approaches;
however,
for
larger
programs,
Ander-sen's
algorithm
can
be
much
slower
than
Steens
+L+
gaard's.
+L+
*
The
second
contribution
is
the
definition
of
two
+L+
new
algorithms.
The
first
algorithm
can
be
"tuned"
+L+
so
that
its
worst-case
time
and
space
requirements,
+L+
as
well
as
its
accuracy
range
from
those
of
Steens-gaard
to
those
of
Andersen.
We
have
experimented
+L+
with
several
versions
of
this
algorithm;
one
version
+L+
provided
a
significant
increase
in
accuracy
over
+L+
Steensgaard's
algorithm,
while
keeping
the
running
time
within
a
factor
of
two.
+L+
The
second
algorithm
uses
the
first
as
a
subroutine.
Its
worst-case
time
and
space
requirements
+L+
are
a
factor
of
log
N
(where
N
is
the
number
of
+L+
variables
in
the
program)
worse
than
those
of
+L+
Steensgaard's
algorithm.
In
practice,
it
appears
to
+L+
This
work
was
supported
in
part
by
the
National
Science
Foundation
under
grant
CCR-8958530,
and
by
the
Defense
Advanced
Research
Projects
Agency
under
ARPA
Order
No.
8856
(monitored
by
+L+
the
Office
of
Naval
Research
under
contract
N00014-92-J-1937).
+L+
run
about
ten
times
slower
than
Steensgaard's
algorithm;
however
it
is
significantly
more
accurate
+L+
than
Steensgaard's
algorithm,
and
significantly
+L+
faster
than
Andersen's
algorithm
on
large
+L+
programs.
+L+
1
Introduction

Storage
Estimation
for
Multidimensional
Aggregates
in
+L+
the
Presence
of
Hierarchies
+L+
Amit
Shukla
Prasad
M.
Deshpande
+L+
Jeffrey
F.
Naughton
Karthikeyan
Ramasamy
+L+
famit,pmd,naughton,karthikg@cs.wisc.edu
+L+
Computer
Sciences
Department
+L+
University
of
Wisconsin
-
Madison
+L+
Abstract
+L+
To
speed
up
multidimensional
data
analysis,
+L+
database
systems
frequently
precompute
aggregates
on
some
subsets
of
dimensions
and
+L+
their
corresponding
hierarchies.
This
improves
+L+
query
response
time.
However,
the
decision
of
+L+
what
and
how
much
to
precompute
is
a
difficult
one.
It
is
further
complicated
by
the
fact
+L+
that
precomputation
in
the
presence
of
hierarchies
can
result
in
an
unintuitively
large
increase
in
the
amount
of
storage
required
by
the
+L+
database.
Hence,
it
is
interesting
and
useful
+L+
to
estimate
the
storage
blowup
that
will
result
from
a
proposed
set
of
precomputations
+L+
without
actually
computing
them.
We
propose
+L+
three
strategies
for
this
problem:
one
based
on
+L+
sampling,
one
based
on
mathematical
approximation,
and
one
based
on
probabilistic
counting.
We
investigate
the
accuracy
of
these
algorithms
in
estimating
the
blowup
for
different
+L+
data
distributions
and
database
schemas.
The
+L+
algorithm
based
upon
probabilistic
counting
is
+L+
particularly
attractive,
since
it
estimates
the
+L+
storage
blowup
to
within
provable
error
bounds
+L+
while
performing
only
a
single
scan
of
the
data.
+L+
Work
supported
by
an
IBM
CAS
Fellowship,
NSF
grant
IRI-9157357,
and
a
grant
from
IBM
under
the
University
Partnership
+L+
Program.
+L+
Permission
to
copy
without
fee
all
or
part
of
this
material
is
+L+
granted
provided
that
the
copies
are
not
made
or
distributed
for
+L+
direct
commercial
advantage,
the
VLDB
copyright
notice
and
+L+
the
title
of
the
publication
and
its
date
appear,
and
notice
is
+L+
given
that
copying
is
by
permission
of
the
Very
Large
Data
Base
+L+
Endowment.
To
copy
otherwise,
or
to
republish,
requires
a
fee
+L+
and/or
special
permission
from
the
Endowment.
+L+
Proceedings
of
the
22nd
VLDB
Conference
+L+
Mumbai
(Bombay),
India,
1996
+L+
1
Introduction

Asking
Questions
to
Minimize
Errors
+L+
Nader
H.
Bshouty
+L+
Department
of
Computer
Science
+L+
The
University
of
Calgary
+L+
2500
University
Drive
N.W.
+L+
Calgary,
Alberta,
Canada
T2N
1N4
+L+
bshouty@cpsc.ucalgary.ca
+L+
Sally
A.
Goldman
+L+
Department
of
Computer
Science
+L+
Washington
University
+L+
St.
Louis,
MO
63130
+L+
sg@cs.wustl.edu
+L+
Thomas
R.
Hancock
+L+
Siemens
Corporate
Research,
Inc.
+L+
755
College
Road
East
+L+
Princeton,
NJ
08540
+L+
hancock@learning.siemens.com
+L+
Sleiman
Matar
+L+
Department
of
Computer
Science
+L+
The
University
of
Calgary
+L+
2500
University
Drive
N.W.
+L+
Calgary,
Alberta,
Canada
T2N
1N4
+L+
sleiman@cpsc.ucalgary.ca
+L+
September
1993
+L+
WUCS-93-23
+L+
Abstract
+L+
A
number
of
efficient
learning
algorithms
achieve
exact
identification
of
an
unknown
function
+L+
from
some
class
using
membership
and
equivalence
queries.
Using
a
standard
transformation
+L+
such
algorithms
can
easily
be
converted
to
on-line
learning
algorithms
that
use
membership
+L+
queries.
Under
such
a
transformation
the
number
of
equivalence
queries
made
by
the
query
+L+
algorithm
directly
corresponds
to
the
number
of
mistakes
made
by
the
on-line
algorithm.
In
+L+
this
paper
we
consider
several
of
the
natural
classes
known
to
be
learnable
in
this
setting,
and
+L+
investigate
the
minimum
number
of
equivalence
queries
with
accompanying
counterexamples
+L+
(or
equivalently
the
minimum
number
of
mistakes
in
the
on-line
model)
that
can
be
made
by
a
+L+
learning
algorithm
that
makes
a
polynomial
number
of
membership
queries
and
uses
polynomial
+L+
computation
time.
We
are
able
both
to
reduce
the
number
of
equivalence
queries
used
by
the
+L+
previous
algorithms
and
often
to
prove
matching
lower
bounds.
As
an
example,
consider
the
+L+
class
of
DNF
formulas
over
n
variables
with
at
most
k
=
O(log
n)
terms.
Previously,
the
+L+
algorithm
of
Blum
and
Rudich
[BR92]
provided
the
best
known
upper
bound
of
2
O(k)
log
n
for
+L+
the
minimum
number
of
equivalence
queries
needed
for
exact
identification.
We
greatly
improve
+L+
on
this
upper
bound
showing
that
exactly
k
counterexamples
are
needed
if
the
learner
knows
k
a
+L+
priori
and
exactly
k
+1
counterexamples
are
needed
if
the
learner
does
not
know
k
a
priori.
This
+L+
exactly
matches
known
lower
bounds
[BC92].
For
many
of
our
results
we
obtain
a
complete
+L+
characterization
of
the
tradeoff
between
the
number
of
membership
and
equivalence
queries
+L+
needed
for
exact
identification.
The
classes
we
consider
here
are
monotone
DNF
formulas,
Horn
+L+
sentences,
O(log
n)-term
DNF
formulas,
read-k
sat-j
DNF
formulas,
read-once
formulas
over
+L+
various
bases,
and
deterministic
finite
automata.
+L+
+PAGE+

Optimal
Solution
of
Off-line
and
On-line
Generalized
Caching
+L+
Saied
Hosseini-Khayat
and
Jerome
R.
Cox,
Jr.
+L+
Washington
University
in
St.
Louis
+L+
Abstract.
Network
traffic
can
be
reduced
significantly
if
caching
is
utilized
effectively.
As
an
effort
+L+
in
this
direction
we
study
the
replacement
problem
+L+
that
arises
in
caching
of
multimedia
objects.
The
size
+L+
of
objects
and
the
cost
of
cache
misses
are
assumed
+L+
non-uniform.
The
non-uniformity
of
size
is
inherent
in
multimedia
objects,
and
the
non-uniformity
of
+L+
cost
is
due
to
the
non-uniformity
of
size
and
the
fact
+L+
that
the
objects
are
scattered
throughout
the
network.
+L+
Although
a
special
case
of
this
problem,
i.e.
the
case
+L+
of
uniform
size
and
cost,
has
been
extensively
studied,
the
general
case
needs
a
great
deal
of
study.
We
+L+
present
a
dynamic
programming
method
of
optimally
+L+
solving
the
off-line
and
on-line
versions
of
this
problem,
and
discuss
the
complexity
of
this
method.
+L+
Key
words:
Generalized
caching,
network
traffic,
+L+
network
caching,
file
caching,
optimal
replacement,
+L+
replacement
algorithm.
+L+
I.
Introduction

Techniques
for
Developing
and
Measuring
+L+
High-Performance
Web
Servers
over
ATM
Networks
+L+
James
C.
Hu
,
Sumedh
Mungee
,Douglas
C.
Schmidt
+L+
fjxh,sumedh,schmidtg@cs.wustl.edu
+L+
TEL:
(314)
935-4215
FAX:
(314)
935-7302
+L+
Campus
Box
1045/Bryan
509
+L+
Washington
University
+L+
One
Brookings
Drive
+L+
St.
Louis,
MO
63130,
USA
+L+
This
paper
has
been
submitted
to
the
INFOCOM
'98
conference.
+L+
Abstract
+L+
High-performance
Web
servers
are
essential
to
meet
the
growing
demands
of
the
Internet
and
large-scale
intranets.
Satisfying
these
demands
requires
a
thorough
understanding
of
key
+L+
factors
affecting
Web
server
performance.
This
paper
presents
+L+
empirical
analysis
illustrating
how
dynamic
and
static
adaptivity
can
enhance
Web
server
performance.
Two
research
+L+
contributions
support
this
conclusion.
+L+
First,
the
paper
presents
results
from
a
comprehensive
empirical
study
of
Web
servers
(such
as
Apache,
Netscape
Enterprise,
PHTTPD,
Zeus,
and
JAWS)
over
high-speed
ATM
networks.
This
study
illustrates
their
relative
performance
and
+L+
precisely
pinpoints
the
server
design
choices
that
cause
performance
bottlenecks.
We
found
that
once
network
and
disk
+L+
I/O
overheads
are
reduced
to
negligible
constant
factors,
the
+L+
main
determinants
of
Web
server
performance
are
its
protocol
processing
path
and
concurrency
strategy.
Moreover,
no
+L+
single
strategy
performs
optimally
for
all
load
conditions
and
+L+
traffic
types.
+L+
Second,
we
describe
the
design
techniques
and
optimizations
used
to
develop
JAWS,
our
high-performance,
adaptive
+L+
Web
server.
JAWS
is
an
object-oriented
Web
server
that
was
+L+
explicitly
designed
to
alleviate
the
performance
bottlenecks
+L+
we
identified
in
existing
Web
servers.
It
consistently
outperforms
all
other
Web
servers
over
ATM
networks.
The
performance
optimizations
used
in
JAWS
include
adaptive
pre-spawned
threading,
fixed
headers,
cached
date
processing,
+L+
and
file
caching.
In
addition,
JAWS
uses
a
novel
software
architecture
that
substantially
improves
its
portability
and
flex
+L+
This
work
was
funded
in
part
by
NSF
grant
NCR-9628218,
Object
Technologies
International,
Eastman
Kodak,
and
Siemens
MED.
+L+
ibility,
relative
to
other
Web
servers.
Our
empirical
results
+L+
illustrate
that
highly
efficient
communication
software
is
not
+L+
antithetical
to
highly
flexible
software.
+L+
1
Introduction

MINIMIZING
MEMORY
CACHE
USAGE
FOR
MULTIGRID
+L+
ALGORITHMS
IN
TWO
DIMENSIONS
+L+
CRAIG
C.
DOUGLAS
+L+
Abstract.
Computers
today
rely
heavily
on
good
utilization
of
their
cache
memory
subsystems.
+L+
Compilers
are
optimized
for
business
applications,
not
scientific
computing
ones,
however.
Automatic
+L+
tiling
of
basic
numerical
algorithms
is
simply
not
provided
by
any
compiler.
Thus,
absolutely
terrible
+L+
cache
performance
is
normal
for
scientific
computing
applications.
+L+
Multigrid
algorithms
combine
several
numerical
algorithms
into
a
more
complicated
algorithm.
+L+
In
this
paper,
an
algorithm
is
derived
that
allows
for
data
to
pass
through
cache
exactly
once
per
+L+
multigrid
level
during
a
V
cycle
before
the
level
changes.
This
is
optimal
cache
usage
for
large
+L+
problems
that
do
not
fit
entirely
in
cache.
+L+
The
new
algorithm
would
appear
to
be
quite
complicated
to
implement,
leading
to
spaghetti
+L+
coding.
Actually,
an
efficient
implementation
of
the
algorithm
requires
a
rigid,
highly
structured
+L+
coding
style.
A
coding
example
is
given
that
is
suitable
for
almost
all
common
discretization
methods.
+L+
Numerical
experiments
are
provided
that
show
that
the
new
algorithm
is
up
to
an
integer
factor
+L+
faster
than
the
traditional
implementation
method
for
common
multigrid
parameter
choices.
+L+
Key
words.
multigrid,
cache,
threads,
sparse
matrix,
iterative
methods,
domain
decomposition,
+L+
compiler
optimization.
+L+
AMS
subject
classifications.
65N15,
65N10
+L+
1.
Introduction.
Multigrid
methods
are
widely
known
as
the
fastest
methods

GEMMW:
A
PORTABLE
LEVEL
3
BLAS
WINOGRAD
VARIANT
OF
+L+
STRASSEN'S
MATRIX-MATRIX
MULTIPLY
ALGORITHM
+L+
CRAIG
C.
DOUGLAS
,
MICHAEL
HEROUX
,
GORDON
SLISHMAN
x
AND
ROGER
M.
+L+
SMITH
-
+L+
Abstract.
Matrix-matrix
multiplication
is
normally
computed
using
one
of
the
BLAS
or
a
+L+
reinvention
of
part
of
the
BLAS.
Unfortunately,
the
BLAS
were
designed
with
small
matrices
in
+L+
mind.
When
huge,
well
conditioned
matrices
are
multiplied
together,
the
BLAS
perform
like
the
+L+
blahs,
even
on
vector
machines.
For
matrices
where
the
coefficients
are
well
conditioned,
Winograd's
+L+
variant
of
Strassen's
algorithm
offers
some
relief,
but
is
rarely
available
in
a
quality
form
on
most
+L+
computers.
We
reconsider
this
method
and
offer
a
highly
portable
solution
based
on
the
Level
3
+L+
BLAS
interface.
+L+
Key
Words.
Level
3
BLAS,
matrix
multiplication,
Winograd's
variant
of
Strassen's
algorithm,
+L+
multilevel
algorithms
+L+
AMS(MOS)
subject
classification.
Numerical
Analysis:
Numerical
Linear
Algebra
+L+
1.
Preliminaries.
Matrix-matrix
multiplication
is
a
very
basic
computer
oper

Constructing
Logic
Programs
with
+L+
Higher-Order
Predicates
1
+L+
Jtrgen
Fischer
Nilsson
+L+
Department
of
Computer
Science
+L+
Technical
University
of
Denmark
+L+
Andreas
Hamfelt
+L+
Computing
Science
Department
+L+
Uppsala
University
+L+
Abstract:
This
paper
proposes
a
logic
programming
approach
based
on
the
+L+
application
of
a
system
of
higher-order
predicates
put
at
disposal
within
ordinary
logic
programming
languages
such
as
prolog.
These
higher-order
+L+
predicates
parallel
the
higher-order
functionals
or
combinators
which
form
an
+L+
established
part
of
contemporary
functional
programming
methodology.
+L+
The
suggested
toolbox
of
higher-order
predicates
for
composing
logic
programs
+L+
is
derived
from
one
universal
higher-order
predicate.
They
take
the
form
of
+L+
recursion
operators
(in
particular
for
expressing
recursion
along
lists)
intended
+L+
to
cover
all
commonly
occurring
recursion
schemes
in
logic
programming
practice.
Their
theoretical
sufficiency
is
proved
and
their
practical
adequacy
is
+L+
argued
through
examples.
+L+
The
recursion
operators,
denoting
higher-order
relations
rather
than
functions,
are
brought
about
straightforwardly
through
a
well-known
metalogic
+L+
programming
technique,
rendering
superfluous
the
need
for
special
higher-order
unification
mechanisms.
+L+
Keywords:
Relational
higher-order
and
metalogic
programming.
Logic
program
recursion
schemes.
Declarative
logic
programming
methodology.
+L+
Perche
nessuna
cosa
si
puo
amare
ne
odiare,
+L+
se
prima
no
si
a
cognitio
di
quella.
+L+
-
Leonardo
da
Vinci,
Notebooks
+L+
1
Introduction:
Background
and
Ob
jectives

Experience
with
MPI:
'Converting
+L+
pvmmake
to
mpimake
under
LAM'
+L+
and
'MPI
and
Parallel
Genetic
+L+
Programming'
+L+
Judith
Ellen
Devaney
+L+
NIST
+L+
jdevaney@nist.gov
+L+
June
1995
+L+
Abstract
+L+
This
looks
at
the
issues
which
arose
in
porting
the
pvmmake
utility
+L+
from
PVM
to
MPI.
Pvmmake
is
a
PVM
application
which
allows
+L+
a
user
to
send
files,
execute
commands,
and
receive
results
from
a
+L+
single
machine
on
any
machine
in
the
virtual
machine.
Its
actions
are
+L+
controlled
by
the
contents
of
a
configuration
file.
Its
most
common
+L+
use
is
to
enable
management
of
the
development
of
a
parallel
program
+L+
in
a
heterogeneous
environment.
A
utility
with
the
same
features,
+L+
mpimake,
was
coded
up
to
run
under
LAM.
+L+
Genetic
programming
is
an
algorithm
which
evolves
an
algorithm
+L+
in
the
form
of
a
program
to
solve
your
input
problem.
The
implementation
under
MPI
requires
the
transfer
of
dynamic
data
structures
+L+
such
as
lists
and
trees.
This
paper
discusses
the
match
between
the
+L+
requirements
of
this
algorithm
and
the
datatype
feature
in
MPI.
A
+L+
new
library,
MPI
DataStruct
is
being
developed
which
can
transfer
+L+
dynamic
data
structures,
created
with
pointers,
without
intervention
+L+
by
the
user.
+L+
+PAGE+

Microkernel
Operating
System
Architecture
and
Mach
+L+
David
L.
Black
David
B.
Golub
Daniel
P.
Julin
Richard
F.
Rashid
+L+
Richard
P.
Draves
Randall
W.
Dean
Alessandro
Forin
Joseph
Barrera
+L+
Hideyuki
Tokuda
Gerald
Malan
David
Bohman
+L+
DRAFT
of
June
16,
1991
+L+
Abstract
+L+
Modular
architectures
based
on
a
microkernel
are
suitable
bases
for
the
design
and
implementation
+L+
of
operating
systems.
Prototype
systems
employing
microkernel
architectures
are
achieving
the
levels
of
+L+
functionality
and
performance
expected
and
required
of
commercial
products.
Researchers
at
Carnegie
+L+
Mellon
University,
the
Open
Software
Foundation,
and
other
sites
are
investigating
implementations
of
a
+L+
number
of
operating
systems
(e.g.,
Unix
1
,
MS-DOS
2
)
that
use
the
Mach
microkernel.
This
paper
describes
+L+
the
Mach
microkernel,
its
use
to
support
implementations
of
other
operating
systems,
and
the
status
of
these
+L+
efforts.
+L+
1
Introduction

Weak
and
Strong
Beta
Normalisations
+L+
in
Typed
-Calculi
+L+
Hongwei
Xi
+L+
Department
of
Mathematical
Sciences
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213,
USA
+L+
Abstract.
We
present
a
technique
to
study
relations
between
weak
and
+L+
strong
fi-normalisations
in
various
typed
-calculi.
We
first
introduce
a
+L+
translation
which
translates
a
-term
into
a
I-term,
and
show
that
a
+L+
-term
is
strongly
fi-normalisable
if
and
only
if
its
translation
is
weakly
+L+
fi-normalisable.
We
then
prove
that
the
translation
preserves
typability
+L+
of
-terms
in
various
typed
-calculi.
This
enables
us
to
establish
the
+L+
equivalence
between
weak
and
strong
fi-normalisations
in
these
typed
+L+
-calculi.
This
translation
can
deal
with
Curry
typing
as
well
as
Church
+L+
typing,
strengthening
some
recent
closely
related
results.
This
may
bring
+L+
some
insights
into
answering
whether
weak
and
strong
fi-normalisations
+L+
in
all
pure
type
systems
are
equivalent.
+L+
1
Introduction

Realistic
Parsing:
+L+
Practical
Solutions
of
Difficult
Problems
+L+
SYLVAIN
DELISLE
a
&
STAN
SZPAKOWICZ
b
+L+
a
Dpartement
de
mathmatiques
et
dinformatique
+L+
Universit
du
Qubec
Trois-Rivires
+L+
Trois-Rivires,
Qubec,
Canada
G9A
5H7
+L+
email:
Sylvain_Delisle@uqtr.uquebec.ca,
phone
+1
819
376
5125;
fax
+1
819
376
5185
+L+
b
Department
of
Computer
Science
+L+
University
of
Ottawa
+L+
Ottawa,
Ontario,
Canada
K1N
6N5
+L+
email:
szpak@csi.uottawa.ca,
phone
+1
613
562
5800
ext.
6687;
fax
+1
613
562
5187
+L+
Abstract
+L+
This
paper
describes
work
on
the
linguistic
+L+
analysis
of
texts
within
a
project
devoted
to
+L+
knowledge
acquisition
from
text.
We
focus
+L+
on
syntactic
processing
and
present
some
+L+
key
elements
of
the
projects
parser
that
+L+
allow
it
to
deal
successfully
with
technical
+L+
texts.
The
parser
is
fully
implemented
and
+L+
tested
on
a
variety
of
real
texts;
+L+
improvements
and
enhancements
are
in
+L+
progress.
Because
our
knowledge
acquisition
+L+
method
assumes
no
a
priori
model
of
the
+L+
domain
of
the
source
text,
the
parser
relies
+L+
as
much
as
possible
on
lexical
and
syntactic
+L+
clues.
That
is
why
it
strives
for
full
+L+
syntactic
analysis
rather
than
some
form
of
+L+
text
skimming.
We
present
a
practical
+L+
approach
to
four
acknowledged
difficult
+L+
problems
which
to
date
have
no
generally
+L+
acceptable
answers:
phrase
attachment;
time
+L+
constraints
for
problematic
input
(how
to
+L+
avoid
long
and
unproductive
computation);
+L+
parsing
conjoined
structures
(how
to
+L+
preserve
broad
coverage
without
losing
+L+
control
of
the
parsing
process);
and
the
+L+
treatment
of
fragmentary
input
or
fragments
+L+
that
are
a
byproduct
of
a
fallback
parsing
+L+
strategy.
We
review
recent
related
work
and
+L+
conclude
by
listing
several
future
work
+L+
items.
+L+
Key
Words
+L+
Text
processing,
knowledge
acquisition
from
+L+
text,
broad-coverage
parsing,
parsing
conjoined
+L+
structures.
+L+
1.
Introduction

Handling
Infeasible
Specifications
of
Cryptographic
Protocols
+L+
Li
Gong
+L+
ORA
Corporation
Cornell
University
+L+
301A
Dates
Drive
Dept.
of
Computer
Science
+L+
Ithaca,
NY
14850
Ithaca,
NY
14853
+L+
Abstract
+L+
In
the
verification
of
cryptographic
protocols
along
the
+L+
approach
of
the
logic
for
authentication
by
Burrows,
+L+
Abadi,
and
Needham,
it
is
possible
to
write
a
specification
which
does
not
faithfully
represent
the
real
world
+L+
situation.
Such
a
specification,
though
impossible
or
+L+
unreasonable
to
implement,
can
go
undetected
and
be
+L+
verified
to
be
correct.
It
can
also
lead
to
logical
statements
that
do
not
preserve
causality,
which
in
turn
can
+L+
have
undesirable
consequences.
Such
a
specification,
+L+
called
an
infeasible
specification
here,
can
be
subtle
and
+L+
hard
to
locate.
This
note
shows
how
the
logic
of
cryptographic
protocols
by
Gong,
Needham,
and
Yahalom
+L+
can
be
enhanced
with
a
notion
of
eligibility
to
preserve
+L+
causality
of
beliefs
and
detect
infeasible
specifications.
+L+
It
is
conceivable
that
this
technique
can
be
adopted
in
+L+
other
similar
logics.
+L+
1
Introduction

Abstract
Datatypes
in
PVS
+L+
S.
Owre
N.
Shankar
+L+
Computer
Science
Laboratory
+L+
SRI
International
+L+
Menlo
Park
CA
94025
+L+
Phone:
(415)859-5272
+L+
fowre,
shankarg@csl.sri.com
+L+
URL:
http://www.csl.sri.com/sri-csl-fm.html
+L+
1
The
development
of
PVS
was
funded
by
internal
research
funding
from
SRI
International.
Support
for
the
preparation
of
this
document
came
from
the
National
Aeronautics
and
Space
Administration
Langley
Research
Center
under
Contract
NAS1-18969.
This
report
is
a
revised
and
updated

IEEE
TRANSACTIONS
ON
KNOWLEDGE
AND
DATA
ENGINEERING,
7(5),
OCTOBER
1995
1
+L+
Enriching
the
Expressive
Power
of
Security
Labels
+L+
Li
Gong
and
Xiaolei
Qian
+L+
Abstract|
Common
security
models
such
as
Bell-LaPadula
+L+
focus
on
the
control
of
access
to
sensitive
data
but
leave
+L+
some
important
systems
issues
unspecified,
such
as
the
implementation
of
read-only
objects,
garbage
collection,
and
+L+
object
upgrade
and
downgrade
paths.
Consequently,
different
implementations
of
the
same
security
model
may
have
+L+
conflicting
operational
and
security
semantics.
We
propose
+L+
the
use
of
more
expressive
security
labels
for
specifying
these
+L+
system
issues
within
the
security
model,
so
that
the
semantics
of
a
system
design
are
precisely
understood
and
are
+L+
independent
of
implementation
details.
+L+
Keywords|
Data
security,
garbage
collection,
multilevel
+L+
security,
object
label,
read-only
object.
+L+
I.
Introduction

Indexing
PROLOG
Procedures
into
DAGs
+L+
by
Heuristic
Classification
+L+
Michael
Sintek
+L+
DFKI
+L+
Postfach
2080
+L+
67608
Kaiserslautern
+L+
Germany
+L+
May
5,
1994
+L+
Abstract
+L+
This
paper
first
gives
an
overview
of
standard
PROLOG
indexing
and
+L+
then
shows,
in
a
step-by-step
manner,
how
it
can
be
improved
by
slightly
+L+
extending
the
WAM
indexing
instruction
set
to
allow
indexing
on
multiple
+L+
arguments.
Heuristics
are
described
that
overcome
the
difficulty
of
computing
the
indexing
WAM
code.
In
order
to
become
independent
from
a
+L+
concrete
WAM
instruction
set,
an
abstract
graphical
representation
based
+L+
on
DAGs
(called
DAXes)
is
introduced.
+L+
The
paper
includes
a
COMMON
LISP
listing
of
the
main
heuristics
+L+
implemented;
the
algorithms
were
developed
for
RELFUN,
a
relational-plus-functional
language,
but
can
easily
be
used
in
arbitrary
PROLOG
+L+
implementations.
+L+
The
ideas
described
in
this
paper
were
first
presented
at
the
Workshop
+L+
"Sprachen
fur
KI-Anwendungen,
Konzepte
-
Methoden
Implementierun-gen"
1992
in
Bad
Honnef
[SS92].
This
paper
is
part
of
a
collaborative
work
+L+
together
with
Werner
Stein
[Ste92].
+L+
+PAGE+

Layer
Reassignment
for
Antenna
Effect
+L+
Minimization
in
3-Layer
Channel
Routing
+L+
Zhan
Chen
and
Israel
Koren
+L+
Department
of
Electrical
and
Computer
Engineering
+L+
University
of
Massachusetts,
Amherst,
MA
01003
+L+
Abstract
+L+
As
semiconductor
technology
enters
the
deep
submicron
era,
reliability
has
+L+
become
a
major
challenge
in
the
design
and
manufacturing
of
next
generation
+L+
VLSI
circuits.
In
this
paper
we
focus
on
one
reliability
issue
the
antenna
+L+
effect
in
the
context
of
3-layer
channel
routing.
We
first
present
an
antenna
effect
model
in
3-layer
channel
routing
and,
based
on
this,
an
antenna
effect
cost
+L+
function
is
proposed.
A
layer
reassignment
approach
is
adopted
to
minimize
this
+L+
cost
function
and
we
show
that
the
layer
reassignment
problem
can
be
formulated
as
a
network
bipartitioning
problem.
Experimental
results
show
that
the
+L+
antenna
effect
can
be
reduced
considerably
by
applying
the
proposed
technique.
+L+
Compared
with
previous
work,
one
advantage
of
our
approach
is
that
no
extra
+L+
channel
area
is
required
for
antenna
effect
minimization.
We
show
that
layer
+L+
reassignment
technique
can
be
used
in
yield-related
critical
area
minimization
+L+
in
3-layer
channel
routing
as
well.
The
trade-off
between
these
two
objectives
is
+L+
also
presented.
+L+
1:
Introduction

Optimal
Routing
Control:
Game
Theoretic
Approach
+L+
Richard
J.
La
,
and
Venkat
Anantharam
+L+
Department
of
Electrical
Engineering
and
Computer
Science
+L+
University
of
California
at
Berkeley
+L+
hyongla@eecs.berkeley.edu,
ananth@eecs.berkeley.edu
+L+
Abstract
+L+
Communication
networks
shared
by
selfish
users
are
+L+
considered
and
modeled
as
noncooperative
repeated
+L+
games.
Each
user
is
interested
only
in
optimizing
its
+L+
own
performance
by
controlling
the
routing
of
its
load.
+L+
We
investigate
the
existence
of
a
NEP
that
achieves
+L+
the
system-wide
optimal
cost.
The
existence
of
a
NEP
+L+
that
not
only
achieves
the
system-wide
optimal
cost
but
+L+
also
yields
a
cost
for
each
user
no
greater
than
its
stage
+L+
game
NEP
cost
is
shown
for
two-node
multiple
link
networks.
It
is
shown
that
more
general
networks
where
+L+
all
users
have
the
same
source-destination
pair
have
+L+
a
NEP
that
achieves
the
minimum
total
system
cost
+L+
under
a
mild
technical
condition.
It
is
shown
general
+L+
networks
with
users
having
multiple
source-destination
+L+
pairs
don't
necessarily
have
such
an
NEP.
+L+
1
Introduction

TIME-FREQUENCY
SIGNAL
MODELS
FOR
MUSIC
ANALYSIS,
+L+
TRANSFORMATION,
AND
SYNTHESIS
+L+
Michael
Goodwin
Martin
Vetterli
+L+
Department
of
Electrical
Engineering
and
Computer
Science
&
Center
for
New
Music
and
Audio
Technologies
+L+
University
of
California
at
Berkeley
+L+
ABSTRACT
+L+
In
signal
analysis-synthesis,
the
analysis
derives
a
set
of
parameters
that
the
synthesis
uses
to
reconstruct
the
original
+L+
signal.
In
musical
applications,
this
reconstruction
should
+L+
be
perceptually
accurate,
and
the
parameterization
should
+L+
allow
for
such
desirable
signal
modifications
as
time-scaling,
+L+
pitch-shifting,
and
cross-synthesis;
the
analysis
parameters
+L+
should
correspond
to
a
signal
model
that
is
flexible
enough
+L+
to
allow
these
transformations.
Sinusoidal
modeling
meets
+L+
this
flexibility
requirement,
but
has
difficulty
representing
+L+
some
salient
features
of
musical
signals
such
as
attack
transients
and
noiselike
processes.
In
this
paper,
sinusoidal
+L+
modeling
is
reviewed
and
some
variations
are
proposed
to
+L+
account
for
its
shortcomings;
also,
wavelet-based
representations
of
musical
signals
are
considered.
+L+
1.
SIGNAL
MODELING
FOR
MUSIC

Capacity,
Mutual
Information,
and
Coding
+L+
for
Finite-State
Markov
Channels
+L+
Andrea
J.
Goldsmith
,
Member,
IEEE
and
Pravin
P.
Varaiya
,
Fellow,
IEEE
+L+
Abstract
+L+
The
Finite-State
Markov
Channel
(FSMC)
is
a
discrete-time
varying
channel
whose
variation
is
determined
by
a
finite-state
Markov
process.
These
channels
have
memory
due
to
the
+L+
Markov
channel
variation.
We
obtain
the
FSMC
capacity
as
a
function
of
the
conditional
channel
+L+
state
probability.
We
also
show
that
for
i.i.d.
channel
inputs,
this
conditional
probability
converges
+L+
weakly,
and
the
channel's
mutual
information
is
then
a
closed-form
continuous
function
of
the
input
+L+
distribution.
+L+
We
next
consider
coding
for
FSMCs.
In
general,
the
complexity
of
maximum-likelihood
+L+
decoding
grows
exponentially
with
the
channel
memory
length.
Therefore,
in
practice,
interleaving
+L+
and
memoryless
channel
codes
are
used.
This
technique
results
in
some
performance
loss
relative
+L+
to
the
inherent
capacity
of
channels
with
memory.
We
propose
a
maximum-likelihood
decision-feedback
decoder
with
complexity
that
is
independent
of
the
channel
memory.
We
calculate
the
+L+
capacity
and
cutoff
rate
of
our
technique,
and
show
that
it
preserves
the
capacity
of
certain
FSMCs.
+L+
We
also
compare
the
performance
of
the
decision-feedback
decoder
with
that
of
interleaving
and
+L+
memoryless
channel
coding
on
a
fading
channel
with
4PSK
modulation.
+L+
Index
Terms:
Finite-State
Markov
Channels,
Capacity,
Mutual
Information,
Decision-Feedback
+L+
Maximum-Likelihood
Decoding.
+L+
Work
supported
in
part
by
an
IBM
graduate
fellowship,
and
in
part
by
the
PATH
program,
Institute
of
Transportation
Studies,
University
of
California,
Berkeley.
+L+
A.
Goldsmith
is
with
the
Department
of
Electrical
Engineering,
California
Institute
of
Technology,
Pasadena,
CA
+L+
91125.
+L+
P.
Varaiya
is
with
the
Department
of
Electrical
Engineering
and
Computer
Science,
University
of
California,
+L+
Berkeley,
CA
94720.
+L+
+PAGE+

An
Analysis
of
Geographical
Push-Caching
+L
James
Gwertzman,
Microsoft
Corporation
+L+
Margo
Seltzer,
Harvard
University
+L+
Abstract
+L+
Most
caching
schemes
in
wide-area,
distributed
systems
are
client-initiated.
Decisions
of
when
and
+L+
where
to
cache
information
are
made
without
the
benefit
of
the
server's
global
knowledge
of
the
usage
+L+
patterns.
In
this
paper,
we
present
a
new
caching
strategy:
geographical
push-caching.
Using
the
server's
+L+
global
knowledge
and
a
derived
network
topology,
we
distribute
data
to
cooperating
servers.
The
World
+L+
Wide
Web
is
an
example
of
a
wide-area
system
that
will
benefit
from
distance-sensitive
caching,
and
we
+L+
present
an
architecture
that
allows
a
Web
server
to
autonomously
replicate
HTML
pages.
We
use
a
trace-driven
simulation
to
evaluate
several
competing
caching
strategies.
Our
results
show
that
geographical
+L+
push-caching
reduces
bandwidth
consumption
and
sever
load
by
the
same
amount
as
web
proxy
caching,
+L+
but
with
a
savings
in
global
cache
space
of
almost
two
orders
of
magnitude.
More
importantly,
servers
+L+
that
wish
to
reduce
Internet
bandwidth
consumption
and
their
load
can
do
so
without
waiting
for
web
+L+
proxies
to
be
implemented
world-wide.
Furthermore,
geographical
push-caching
helps
distribute
server
+L+
load
for
all
web
servers,
not
just
the
most
popular
as
is
the
case
with
proxy
caching.
+L+
1
Introduction

Efficient
Formulation
for
Optimal
Modulo
Schedulers
+L+
Alexandre
E.
Eichenberger
Edward
S.
Davidson
+L+
ECE
Department
EECS
Department
+L+
North
Carolina
State
University
University
of
Michigan
+L+
Raleigh,
NC
27695-7911
Ann
Arbor,
MI
48109-2122
+L+
alexe@eos.ncsu.edu
davidson@eecs.umich.edu
+L+
Abstract
+L+
Modulo
scheduling
algorithms
based
on
optimal
+L+
solvers
have
been
proposed
to
investigate
and
tune
the
+L+
performance
of
modulo
scheduling
heuristics.
While
+L+
recent
advances
have
broadened
the
scope
for
which
+L+
the
optimal
approach
is
applicable,
this
approach
+L+
increasingly
suffers
from
large
execution
times.
In
this
+L+
paper,
we
propose
a
more
efficient
formulation
of
the
+L+
modulo
scheduling
space
that
significantly
decreases
+L+
the
execution
time
of
solvers
based
on
integer
linear
+L+
programs.
For
example,
the
total
execution
time
is
+L+
reduced
by
a
factor
of
8.6
when
782
loops
from
the
+L+
Perfect
Club,
SPEC,
and
Livermore
Fortran
Kernels
+L+
are
scheduled
for
minimum
register
requirements
using
+L+
the
more
efficient
formulation
instead
of
the
traditional
+L+
formulation.
Experimental
evidence
further
indicates
+L+
that
significantly
larger
loops
can
be
scheduled
under
+L+
realistic
machine
constraints.
+L+
1
Introduction

A
Scalable
Key
Distribution
Hierarchy
+L+
Patrick
McDaniel
Sugih
Jamin
+L+
Electrical
Engineering
and
Computer
Science
Department
+L+
University
of
Michigan
+L+
Ann
Arbor,
MI
48109-2122
+L+
fpdmcdan,jaming@eecs.umich.edu
+L+
April
13,
1998
+L+
Abstract
+L+
As
the
use
of
the
Internet
for
electronic
commerce,
audio
+L+
and
video
conferencing,
and
other
applications
with
sensitive
content
grows,
the
need
for
secure
services
becomes
+L+
critical.
Central
to
the
success
of
these
services
is
the
support
for
secure
public
key
distribution.
Although
there
are
+L+
several
existing
services
available
for
this
purpose,
they
+L+
are
not
very
scalable,
either
because
they
depend
on
a
centralized
server
or
rely
on
ad
hoc
trust
relationships.
+L+
In
this
paper,
we
present
and
examine
a
flexible
approach
to
certificate
distribution
scalable
to
arbitrarily
+L+
large
networks.
We
propose
a
two
level
hierarchy
where
+L+
certificates
can
be
independently
authenticated
by
one
or
+L+
more
peer
authorities,
called
keyservers.
Certificates
for
+L+
end-user
and
host
entities
are
managed
within
local
domains,
called
enterprises.
By
administering
certificates
+L+
close
to
the
source,
we
reduce
the
load
on
the
key
servers
+L+
and
the
effects
of
network
topology
changes.
We
describe
+L+
the
design
of
our
system
and
present
a
preliminary
performance
analysis
based
on
traces
of
present-day
DNS
requests.
+L+
1
Introduction

A
Novel
Framework
for
Decentralized
Supervisory
Control
with
Communication
+L+
George
Barrett
+L+
grbarret@eecs.umich.edu
+L+
Stephane
Lafortune
+L+
stephane@eecs.umich.edu
+L+
Department
of
Electrical
Engineering
and
Computer
Science
+L+
The
University
of
Michigan
+L+
1301
Beal
Avenue
+L+
Ann
Arbor,
MI
48109-2122
+L+
ABSTRACT
+L+
The
decentralized
control
problem
that
we
address
in
+L+
this
paper
is
that
of
several
communicating
supervisory
+L+
controllers,
each
with
different
information,
working
in
+L+
concert
to
exactly
achieve
a
given
legal
sublanguage
of
+L+
the
uncontrolled
system's
language
model.
We
present
+L+
a
novel
information
structure
formalism
for
dealing
with
+L+
this
class
of
problems.
Preliminary
results
are
presented
+L+
which
elucidate
a
fundamental
concept
in
decentralized
+L+
control
problems:
the
importance
of
controllers
anticipating
future
possible
communications.
+L+
1.
INTRODUCTION

Internet
Routing
Instability
+L+
Craig
Labovitz
,
G.
Robert
Malan
,
and
Farnam
Jahanian
+L+
University
of
Michigan
+L+
Department
of
Electrical
Engineering
and
Computer
Science
+L+
1301
Beal
Ave.
+L+
Ann
Arbor,
Michigan
48109-2122
+L+
flabovit,
rmalan,
farnamg@eecs.umich.edu
+L+
Abstract
+L+
This
paper
examines
the
network
inter-domain
routing
information
exchanged
between
backbone
service
providers
at
+L+
the
major
U.S.
public
Internet
exchange
points.
Internet
+L+
routing
instability,
or
the
rapid
fluctuation
of
network
reach-ability
information,
is
an
important
problem
currently
facing
the
Internet
engineering
community.
High
levels
of
network
instability
can
lead
to
packet
loss,
increased
network
+L+
latency
and
time
to
convergence.
At
the
extreme,
high
levels
of
routing
instability
have
lead
to
the
loss
of
internal
+L+
connectivity
in
wide-area,
national
networks.
In
this
paper,
+L+
we
describe
several
unexpected
trends
in
routing
instability,
+L+
and
examine
a
number
of
anomalies
and
pathologies
observed
in
the
exchange
of
inter-domain
routing
information.
+L+
The
analysis
in
this
paper
is
based
on
data
collected
from
+L+
BGP
routing
messages
generated
by
border
routers
at
five
+L+
of
the
Internet
core's
public
exchange
points
during
a
nine
+L+
month
period.
We
show
that
the
volume
of
these
routing
updates
is
several
orders
of
magnitude
more
than
expected
and
+L+
that
the
majority
of
this
routing
information
is
redundant,
+L+
or
pathological.
Furthermore,
our
analysis
reveals
several
+L+
unexpected
trends
and
ill-behaved
systematic
properties
in
+L+
Internet
routing.
We
finally
posit
a
number
of
explanations
+L+
for
these
anomalies
and
evaluate
their
potential
impact
on
+L+
the
Internet
infrastructure.
+L+
1
Introduction

Distributed
Pipeline
Scheduling:
End-to-End
Analysis
of
+L+
Heterogeneous,
Multi-Resource
Real-Time
Systems
+L+
Saurav
Chatterjee
and
Jay
Strosnider
+L+
Department
of
Electrical
&
Computer
Engineering
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213
+L+
This
research
was
supported
in
part
by
a
grant
from
the
Office
+L+
of
Naval
Research,
in
part
by
a
grant
from
the
Naval
Research
+L+
and
Development
Laboratory,
and
in
part
by
a
grant
from
+L+
Siemens
Corporate
Research.
+L+
In
15th
IEEE
International
Conference
on
Distributed
Computing
Systems,
May
1995.
+L+
Abstract
+L+
This
paper
presents
an
hierarchical
end-to-end
+L+
analysis
technique
that
decomposes
the
very
complex
+L+
heterogeneous
multi-resource
scheduling
problem
into
+L+
a
set
of
single
resource
scheduling
problems
with
well
+L+
defined
interactions.
We
define
heterogeneity
both
in
+L+
resource
types,
e.g.,
CPU,
and
in
scheduling
policies,
+L+
e.g.,
rate-monotonic
scheduling.
This
analysis
+L+
technique
is
one
phase
of
our
systems
integration
+L+
framework
for
designing
large-scale,
heterogeneous,
+L+
distributed
real-time
systems
whose
timing
properties
+L+
can
be
strictly
controlled
and
analyzed.
This
approach,
+L+
denoted
the
Distributed
Pipelining
Framework,
+L+
exploits
the
natural
pipelining
execution
pattern
found
+L+
in
a
large
number
of
continuous
(periodic)
applications
+L+
executing
over
heterogenous
resources.
A
teleconference
application
is
used
in
this
paper
to
show
the
+L+
utility
of
the
approach.
+L+
1.
Introduction

Some
Geographical
Applications
of
+L+
Genetic
Programming
on
the
Cray
T3D
+L+
Supercomputer
+L+
I.
Turton
,
S.
Openshawand
G.
Diplock
+L+
School
of
Geography,
University
of
Leeds,
Leeds,
UK
+L+
email:
ian@geog.leeds.ac.uk,
stan@geog.leeds.ac.uk,
gary@geog.leeds.ac.uk
+L+
April
15,
1996
+L+
Abstract
+L+
The
paper
describes
some
geographical
applications
of
a
parallel
GP
code
+L+
which
is
run
on
a
Cray
T3D
512
processor
supercomputer
to
create
new
+L+
types
of
well
performing
mathematical
models.
A
series
of
results
are
described
which
allude
to
the
potential
power
of
the
method
for
which
there
+L+
are
many
practical
applications
in
spatial
data
rich
environments
where
+L+
there
are
no
suitable
existing
models
and
no
soundly
based
theoretical
+L+
framework
on
which
to
base
them.
+L+
1
Introduction

,
,
1-30
()
+L+
c
Kluwer
Academic
Publishers,
Boston.
Manufactured
in
The
Netherlands.
+L+
On
the
Optimality
of
the
Simple
Bayesian
+L+
Classifier
under
Zero-One
Loss
+L+
PEDRO
DOMINGOS
pedrod@ics.uci.edu
+L+
MICHAEL
PAZZANI
pazzani@ics.uci.edu
+L+
Department
of
Information
and
Computer
Science,
University
of
California,
Irvine,
CA
92697
+L+
Editor:
Gregory
Provan
+L+
Abstract.
The
simple
Bayesian
classifier
is
known
to
be
optimal
when
attributes
are
independent
+L+
given
the
class,
but
the
question
of
whether
other
sufficient
conditions
for
its
optimality
exist
has
+L+
so
far
not
been
explored.
Empirical
results
showing
that
it
performs
surprisingly
well
in
many
+L+
domains
containing
clear
attribute
dependences
suggest
that
the
answer
to
this
question
may
be
+L+
positive.
This
article
shows
that,
although
the
Bayesian
classifier's
probability
estimates
are
only
+L+
optimal
under
quadratic
loss
if
the
independence
assumption
holds,
the
classifier
itself
can
be
+L+
optimal
under
zero-one
loss
(misclassification
rate)
even
when
this
assumption
is
violated
by
a
+L+
wide
margin.
The
region
of
quadratic-loss
optimality
of
the
Bayesian
classifier
is
in
fact
a
second-order
infinitesimal
fraction
of
the
region
of
zero-one
optimality.
This
implies
that
the
Bayesian
+L+
classifier
has
a
much
greater
range
of
applicability
than
previously
thought.
For
example,
in
this
+L+
article
it
is
shown
to
be
optimal
for
learning
conjunctions
and
disjunctions,
even
though
they
+L+
violate
the
independence
assumption.
Further,
studies
in
artificial
domains
show
that
it
will
often
+L+
outperform
more
powerful
classifiers
for
common
training
set
sizes
and
numbers
of
attributes,
even
+L+
if
its
bias
is
a
priori
much
less
appropriate
to
the
domain.
This
article's
results
also
imply
that
+L+
detecting
attribute
dependence
is
not
necessarily
the
best
way
to
extend
the
Bayesian
classifier,
+L+
and
this
is
also
verified
empirically.
+L+
Keywords:
Simple
Bayesian
classifier,
naive
Bayesian
classifier,
zero-one
loss,
optimal
classification,
induction
with
attribute
dependences
+L+
1.
Introduction

INTERNATIONAL
COMPUTER
SCIENCE
INSTITUTE
+L+
I
1947
Center
St.
*
Suite
600
*
Berkeley,
California
94704-1198
*
(510)
643-9153
*
FAX
(510)
643-7684
+L+
Some
MPEG
Decoding
Functions
+L+
on
Spert
+L+
An
Example
for
Assembly
+L+
Programmers
+L+
Arno
Formella
+L+
TR-94-027
+L+
October
1994
+L+
Abstract
+L+
We
describe
our
method
how
to
implement
C-program
sequences
in
torrent
(T0)
+L+
assembler
code
while
there
is
no
efficient
automatic
tool.
We
use
re-structuring
of
+L+
the
source
code,
vectorization,
dataflow
graphs,
a
simple
scheduling
strategy
and
a
+L+
straight
forward
register
allocation
algorithm.
We
define
some
lower
and
an
upper
+L+
bound
for
the
expected
run
time.
For
two
functions,
namely
the
color
transformation
+L+
and
reverse
DCT,
we
achieve
almost
54,
respectively
16
times
the
performance
of
a
+L+
Sparc
2
workstation.
+L+
+PAGE+

Storage
of
Two
and
Three
Dimensional
Raster
Type
Data
for
+L+
Optimized
Retrieval
of
One,
Two
or
Three
Dimensional
Features
+L+
Kjell
Bratbergsengen
+L+
Department
of
Computer
Systems
and
Information
Science
+L+
Norwegian
University
of
Science
and
Technology,
Trondheim,
Norway
+L+
email:
kjellb@idi.ntnu.no
+L+
Abstract
+L+
We
are
analyzing
storage
structures
for
two
and
three
dimensional
raster
type
data
which
are
used
+L+
for
feature
retrieval.
The
features
are
one,
two
or
three
dimensional
objects
with
regular
outlines
+L+
like
a
rectangle
or
a
prism.
The
features
could
be
parts
of
a
map
or
image,
an
area
of
special
+L+
interest
for
searching
after
oil,
a
sequence
of
ultra
sound
images,
and
so
on.
The
storage
medium
+L+
is
magnetic
disk.
The
data
are
stored
in
chunks
or
blocks
representing
a
regular
part
of
the
source
+L+
object.
We
analyze
the
shape
and
the
size
to
minimize
the
cost
of
retrieval.
The
optimization
is
+L+
based
on
minimum
time
to
do
retrieval.
We
have
five
combinations:
Lines
and
areas
from
areas
and
+L+
volumes,
and
volumes
from
volumes.
The
optimal
block
sizes
for
random
retrieval
varies,
with
case,
+L+
feature
size
and
disk
characteristics.
One
general
observation
is
that
longer
disk
tracks
gives
larger
+L+
blocks.
For
line
retrieval
the
optimal
block
size
is
only
depending
on
disk
track
length.
For
other
+L+
cases
it
is
also
depending
on
the
feature
size.
For
partly
sequential
retrieval
the
block
size
is
not
the
+L+
actual
block
size
used
during
retrieval,
but
the
smallest
addressing
unit,
and
the
optimal
addressing
+L+
unit
could
be
rather
small.
+L+
The
analysis
reveals
that
using
too
small
blocks
could
be
very
costly.
The
time
could
easily
+L+
double
or
triple
if
small
blocks
are
used.
In
many
cases
the
optimal
block
size
is
several
tracks.
+L+
Keywords:
storage
and
retrieval,
matrix,
optimization
+L+
1
Problem
Introduction

Write
Optimized
Object-Oriented
Database
Systems
+L+
Kjetil
Nrvag
and
Kjell
Bratbergsengen
+L+
Department
of
Computer
and
Information
Science
+L+
Norwegian
University
of
Science
and
Technology
+L+
7034
Trondheim,
Norway
+L+
fnoervaag,
kjellbg@idi.ntnu.no
+L+
Abstract
+L+
In
a
database
system,
read
operations
are
much
more
+L+
common
than
write
operations,
and
consequently,
database
+L+
systems
have
been
read
optimized.
As
the
size
of
main
memory
increases,
more
of
the
database
read
requests
will
be
satisfied
from
the
buffer
system,
and
the
amount
of
disk
write
+L+
operations
relative
to
disk
read
operations
will
increase.
+L+
This
calls
for
a
focus
on
write
optimized
database
systems.
+L+
In
this
paper,
we
present
solutions
to
this
problem.
We
describe
in
detail
the
data
structures
and
algorithms
needed
+L+
to
realize
a
write
optimized
object-oriented
database
system
+L+
in
the
context
of
Vagabond,
an
OODB
currently
being
implemented
at
our
department.
In
Vagabond,
focus
has
been
+L+
to
provide
support
for
applications
which
have
earlier
used
+L+
file
systems
because
of
the
limited
data
bandwidth
in
current
+L+
database
systems,
typical
examples
are
super
computing
applications
and
geographical
information
systems
+L+
1.
Introduction

Signal
and
Image
Processing
in
Java
+L+
Jonathan
Campbell
and
Fionn
Murtagh
+L+
University
of
Ulster,
Magee
College,
Derry,
BT48
7JL
+L+
email:
jg.campbell@ulst.ac.uk.
+L+
Revisions
available
from
+L+
http://www.infm.ulst.ac.uk/research/preprints.html
+L+
Original
paper
presented
at
IMVIP
'97
+L+
University
of
Ulster,
Magee
College,
Derry
+L+
10-13
September
1997.
+L+
9
September
1997
+L+
Revised
20
September
1997
+L+
Revised
6
November
1997
+L+
Abstract
+L+
We
describe
the
implementation
of
a
multi-purpose
data
analysis
laboratory,
DataLab-J,
in
+L+
the
programming
language
Java.
We
briefly
trace
the
stages
of
the
evolution
of
DataLab
from
a
+L+
FORTRAN-IV
system
in
1973
to
the
current
Java
development.
Description
of
this
evolution
allows
+L+
us
to
discuss
some
key
design
and
functionality
decisions
and
issues
that
arose
throughout
the
years;
+L+
many
of
these
issues
remain
topical,
so,
in
addition
to
an
evaluation
of
Java,
we
identify
and
discuss
+L+
what
are
for
us
the
major
issues
in
the
design
of
such
software.
Moreover,
we
address
questions
+L+
raised
by
the
need
to
convert
legacy
systems,
e.g.
those
programmed
in
C
and
various
versions
of
+L+
FORTRAN.
The
experience
of
redesign
and
implementation
in
Java
is
described,
together
with
a
+L+
brief
evaluation
of
the
suitability
of
Java
for
'number-crunching'.
Overall
conclusions
are
drawn,
+L+
regarding
design
of
such
software,
lessons
learned,
traps
to
avoid,
and
on
Java
itself.
+L+
1
Introduction

SEE
ME,
HEAR
ME:
INTEGRATING
AUTOMATIC
SPEECH
RECOGNITION
AND
+L+
LIP-READING
+L+
Paul
Duchnowski
1
Uwe
Meier
1
Alex
Waibel
1;2
+L+
1
University
of
Karlsruhe,
Karlsruhe,
Germany
2
Carnegie
Mellon
University,
Pittsburgh
PA,
USA
+L+
ABSTRACT
+L+
We
present
recent
work
on
integration
of
visual
information
(automatic
lip-reading)
with
acoustic
speech
for
better
overall
speech
recognition.
A
Multi-State
Time
Delay
+L+
Neural
Network
performs
the
recognition
of
spelled
letter
+L+
sequences
taking
advantage
of
lip
images
from
a
standard
+L+
camera.
The
problems
addressed
include
efficient
but
effective
representation
of
the
visual
information
and
optimum
+L+
manner
of
combining
the
two
modalities
when
rendering
a
+L+
decision.
We
show
results
for
several
alternatives
to
direct
+L+
gray
level
image
as
the
visual
evidence.
These
are:
Principal
+L+
Components,
Linear
Discriminants,
and
DFT
coefficients.
+L+
Dimensionality
of
the
input
is
decreased
by
a
factor
of
12
+L+
while
maintaining
recognition
rates.
Combination
of
the
+L+
visual
and
acoustic
information
is
performed
at
three
different
levels
of
abstraction.
Results
suggest
that
integration
+L+
of
higher
order
input
features
works
best.
On
a
continuous
+L+
spelling
task,
visual-alone
recognition
of
45-55%,
when
combined
with
acoustic
data,
lowers
audio-alone
error
rates
by
+L+
30-40%.
+L+
1.
INTRODUCTION

Tracking
Drifting
Concepts
+L+
By
Minimizing
Disagreements
+L+
David
P.
Helmbold
and
Philip
M.
Long
+L+
CIS
Board
+L+
UC
Santa
Cruz
+L+
Santa
Cruz,
CA
95064
+L+
March
24,
1994
+L+
Abstract
+L+
In
this
paper
we
consider
the
problem
of
tracking
a
subset
of
a
domain
(called
the
target)
which
+L+
changes
gradually
over
time.
A
single
(unknown)
probability
distribution
over
the
domain
is
used
+L+
to
generate
random
examples
for
the
learning
algorithm
and
measure
the
speed
at
which
the
target
+L+
changes.
+L+
Clearly,
the
more
rapidly
the
target
moves,
the
harder
it
is
for
the
algorithm
to
maintain
a
good
+L+
approximation
of
the
target.
Therefore
we
evaluate
algorithms
based
on
how
much
movement
of
+L+
the
target
can
be
tolerated
between
examples
while
predicting
with
accuracy
*.
Furthermore,
the
+L+
complexity
of
the
class
H
of
possible
targets,
as
measured
by
d,
its
VC-dimension,
also
effects
the
+L+
difficulty
of
tracking
the
target
concept.
+L+
We
show
that
if
the
problem
of
minimizing
the
number
of
disagreements
with
a
sample
from
+L+
among
concepts
in
a
class
H
can
be
approximated
to
within
a
factor
k,
then
there
is
a
simple
tracking
+L+
algorithm
for
H
which
can
achieve
a
probability
*
of
making
a
mistake
if
the
target
movement
rate
+L+
is
at
most
a
constant
times
*
2
=(k(d
+
k)
ln
1
+L+
*
),
where
d
is
the
Vapnik-Chervonenkis
dimension
of
+L+
H.
Also,
we
show
that
if
H
is
properly
PAC-learnable,
then
there
is
an
efficient
(randomized)
+L+
algorithm
that
with
high
probability
approximately
minimizes
disagreements
to
within
a
factor
of
+L+
7d
+
1,
yielding
an
efficient
tracking
algorithm
for
H
which
tolerates
drift
rates
up
to
a
constant
+L+
times
*
2
=(d
2
ln
1
+L+
In
addition,
we
prove
complementary
results
for
the
classes
of
halfspaces
and
axis-aligned
hy
+L+
perrectangles
showing
that
the
maximum
rate
of
drift
that
any
algorithm
(even
with
unlimited
+L+
computational
power)
can
tolerate
is
a
constant
times
*
2
=d.
+L+
1
Introduction

Understanding
Neural
Networks
via
Rule
Extraction
+L+
Rudy
Setiono
and
Huan
Liu
+L+
Department
of
Information
Systems
and
Computer
Science
+L+
National
University
of
Singapore
+L+
Kent
Ridge,
Singapore
0511
+L+
frudys,liuhg@iscs.nus.sg
+L+
Abstract
+L+
Although
backpropagation
neural
networks
+L+
generally
predict
better
than
decision
trees
do
+L+
for
pattern
classification
problems,
they
are
often
regarded
as
black
boxes,
i.e.,
their
predictions
are
not
as
interpretable
as
those
of
decision
trees.
This
paper
argues
that
this
is
because
there
has
been
no
proper
technique
that
+L+
enables
us
to
do
so.
With
an
algorithm
that
+L+
can
extract
rules
1
,
by
drawing
parallels
with
+L+
those
of
decision
trees,
we
show
that
the
predictions
of
a
network
can
be
explained
via
rules
extracted
from
it,
thereby,
the
network
can
be
understood.
Experiments
demonstrate
that
rules
+L+
extracted
from
neural
networks
are
comparable
with
those
of
decision
trees
in
terms
of
predictive
accuracy,
number
of
rules
and
average
+L+
number
of
conditions
for
a
rule;
they
preserve
+L+
high
predictive
accuracy
of
original
networks.
+L+
1
Introduction

To
appear
in
the
journal
Presence
+L+
06/02/97
4:57
PM
1
+L+
Integrating
Pedagogical
Agents
into
Virtual
Environments
+L+
W.
Lewis
Johnson
+L+
Jeff
Rickel
+L+
Randy
Stiles
+L+
Allen
Munro
1
+L+
Abstract
+L+
In
order
for
a
virtual
environment
to
be
effective
as
a
training
tool,
it
is
not
enough
to
+L+
concentrate
on
the
fidelity
of
the
renderings
and
the
accuracy
of
the
simulated
behaviors.
+L+
The
environment
should
help
trainees
develop
an
understanding
of
the
task
being
trained,
+L+
and
should
provide
guidance
and
assistance
as
needed.
This
paper
describes
a
system
for
+L+
developing
virtual
environments
in
which
pedagogical
capabilities
are
incorporated
into
+L+
autonomous
agents
that
interact
with
trainees.
These
pedagogical
agents
can
monitor
+L+
trainees
progress
and
provide
guidance
and
assistance.
The
agents
interact
with
+L+
simulations
of
objects
in
the
environment,
and
with
trainees.
The
paper
describes
the
+L+
architectural
features
of
the
environment
and
of
the
agents
that
permit
the
agents
to
meet
+L+
instructional
objectives
within
the
virtual
environment.
It
also
discusses
how
agent-based
+L+
instruction
is
combined
with
other
methods
of
delivering
instruction.
+L+
1.
Introduction

A
Multicast
Congestion
Control
Mechanism
Using
Representatives
+L+
Dante
DeLucia
Katia
Obraczka
+L+
Hughes
Research
Laboratories
USC
Information
Sciences
Institute
+L+
3011
Malibu
Canyon
Road
4676
Admiralty
Way
Suite
1001
+L+
Malibu
CA
90265
Marina
Del
Rey,
CA
90292
+L+
email:
dante@usc.edu
email:
katia@isi.edu
+L+
Abstract
+L+
In
this
paper,
we
propose
a
congestion
control
+L+
mechanism
for
reliable
multicast
applications
that
+L+
uses
a
small
set
of
group
members,
or
representatives,
+L+
to
provide
timely
and
accurate
feedback
on
behalf
of
+L+
congested
subtrees
of
a
multicast
distribution
tree.
+L+
Our
algorithm
does
not
need
to
compute
round-trip
+L+
time
(RTT)
from
all
receivers
to
the
source,
nor
does
+L+
it
require
knowledge
of
group
membership
or
network
+L+
topology.
Through
simulations,
we
evaluate
our
algorithm
with
and
without
TCP
cross
traffic.
This
initial
evaluation
study
shows
that
our
algorithm
takes
+L+
advantage
of
network
bandwidth
when
available,
yet
+L+
does
not
starve
competing
flows.
+L+
1
Introduction

Information
Gathering
Plans
With
Sensing
Actions
+L+
Naveen
Ashish
and
Craig
A.
Knoblock
Alon
Levy
+L+
Information
Sciences
Institute
and
AT&T
Bell
Laboratories
+L+
Department
of
Computer
Science
AI
Principles
Research
Dept.
+L+
University
of
Southern
California
600
Mountain
Ave.,
Room
2A-440
+L+
4676
Admiralty
Way
Murray
Hill,
NJ
07974
+L+
Marina
del
Rey,
CA
90292
levy@research.att.com
+L+
fashish,knoblockg@isi.edu
+L+
Abstract
+L+
Information
gathering
agents
can
automate
the
task
of
retrieving
and
integrating
data
+L+
from
a
large
number
of
diverse
information
sources.
The
key
issue
in
their
performance
is
+L+
efficient
query
planning
that
minimizes
the
number
of
information
sources
used
to
answer
+L+
a
query.
Previous
work
on
query
planning
has
considered
generating
information
gathering
+L+
plans
solely
based
on
compile-time
analysis
of
the
query
and
the
models
of
the
information
+L+
sources.
We
argue
that
at
compile-time
it
may
not
be
possible
to
generate
an
efficient
+L+
plan
for
retrieving
the
requested
information
because
of
the
large
number
of
possibly
+L+
relevant
sources.
We
describe
an
approach
that
naturally
extends
query
planning
to
use
+L+
run-time
information
to
optimize
queries
that
involve
many
sources.
First,
we
describe
an
+L+
algorithm
for
generating
a
discrimination
matrix,
which
is
a
data
structure
that
identifies
+L+
the
information
that
can
be
sensed
at
run-time
to
optimize
a
query
plan.
Next,
we
describe
+L+
how
the
discrimination
matrix
is
used
to
decide
which
of
the
possible
run-time
sensing
+L+
actions
to
perform.
Finally,
we
demonstrate
that
this
approach
yields
significant
savings
+L+
(over
90%
for
some
queries)
in
a
real-world
task.
+L+
The
first
and
second
authors
is
supported
in
part
by
Rome
Laboratory
of
the
Air
Force
Systems
Command
+L+
and
the
Advanced
Research
Projects
Agency
under
contract
no.
F30602-94-C-0210,
and
in
part
by
the
National
+L+
Science
Foundation
under
grant
number
IRI-9313993.
The
views
and
conclusions
contained
in
this
paper
are
the
+L+
author's
and
should
not
be
interpreted
as
representing
the
official
opinion
or
policy
of
ARPA,
RL,
NSF,
AT&T
+L+
Labs,
or
any
person
or
agency
connected
with
them.
+L+
+PAGE+

A
Synergy
of
Agent
Components:
+L+
Social
Comparison
for
Failure
Detection
+L+
Gal
A.
Kaminka
Milind
Tambe
+L+
Information
Sciences
Institute
and
Computer
Science
Department
+L+
University
of
Southern
California
+L+
4676
Admiralty
Way,
Marina
del
Rey,
CA
90292
+L+
galk,
tambe-@isi.edu
+L+
1
Overview

Adaptive
Agent
Tracking
in
Real-world
+L+
Multi-Agent
Domains:
A
Preliminary
Report
+L+
Milind
Tambe
,
Lewis
Johnson
and
Wei-Min
Shen
+L+
Information
Sciences
Institute
and
Computer
Science
Department
+L+
University
of
Southern
California
+L+
4676
Admiralty
Way,
Marina
del
Rey,
CA
90292
+L+
ftambe,johnson,sheng@isi.edu
+L+
October
30,
1996
+L+
Abstract
+L+
Intelligent
interaction
in
multi-agent
domains
frequently
requires
an
agent
to
track
other
+L+
agents'
mental
states:
their
current
goals,
beliefs,
and
intentions.
Accuracy
in
this
agent
+L+
tracking
task
is
critically
dependent
on
the
accuracy
of
the
tracker's
(tracking
agent's)
model
+L+
of
the
trackee
(tracked
agent).
Unfortunately,
in
real-world
situations,
model
imperfections
+L+
arise
due
to
the
tracker's
resource
and
information
constraints,
as
well
as
due
to
trackees'
+L+
dynamic
behavior
modification.
While
such
model
imperfections
are
unavoidable,
a
tracker
+L+
must
nonetheless
attempt
to
be
adaptive
in
its
agent
tracking.
This
article
identifies
key
issues
+L+
in
adaptive
agent
tracking
and
presents
an
approach
called
DEFT.
At
its
core,
DEFT
is
based
+L+
on
discrimination-based
learning.
The
main
idea
is
to
identify
the
deficiency
of
a
model
based
+L+
on
tracking
failures,
and
revise
the
model
by
using
features
that
are
critical
in
discriminating
+L+
successful
and
failed
tracking
episodes.
Because
in
real-world
situations
the
set
of
candidate
+L+
discriminating
features
is
very
large,
DEFT
relies
on
knowledge-based
focusing
to
limit
the
+L+
discrimination
to
those
features
that
it
determines
were
relevant
in
successful
tracking
episodes
+L+
with
an
autonomous
explanation
capability
as
a
major
source
of
this
knowledge.
This
article
+L+
reports
on
experiments
with
an
implementation
of
key
aspects
of
DEFT
in
a
complex
synthetic
+L+
air-to-air
combat
domain.
+L+
+PAGE+

To
appear
in
Proceedings
of
the
Second
International
Conference
on
AI
Planning
Systems
(1994).
Chicago:
AAAI
Press.
+L+
Reactive
and
Automatic
Behavior
in
Plan
Execution
+L+
Pat
Langley
Wayne
Iba
Jeff
Shrager
+L+
Robotics
Laboratory
Recom
Technologies
Palo
Alto
Research
Center
+L+
Computer
Science
Dept.
Mail
Stop
269-2
Xerox
Corporation
+L+
Stanford
University
NASA
Ames
Research
Center
3333
Coyote
Hill
Road
+L+
Stanford,
CA
94305
Moffett
Field,
CA
94035
Palo
Alto,
CA
94304
+L+
langley@cs.stanford.edu
iba@wind.arc.nasa.gov
shrager@xerox.com
+L+
Abstract
+L+
Much
of
the
work
on
execution
assumes
that
the
agent
+L+
constantly
senses
the
environment,
which
lets
it
respond
+L+
immediately
to
errors
or
unexpected
events.
In
this
paper,
we
argue
that
this
purely
reactive
strategy
is
only
+L+
optimal
if
sensing
is
inexpensive,
and
we
formulate
a
simple
model
of
execution
that
incorporates
the
cost
of
sensing.
We
present
an
average-case
analysis
of
this
model,
+L+
which
shows
that
in
domains
with
high
sensing
cost
or
+L+
low
probability
of
error,
a
more
`automatic'
strategy
-
+L+
one
with
long
intervals
between
sensing
can
lead
to
+L+
less
expensive
execution.
The
analysis
also
shows
that
+L+
the
distance
to
the
goal
has
no
effect
on
the
optimal
sensing
interval.
These
results
run
counter
to
the
prevailing
+L+
wisdom
in
the
planning
community,
but
they
promise
a
+L+
more
balanced
approach
to
the
interleaving
of
execution
+L+
and
sensing.
+L+
Reactive
and
Automatic
Execution

Advanced
Transaction
Processing
in
Multilevel
Secure
File
+L+
Stores
+L+
Elisa
Bertino
Sushil
Jajodia
Luigi
Mancini
Indrajit
Ray
x
+L+
Abstract
+L+
The
concurrency
control
requirements
for
transaction
processing
in
a
multilevel
secure
file
system
are
different
from
those
in
conventional
transaction
processing
systems.
+L+
In
particular,
there
is
the
need
to
coordinate
transactions
at
different
security
levels
+L+
avoiding
both
potential
timing
covert
channels
and
the
starvation
of
transactions
at
+L+
higher
security
levels.
Suppose
a
transaction
at
a
lower
security
level
attempts
to
write
+L+
a
data
item
that
is
being
read
by
a
transaction
at
a
higher
security
level.
On
the
one
+L+
hand,
a
timing
covert
channel
arises
if
the
transaction
at
the
lower
security
level
is
either
+L+
delayed
or
aborted
by
the
scheduler.
On
the
other
hand,
the
transaction
at
the
high
+L+
security
level
may
be
subjected
to
an
indefinite
delay
if
it
is
forced
to
abort
repeatedly.
+L+
This
paper
extends
the
classical
two-phase
locking
mechanism
to
multilevel
secure
+L+
file
systems.
The
scheme
presented
here
prevents
potential
timing
covert
channels
and
+L+
avoids
the
abort
of
higher
level
transactions
nonetheless
guaranteeing
serializability.
+L+
The
programmer
is
provided
with
a
powerful
set
of
linguistic
constructs
that
supports
+L+
exception
handling,
partial
rollback
and
forward
recovery.
The
proper
use
of
these
+L+
constructs
can
prevent
the
indefinite
delay
in
completion
of
a
higher
level
transaction,
+L+
and
allows
the
programmer
to
trade
off
starvation
with
transaction
isolation.
+L+
Index
Terms|Data
management
system,
File
system
management,
Transaction
processing,
Concurrency
control,
Two-phase
locking,
Exception
handling,
Security
kernel,
+L+
Mandatory
access
control,
Covert
channels.
+L+
1
Introduction

Proceedings
of
the
Thirteenth
International
Joint
Conference
on
Artificial
Intelligence,
1995.
+L+
Using
Introspective
Reasoning
to
Refine
Indexing
+L+
Susan
Fox
and
David
B.
Leake
+L+
Computer
Science
Department
+L+
Lindley
Hall
215
+L+
Indiana
University
+L+
Bloomington,
IN
47405
USA
+L+
E-mail:
fsfox,leakeg@cs.indiana.edu
+L+
Abstract
+L+
Introspective
reasoning
about
a
system's
own
+L+
reasoning
processes
can
form
the
basis
for
+L+
learning
to
refine
those
reasoning
processes.
+L+
The
ROBBIE
1
system
uses
introspective
reasoning
to
monitor
the
retrieval
process
of
a
+L+
case-based
planner
to
detect
retrieval
of
inappropriate
cases.
When
retrieval
problems
+L+
are
detected,
the
source
of
the
problems
is
explained
and
the
explanations
are
used
to
determine
new
indices
to
use
during
future
case
+L+
retrieval.
The
goal
of
ROBBIE's
learning
is
to
+L+
increase
its
ability
to
focus
retrieval
on
relevant
+L+
cases,
with
the
aim
of
simultaneously
decreasing
the
number
of
candidates
to
consider
and
+L+
increasing
the
likelihood
that
the
system
will
be
+L+
able
to
successfully
adapt
the
retrieved
cases
to
+L+
fit
the
current
situation.
We
evaluate
the
benefits
of
the
approach
in
light
of
empirical
results
+L+
examining
the
effects
of
index
learning
in
the
+L+
ROBBIE
system.
+L+
1
Introduction

NEURAL
NETWORKS
FOR
CONTROL
+L+
Eduardo
D.
Sontag
+L+
Department
of
Mathematics,
Rutgers
University
+L+
New
Brunswick,
NJ
08903,
USA
+L+
Abstract
+L+
This
paper
starts
by
placing
neural
net
techniques
in
a
general
nonlinear
+L+
control
framework.
After
that,
several
basic
theoretical
results
on
networks
+L+
are
surveyed.
+L+
1
Introduction

A
Construction
of
a
Cipher
+L+
From
a
Single
Pseudorandom
Permutation
+L+
Shimon
Even
1
and
Yishay
Mansour
2
+L+
Abstract
+L+
We
suggest
a
scheme
for
a
block
cipher
which
uses
only
one
randomly
chosen
permutation,
F
.
The
key,
consisting
of
two
blocks,
K
1
+L+
and
K
2
is
used
in
the
following
way:
The
message
block
is
XORed
+L+
with
K
1
before
applying
F
,
and
the
outcome
is
XORed
with
K
2
,
to
+L+
produce
the
cryptogram
block.
We
show
that
the
resulting
cipher
+L+
is
secure
(when
the
permutation
is
random
or
pseudorandom).
This
+L+
removes
the
need
to
store,
or
generate
a
multitude
of
permutations.
+L+
1
Comp.
Sci.
Dept.,
Technion,
Israel
Institute
of
Technology,
Haifa,
Israel
32000.

Constructing
Small
Sample
Spaces
Satisfying
Given
+L+
Constraints
+L+
Daphne
Koller
+L+
e-mail:
daphne@cs.stanford.edu
+L+
Nimrod
Megiddo
+L+
e-mail:
megiddo@almaden.ibm.com
+L+
Abstract
+L+
Abstract.
The
subject
of
this
paper
is
finding
small
sample
spaces
for
joint
distributions
of
+L+
n
discrete
random
variables.
Such
distributions
+L+
are
often
only
required
to
obey
a
certain
limited
set
of
constraints
of
the
form
P
r(E)
=
.
+L+
We
show
that
the
problem
of
deciding
whether
+L+
there
exists
any
distribution
satisfying
a
given
+L+
set
of
constraints
is
NP-hard.
However,
if
the
+L+
constraints
are
consistent,
then
there
exists
a
distribution
satisfying
them
which
is
supported
by
+L+
a
"small"
sample
space
(one
whose
cardinality
is
+L+
equal
to
the
number
of
constraints).
For
the
important
case
of
independence
constraints,
where
+L+
the
constraints
have
a
certain
form
and
are
consistent
with
a
joint
distribution
of
n
independent
+L+
random
variables,
a
small
sample
space
can
be
+L+
constructed
in
polynomial
time.
This
last
result
+L+
is
also
useful
for
de-randomizing
algorithms.
We
+L+
demonstrate
this
technique
by
an
application
to
+L+
the
problem
of
finding
large
independent
sets
in
+L+
sparse
hypergraphs.
+L+
Department
of
Computer
Science,
Stanford
University,
Stanford,
CA
94305;
and
IBM
Almaden
Research
+L+
Center,
650
Harry
Road,
San
Jose,
CA
95120.
+L+
IBM
Almaden
Research
Center,
650
Harry
Road,
San
+L+
Jose,
CA
95120;
and
School
of
Mathematical
Sciences,
Tel
+L+
Aviv
University,
Tel
Aviv,
Israel.
+L+
Research
supported
in
part
by
ONR
Contract
N00014-91-C-0026
and
by
the
Air
Force
Office
of
Scientific
+L+
Research
(AFSC),
under
Contract
F49620-91-C-0080.
+L+
(Paste
copyright
notice
here.)
+L+
1.
Introduction

Authoring
and
Transcription
Tools
+L+
for
Speech-Based
Hypermedia
Systems
+L+
Barry
Arons
+L+
MIT
Media
Laboratory
+L+
20
Ames
Street,
E15-353
+L+
Cambridge
MA,
02139
+L+
Phone:
+1
617-253-2245
+L+
E-mail:
barons@media-lab.mit.edu
+L+
Abstract
+L+
Authoring
is
usually
one
of
the
most
difficult
parts
in
the
design
and
implementation
of
hypertext
and
+L+
hypermedia
systems.
This
problem
is
exacerbated
if
the
data
to
be
presented
by
the
system
is
speech,
+L+
rather
than
text
or
graphics,
because
of
the
slow
and
serial
nature
of
speech.
This
paper
provides
an
+L+
overview
of
speech-only
hypermedia,
discusses
the
difficulties
associated
with
authoring
databases
for
+L+
such
a
system,
and
explores
a
variety
of
techniques
to
assist
in
the
authoring
process.
+L+
Speech-Only
Hypermedia

DESIGNING
AN
ECOLOGY
OF
DISTRIBUTED
AGENTS
+L+
by
+L+
Nelson
Minar
+L+
B.A.
Mathematics
(1994)
+L+
Reed
College
+L+
&lt;nelson@media.mit.edu&gt;
+L+
http://www.media.mit.edu/nelson/
+L+
Submitted
to
the
Program
in
Media
Arts
and
Sciences,
School
of
Architecture
and
+L+
Planning,
in
partial
fulfillment
of
the
requirements
for
the
degree
of
Master
of
Science
in
+L+
Media
Arts
and
Sciences
at
the
Massachusetts
Institute
of
Technology
+L+
September
1998
+L+
c
fl1998
Massachusetts
Institute
of
Technology.
All
rights
reserved.
+L+
Author
+L+
Nelson
Minar
+L+
Department
of
Media
Arts
and
Sciences
+L+
August
7,
1998
+L+
Certified
by
+L+
Pattie
Maes
+L+
Associate
Professor
of
Media
Arts
and
Sciences
+L+
MIT
Media
Lab
+L+
Accepted
by
+L+
Stephen
A.
Benton
+L+
Professor
of
Media
Arts
and
Sciences
+L+
Chair,
Departmental
Committee
on
Graduate
Students
+L+
Program
in
Media
Arts
and
Sciences
+L+
+PAGE+

Stability
of
the
replica
symmetric
solution
for
the
information
+L+
conveyed
by
a
neural
network
+L+
Simon
Schultzy
and
Alessandro
Trevesz
+L+
Department
of
Experimental
Psychology,
South
Parks
Rd.,
University
of
Oxford,
Oxford
OX1
+L+
3UD,
U.K.
+L+
Programme
in
Neuroscience,
International
School
for
Advanced
Studies,
via
Beirut
2-4,
34013
+L+
Trieste,
Italy
+L+
(November
7,
1997)
+L+
Abstract
+L+
The
information
that
a
pattern
of
firing
in
the
output
layer
of
a
feedforward
+L+
network
of
threshold-linear
neurons
conveys
about
the
network's
inputs
is
+L+
considered.
A
replica-symmetric
solution
is
found
to
be
stable
for
all
but
+L+
small
amounts
of
noise.
The
region
of
instability
depends
on
the
contribution
+L+
of
the
threshold
and
the
sparseness:
for
distributed
pattern
distributions,
+L+
the
unstable
region
extends
to
higher
noise
variances
than
for
very
sparse
+L+
distributions,
for
which
it
is
almost
nonexistant.
+L+
84.35.+i,89.70.+c,87.10.+e
+L+
Typeset
using
REVT
E
X
+L+
+PAGE+

A
Comparative
Study
of
Reliable
Error
Estimators
+L+
for
Pruning
Regression
Trees
+L+
Lus
Torgo
+L+
LIACC/FEP
University
of
Porto
+L+
R.
Campo
Alegre,
823,
2
-
4150
PORTO
-
PORTUGAL
+L+
Phone
:
(+351)
2
607
8830
Fax
:
(+351)
2
600
3654
+L+
email
:
ltorgo@ncc.up.pt
WWW
:
http://www.ncc.up.pt/~ltorgo
+L+
Abstract.
This
paper
presents
a
comparative
study
of
several
methods
for
estimating
+L+
the
true
error
of
treestructured
regression
models.
We
evaluate
these
methods
in
the
+L+
context
of
regression
tree
pruning.
Pruning
is
considered
a
key
issue
for
obtaining
+L+
reliable
treestructured
models
in
a
real
world
scenario.
The
major
step
of
a
pruning
+L+
process
consists
of
obtaining
accurate
estimates
of
the
error
of
alternative
tree
+L+
models.
We
evaluate
experimentally
four
methods
for
obtaining
these
estimates
in
+L+
twelve
domains.
The
goal
of
this
evaluation
was
to
characterise
the
performance
of
+L+
the
methods
in
the
task
of
selecting
the
best
possible
tree
among
the
set
of
trees
+L+
considered
during
pruning.
The
results
of
the
comparison
show
that
certain
+L+
estimators
lead
to
poor
decisions
in
some
domains.
The
Cross
Validation
variant
that
+L+
we
have
proposed
achieved
the
best
results
on
the
setups
we
have
considered.
+L+
Keywords
:
Machine
Learning,
Regression
Trees,
Pruning
methods.
+L+
1
Introduction

Rule
Revision
with
Recurrent
Neural
Networks
+L+
Christian
W.
Omlin
a;b
and
C.L.
Giles
a;c
+L+
a
NEC
Research
Institute,
4
Independence
Way,
Princeton,
New
Jersey
+L+
b
Computer
Science
Department,
Rensselaer
Polytechnic
Institute,
Troy,
New
York
+L+
c
Institute
for
Advanced
Computer
Studies,
University
of
Maryland,
College
Park,
Maryland
+L+
Abstract
+L+
Recurrent
neural
networks
readily
process,
recognize
and
generate
temporal
sequences.
By
encoding
+L+
grammatical
strings
as
temporal
sequences,
recurrent
neural
networks
can
be
trained
to
behave
like
deterministic
sequential
finite-state
automata.
Algorithms
have
been
developed
for
extracting
grammatical
+L+
rules
from
trained
networks.
Using
a
simple
method
for
inserting
prior
knowledge
(or
rules)
into
recurrent
+L+
neural
networks,
we
show
that
recurrent
neural
networks
are
able
to
perform
rule
revision.
Rule
revision
+L+
is
performed
by
comparing
the
inserted
rules
with
the
rules
in
the
finite-state
automata
extracted
from
+L+
trained
networks.
The
results
from
training
a
recurrent
neural
network
to
recognize
a
known
non-trivial,
+L+
randomly
generated
regular
grammar
show
that
not
only
do
the
networks
preserve
correct
rules
but
that
+L+
they
are
able
to
correct
through
training
inserted
rules
which
were
initially
incorrect.
(By
incorrect,
we
+L+
mean
that
the
rules
were
not
the
ones
in
the
randomly
generated
grammar.)
+L+
Index
Terms:
Deterministic
Finite-State
Automata,
Genuine
and
Incorrect
Rules,
Knowledge
Insertion
+L+
and
Extraction,
Recurrent
Neural
Networks,
Regular
Languages,
Rule
Revision.
+L+
Published
in
IEEE
Trans.
on
Knowledge
and
Data
Engineering,
vol.
8,
no.
1,
p.
183,
1996.
Copyright
IEEE.
+L+
+PAGE+

What
Size
Neural
Network
Gives
Optimal
Generalization?
+L+
Convergence
Properties
of
Backpropagation
+L+
Steve
Lawrence
1;2
,
C.
Lee
Giles
1
,
Ah
Chung
Tsoi
2
+L+
flawrence,actg@elec.uq.edu.au,
giles@research.nj.nec.com
+L+
1
NEC
Research
Institute,
4
Independence
Way,
Princeton,
NJ
08540
+L+
2
Department
of
Electrical
and
Computer
Engineering
+L+
University
of
Queensland,
St.
Lucia
4072,
Australia
+L+
Technical
Report
+L+
UMIACS-TR-96-22
and
CS-TR-3617
+L+
Institute
for
Advanced
Computer
Studies
+L+
University
of
Maryland
+L+
College
Park,
MD
20742
+L+
June
1996
(Revised
August
1996)
+L+
Abstract
+L+
One
of
the
most
important
aspects
of
any
machine
learning
paradigm
is
how
it
scales
according
+L+
to
problem
size
and
complexity.
Using
a
task
with
known
optimal
training
error,
and
a
pre-specified
+L+
maximum
number
of
training
updates,
we
investigate
the
convergence
of
the
backpropagation
algorithm
+L+
with
respect
to
a)
the
complexity
of
the
required
function
approximation,
b)
the
size
of
the
network
in
+L+
relation
to
the
size
required
for
an
optimal
solution,
and
c)
the
degree
of
noise
in
the
training
data.
In
+L+
general,
for
a)
the
solution
found
is
worse
when
the
function
to
be
approximated
is
more
complex,
for
+L+
b)
oversized
networks
can
result
in
lower
training
and
generalization
error
in
certain
cases,
and
for
c)
+L+
the
use
of
committee
or
ensemble
techniques
can
be
more
beneficial
as
the
level
of
noise
in
the
training
+L+
data
is
increased.
For
the
experiments
we
performed,
we
do
not
obtain
the
optimal
solution
in
any
case.
+L+
We
further
support
the
observation
that
larger
networks
can
produce
better
training
and
generalization
+L+
error
using
a
face
recognition
example
where
a
network
with
many
more
parameters
than
training
points
+L+
generalizes
better
than
smaller
networks.
+L+
Keywords:
Local
Minima,
Generalization,
Committees,
Ensembles,
Convergence,
Backpropagation,
Smoothness,
+L+
Network
Size,
Problem
Complexity,
Function
Approximation,
Curse
of
Dimensionality.
+L+
http://www.neci.nj.nec.com/homepages/lawrence
+L+
Also
with
the
Institute
for
Advanced
Computer
Studies,
University
of
Maryland,
College
Park,
MD
20742.
+L+
http://www.neci.nj.nec.com/homepages/giles.html
+L+
+PAGE+

On
the
uniqueness
of
the
convolution
theorem
for
+L+
the
fourier
transform
+L+
Harold
S.
Stone
+L+
Lance
R.
Williams
+L+
NEC
Research
Institute
+L+
4
Independence
Way
+L+
Princeton,
NJ
08540
+L+
Revision
1,
13
February
1995
+L+
Abstract
+L+
This
paper
shows
that
members
of
the
fourier
transform
family
are
the
only
linear
+L+
transforms
that
have
a
convolution
theorem,
that
is,
that
can
replace
O(N
2
)
operations
+L+
of
a
convolution
in
a
time
domain
by
O(N)
operations
in
a
transform
domain.
Generally,
+L+
there
is
an
additional
cost
to
compute
the
transform
itself.
Our
observation
is
motivated
by
+L+
recent
activity
in
wavelet
and
subband
decompositions
and
related
spectral
analyses,
which
+L+
are
attractive
alternatives
for
signal
compression
applications.
A
natural
question
when
+L+
using
such
techniques
is
to
determine
if
convolutions
of
N
-point
signals
can
be
calculated
+L+
with
fewer
operations
in
a
compressed
transform
domain
than
in
an
uncompressed
time
+L+
domain.
The
answer
is
negative
for
a
broad
set
of
assumptions.
This
paper
indicates
what
+L+
assumptions
must
be
relaxed
in
seeking
a
linear
transform
that
has
a
convolution
theorem
+L+
comparable
to
the
convolution
theorem
for
fourier
transforms.
+L+
1
Introduction

The
Anisotropy
in
the
Cosmic
Microwave
Background
+L+
At
Degree
Angular
Scales.
+L+
C.
B.
Netterfield
,
N.
Jarosik
,
L.
Page
,
D.
Wilkinson
,
&
E.
Wollack
1
+L+
Princeton
University,
Department
of
Physics,
Jadwin
Hall,
P.O.
Box
708,
Princeton,
NJ
+L+
08544
+L+
Received
;
accepted
+L+
Submitted
Ap.
J.
Letters
+L+
1
NRAO,
2015
Ivy
Rd.,
Charlottesville,
VA,
22903
+L+
+PAGE+

Laser
Remote
Sensing
Techniques
for
Vertical
Profiling
of
Cloud
and
+L+
Aerosol
Extinction
and
Back-scatter
in
the
Lower
Atmosphere.
+L+
A
Brief
Review
+L+
$flfl
Papayannis*
,
E.
Fokitis
+L+
National
Technical
University
of
Athens,
Physics
Department
+L+
Zografou
Campus,
15780
Zografou,
GREECE
+L+
Email:
apdlidar@central.ntua.gr
+L+
Abstract
+L+
In
this
brief
contribution
we
present
the
three
principal
laser
remote
+L+
sensing
(
lidar)
techniques
developed
to
retrieve
the
vertical
profiling
of
+L+
clouds
and
of
the
suspended
aerosols
(extinction
and
backscatter)
in
the
+L+
lower
atmosphere,
namely
in
the
0-7
km
altitude
region.
The
three
lidar
+L+
techniques
include
the
elastic
(
Klett
inversion,
Doppler
broadening)
and
+L+
the
nonelastic
backscattering
techniques
(
Raman
scattering).
We
report
+L+
on
the
potential
of
these
techniques,
as
well
as
on
the
typical
accuracies
+L+
of
these
techniques
in
the
retrieval
of
the
cloud
and
aerosol
extinction
+L+
and
backscatter
vertical
profiles
in
the
troposphere
(0-7
km
ASL).
+L+
flfl
Introduction

Proceedings
of
the
19th
Annual
Conference
of
the
Cognitive
+L+
Science
Society,
Mahwah,
NJ:Erlbaum
p.
253-258
(1997).
+L+
253
+L+
The
Dynamics
of
Prefrontal
Cortico-Thalamo-Basal
Ganglionic
Loops
and
+L+
Short-Term
Memory
Interference
Phenomena
+L+
Jack
Gelfand
1
,
Vijay
Gullapalli
1
,
Marcia
Johnson
1
,
Carol
Raye
1
and
+L+
Jeffrey
Henderson
2
+L+
Department
of
Psychology
1
and
Department
of
Computer
Science
2
+L+
Princeton
University
+L+
Princeton,
NJ
08544
+L+
jjg@princeton.edu
+L+
Abstract
+L+
We
present
computer
simulations
of
a
model
of
the
brain
+L+
mechanisms
operating
in
short-term
memory
tasks
that
are
+L+
consistent
with
the
anatomy
and
physiology
of
prefrontal
+L+
cortex
and
associated
subcortical
structures.
These
+L+
simulations
include
dynamical
processes
in
thalamo-cortical
loops
which
are
used
to
generate
short-term
+L+
persistent
responses
in
prefrontal
cortex.
We
discuss
this
+L+
model
in
terms
of
the
representation
of
input
stimuli
in
+L+
cortical
association
areas
and
prefrontal
short-term
+L+
memory
areas.
We
report
on
interference
phenomena
that
+L+
result
from
the
interaction
of
these
dynamical
processes
+L+
and
lateral
projections
within
cortical
columns.
These
+L+
interference
phenomena
can
be
used
to
elucidate
the
+L+
representational
organization
of
short-term
memory.
+L+
Introduction

On
the
Boosting
Ability
of
Top-Down
+L+
Decision
Tree
Learning
Algorithms
+L+
Michael
Kearns
+L+
AT&T
Research
+L+
Yishay
Mansour
+L+
Tel-Aviv
University
+L+
May
1996
+L+
Abstract
+L+
We
analyze
the
performance
of
top-down
algorithms
for
decision
tree
learning,
such
as
those
employed
+L+
by
the
widely
used
C4.5
and
CART
software
packages.
Our
main
result
is
a
proof
that
such
algorithms
+L+
are
boosting
algorithms.
By
this
we
mean
that
if
the
functions
that
label
the
internal
nodes
of
the
+L+
decision
tree
can
weakly
approximate
the
unknown
target
function,
then
the
top-down
algorithms
we
+L+
study
will
amplify
this
weak
advantage
to
build
a
tree
achieving
any
desired
level
of
accuracy.
The
bounds
+L+
we
obtain
for
this
amplification
show
an
interesting
dependence
on
the
splitting
criterion
used
by
the
+L+
top-down
algorithm.
More
precisely,
if
the
functions
used
to
label
the
internal
nodes
have
error
1=2
+L+
as
approximations
to
the
target
function,
then
for
the
splitting
criteria
used
by
CART
and
C4.5,
trees
+L+
of
size
(1=*)
O(1=
2
*
2
)
and
(1=*)
O(log(1=*)=
2
)
(respectively)
suffice
to
drive
the
error
below
*.
Thus
(for
+L+
example),
a
small
constant
advantage
over
random
guessing
is
amplified
to
any
larger
constant
advantage
+L+
with
trees
of
constant
size.
For
a
new
splitting
criterion
suggested
by
our
analysis,
the
much
stronger
+L+
bound
of
(1=*)
O(1=
2
)
(which
is
polynomial
in
1=*)
is
obtained,
which
is
provably
optimal
for
decision
+L+
tree
algorithms.
The
differing
bounds
have
a
natural
explanation
in
terms
of
concavity
properties
of
the
+L+
splitting
criterion.
+L+
The
primary
contribution
of
this
work
is
in
proving
that
some
popular
and
empirically
successful
+L+
heuristics
that
are
based
on
first
principles
meet
the
criteria
of
an
independently
motivated
theoretical
+L+
model.
+L+
A
preliminary
version
of
this
paper
appears
in
Proceedings
of
the
Twenty-Eighth
Annual
ACM
Symposium
on
the
Theory
of
+L+
Computing,
pages
459-468,
ACM
Press,
1996.
Authors'
addresses:
M.
Kearns,
AT&T
Research,
600
Mountain
Avenue,
Room
+L+
2A-423,
Murray
Hill,
New
Jersey
07974;
electronic
mail
mkearns@research.att.com.
Y.
Mansour,
Department
of
Computer
+L+
Science,
Tel
Aviv
University,
Tel
Aviv,
Israel;
electronic
mail
mansour@math.tau.ac.il.
Y.
Mansour
was
supported
in
part
by
+L+
the
Israel
Science
Foundation,
administered
by
the
Israel
Academy
of
Science
and
Humanities,
and
by
a
grant
of
the
Israeli
+L+
Ministry
of
Science
and
Technology.
+L+
+PAGE+

A
Systematic
Approach
to
+L+
Host
Interface
Design
for
High-Speed
Networks
+L+
Peter
Steenkiste
+L+
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
5000
Forbes
Avenue
+L+
Pittsburgh,
Pennsylvania
15213-3891
+L+
Abstract
+L+
In
recent
years,
networks
with
media
rates
of
100
Mbit/second
or
more
have
become
widely
available
(FDDI,
ATM,
+L+
HIPPI,
..).
However,
many
computer
systems
cannot
make
use
of
the
available
bandwidth
because
of
the
high
+L+
overhead
associated
with
network
communication.
In
this
paper
we
review
the
operations
involved
in
communication
over
high-speed
networks,
and
we
describe
optimizations
of
the
network
interface
that
improve
network
+L+
throughput.
We
also
discuss
how
the
payoff
of
the
optimizations
is
influenced
by
features
of
the
host
software
and
+L+
architecture.
This
paper
is
based
on
our
experience
with
the
interfaces
for
the
Nectar
and
Gigabit
Nectar
networks.
+L+
Keywords:
network
interfaces,
high-speed
networks,
buffer
management,
memory
hierarchy
+L+
This
research
was
sponsored
by
the
Defense
Advanced
Research
Projects
Agency
(DOD)
under
contract
number
MDA972-90-C-0035,
in
part
by
the
National
Science
Foundation
and
the
Defense
Advanced
+L+
Research
Projects
Agency
under
Cooperative
Agreement
NCR-8919038
with
the
Corporation
for
National
Research
Initiatives.
+L+
+PAGE+

DYNAMIC
COUPLING
OF
+L+
UNDERACTUATED
MANIPULATORS
+L+
Marcel
Bergerman
Christopher
Lee
Yangsheng
Xu
+L+
The
Robotics
Institute
+L+
Carnegie
Mellon
University
+L+
Pittsburgh
PA
15213
+L+
-mbergerm|chrislee|xu+-@cs.cmu.edu
+L+
Proceedings
of
the
4th
IEEE
Conference
on
Control
Applications,
Albany,
USA,
Sep.
1995,
pp.
500-505.
+L+
Abstract
+L+
In
recent
years,
researchers
have
been
dedicated
to
the
+L+
study
of
underactuated
manipulators
which
have
more
+L+
joints
than
control
actuators.
In
previous
works,
+L+
assumptions
were
made
as
to
the
existence
of
enough
+L+
dynamic
coupling
between
the
active
and
the
passive
+L+
joints
of
the
manipulator
for
it
to
be
possible
to
control
+L+
the
position
of
the
passive
joints
via
the
dynamic
+L+
coupling.
In
this
work,
the
authors
aim
to
develop
an
+L+
index
to
measure
the
dynamic
coupling,
so
as
to
address
+L+
when
control
of
the
underactuated
system
is
possible,
+L+
and
how
the
motion
and
robot
configuration
can
be
+L+
designed.
We
discuss
extensively
the
nature
of
the
+L+
dynamic
coupling
and
of
the
proposed
coupling
index,
+L+
and
their
applications
in
the
analysis
and
design
of
+L+
underactuated
systems,
and
in
control
and
planning
of
+L+
robot
motion
configuration.
+L+
1
Introduction

A
Linear
Spine
Calculus
+L+
Iliano
Cervesato
and
Frank
Pfenning
1
+L+
April
10,
1997
+L+
CMU-CS-97-125
+L+
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213
+L+
Abstract
+L+
We
present
the
spine
calculus
S
!ffi&&gt;
as
an
efficient
representation
for
the
linear
-calculus
!ffi&&gt;
+L+
which
includes
intuitionistic
functions
(!),
linear
functions
(ffi),
additive
pairing
(&),
and
additive
unit
+L+
(&gt;).
S
!ffi&&gt;
enhances
the
representation
of
Church's
simply
typed
-calculus
as
abstract
Bohm
trees
+L+
by
enforcing
extensionality
and
by
incorporating
linear
constructs.
This
approach
permits
procedures
+L+
such
as
unification
to
retain
the
efficient
head
access
that
characterizes
first-order
term
languages
without
+L+
the
overhead
of
performing
-conversions
at
run
time.
Potential
applications
lie
in
proof
search,
logic
+L+
programming,
and
logical
frameworks
based
on
linear
type
theories.
We
define
the
spine
calculus,
give
+L+
translations
of
!ffi&&gt;
into
S
!ffi&&gt;
and
vice-versa,
prove
their
soundness
and
completeness
with
respect
+L+
to
typing
and
reductions,
and
show
that
the
spine
calculus
is
strongly
normalizing
and
admits
unique
+L+
canonical
forms.
+L+
1
The
authors
can
be
reached
at
iliano@cs.cmu.edu
and
fp@cs.cmu.edu.

Tactile
Gestures
for
Human/Robot
Interaction
+L+
Richard
M.
Voyles
,
Jr.
Pradeep
K.
Khosla
+L+
Robotics
Ph.D.
Program
+L+
Dept.
of
Electrical
and
Computer
Engineering
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213
+L+
7
+L+
Abstract
+L+
Gesture-Based
Programming
is
a
new
paradigm
to
ease
+L+
the
burden
of
programming
robots.
By
tapping
in
to
the
+L+
users
wealth
of
experience
with
contact
transitions,
+L+
compliance,
uncertainty
and
operations
sequencing,
we
+L+
hope
to
provide
a
more
intuitive
programming
environment
+L+
for
complex,
real-world
tasks
based
on
the
expressiveness
+L+
of
non-verbal
communication.
A
requirement
for
this
to
be
+L+
accomplished
is
the
ability
to
interpret
gestures
to
infer
the
+L+
intentions
behind
them.
As
a
first
step
toward
this
goal,
this
+L+
paper
presents
an
application
of
distributed
perception
for
+L+
inferring
a
users
intentions
by
observing
tactile
gestures.
+L+
These
gestures
consist
of
sparse,
inexact,
physical
+L+
nudges
applied
to
the
robots
end
effector
for
the
+L+
purpose
of
modifying
its
trajectory
in
free
space.
A
set
of
+L+
independent
agents
-
each
with
its
own
local,
fuzzified,
+L+
heuristic
model
of
a
particular
trajectory
parameter
-
+L+
observes
data
from
a
wrist
force/torque
sensor
to
evaluate
+L+
the
gestures.
The
agents
then
independently
determine
the
+L+
confidence
of
their
respective
findings
and
distributed
+L+
arbitration
resolves
the
interpretation
through
voting.
+L+
1
Gesture-based
programming

Using
a
DEM
to
Determine
Geospatial
Object
Trajectories
+L+
Robert
T.
Collins
,
Yanghai
Tsin
,
J.
Ryan
Miller
and
Alan
J.
Lipton
+L+
The
Robotics
Institute,
Carnegie
Mellon
University,
Pittsburgh,
PA.
15213
+L+
Email:
frcollins,ytsin,jmce,ajlgcs.cmu.edu
+L+
Abstract
+L+
This
paper
addresses
the
estimation
of
moving
object
trajectories
within
a
geospatial
coordinate
system,
+L+
using
a
network
of
video
sensors.
A
high-resolution
+L+
(0.5m
grid
spacing)
digital
elevation
map
(DEM)
has
+L+
been
constructed
using
a
helicopter-based
laser
range-finder.
Object
locations
are
estimated
by
intersecting
viewing
rays
from
a
calibrated
sensor
platform
+L+
with
the
DEM.
Continuous
object
trajectories
can
then
+L+
be
assembled
from
sequences
of
single-frame
location
+L+
estimates
using
spatio-temporal
filtering
and
domain
+L+
knowledge.
+L+
1
Introduction

Formal
Aspects
of
Computing
(1998)
3:
1-000
+L+
c
1998
BCS
+L+
Protective
Interface
Specifications
+L+
Gary
T.
Leavens
2
and
Jeannette
M.
Wing
3
+L+
1
Department
of
Computer
Science,
Iowa
State
University,
Ames,
IA
50011
USA
+L+
2
Computer
Science
Department,
Carnegie
Mellon
University,
Pittsburgh,
PA
15213
USA
+L+
Abstract.
The
interface
specification
of
a
procedure
describes
the
procedure's
+L+
behavior
using
pre-
and
postconditions.
These
pre-
and
postconditions
are
written
using
various
functions.
If
some
of
these
functions
are
partial,
or
underspec-ified,
then
the
procedure
specification
may
not
be
well-defined.
+L+
We
show
how
to
write
pre-
and
postcondition
specifications
that
avoid
such
+L+
problems,
by
having
the
precondition
"protect"
the
postcondition
from
the
effects
+L+
of
partiality
and
underspecification.
We
formalize
the
notion
of
protection
from
+L+
partiality
in
the
context
of
specification
languages
like
VDM-SL
and
COLD-K.
+L+
We
also
formalize
the
notion
of
protection
from
underspecification
for
the
Larch
+L+
family
of
specification
languages,
and
for
Larch
show
how
one
can
prove
that
a
+L+
procedure
specification
is
protected
from
the
effects
of
underspecification.
+L+
1.
The
Problem

Geometric
Sensing
of
Known
Planar
Shapes
+L+
Yan-Bin
Jia
Michael
Erdmann
+L+
The
Robotics
Institute
and
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
Pennsylvania
15213-3891
+L+
March
12,
1995
+L+
International
Journal
of
Robotics
Research,
15(4):365-392,
1996.
+L+
+PAGE+

Remote
Access
to
Interactive
Media
+L+
Roger
B.
Dannenberg
+L+
Carnegie
Mellon
University,
School
of
Computer
Science
+L+
Pittsburgh,
PA
15213
USA
+L+
Email:
dannenberg@cs.cmu.edu
+L+
ABSTRACT
+L+
Digital
interactive
media
augments
interactive
computing
with
video,
audio,
computer
graphics
and
text,
+L+
allowing
multimedia
presentations
to
be
individually
and
dynamically
tailored
to
the
user.
Multimedia,
and
+L+
particularly
continuous
media
pose
interesting
problems
for
system
designers,
including
those
of
latency
+L+
and
synchronization.
These
problems
are
especially
evident
when
multimedia
data
is
remote
and
must
be
+L+
accessed
via
networks.
Latency
and
synchronization
issues
are
discussed,
and
an
integrated
system,
+L+
Tactus,
is
described.
Tactus
facilitates
the
implementation
of
interactive
multimedia
computer
programs
by
+L+
managing
latency
and
synchronization
in
the
framework
of
an
object-oriented
graphical
user
interface
+L+
toolkit.
+L+
1.
Introduction

A
Whole
Sentence
+L+
Maximum
Entropy
Language
Model
+L+
R.
Rosenfeld
+L+
School
of
Computer
Science
+L+
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213
+L+
Abstract
We
introduce
a
new
kind
of
language
model,
which
models
whole
sentences
or
utterances
directly
using
the
Maximum
Entropy
+L+
paradigm.
The
new
model
is
conceptually
simpler,
and
more
naturally
+L+
suited
to
modeling
whole-sentence
phenomena,
than
the
conditional
ME
+L+
models
proposed
to
date.
By
avoiding
the
chain
rule,
the
model
treats
+L+
each
sentence
or
utterance
as
a
"bag
of
features",
where
features
are
+L+
arbitrary
computable
properties
of
the
sentence.
The
model
is
unnor-malizable,
but
this
does
not
interfere
with
training
(done
via
sampling)
+L+
or
with
use.
Using
the
model
is
computationally
straightforward.
The
+L+
main
computational
cost
of
training
the
model
is
in
generating
sample
+L+
sentences
from
a
Gibbs
distribution.
Interestingly,
this
cost
has
different
dependencies,
and
is
potentially
lower,
than
in
the
comparable
+L+
conditional
ME
model.
+L+
1
Motivation

Tolerating
Latency
Through
Software-Controlled
Prefetching
+L+
in
Shared-Memory
Multiprocessors
+L+
Todd
Mowry
and
Anoop
Gupta
+L+
Computer
Systems
Laboratory
+L+
Stanford
University,
CA
94305
+L+
To
appear
in
the
Journal
of
Parallel
and
Distributed
Computing,
June
1991.
+L+
Abstract
+L+
The
large
latency
of
memory
accesses
is
a
major
obstacle
in
obtaining
high
processor
utilization
in
large
+L+
scale
shared-memory
multiprocessors.
Although
the
provision
of
coherent
caches
in
many
recent
machines
+L+
has
alleviated
the
problem
somewhat,
cache
misses
still
occur
frequently
enough
that
they
significantly
lower
+L+
performance.
In
this
paper
we
evaluate
the
effectiveness
of
non-binding
software-controlled
prefetching,
as
+L+
proposed
in
the
Stanford
DASH
Multiprocessor,
to
address
this
problem.
The
prefetches
are
non-binding
in
+L+
the
sense
that
the
prefetched
data
is
brought
to
a
cache
close
to
the
processor,
but
is
still
available
to
the
cache
+L+
coherence
protocol
to
keep
it
consistent.
Prefetching
is
software-controlled
since
the
program
must
explicitly
+L+
issue
prefetch
instructions.
+L+
The
paper
presents
results
from
detailed
simulation
studies
done
in
the
context
of
the
Stanford
DASH
+L+
multiprocessor.
Our
results
show
that
for
applications
with
regular
data
access
patterns|we
evaluate
a
particle-based
simulator
used
in
aeronautics
and
an
LU-decomposition
application|prefetching
can
be
very
effective.
+L+
It
was
easy
to
augment
the
applications
to
do
prefetching
and
it
increased
their
performance
by
100-150%
when
+L+
we
prefetched
directly
into
the
processor's
cache.
However,
for
applications
with
complex
data
usage
patterns,
+L+
prefetching
was
less
successful.
After
much
effort,
the
performance
of
a
distributed-time
logic
simulation
+L+
application
that
made
extensive
use
of
pointers
and
linked
lists
could
be
increased
only
by
30%.
The
paper
+L+
also
evaluates
the
effects
of
various
hardware
optimizations
such
as
separate
prefetch
issue
buffers,
prefetching
+L+
with
exclusive
ownership,
lockup-free
caches,
and
weaker
memory
consistency
models
on
the
performance
of
+L+
prefetching.
+L+
1
Introduction

Learning
Maps
for
Indoor
Mobile
Robot
Navigation
+L+
Sebastian
Thrun
+L+
Computer
Science
Department
and
Robotics
Institute
+L+
Carnegie
Mellon
University,
Pittsburgh
+L+
Accepted
for
Publication
in
Artificial
Intelligence
+L+
Abstract
+L+
Autonomous
robots
must
be
able
to
learn
and
maintain
models
of
their
environments.
+L+
Research
on
mobile
robot
navigation
has
produced
two
major
paradigms
for
mapping
indoor
+L+
environments:
grid-based
and
topological.
While
grid-based
methods
produce
accurate
+L+
metric
maps,
their
complexity
often
prohibits
efficient
planning
and
problem
solving
in
+L+
large-scale
indoor
environments.
Topological
maps,
on
the
other
hand,
can
be
used
much
+L+
more
efficiently,
yet
accurate
and
consistent
topological
maps
are
often
difficult
to
learn
+L+
and
maintain
in
large-scale
environments,
particularly
if
momentary
sensor
data
is
highly
+L+
ambiguous.
This
paper
describes
an
approach
that
integrates
both
paradigms:
grid-based
+L+
and
topological.
Grid-based
maps
are
learned
using
artificial
neural
networks
and
naive
+L+
Bayesian
integration.
Topological
maps
are
generated
on
top
of
the
grid-based
maps,
by
+L+
partitioning
the
latter
into
coherent
regions.
By
combining
both
paradigms,
the
approach
+L+
presented
here
gains
advantages
from
both
worlds:
accuracy/consistency
and
efficiency.
+L+
The
paper
gives
results
for
autonomous
exploration,
mapping
and
operation
of
a
mobile
+L+
robot
in
populated
multi-room
environments.
+L+
?
This
research
was
sponsored
in
part
by
the
National
Science
Foundation
under
award
IRI-9313367,
and
by
the
Wright
Laboratory,
Aeronautical
Systems
Center,
Air
Force
Materiel
+L+
Command,
USAF,
and
the
Darpa
Advanced
Research
Projects
Agency
(DARPA)
under
+L+
grant
number
F33615-93-1-1330.
We
also
acknowledge
financial
support
by
Daimler
Benz
+L+
Corp.
+L+
Preprint
submitted
to
Elsevier
Science
15
September
1997
+L+
+PAGE+

Bayesian
Analysis
of
Variance
Component
Models
via
Rejection
+L+
Sampling
+L+
Russell
D.
Wolfinger
+L+
SAS
Institute
Inc.,
SAS
Campus
Drive,
+L+
Cary,
NC
27513,
U.S.A.
+L+
and
Robert
E.
Kass
+L+
Department
of
Statistics,
Carnegie
Mellon
University
+L+
Pittsburgh,
PA
15213,
U.S.A.
+L+
January,
1996
+L+
Abstract
+L+
We
consider
the
usual
Normal
linear
mixed
model
for
"components
of
variance"
from
a
Bayesian
+L+
viewpoint.
Instead
of
using
Gibbs
sampling
or
other
Markov
Chain
schemes
that
rely
on
full
+L+
conditional
distributions,
we
propose
and
investigate
a
method
for
simulating
from
posterior
distributions
based
on
rejection
sampling.
The
method
applies
with
arbitrary
prior
distributions
but
+L+
we
also
employ
as
a
default
reference
prior
a
version
of
Jeffreys's
prior
based
on
the
integrated
+L+
("restricted")
likelihood.
We
demonstrate
the
ease
of
application
and
flexibility
of
this
approach
+L+
in
several
familiar
settings,
even
in
the
presence
of
unbalanced
data.
A
program
implementing
the
+L+
algorithm
discussed
here
will
be
available
in
the
SAS
MIXED
procedure.
+L+
Some
key
words:
Jeffreys's
prior,
Mixed
model,
Posterior
simulation,
Reference
prior,
REML.
+L+
+PAGE+

Working
Paper
IS-98-01
(Information
Systems)
+L+
Leonard
N.
Stern
School
of
Business,
New
York
University.
+L+
In:
Proceedings
of
the
IEEE/IAFE/INFORMS
Conference
on
Computational
Intelligence
for
Financial
Engineering
+L+
(CIFEr'98,
New
York,
March
1998)
+L+
http://www.stern.nyu.edu/~aweigend/Research/Papers/TradeStyles
+L+
Uncovering
Hidden
Structure
in
Bond
Futures
Trading
+L+
Fei
CHEN
,
Stephen
FIGLEWSKI
,
+L+
Jeffrey
HEISLER
zz
,
Andreas
S.
WEIGEND
+L+
Abstract.
This
study
uncovers
trading
styles
in
the
transaction
records
of
US
Treasury
bond
futures.
+L+
It
uses
transaction-by-transaction
data
from
the
Commodity
Futures
Trading
Commissions'
(CFTC)
+L+
Computerized
Trade
Reconstruction
(CTR)
records.
The
data
set
consists
of
30
million
transaction|
+L+
the
complete
US
T-bond
futures
market
for
3
years.
Each
transaction
record
consists
of
time
(by
the
+L+
minute),
price,
volume,
buy/sell,
and
an
identifier
of
the
specific
account.
+L+
We
use
statistical
clustering
techniques
to
group
together
trades
that
are
similar.
Two
sets
of
+L+
assumptions
have
to
be
made:
(1)
What
is
a
trade?
We
define
a
trade
to
begin
when
an
account
opens
+L+
a
position,
and
to
end
when
its
position
size
returns
to
zero.
We
describe
each
trade
by
several
trade-specific
variables
(e.g.,
length
of
trade,
maximum
position
size,
opening
move,
long
or
short)
and
several
+L+
exogenous,
market-specific
variables
(e.g.,
price,
volatility,
trading
volume).
(2)
What
process
generated
+L+
the
data?
We
assume
a
mixture
of
Gaussians.
An
observed
trade
is
interpreted
as
a
noisy
realization
of
+L+
one
of
the
mixture
components.
This
paper
assumes
identity
covariance
matrices.
Furthermore,
each
+L+
trade
is
fully
assigned
to
a
single
cluster.
We
compare
this
approach
to
diagonal
and
to
full
covariance
+L+
structure
with
probabilistic
assignments.
+L+
Trade
profit
was
held
back
in
the
clustering
process.
It
turns
out
that
the
clusters
differ
significantly
in
their
profit
and
risk
characteristics.
Using
conditional
distributions,
we
summarize
features
+L+
of
profitable
trading
styles
and
contrast
them
with
losing
strategies.
We
find
that
profitable
styles
tend
+L+
to
hold
trades
longer,
trade
at
higher
volatility,
and
trade
earlier
in
the
contracts.
We
also
show
how
+L+
some
clusters
uncover
"technical"
traders.
Using
the
information
about
the
individual
accounts,
the
+L+
assignments
of
accounts
to
clusters
are
described
by
entropy,
and
the
transitions
of
a
given
account
+L+
through
clusters
is
modeled
by
a
first
order
Markov
model.
+L+
1
Motivation
and
Overview

An
Extensible
Protocol
Architecture
for
+L+
Application-Specific
Networking
+L+
Marc
E.
Fiuczynski
+L+
Brian
N.
Bershad
+L+
fmef,bershadg@cs.washington.edu
+L+
Department
of
Computer
Science
and
Engineering
+L+
University
of
Washington
+L+
Seattle,
WA
98195
+L+
Abstract
+L+
Plexus
is
a
networking
architecture
that
allows
applications
to
achieve
high
performance
with
customized
+L+
protocols.
Application-specific
protocols
are
written
in
+L+
a
typesafe
language
and
installed
dynamically
into
the
+L+
operating
system
kernel.
Because
these
protocols
execute
within
the
kernel,
they
can
access
the
network
+L+
interface
and
other
operating
system
services
with
low
+L+
overhead.
Protocols
implemented
with
Plexus
outperform
equivalent
protocols
implemented
on
conventional
monolithic
systems.
Plexus
runs
in
the
context
+L+
of
the
SPIN
extensible
operating
system.
+L+
1
Introduction

Two
Computer
Systems
Paradoxes:
Serialize-to-Parallelize,
+L+
and
Queuing
Concurrent-Writes
+L+
Rimon
Orni
and
Uzi
Vishkin
+L+
September
17,
1995
+L+
Abstract
+L+
We
present
and
examine
the
following
Serialize-to-Parallelize
Paradox:
suppose
a
+L+
programmer
has
a
parallel
algorithm
in
mind;
the
programmer
must
serialize
the
algorithm,
and
is
actually
trained
to
suppress
its
parallelism,
while
writing
code;
later,
+L+
however,
compilation
and
runtime
techniques
are
used
to
reverse
the
results
of
this
serialization
effort
and
extract
as
much
parallelism
as
possible.
This
work
actually
provides
+L+
examples
where
parallel
or
parallel-style
code
enables
extracting
more
parallelism
than
+L+
standard
serial
code.
+L+
The
"arbitrary
concurrent-write"
convention
is
useful
in
parallel
algorithms
and
programs
and
appears
to
be
not
too
difficult
to
implement
in
hardware
for
serial
machines.
+L+
Still,
typically
concurrent-writes
to
the
same
memory
location
in
a
program
are
implemented
by
queuing
the
write
operations,
thus
requiring
time
linear
in
the
number
of
+L+
writes.
We
call
this
the
Queuing
Concurrent-Writes
Paradox.
+L+
Assuming
that
providing
useful,
easy-to-program
programming
paradigms
to
improve
the
overall
effectiveness
of
computer
systems
is
of
interest,
this
work
is
a
modest
+L+
example
for
applying
such
software-driven
considerations
to
computer
architecture
issues.
This
work
may
be
the
first
to
relate
parallel
algorithms
and
parallel
programming
+L+
with
the
technology
of
instruction
level
parallelism.
+L+
1
Introduction

Learning
and
Vision
Algorithms
for
+L+
Robot
Navigation
+L+
by
+L+
Margrit
Betke
+L+
S.M.,
Massachusetts
Institute
of
Technology
(1992)
+L+
Submitted
to
the
Department
of
Electrical
Engineering
and
Computer
+L+
Science
in
partial
fulfillment
of
the
requirements
for
the
degree
of
+L+
Doctor
of
Philosophy
in
Electrical
Engineering
and
Computer
Science
+L+
at
the
+L+
MASSACHUSETTS
INSTITUTE
OF
TECHNOLOGY
+L+
June
1995
+L+
c
Massachusetts
Institute
of
Technology
1995.
All
rights
reserved.
+L+
Author
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
+L+
Department
of
Electrical
Engineering
and
Computer
Science
+L+
May
18,
1995
+L+
Certified
by
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
+L+
Ronald
L.
Rivest
+L+
Professor
+L+
Thesis
Supervisor
+L+
Accepted
by
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
+L+
F.
R.
Morgenthaler
+L+
Chairman,
Department
Committee
on
Graduate
Students
+L+
+PAGE+

Augmenting
Collective
Adaptation
+L+
with
Simple
Process
Agents
+L+
Thomas
Haynes
+L+
Department
of
Mathematical
&
Computer
Sciences
+L+
600
South
College
Ave.
+L+
The
University
of
Tulsa
+L+
Tulsa,
OK
74104-3189
+L+
e-mail:
haynes@euler.mcs.utulsa.edu
+L+
Abstract
+L+
We
have
integrated
the
distributed
search
of
genetic
+L+
programming
based
systems
with
collective
memory
+L+
to
form
a
collective
adaptation
search
method.
Such
a
+L+
system
significantly
improves
search
as
problem
complexity
is
increased.
However,
there
is
still
considerable
scope
for
improvement.
In
collective
adaptation,
search
agents
gather
knowledge
of
their
environment
and
deposit
it
in
a
central
information
repository.
Process
agents
are
then
able
to
manipulate
that
+L+
focused
knowledge,
exploiting
the
exploration
of
the
+L+
search
agents.
We
examine
the
utility
of
increasing
+L+
the
capabilities
of
the
centralized
process
agents.
+L+
Introduction

Trainable
Cataloging
for
Digital
Image
Libraries
with
Applications
+L+
to
Volcano
Detection
+L+
M.C.
Burl
yz
,
U.M.
Fayyad
,
P.
Perona
,
P.
Smyth
+L+
California
Institute
of
Technology
Jet
Propulsion
Laboratory
+L+
MS
116-81
|
Pasadena,
CA
91125
MS
525-3660
|
Pasadena,
CA
91109
+L+
fburl,peronag@systems.caltech.edu
ffayyad,pjsg@aig.jpl.nasa.gov
+L+
Computation
and
Neural
Systems
Technical
Report
+L+
CNS-TR-96-01
|
October
2,
1996
+L+
Abstract
+L+
Users
of
digital
image
libraries
are
often
not
interested
in
image
data
per
se
but
in
derived
+L+
products
such
as
catalogs
of
objects
of
interest.
Converting
an
image
database
into
a
usable
+L+
catalog
is
typically
carried
out
manually
at
present.
For
many
larger
image
databases
the
+L+
purely
manual
approach
is
completely
impractical.
In
this
paper
we
describe
the
development
+L+
of
a
trainable
cataloging
system:
the
user
indicates
the
location
of
the
objects
of
interest
for
+L+
a
number
of
training
images
and
the
system
learns
to
detect
and
catalog
these
objects
in
the
+L+
rest
of
the
database.
In
particular
we
describe
the
application
of
this
system
to
the
cataloging
+L+
of
small
volcanoes
in
radar
images
of
Venus.
The
volcano
problem
is
of
interest
because
of
the
+L+
scale
(30,000
images,
order
of
1
million
detectable
volcanoes),
technical
difficulty
(the
variability
+L+
of
the
volcanoes
in
appearance)
and
the
scientific
importance
of
the
problem.
The
problem
of
+L+
uncertain
or
subjective
ground
truth
is
of
fundamental
importance
in
cataloging
problems
of
this
+L+
nature
and
is
discussed
in
some
detail.
Experimental
results
are
presented
which
quantify
and
+L+
compare
the
detection
performance
of
the
system
relative
to
human
detection
performance.
The
+L+
paper
concludes
by
discussing
the
limitations
of
the
proposed
system
and
the
lessons
learned
of
+L+
general
relevance
to
the
development
of
digital
image
libraries.
+L+
Keywords:
digital
image
libraries,
pattern
recognition,
science
data
analysis,
volcanoes,
Venus,
+L+
SAR,
detection,
classification,
learning,
remote
sensing
+L+
+PAGE+

127
+L+
Progress
for
Local
Variables
in
UNITY
+L+
Rob
Udink
and
Ted
Herman
and
Joost
Kok
+L+
Department
of
Computer
Science,
Utrecht
University,
+L+
P.O.
Box
80089,
3508
TB
Utrecht,
The
Netherlands
+L+
e-mail:
frob,ted,joostg@cs.ruu.nl
+L+
A
new
notion
of
refinement
for
UNITY
programs
with
local
variables
is
defined.
This
+L+
notion
is
compositional
in
the
following
sense:
programs
can
be
refined
in
arbitrary
contexts
such
that
all
unless
and
leadsto
properties
(i.e.
temporal
properties
for
both
safety
+L+
and
progress)
of
the
composition
are
preserved.
The
refinement
notion
is
based
on
preservation
of
a
new
kind
of
UNITY-like
property
that
takes
into
account
the
locality
of
+L+
variables.
We
do
a
small
case
study
about
registers.
+L+
Keyword
Codes:
F.3.1
+L+
Keywords:
Specifying
and
Verifying
and
Reasoning
about
Programs
+L+
1.
INTRODUCTION

Adaptive
Information
Filtering
using
Evolutionary
Computation
+L+
D.R.
Tauritz
&
J.N.
Kok
&
I.G.
Sprinkhuizen-Kuyper
+L+
Department
of
Computer
Science,
Leiden
University
+L+
P.O.
Box
9512,
2300
RA
Leiden,
The
Netherlands
+L+
1
Introduction

