12 - 11
abstract:
categories and subject descriptors:
general terms:
keywords:
introduction:
method: SSE We now explain some of the ideas behind the construction of the subsource somewhere extractor SSE of Theorem 2.6. Consider the source X. We are seeking to find in it a somewhere c-block-source, so that we can use it (together with Y ) in the block-source extractor of Theorem 2.8. Like in previous works in the extractor literature (e.g. [19, 13]) we use a "win-win" analysis which shows that either X is already a somewhere c-block-source, or it has a condensed part which contains a lot of the entropy of the source. In this case we proceed recursively on that part. Continuing this way we eventually reach a source so condensed that it must be a somewhere block source. Note that in [4], the challenge response mechanism was used to find a block source also, but there the entropy was so high that they could afford to use 675 t blocks low high med n bits total t blocks med med low high responded Challenge Challenge responded Challenge Unresponded med med n/t bits total SB SB Outputs Somewhere Block Source! Not Somewhere block source X Random Row &lt; k' 0&lt; low &lt; k'/t k'/c &lt; high &lt; k' k'/t &lt; med &lt; k'/c Figure 1: Analysis of the subsource somewhere extractor. a tree of depth 1. They did not need to recurse or condense the sources. Consider the tree of parts of the source X evolved by such recursion. Each node in the tree corresponds to some interval of bit locations of the source, with the root node corresponding to the entire source. A node is a child of another if its interval is a subinterval of the parent. It can be shown that some node in the tree is "good"; it corresponds to a somewhere c-source, but we don't know which node is good. Since we only want a somewhere extractor, we can apply to each node the somewhere block-source extractor of Corollary 2.8 ­ this will give us a random output in every "good" node of the tree. The usual idea is output all these values (and in seeded extractors, merge them using the ex-ternally given random seed). However, we cannot afford to do that here as there is no external seed and the number of these outputs (the size of the tree) is far too large. Our aim then will be to significantly prune this number of candidates and in fact output only the candidates on one path to a canonical "good" node. First we will give a very informal description of how to do this (Figure 1). Before calling SSE recursively on a subpart of a current part of X, we'll use the "Challenge-Response" mechanism described above to check if "it has entropy". 4 We will recurse only with the first (in left-to-right order) part which passes the "entropy test". Thus note that we will follow a single path on this tree. The algorithm SSE will output only the sets of strings produced by applying the somewhere c-block-extractor SB on the parts visited along this path. Now let us describe the algorithm for SSE. SSE will be initially invoked as SSE(x, y), but will recursively call itself with different inputs z which will always be substrings of x. 4 We note that we ignore the additional complication that SSE will actually use recursion also to compute the challenge in the challenge-response mechanism. Algorithm: SSE (z, y) Let pSE(., .) be the somewhere extractor with a polynomial number of outputs of Proposition 2.3. Let SB be the somewhere block source extractor of Corollary 2.8. Global Parameters: t, the branching factor of the tree. k the original entropy of the sources. Output will be a set of strings. 1. If z is shorter than k, return the empty set, else continue. 2. Partition z into t equal parts z = z 1 , z 2 , . . . , z t . 3. Compute the response set R(z, y) which is the set of strings output by pSE(z, y). 4. For i  [t], compute the challenge set C(z i , y), which is the set of outputs of SSE(z i , y). 5. Let h be the smallest index for which the challenge set C (z h , y) is not contained in the response set (set h = t if no such index exists). 6. Output SB(z, y) concatenated with SSE(z h , y). Proving that indeed there are subsources on which SSE will follow a path to a "good" (for these subsources) node, is the heart of the analysis. It is especially complex due to the fact that the recursive call to SSE on subparts of the current part is used to generate the Challenges for the Challenge-Response mechanism. Since SSE works only on a subsources we have to guarantee that restriction to these does not hamper the behavior of SSE in past and future calls to it. Let us turn to the highlights of the analysis, for the proof of Theorem 2.6. Let k  be the entropy of the source Z at some place in this recursion. Either one of its blocks Z i has 676 entropy k  /c, in which case it is very condensed, since its size is n/t for t  c), or it must be that c of its blocks form a c-block source with block entropy k  /t (which is sufficient for the extractor B used by SB). In the 2nd case the fact that SB(z, y) is part of the output of of our SSE guarantees that we are somewhere random. If the 2nd case doesn't hold, let Z i be the leftmost condensed block. We want to ensure that (on appropriate subsources) SSE calls itself on that ith subpart. To do so, we fix all Z j for j &lt; i to constants z j . We are now in the position described in the Challenge-Response mechanism section, that (in each of the first i parts) there is either no entropy or lots of entropy. We further restrict to subsources as explained there which make all first i - 1 blocks fail the "entropy test", and the fact that Z i still has lots of entropy after these restrictions (which we need to prove) ensures that indeed SSE will be recursively applied to it. We note that while the procedure SSE can be described recursively , the formal analysis of fixing subsources is actually done globally, to ensure that indeed all entropy requirements are met along the various recursive calls. Let us remark on the choice of the branching parameter t. On the one hand, we'd like to keep it small, as it dominates the number of outputs t c of SB, and thus the total number of outputs (which is t c log t n). For this purpose, any t = n o(1) will do. On the other hand, t should be large enough so that condensing is faster than losing entropy. Here note that if Z is of length n, its child has length n/t, while the entropy shrinks only from k  to k  /c. A simple calculation shows that if k (log t)/ log c) &gt; n 2 then a c block-source must exist along such a path before the length shrinks to k. Note that for k = n (1) a (large enough) constant t suffices (resulting in only logarithmic number of outputs of SSE). This analysis is depicted pictorially in Figure 1.  D Following is a rough description of our disperser D proving Theorem 2.1. The high level structure of D will resemble the structure of SSE - we will recursively split the source X and look for entropy in the parts. However now we must output a single value (rather than a set) which can take both values 0 and 1. This was problematic in SSE, even knowing where the "good" part (containing a c-block-source) was! How can we do so now? We now have at our disposal a much more powerful tool for generating challenges (and thus detecting entropy), namely the subsource somewhere disperser SSE. Note that in constructing SSE we only had essentially the somewhere c-block-source extractor SB to (recursively) generate the challenges, but it depended on a structural property of the block it was applied on. Now SSE does not assume any structure on its input sources except sufficient entropy 5 . Let us now give a high level description of the disperser D . It too will be a recursive procedure. If when processing some part Z of X it "realizes" that a subpart Z i of Z has entropy, but not all the entropy of Z (namely Z i , Z is a 2-block-source) then we will halt and produce the output of D. Intuitively, thinking about the Challenge-Response mechanism described above, the analysis implies that we 5 There is a catch ­ it only works on subsources of them! This will cause us a lot of head ache; we will elaborate on it later. can either pass or fail Z i (on appropriate subsources). But this means that the outcome of this "entropy test" is a 1-bit disperser! To capitalize on this idea, we want to use SSE to identify such a block-source in the recursion tree. As before, we scan the blocks from left to right, and want to distinguish three possibilities. low Z i has low entropy. In this case we proceed to i + 1. medium Z i has "medium" entropy (Z i , Z is a block-source). In which case we halt and produce an output (zero or one). high Z i has essentially all entropy of Z. In this case we recurse on the condensed block Z i . As before, we use the Challenge-Response mechanism (with a twist). We will compute challenges C(Z i , Y ) and responses R (Z, Y ), all strings of length m. The responses are computed exactly as before, using the somewhere extractor pSE. The Challenges are computed using our subsource somewhere extractor SSE. We really have 4 possibilities to distinguish, since when we halt we also need to decide which output bit we give. We will do so by deriving three tests from the above challenges and responses: (C H , R H ), (C M , R M ), (C L , R L ) for high, medium and low respectively, as follows. Let m  m H &gt;&gt; m M &gt;&gt; m L be appropriate integers: then in each of the tests above we restrict ourselves to prefixes of all strings of the appropriate lengths only. So every string in C M will be a prefix of length m M of some string in C H . Similarly, every string in R L is the length m L prefix of some string in R H . Now it is immediately clear that if C M is contained in R M , then C L is contained in R L . Thus these tests are monotone, if our sample fails the high test, it will definitely fail all tests. Algorithm: D (z, y) Let pSE(., .) be the somewhere extractor with a polynomial number of outputs of Proposition 2.3. Let SSE(., .) be the subsource somewhere extractor of Theorem 2.6. Global Parameters: t, the branching factor of the tree. k the original entropy of the sources. Local Parameters for recursive level: m L m M m H . Output will be an element of {0, 1}. 1. If z is shorter than k, return 0. 2. Partition z into t equal parts z = z 1 , z 2 , . . . , z t . 3. Compute three response sets R L , R M , R H using pSE(z, y). R j will be the prefixes of length m j of the strings in pSE (z, y). 4. For each i  [t], compute three challenge sets C i L , C i M , C i H using SSE(z i , y). C i j will be the prefixes of length m j of the strings in SSE(z i , y). 5. Let h be the smallest index for which the challenge set C L is not contained in the response set R L , if there is no such index, output 0 and halt. 6. If C h H is contained in R H and C h H is contained in R M , output 0 and halt. If C h H is contained in R H but C h H is not contained in R M , output 1 and halt. 677 t blocks t blocks t blocks fail fail fail pass pass pass fail fail fail fail fail fail fail fail fail fail fail fail pass pass fail pass fail fail low low high low low low high low med n bits total n/t bits total X low low Output 0 Output 1 n/t^2 bits total X_3 (X_3)_4 Figure 2: Analysis of the disperser. 7. Output D(z h , y), First note the obvious monotonicity of the tests. If Z i fails one of the tests it will certainly fail for shorter strings. Thus there are only four outcomes to the three tests, written in the order (low, medium, high): (pass, pass, pass), (pass, pass, fail), (pass, fail, fail) and (fail, fail, fail). Conceptually, the algorithm is making the following decisions using the four tests: 1. (fail, fail, fail): Assume Z i has low entropy and proceed to block i + 1. 2. (pass, fail, fail): Assume Z i is medium, halt and output 0. 3. (pass, pass, fail): Assume Z i is medium, halt and output 1. 4. (pass, pass, pass): Assume Z i is high and recurse on Z i . The analysis of this idea (depicted in Figure 2).turns out to be more complex than it seems. There are two reasons for that. Now we briefly explain them and the way to overcome them in the construction and analysis. The first reason is the fact mentioned above, that SSE which generates the challenges, works only on a subsources of the original sources. Restricting to these subsources at some level of the recursion (as required by the analysis of of the test) causes entropy loss which affects both definitions (such as these entropy thresholds for decisions) and correct-ness of SSE in higher levels of recursion. Controlling this entropy loss is achieved by calling SSE recursively with smaller and smaller entropy requirements, which in turn limits the entropy which will be lost by these restrictions. In order not to lose all the entropy for this reason alone, we must work with special parameters of SSE, essentially requiring that at termination it has almost all the entropy it started with. The second reason is the analysis of the test when we are in a medium block. In contrast with the above situation, we cannot consider the value of Z i fixed when we need it to fail on the Medium and Low tests. We need to show that for these two tests (given a pass for High), they come up both (pass, fail) and (fail, fail) each with positive probability. Since the length of Medium challenges and responses is m M , the probability of failure is at least exp(-(m M )) (this follows relatively easily from the fact that the responses are somewhere random). If the Medium test fails so does the Low test, and thus (fail, fail) has a positive probability and our disperser D outputs 0 with positive probability. To bound (pass, fail) we first observe (with a similar reasoning) that the low test fails with probability at least exp(-(m L )). But we want the medium test to pass at the same time. This probability is at least the probability that low fails minus the probability that medium fails. We already have a bound on the latter: it is at most poly(n)exp(-m M ). Here comes our control of the different length into play - we can make the m L sufficiently smaller than m M to yield this difference positive. We conclude that our disperser D outputs 1 with positive probability as well. Finally, we need to take care of termination: we have to ensure that the recurrence always arrives at a medium subpart , but it is easy to chose entropy thresholds for low, medium and high to ensure that this happens. 678  In this section we will breifly discuss an issue which arises in our construction that we glossed over in the previous sections . Recall our definition of subsources: Definition 6.1 (Subsources). Given random variables Z and ^ Z on {0, 1} n we say that ^ Z is a deficiency d subsource of Z and write ^ Z  Z if there exists a set A  {0,1} n such that (Z|A) = ^Z and Pr[Z  A]  2 -d . Recall that we were able to guarantee that our algorithms made the right decisions only on subsources of the original source. For example, in the construction of our final disperser , to ensure that our algorithms correctly identify the right high block to recurse on, we were only able to guarantee that there are subsources of the original sources in which our algorithm makes the correct decision with high probability. Then, later in the analysis we had to further restrict the source to even smaller subsources. This leads to complications, since the original event of picking the correct high block, which occurred with high probability, may become an event which does not occur with high probability in the current subsource. To handle these kinds of issues, we will need to be very careful in measuring how small our subsources are. In the formal analysis we introduce the concept of resiliency to deal with this. To give an idea of how this works, here is the actual definition of somewhere subsource extractor that we use in the formal analysis. Definition 6.2 (subsource somewhere extractor). A function SSE : {0, 1} n × {0, 1} n ({0, 1} m )  is a subsource somewhere extractor with nrows output rows, entropy threshold k, deficiency def, resiliency res and error  if for every (n, k)-sources X, Y there exist a deficiency def subsource X good of X and a deficiency def subsource Y good of Y such that for every deficiency res subsource X  of X good and deficiency res subsource Y  of Y good , the random variable SSE(X  , Y  ) is -close to a  × m somewhere random distribution. It turns out that our subsource somewhere extractor does satisfy this stronger definition. The advantage of this definition is that it says that once we restrict our attention to the good subsources X good , Y good , we have the freedom to further restrict these subsources to smaller subsources, as long as our final subsources do not lose more entropy than the resiliency permits. This issue of managing the resiliency for the various objects that we construct is one of the major technical challenges that we had to overcome in our construction. 
discussions:
conclusions:
