Using Long Runs as Predictors of Semantic Coherence in a |Structure| header
Partial Document Retrieval System |Structure| header
Hyopil Shin |Structure| header
Computing Research Laboratory, NMSU |Structure| header
PO Box 30001 |Structure| header
Las Cruces, NM, 88003 |Structure| header
hshin@crl.nmsu.edu |Structure| header
Jerrold F. Stach |Structure| header
Computer Science Telecommunications, UMKC |Structure| header
5100 Rockhill Road |Structure| header
Kansas City, MO, 64110 |Structure| header
stach@cstp.umkc.edu |Structure| header
Abstract |Structure| header
We propose a method for dealing with  |Structure| header
semantic complexities occurring in  |Structure| header
information retrieval systems on the basis of  |Structure| header
linguistic observations. Our method follows  |Structure| header
from an analysis indicating that long runs of  |Structure| header
content words appear in a stopped document  |Structure| header
cluster, and our observation that these long  |Structure| header
runs predominately originate from the  |Structure| header
prepositional phrase and subject  |Structure| header
complement positions and as such, may be  |Structure| header
useful predictors of semantic coherence.  |Structure| header
From this linguistic basis, we test three  |Structure| header
statistical hypotheses over a small collection  |Structure| header
of documents from different genre. By  |Structure| header
coordinating thesaurus semantic categories  |Structure| header
(SEMCATs) of the long run words to the  |Structure| header
semantic categories of paragraphs, we  |Structure| header
conclude that for paragraphs containing both  |Structure| header
long runs and short runs, the SEMCAT  |Structure| header
weight of long runs of content words is a  |Structure| header
strong predictor of the semantic coherence  |Structure| header
of the paragraph. |Structure| header
Introduction |Structure| body
One of the fundamental deficiencies of current  |Structure| body
information retrieval methods is that the words  |Structure| body
searchers use to construct terms often are not the  |Structure| body
same as those by which the searched information  |Structure| body
has been indexed. There are two components to  |Structure| body
this problem, synonymy and polysemy  |Structure| body
(Deerwester et. al., 1990). By definition of  |Structure| body
polysemy, a document containing the search  |Structure| body
terms or indexed with the search terms is not  |Structure| body
necessarily relevant. Polysemy contributes |Structure| body
heavily to poor precision. Attempts to deal with  |Structure| body
the synonymy problem have relied on  |Structure| body
intellectual or automatic term expansion, or the  |Structure| body
construction of a thesaurus. |Structure| body
Also the ambiguity of natural language causes  |Structure| body
semantic complexities that result in poor  |Structure| body
precision. Since queries are mostly formulated  |Structure| body
as words or phrases in a language, and the  |Structure| body
expressions of a language are ambiguous in  |Structure| body
many cases, the system must have ways to  |Structure| body
disambiguate the query. |Structure| body
In order to resolve semantic complexities in  |Structure| body
information retrieval systems, we designed a  |Structure| body
method to incorporate semantic information into  |Structure| body
current IR systems. Our method ( 1 ) adopts  |Structure| body
widely used Semantic Information or  |Structure| body
Categories, (2) calculates Semantic Weight  |Structure| body
based on probability, and (3) (for the purpose of  |Structure| body
verifying the method) performs partial text  |Structure| body
retrieval based upon Semantic Weight or  |Structure| body
Coherence to overcome cognitive overload of  |Structure| body
the human agent. We make two basic  |Structure| body
assumptions: 1. Matching search terms to  |Structure| body
semantic categories should improve retrieval  |Structure| body
precision. 2. Long runs of content words have a  |Structure| body
linguistic basis for Semantic Weight and can  |Structure| body
also be verified statistically. |Structure| body
1 A Brief Overview of Previous Approaches |Structure| body
There have been several attempts to deal with  |Structure| body
complexity using semantic information. These  |Structure| body
methods are hampered by the lack of  |Structure| body
dictionaries containing proper semantic  |Structure| body
categories for classifying text. Semantic  |Structure| body
methods designed by Boyd et. al. (1994) and  |Structure| body
Wendlandt et. al. (1991) demonstrate only  |Structure| body
simple examples and are restricted to small  |Structure| body
numbers of words. In order to overcome this |Structure| body
6 |Structure| body
deficiency, we propose to incorporate the  |Structure| body
structural information of the thesaurus, semantic  |Structure| body
categories (SEMCATs). However, we must also  |Structure| body
incorporate semantic categories into current IR  |Structure| body
systems in a compatible manner. The problem  |Structure| body
we deal with is partial text retrieval when all the  |Structure| body
terms of the traditional vector equations are not  |Structure| body
known. This is the case when retrieval is  |Structure| body
associated with a near real time filter, or when  |Structure| body
the size or number of documents in a corpus is  |Structure| body
unknown. In such cases we can retrieve only  |Structure| body
partial text, a paragraph or page. But since there  |Structure| body
is no document wide or corpus wide statistics, it  |Structure| body
is difficult to judge whether or not the text  |Structure| body
fragment is relevant. The method we employ in  |Structure| body
this paper identifies semantic "hot spots" in  |Structure| body
partial text. These "hot spots" are loci of  |Structure| body
semantic coherence in a paragraph of text. Such  |Structure| body
paragraphs are likely to convey the central ideas  |Structure| body
of the document, |Structure| body
We also deal with the computational aspects  |Structure| body
of partial text retrieval. We use a simple  |Structure| body
stop/stem method to expose long runs of context  |Structure| body
words that are evaluated relative to the search  |Structure| body
terms. Our goal is not to retrieve a highly  |Structure| body
relevant sentence, but rather to retrieve a portion  |Structure| body
of text that is semantically coherent with respect  |Structure| body
to the search terms. This locale can be returned  |Structure| body
to the searcher for evaluation and if it is  |Structure| body
relevant, the search terms can be refined. This  |Structure| body
approach is compatible with Latent Semantic  |Structure| body
Indexing (LSI) for partial text retrieval when the  |Structure| body
terms of the vector space are not known. LSI is  |Structure| body
based on a vector space information retrieval  |Structure| body
method that has demonstrated improved  |Structure| body
performance over the traditional vector space  |Structure| body
techniques. So when incorporating semantic  |Structure| body
information, it is necessary to adopt existing  |Structure| body
mathematical methods including probabilistic  |Structure| body
methods and statistical methods. |Structure| body
2 Theoretical Background |Structure| body
2.1 Long Runs |Structure| body
Partial Information Retrieval has to with  |Structure| body
detection of main ideas. Main ideas are topic  |Structure| body
sentences that have central meaning to the text.  |Structure| body
Our method of detecting main idea paragraphs  |Structure| body
extends from Jang (1997) who observed that  |Structure| body
after stemming and stopping a document, long  |Structure| body
runs of content words cluster. Content word runs |Structure| body
are a sequence of content words with a function  |Structure| body
word(s) prefix and suffix. These runs can be  |Structure| body
weighted for density in a stopped document and  |Structure| body
vector processed. We observed that these long  |Structure| body
content word runs generally originate from the  |Structure| body
prepositional phrase and subject complement  |Structure| body
positions, providing a linguistic basis for a dense  |Structure| body
neighbourhood of long runs of content words  |Structure| body
signalling a semantic locus of the writing. We  |Structure| body
suppose that these neighbourhoods may contain  |Structure| body
main ideas of the text. In order to verify this, we  |Structure| body
designed a methodology to incorporate semantic  |Structure| body
features into information retrieval and examined  |Structure| body
long runs of content words as a semantic  |Structure| body
predictor. |Structure| body
We examined all the long runs of the Jang  |Structure| body
(1997) collection and discovered most of them  |Structure| body
originate from the prepositional phrase and  |Structure| body
subject complement positions. According to  |Structure| body
Halliday (1985), a preposition is explained as a  |Structure| body
minor verb. It functions as a minor Predicator  |Structure| body
having a nominal group as its complement. Thus  |Structure| body
the internal structure of 'across the lake' is like  |Structure| body
that of 'crossing the lake', with a non-finite  |Structure| body
verb as Predicator (thus our choice of 3 words  |Structure| body
as a long run). When we interpret the  |Structure| body
preposition as a "minor Predicator" and "minor  |Structure| body
Process", we are interpreting the prepositional  |Structure| body
phrase as a kind of minor clause. That is,  |Structure| body
prepositional phrases function as a clause and  |Structure| body
their role is predication. |Structure| body
Traditionally, predication is what a statement  |Structure| body
says about its subject. A named predication  |Structure| body
corresponds to an externally defined function,  |Structure| body
namely what the speaker intends to say his or  |Structure| body
her subject, i.e. their referent. If long runs  |Structure| body
largely appear in predication positions, it would  |Structure| body
suggest that the speaker is saying something  |Structure| body
important and the longer runs of content words  |Structure| body
would signal a locus of the speaker's intention. |Structure| body
Extending from the statistical analysis of Jang  |Structure| body
(1997) and our observations of those long runs  |Structure| body
in the collection, we give a basic assumption of |Structure| body
OUT study: |Structure| body
Long runs of content words contain  |Structure| body
significant semantic information that a  |Structure| body
speaker wants to express and focus,  |Structure| body
and thus are semantic indicators or loci  |Structure| body
or main ideas. |Structure| body
7 |Structure| body
In this paper, we examine the SEMCAT  |Structure| body
values of long and short runs, extracted from a  |Structure| body
random document of the collection in Jang  |Structure| body
(1997), to determine if the SEMCAT weights of  |Structure| body
long runs of content words are semantic  |Structure| body
predictors. |Structure| body
2.2 SEMCATs |Structure| body
We adopted Roget's Thesaurus for our basic  |Structure| body
semantic categories (SEMCATs). We extracted  |Structure| body
the semantic categories from the online  |Structure| body
Thesaurus for convenience. We employ the 39  |Structure| body
intermediate categories as basic semantic  |Structure| body
information, since the 6 main categories are too  |Structure| body
general, and the many sub-categories are too  |Structure| body
narrow to be taken into account. We refer to  |Structure| body
these 39 categories as SEMCATs. |Structure| body
Table 1: Semantic Categories (SEMCATs) |Structure| body
	Abbreviation	Full Description |Structure| body
1	AFIG	Affection in General |Structure| body
2	ANT	Antagonism |Structure| body
3	CAU	Causation |Structure| body
4	CHN	Change |Structure| body
5	COIV	Conditional Intersocial Volition |Structure| body
6	CRTH	Creative Thought |Structure| body
7	DIM	Dimensions |Structure| body
	EXIS	Existence |Structure| body
9	EXOT	Extension of Thought |Structure| body
1°	FORM	Form |Structure| body
11	GINV	General Inter social Volition |Structure| body
12	INOM	Inorganic Matter |Structure| body
13	MECO	Means of Communication |Structure| body
14	MFRE	Materials for Reasoning |Structure| body
15	MIG	Matter ingeneral |Structure| body
16	MOAF	Moral Affections |Structure| body
17	MOCO	Modes of Communication |Structure| body
18	MOT	Motion |Structure| body
19	NOIC	Nature of Ideas Communicated |Structure| body
20	NUM	Number |Structure| body
21	opm	Operations of Intelligence |Structure| body
		In General |Structure| body
22	ORD	Order |Structure| body
23	ORGM	Organic Matter |Structure| body
24	pEAF	Personal Affections |Structure| body
25	PORE	Possessive Relations |Structure| body
26	PRCO	Precursory Conditions and Operations |Structure| body
27	PRVO	Prospective Volition |Structure| body
28	QUAN	Quantity |Structure| body
29	REAF	Religious Affections |Structure| body
ao	RELN	Relation |Structure| body
31	REOR	Reasoning Organization |Structure| body
32	REPR	Reasoning Process |Structure| body
33	ROVO	Result of Voluntary Action |Structure| body
34	SIG	Space in General |Structure| body
35	S IVO	Special Inter social Volition |Structure| body
36	SYAF	Sympathetic Affections |Structure| body
37	TIME	Time |Structure| body
38	VOAC	Voluntary Action |Structure| body
39	VOIG	Volition in General |Structure| body
2.3 Indexing Space and Stop Lists |Structure| body
Many of the most frequently occurring words in  |Structure| body
English, such as "the," "of," "and," "to," etc. are  |Structure| body
non-discriminators with respect to information  |Structure| body
filtering. Since many of these function words  |Structure| body
make up a large fraction of the text of Most  |Structure| body
documents, their early elimination in the  |Structure| body
indexing process speeds processing, saves  |Structure| body
significant amounts of index space and does not  |Structure| body
compromise the filtering process. In the Brown  |Structure| body
Corpus, the frequency of stop words is 551,057  |Structure| body
out of 1,013,644 total words. Function words  |Structure| body
therefore account for about 54.5% of the tokens  |Structure| body
in a document. |Structure| body
The Brown Corpus is useful in text retrieval  |Structure| body
because it is small and efficiently exposes  |Structure| body
content word runs. Furthermore, minimizing the  |Structure| body
document token size is very important in NLP- |Structure| body
based methods, because NLP-based methods  |Structure| body
usually need much larger indexing spaces than  |Structure| body
statistical-based methods due to processes for  |Structure| body
tagging and parsing. |Structure| body
3 Experimental Basis |Structure| body
In order to verify that long runs contribute to  |Structure| body
resolve semantic complexities and can be used  |Structure| body
as predictors of semantic intent, we employed a  |Structure| body
probabilistic, vector processing methodology. |Structure| body
3.1 Revised Probability and Vector Processing |Structure| body
In order to understand the calculation of  |Structure| body
SEMCATs, it is helpful to look at the structure |Structure| body
8 |Structure| body
of a preprocessed document. One document  |Structure| body
"Barbie" in the Jang (1997) collection has a total  |Structure| body
of 1,468 words comprised of 755 content words  |Structure| body
and 713 function words. The document has 17  |Structure| body
paragraphs. Filtering out function words using  |Structure| body
the Brown Corpus exposed the runs of content  |Structure| body
words as shown in Figure 1. |Structure| body
Figure 1: Preprocessed Text Document |Structure| body
BARBIE * * * * FAVORITE COMPANION  |Structure| body
DETRACTORS LOVE * * * PLASTIC  |Structure| body
PERFECTION * FASHION DOLL * *  |Structure| body
IMPOSSIBLE FIGURE * LONG * * * POPULAR  |Structure| body
GIRL * MA ITEL * WORLD * TOYMAKER *  |Structure| body
PRODUCTS RANGE * FISHER PRICE INFANT *  |Structure| body
SALES * * * TALL MANNEQUIN * BARBIE * *  |Structure| body
AGE * * * BEST SELLING GIRLS BRAND * *  |Structure| body
POISED * STRUT * * CHANGE * * MALE  |Structure| body
DOMINATED WORLD * MULTIMEDIA  |Structure| body
SOFTWARE * VIDEO GAMES |Structure| body
In Figure 1, asterisks occupy positions where  |Structure| body
function words were filtered out. The bold type  |Structure| body
indicates the location of the longest runs of  |Structure| body
content words. The run length distribution of  |Structure| body
Figure 1 is shown below: |Structure| body
Table 2: Distribution of Content Run Lengths in  |Structure| body
a sam le Document |Structure| body
Run Length	Frequency |Structure| body
1	II |Structure| body
2	8 |Structure| body
3	2 |Structure| body
4	2 |Structure| body
The traditional vector processing model  |Structure| body
requires the following set of terms: |Structure| body
•	(dl) the number of documents in the  |Structure| body
collection that each word occurs in |Structure| body
•	(id° the inverse document frequency of each  |Structure| body
word determined by logio(N/df) where N is  |Structure| body
the total number of documents. If a word  |Structure| body
appears in a query but not in a document, its  |Structure| body
idf is undefined. |Structure| body
•	The category probability of each query  |Structure| body
word. |Structure| body
Wendlandt (1991) points out that it is useful to  |Structure| body
retrieve a set of documents based upon key  |Structure| body
words only, and then considers only those  |Structure| body
documents for semantic category and attribute  |Structure| body
analysis. Wendlandt (1991) appends the s  |Structure| body
category weights to the t term weights of each  |Structure| body
document vector Di and the Query vector Q. |Structure| body
Since our basic query unit is a paragraph,  |Structure| body
document frequency (dl) and inverse document  |Structure| body
frequency (idf) have to be redefined. As we  |Structure| body
pointed out in Section 1, all terms are not known  |Structure| body
in partial text retrieval. Further, our approach is  |Structure| body
based on semantic weight rather than word  |Structure| body
frequency. Therefore any frequency based  |Structure| body
measures defined by Boyd et al. (1994) and  |Structure| body
Wendlandt (1991) need to be built from the  |Structure| body
probabilities of individual semantic categories.  |Structure| body
Those modifications are described below. As a  |Structure| body
simplifying assumption, we assume SEMCATs  |Structure| body
have a uniform probability distribution with  |Structure| body
regard to a word. |Structure| body
3.2 Calculating SEMCATs |Structure| body
Our first task in computing SEMCAT values  |Structure| body
was to create a SEMCAT dictionary for our  |Structure| body
method. We extracted SEMCATs for every  |Structure| body
word from the World Wide Web version of  |Structure| body
Roget's thesaurus. SEMCATs give probabilities  |Structure| body
of a word corresponding to a semantic category.  |Structure| body
The content word run 'favorite companion  |Structure| body
detractors love' is of length 4. Each word of the  |Structure| body
run maps to at least one SEMCAT. The word  |Structure| body
`favorite' maps to categories `PEAF and SYAF'.  |Structure| body
'companion' maps to categories 'ANT, MECO,  |Structure| body
NUM, ORD, ORGM, PEAF, PRVO, QUAN,  |Structure| body
and SYAF'. 'detractor' maps to `MOAF'. 'love'  |Structure| body
maps to `AFIG, ANT, MECO, MOAF, MOCO,  |Structure| body
ORGM, PEAF, PORE, PRVO, SYAF, and  |Structure| body
VOIG'. We treat the long runs as a semantic  |Structure| body
core from which to calculate SEMCAT values.  |Structure| body
SEMCAT weights are calculated based on the  |Structure| body
following equations. |Structure| body
Eq.1 Pik(Probability) - The likelihood of  |Structure| body
SEMCAT Si occurring due to the le  |Structure| body
trigger. For example, assuming a  |Structure| body
uniform probability distribution, the  |Structure| body
category PEAF triggered by the word  |Structure| body
favorite above, has the following  |Structure| body
probability: |Structure| body
PPEAF, favorite = 0.5(112) |Structure| body
Eq.2 Sw; (SEMCAT Weights in Long runs)  |Structure| body
is the sum of each SEMCATO weight  |Structure| body
of long runs based on their probabilities.  |Structure| body
In the above example, the long run |Structure| body
9 |Structure| body
'favorite companion detractors love,' the  |Structure| body
SEMCAT `MOAF' has SWMOAF  |Structure| body
(detractor(1)	love(.09)) = 1.09. We |Structure| body
can write; |Structure| body
SWi = I p,, |Structure| body
Eq.3 edwj (Expected data weights in a  |Structure| body
paragraph) - Given a set of N content  |Structure| body
words (data) in a paragraph, the  |Structure| body
expected weight of the SEMCATs of  |Structure| body
long runs in a paragraph is: |Structure| body
edwj =	pi; |Structure| body
,=1 |Structure| body
Eq.4 idwj (Inverse data weights in a  |Structure| body
paragraph) - The inverse data weight of  |Structure| body
SEMCATs of long runs for a set of N  |Structure| body
content words in a paragraph is |Structure| body
N ), |Structure| body
ichvi=logio((- |Structure| body
edwi |Structure| body
Eq.5 Weight(W) - The weight of SEMCAT  |Structure| body
Si in a paragraph is |Structure| body
W; = Swjxidw; |Structure| body
Eq.6	Relevance Weights	(Semantic |Structure| body
Coherence) |Structure| body
Our method performs the following steps: |Structure| body
1. calculate the SEMCAT weight of each long  |Structure| body
content word run in every paragraph (Sw) |Structure| body
2. calculate the expected data weight of each  |Structure| body
paragraph (edw) |Structure| body
3. calculate the inverse expected data weight of  |Structure| body
each paragraph (idw) |Structure| body
4. calculate the actual weight of each  |Structure| body
paragraph (Swxidw) |Structure| body
5. calculate coherence weights (total relevance) |Structure| body
by summing the weights of (Swxidw). |Structure| body
In every paragraph, extraction of SEMCATs  |Structure| body
from long runs is done first. The next step is  |Structure| body
finding the same SEMCATs of long runs  |Structure| body
through every word in a paragraph (expected  |Structure| body
data weight), then calculate idw, and finally  |Structure| body
Swxidw. The final, total relevance weights are  |Structure| body
an accumulation of all weights of SEMCATs of  |Structure| body
content words in a paragraph. Total relevance  |Structure| body
tells how many SEMCATs of the Query's long  |Structure| body
runs appear in a paragraph. Higher values imply  |Structure| body
that the paragraph is relevant to the long runs of  |Structure| body
the Query. |Structure| body
The following is a program output for  |Structure| body
calculating SEMCAT weights for an arbitrary  |Structure| body
long run: "SEVEN INTERACTIVE  |Structure| body
PRODUCTS LED" |Structure| body
SEMCAT: EXOT Sw : 1.00 edw : 1.99 idw :  |Structure| body
1.44 Swxidw : 1.44 |Structure| body
SEMCAT: GINV Sw : 0.33 edw : 1.62 idw :  |Structure| body
1.53 Swxidw : 0.51 |Structure| body
SEMCAT: MOT Sw : 0.20 edw : 0.71 idw :  |Structure| body
1.89 Swxidw : 0.38 |Structure| body
SEMCAT: NUM Sw : 0.20 edw : 1.76 idw :  |Structure| body
1.49 Swxidw : 0.30 |Structure| body
SEMCAT: ORGM Sw : 0.20 edw : 1.67 idw  |Structure| body
1.52 Swxidw ; 0,30 |Structure| body
SEMCAT: PEAF Sw : 0.53 edw : 1.50 idw :  |Structure| body
1.56 Swxidw : 0.83 |Structure| body
SEMCAT: REAF Sw : 0.20 edw : 0.20 idw :  |Structure| body
2.44 Swxidw : 0.49 |Structure| body
SEMCAT: SYAF Sw : 0.33 edw : 1.19 idw :  |Structure| body
1.66 Swxidw : 0.55 |Structure| body
Total (Swxidw) : 4,79 |Structure| body
4	Experimental Results |Structure| body
The goal of employing probability and vector  |Structure| body
processing is to prove the linguistic basis that  |Structure| body
long runs of content words can be used as  |Structure| body
predictors of semantic intent But we also want to  |Structure| body
exploit the computational advantage of  |Structure| body
removing the function words from the  |Structure| body
document, which reduces the number of tokens  |Structure| body
processed by about 50% and thus reduces vector  |Structure| body
space and probability computations. If it is true  |Structure| body
that long runs of content words are predictors of  |Structure| body
semantic coherence, we can further reduce the  |Structure| body
complexity of vector computations: (1) by  |Structure| body
eliminating those paragraphs without long runs  |Structure| body
from consideration, (2) within remaining  |Structure| body
paragraphs with long runs, computing and  |Structure| body
summing the semantic coherence of the longest  |Structure| body
runs only, (3) ranking the eligible paragraphs for  |Structure| body
retrieval based upon their semantic weights  |Structure| body
relative to the query. |Structure| body
Jang (1997) established that the distribution  |Structure| body
of long runs of content words and short runs of  |Structure| body
content words in a collection of paragraphs are  |Structure| body
drawn from different populations. This implies |Structure| body
10 |Structure| body
that either long runs or short runs are predictors,  |Structure| body
but since all paragraphs contain short runs, i.e. a  |Structure| body
single content word separated by function  |Structure| body
words, only long runs can be useful predictors.  |Structure| body
Furthermore, only long runs as we define them  |Structure| body
can be used as predictors because short runs are  |Structure| body
insufficient to construct the language constructs  |Structure| body
for prepositional phrase and subject complement  |Structure| body
positions. If short runs were discriminators, the  |Structure| body
linguistic assumption of this research would be  |Structure| body
violated. The statistical analysis of Jang (1997)  |Structure| body
does not indicate this to be the case. |Structure| body
To proceed in establishing the viability of  |Structure| body
our approach, we proposed the following  |Structure| body
experimental hypotheses: |Structure| body
(111) The SEMCAT weights for long runs  |Structure| body
of content words are statistically greater  |Structure| body
than weights for short runs of content  |Structure| body
words. Since each content word can map  |Structure| body
to multiple SEMCATs, we cannot  |Structure| body
assume that the semantic weight of a  |Structure| body
long run is a function of its length. The  |Structure| body
semantic coherence of long runs should  |Structure| body
be a more granular discriminator. |Structure| body
(112) For paragraphs containing long runs  |Structure| body
and short runs, the distribution of long  |Structure| body
run SEMCAT weights is statistically  |Structure| body
different from the distribution of short  |Structure| body
run SEMCAT weights. |Structure| body
(H3) There is a positive correlation  |Structure| body
between the sum of long run SEMCAT  |Structure| body
weights and the semantic coherence of a  |Structure| body
paragraph, the total paragraph SEMCAT  |Structure| body
weight. |Structure| body
A detailed description of these experiments  |Structure| body
and their outcome are described in Shin (1997,  |Structure| body
1999). The results of the experiments and the  |Structure| body
implications of those results relative to the  |Structure| body
method we propose are discussed below. Table 3  |Structure| body
gives the SEMCAT weights for seventeen  |Structure| body
paragraphs randomly chosen from one document  |Structure| body
in the collection of Jang (1997). |Structure| body
Table 3: SEMCAT Weights of 17 Paragraphs Chosen  |Structure| body
Randomly From a Collection |Structure| body
Paragraph	Short Runs	Long Runs |Structure| body
	Weight	Weight |Structure| body
1	29.84	18.60 |Structure| body
2	31.29	12.81 |Structure| body
3	23.29	4.25 |Structure| body
4	23.94	11.63 |Structure| body
5	34.63	35.00 |Structure| body
6	22.85	03.32 |Structure| body
7	21.74	00.00 |Structure| body
8	35.84	15.94 |Structure| body
9	30.15	00.00 |Structure| body
10	13.40	00.00 |Structure| body
11	23.01	07.82 |Structure| body
12	31.69	04.79 |Structure| body
13	36.54	00.00 |Structure| body
14	17.91	10.55 |Structure| body
15	19.70	05.83 |Structure| body
16	17.11	00.00 |Structure| body
17	31.86	00.00 |Structure| body
The data was evaluated using a standard two way  |Structure| body
F test and analysis of variance table with ot = .05.  |Structure| body
The analysis of variance table for the paragraphs  |Structure| body
in Table 3 is shown in Table 4. |Structure| body
Table 4: Analysis of Variance for Table 2 Data |Structure| body
Variation	Degrees of	Mean Square	F |Structure| body
	Freedom		 |Structure| body
Between	1	2904.51	68.56 |Structure| body
Treatments			 |Structure| body
V, = 2904.51			 |Structure| body
Between Blocks	16	93.92	2.21 |Structure| body
yr = 1502.83			 |Structure| body
Residual	or	16	42.36	 |Structure| body
Random			 |Structure| body
V,= 677.77			 |Structure| body
Total	33		 |Structure| body
V = 5085.11			 |Structure| body
At the .05 significance level, Fa 05 = 4.49 for  |Structure| body
1,16 degrees of freedom. Since 68.56 > 4.49 we  |Structure| body
reject the assertion that column means (run  |Structure| body
weights) are equal in Table 2. Long run and  |Structure| body
short run weights come from different  |Structure| body
populations. We accept Hl. |Structure| body
For the between paragraph treatment, the  |Structure| body
row means (paragraph weights) have an F value  |Structure| body
of 2.21. At the .05 significance level, Fa . 05 =  |Structure| body
2.28 for 16,16 degrees of freedom. Since 2.21 <  |Structure| body
2.28 we cannot reject the assertion that there is  |Structure| body
no significant difference in SEMCAT weights  |Structure| body
between paragraphs. That is, paragraph weights  |Structure| body
do not appear to be taken from different  |Structure| body
populations, as do the long run and short run  |Structure| body
weight distributions. Thus, the semantic weight |Structure| body
11 |Structure| body
of the content words in a paragraph cannot be  |Structure| body
used to predict the semantic weight of the  |Structure| body
paragraph. We therefore proceed to examine H2. |Structure| body
Notice that two paragraphs in Table 2 are  |Structure| body
without long runs. We need to repeat the  |Structure| body
analysis of variance for only those paragraphs  |Structure| body
with long runs to see if long runs are  |Structure| body
discriminators. Table 5 summarizes those  |Structure| body
paragraphs. |Structure| body
Table 5: SEMCAT weights of 11 paragraphs  |Structure| body
containing Ion runs and short runs |Structure| body
Paragraph	Short Runs	Long Runs |Structure| body
	Weight	Weight |Structure| body
1	29.84	18.60 |Structure| body
2	31.29	12.81 |Structure| body
3	23.29	4.25 |Structure| body
4	23.94	11,63 |Structure| body
5	34.63	35.00 |Structure| body
6	22.85	03.32 |Structure| body
8	35.84	15.94 |Structure| body
11	23.01	07.82 |Structure| body
12	31.69	04.79 |Structure| body
14	17.91	10.55 |Structure| body
15	19.70	05.83 |Structure| body
This data was evaluated using a standard two way  |Structure| body
F test and analysis of variance with a = .05. The  |Structure| body
analysis of variance table for the paragraphs in  |Structure| body
Table 5 follows. |Structure| body
Table 6: Analysis of Variance for Table 5 Data |Structure| body
Variation	._	Mean Square	F |Structure| body
	Degrees		 |Structure| body
	of Freedom		 |Structure| body
Between Treatments	1	1430.98	291.44 |Structure| body
V= 1430.98			 |Structure| body
Between Blocks	10	94.40	19.22 |Structure| body
V= 944.05			 |Structure| body
Residual	or	10	4.91	 |Structure| body
Random			 |Structure| body
V,...- 49.19			 |Structure| body
Total	21		 |Structure| body
V = 2424.26			 |Structure| body
At the .05 significance level, F. .05 = 4.10 for  |Structure| body
2,10 degrees of freedom. 4.10 < 291.44. At the  |Structure| body
.05 significance level, F. = 2.98 for 10,10  |Structure| body
degrees of freedom. 2.98 < 19.22. For  |Structure| body
paragraphs in a collection containing both long  |Structure| body
and short runs: the SEMCAT weights of the |Structure| body
long runs and short runs are drawn from  |Structure| body
different distributions. We accept 112. |Structure| body
For paragraphs containing long runs and  |Structure| body
short runs, the distributions of long run  |Structure| body
SEMCAT weights is different from the  |Structure| body
distribution of short run SEMCAT weights. We  |Structure| body
know from the linguistic basis for long runs that  |Structure| body
short runs cannot be used as predictors. We  |Structure| body
therefore proceed to examine the Pearson  |Structure| body
correlation between the long run SEMCAT  |Structure| body
weights and paragraph SEMCAT weights for  |Structure| body
those paragraphs with both long and short  |Structure| body
content word runs. |Structure| body
Table 7: Correlation of Long Run SEMCAT  |Structure| body
Wei hts to Para ra h SEMCAT Weight |Structure| body
Paragraph	Long Runs Semantic Weight	Paragraph Semantic Weight |Structure| body
1	18.60	48.44 |Structure| body
2	12.81	44.10 |Structure| body
3	4.25	27.54 |Structure| body
4	11.63	35.57 |Structure| body
5	35.00	69.63 |Structure| body
6	03.32	26.17 |Structure| body
8	15.94	51.78 |Structure| body
11	07.82	30.83 |Structure| body
12	-04.79	31.69 |Structure| body
14	10.55	28.46 |Structure| body
15	05.83	25.53 |Structure| body
The weights in Table have a positive Pearson  |Structure| body
Product Correlation coefficient of .952. We  |Structure| body
therefore accept 1-13. There is a positive  |Structure| body
correlation between the sum of long run  |Structure| body
SEMCAT weights and the semantic coherence  |Structure| body
of a paragraph, the total paragraph SEMCAT  |Structure| body
weight. |Structure| body
5. Conclusion |Structure| body
This research tested three statistical hypotheses  |Structure| body
extending from two observations: (1) fang  |Structure| body
(1997) observed the clustering of long runs of  |Structure| body
content words and established the distribution of  |Structure| body
long run lengths and short run lengths are drawn  |Structure| body
from different populations, (2) our observation  |Structure| body
that these long runs of content words originate  |Structure| body
from the prepositional phrase and subject  |Structure| body
complement positions. According to Halliday  |Structure| body
(1985) those grammar structures function as |Structure| body
12 |Structure| body
minor predication and as such are loci of  |Structure| body
semantic intent or coherence. In order to  |Structure| body
facilitate the use of long runs as predictors, we  |Structure| body
modified the traditional measures of Boyd et al.  |Structure| body
(1994), Wendlandt (1991) to accommodate  |Structure| body
semantic categories and partial text retrieval.  |Structure| body
The revised metrics and the computational  |Structure| body
method we propose were used in the statistical  |Structure| body
experiments presented above. The main findings  |Structure| body
of this work are |Structure| body
1.	the distribution semantic coherence |Structure| body
(SEMCAT weights) of long runs is not  |Structure| body
statistically greater than that of short  |Structure| body
runs, |Structure| body
2.	for paragraphs containing both long runs |Structure| body
and short runs, the SEMCAT weight  |Structure| body
distributions are drawn from different  |Structure| body
populations |Structure| body
3. there is a positive correlation between |Structure| body
the sum of long run SEMCAT weights  |Structure| body
and the total SEMCAT weight of the  |Structure| body
paragraph (its semantic coherence). |Structure| body
Significant additional work is required to  |Structure| body
validate these preliminary results. The collection  |Structure| body
employed in Jang (1997) is not a standard  |Structure| body
Corpus so we have no way to test precision and  |Structure| body
relevance of the proposed method. The results of  |Structure| body
the proposed method are subject to the accuracy  |Structure| body
of the stop lists and filtering function. |Structure| body
Nonetheless, we feel the approach proposed  |Structure| body
has potential to improve performance through  |Structure| body
reduced token processing and increased  |Structure| body
relevance through consideration of semantic  |Structure| body
coherence of long runs. Significantly, our  |Structure| body
approach does not require knowledge of the  |Structure| body
collection. |Structure| body
References |Structure| body
Boyd R., Driscoll J, and Syu I. (1994) incorporating  |Structure| reference
Semantics Within a Connectionist Model and a  |Structure| reference
Vector Processing Model. In Proceedings of the  |Structure| reference
TREC-2, NIST. |Structure| reference
Deerwester S., Furnas G., Landauer T., and  |Structure| reference
Harshman R. (1990) Indexing by Latent Semantic  |Structure| reference
Anaysis. Journal of the American Society of  |Structure| reference
Information Science 41-6. |Structure| reference
Halliday M.A.K. (1985) An Introduction to |Structure| reference
Functional Grammar. Edward Arnold, London. |Structure| reference
Jang S. (1997) Extracting Context from Unstructured  |Structure| reference
Text Documents by Content Word Density. M.S.  |Structure| reference
Thesis, University of Missouri-Kansas City. |Structure| reference
Moffat A., Davis R., Wilkinson, R., and Zobel J.  |Structure| reference
(1994) Retrieval of Partial Documents. In  |Structure| reference
Proceedings of TREC-2. |Structure| reference
Shin H. (1997) Incorporating Semantic Categories  |Structure| reference
(SEMCATs) into a Partial Information Retrieval  |Structure| reference
System. M.S. Thesis, University of Missouri  |Structure| reference
Kansas City. |Structure| reference
Shin H., Stach J. (1999) Incorporating Probabilistic  |Structure| reference
Semantic Categories (SEMCATs) Into Vector  |Structure| reference
Space Techniques for Partial Document Retrieval.  |Structure| reference
Journal of Computer Science and Information  |Structure| reference
Management, vol. 2, No. 4, December 1999, to  |Structure| reference
appear. |Structure| reference
Wendlandt E. and Driscoll R. (1991) Incorporating a  |Structure| reference
semantic analysis into a document retrieval  |Structure| reference
strategy. CACM 31, pp. 54-48. |Structure| reference
13 |Structure| reference
