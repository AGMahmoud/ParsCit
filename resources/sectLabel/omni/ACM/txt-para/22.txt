# Para 0 1
A Two-Phase Sampling Technique for Information
# Para 1 1
Extraction from Hidden Web Databases
# Para 2 1
Y.L. Hedley, M. Younas, A. James
# Para 3 2
School of Mathematical and Information Sciences
Coventry University, Coventry CV1 5FB, UK
# Para 5 2
{y.hedley, m.younas, a.james}@coventry.ac.uk 
ABSTRACT
# Para 7 16
Hidden Web databases maintain a collection of specialised 
documents, which are dynamically generated in response to users’ 
queries. However, the documents are generated by Web page 
templates, which contain information that is irrelevant to queries. 
This paper presents a Two-Phase Sampling (2PS) technique that 
detects templates and extracts query-related information from the 
sampled documents of a database. In the first phase, 2PS queries 
databases with terms contained in their search interface pages and 
the subsequently sampled documents. This process retrieves a 
required number of documents. In the second phase, 2PS detects 
Web page templates in the sampled documents in order to extract 
information relevant to queries. We test 2PS on a number of real- 
world Hidden Web databases. Experimental results demonstrate 
that 2PS effectively eliminates irrelevant information contained in 
Web page templates and generates terms and frequencies with 
improved accuracy.
# Para 23 1
Categories and Subject Descriptors
# Para 24 2
H.3.5 [Information Storage and Retrieval]: Online Information 
Services – Web-based services.
# Para 26 1
General Terms
# Para 27 1
Algorithms, Experimentation.
# Para 28 1
Keywords
# Para 29 2
Hidden Web Databases, Document Sampling, Information 
Extraction.
# Para 31 1
1. INTRODUCTION
# Para 32 8
An increasing number of databases on the Web maintain a 
collection of documents such as archives, user manuals or news 
articles. These databases dynamically generate documents in 
response to users’ queries and are referred to as Hidden Web 
databases [5]. As the number of databases proliferates, it has 
become prohibitive for specialised search services (such as 
search.com) to evaluate databases individually in order to answer 
users’ queries.
# Para 40 1
Current techniques such as database selection and categorisation
# Para 41 6
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee.
# Para 47 1
WIDM’04, November 12–13, 2004, Washington, DC, USA.
# Para 48 1
Copyright 2004 ACM 1-58113-978-0/04/0011...$5.00. 
# Para 49 1
M. Sanderson
# Para 50 2
Department of Information Studies
University of Sheffield, Sheffield, S1 4DP, UK
# Para 52 1
m.sanderson@sheffield.ac.uk
# Para 53 15
have been employed to enhance the effectiveness of information 
retrieval from databases [2, 5, 10, 11, 15]. In the domain of the 
Hidden Web, knowledge about the contents of databases is often 
unavailable. Existing approaches such as in [2, 10, 15] acquire 
knowledge through sampling documents from databases. For 
instance, query-based sampling [2] queries databases with terms 
that are randomly selected from those contained in the sampled 
documents. The techniques in [10, 15] sample databases with 
terms obtained from Web logs to retrieve additional topic terms. 
A major issue associated with existing techniques is that they also 
extract information irrelevant to queries. That is, information 
extracted is often found in Web page templates, which contain 
navigation panels, search interfaces and advertisements. 
Consequently, the accuracy of terms and frequencies generated 
from sampled documents has been reduced.
# Para 68 10
In addition, approximate string matching techniques are adopted 
by [13] to extract information from Web pages, but this approach 
is limited to textual contents only. Alternatively, the approaches 
proposed in [3, 4] analyse Web pages in tree-like structures. 
However, such an approach requires Web pages with well- 
conformed HTML tag trees. Furthermore, [3] discovers 
dynamically generated objects from Web pages, which are 
clustered into groups of similar structured pages based on a set of 
pre-defined templates, such as exception page templates and 
result page templates.
# Para 78 11
In this paper, we propose a sampling and extraction technique, 
which is referred to as Two-Phase Sampling (2PS). 2PS aims to 
extract information relevant to queries in order to acquire 
information contents of underlying databases. Our technique is 
applied in two phases. First, it randomly selects a term from those 
found in the search interface pages of a database to initiate the 
process of sampling documents. Subsequently, 2PS queries the 
database with terms randomly selected from those contained in 
the sampled documents. Second, 2PS detects Web page templates 
and extracts query-related information from which terms and 
frequencies are generated to summarise the database contents.
# Para 89 11
Our approach utilises information contained in search interface 
pages of a database to initiate the sampling process. This differs 
from current sampling techniques such as query-based sampling, 
which performs an initial query with a frequently used term. 
Furthermore, 2PS extracts terms that are relevant to queries thus 
generating statistics (i.e., terms and frequencies) that represent 
database contents with improved accuracy. By contrast, the 
approaches in [2, 10, 15] extract all terms from sampled 
documents, including those contained in Web page templates. 
Consequently, information that is irrelevant to queries is also 
extracted.
# Para 100 1
1
# Para 101 1
Figure 1. The Two-Phase Sampling (2PS) technique.
# Para 102 6
2PS is implemented as a prototype system and tested on a number 
of real-world Hidden Web databases, which contain computer 
manuals, healthcare archives and news articles. Experimental 
results show that our technique effectively detects Web page 
templates and generates terms and frequencies (from sampled 
documents) that are relevant to the queries.
# Para 108 7
The remainder of the paper is organised as follows. Section 2 
introduces current approaches to the discovery of information 
contents of Hidden Web databases. Related work on the 
information extraction from Web pages or dynamically generated 
documents is also discussed. Section 3 describes the proposed 
2PS technique. Section 4 presents experimental results. Section 5 
concludes the paper.
# Para 115 1
2. RELATED WORK
# Para 116 9
A major area of current research into the information retrieval of 
Hidden Web databases focuses on the automatic discovery of 
information contents of databases, in order to facilitate their 
selection or categorisation. For instance, the technique proposed 
in [6] analyses the hyperlink structures of databases in order to 
facilitate the search for databases that are similar in content. The 
approach adopted by [10, 15] examines the textual contents of 
search interface pages maintained by data sources to gather 
information about database contents.
# Para 125 12
A different approach is to retrieve actual documents to acquire 
such information. However, in the domain of Hidden Web 
databases, it is difficult to obtain all documents from a database. 
Therefore, a number of research studies [2, 10, 15] obtain 
information by retrieving a set of documents through sampling. 
For instance, query-based sampling [2] queries databases with 
terms that are randomly selected from those contained in the 
sampled documents. The techniques in [10, 15] sample databases 
with terms extracted from Web logs to obtain additional topic 
terms. These techniques generate terms and frequencies from 
sampled documents, which are referred to as Language Models 
[2], Textual Models [10, 15] or Centroids [11].
# Para 137 10
A key issue associated with the aforementioned sampling 
techniques is that they extract information that is often irrelevant 
to queries, since information contained in Web page templates 
such as navigation panels, search interfaces and advertisements is 
also extracted. For example, a language model generated from the 
sampled documents of the Combined Health Information 
Database (CHID) contains terms (such as ‘author’ and ‘format’) 
with high frequencies. These terms are not relevant to queries but 
are used for descriptive purposes. Consequently, the accuracy of 
terms and frequencies generated from sampled documents has
# Para 147 4
been reduced. The use of additional stop-word lists has been 
considered in [2] to eliminate irrelevant terms - but it is 
maintained that such a technique can be difficult to apply in 
practice.
# Para 151 12
Existing techniques in information extraction from Web pages are 
of varying degrees of complexity. For instance, approximate 
string matching techniques are adopted by [13] to extract texts 
that are different. This approach is limited to finding textual 
similarities and differences. The approaches proposed in [3, 4] 
analyse textual contents and tag structures in order to extract data 
from Web pages. However, such an approach requires Web pages 
that are produced with well-conformed HTML tag-trees. 
Computation is also needed to convert and analyse Web pages in 
a tree-like structure. Moreover, [3] identifies Web page templates 
based on a number of pre-defined templates, such as exception 
page templates and result page templates.
# Para 163 6
Our technique examines Web documents based on textual 
contents and the neighbouring tag structures rather than analysing 
their contents in a tree-like structure. We also detect information 
contained in different templates through which documents are 
generated. Therefore, it is not restricted to a pre-defined set of 
page templates.
# Para 169 14
Furthermore, we focus on databases that contain documents such 
as archives and new articles. A distinct characteristic of 
documents found in such a domain is that the content of a 
document is often accompanied by other information for 
supplementary or navigation purposes. The proposed 2PS 
technique detects and eliminates information contained in 
templates in order to extract the content of a document. This 
differs from the approaches in [1, 4], which attempt to extract a 
set of data from Web pages presented in a particular pattern. For 
example, the Web pages of a bookstore Web site contain 
information about authors followed by their associated list of 
publications. However, in the domain of document databases, 
information contained in dynamically generated Web pages is 
often presented in a structured fashion but irrelevant to queries.
# Para 183 3
Other research studies [9, 8, 12] are specifically associated with 
the extraction of data from query forms in order to further the 
retrieval of information from the underlying databases.
# Para 186 1
3. TWO-PHASE SAMPLING
# Para 187 4
This section presents the proposed technique for extracting 
information from Hidden Web document databases in two phases, 
which we refer to as Two-Phase Sampling (2PS). Figure 1 depicts 
the process of sampling a database and extracting query-related
# Para 191 1
2
# Para 192 6
information from the sampled documents. In phase one, 2PS 
obtains randomly sampled documents. In phase two, it detects 
Web page templates. This extracts information relevant to the 
queries and then generates terms and frequencies to summarise 
the database content. The two phases are detailed in section 3.1 
and 3.2.
# Para 198 1
3.1 Phase One: Document Sampling
# Para 199 9
In the first phase we initiate the process of sampling documents 
from a database with a randomly selected term from those 
contained in the search interface pages of the database. This 
retrieves top N documents where N represents the number of 
documents that are the most relevant to the query. A subsequent 
query term is then randomly selected from terms extracted from 
the sampled documents. This process is repeated until a required 
number of documents are sampled. The sampled documents are 
stored locally for further analysis.
# Para 208 3
Figure 2 illustrates the algorithm that obtains a number of 
randomly sampled documents. tq denotes a term extracted from 
the search interface pages of a database, D. qtp represents a query
# Para 211 1
term selected from a collection of terms, Q, qtp e Q, 1 &lt;_ p &lt;_ m;
# Para 212 4
where m is the distinct number of terms extracted from the search 
interface pages and the documents that have been sampled. R 
represents the set of documents randomly sampled from D. tr is a 
term extracted from di. di represents a sampled document from D,
# Para 216 1
di e D, 1 &lt;_ i &lt;_ n, where n is the number of document to sample.
# Para 217 1
Algorithm SampleDocument
# Para 218 2
Extract tq from search interface pages of D, Q = tq 
For i = 1 to n
# Para 220 1
Randomly select qtp from Q
# Para 221 2
If (qtp has not been selected previously)
Execute the query with qtp on D
# Para 223 1
j = 0
# Para 224 1
While j &lt;= N
# Para 225 1
If (di o R)
# Para 226 1
Retrieve di from D
# Para 227 1
Extract tr from di,
# Para 228 3
R = di 
Q = tr 
Increase j by 1
# Para 231 1
End if
# Para 232 1
End while
# Para 233 1
End if
# Para 234 1
End for
# Para 235 2
Figure 2. The algorithm for sampling documents from a
database.
# Para 237 8
2PS differs from query-based sampling in terms of selecting an 
initial query. The latter selects an initial term from a list of 
frequently used terms. 2PS initiates the sampling process with a 
term randomly selected from those contained in the search 
interface pages of the database. This utilises a source of 
information that is closely related to its content. Moreover, 2PS 
analyses the sampled documents in the second phase in order to 
extract query-related information. By contrast, query-based
# Para 245 2
sampling does not analyse their contents to determine whether 
terms are relevant to queries.
# Para 247 2
3.2 Phase Two: Document Content Extraction 
and Summarisation
# Para 249 5
The documents sampled from the first phase are further analysed 
in order to extract information relevant to the queries. This is then 
followed by the generation of terms and frequencies to represent 
the content of the underlying database. This phase is carried out 
through the following processes.
# Para 254 1
3.2.1 Generate Document Content Representations
# Para 255 14
The content of each sampled document is converted into a list of 
text and tag segments. Tag segments include start tags, end tags 
and single tags specified in HyperText Markup Language 
(HTML). Text segments are text that resides between two tag 
segments. The document content is then represented by text 
segments and their neighbouring tag segments, which we refer to 
as Text with Neighbouring Adjacent Tag Segments (TNATS). The 
neighbouring adjacent tag segments of a text segment are defined 
as the list of tag segments that are located immediately before and 
after the text segment until another text segment is reached. The 
neighbouring tag segments of a text segment describe how the 
text segment is structured and its relation to the nearest text 
segments. Assume that a document contains n segments, a text 
segment, txs, is defined as: txs = (txi, tg-lstj, tg-lstk), where txi is
# Para 269 1
the textual content of the ith text segment, 1 &lt;_ i &lt;_ n; tg-lstj
# Para 270 2
represents p tag segments located before txi and tg-lstk represents 
q tag segments located after txi until another text segment is
# Para 272 1
reached. tg-lstj = (tg1, ..., tgp), 1 &lt;_ j &lt;_ p and tg-lstk = (tg1, ..., tgq),
# Para 273 1
1 &lt;_ k &lt;_ q.
# Para 274 1
Figure 3. A template-generated document from CHID.
# Para 275 12
Figure 3 shows a template-generated document retrieved from the 
CHID database. The source code for this document is given in 
Figure 4. For example, text segment, ‘1. Equipos Mas Seguros: 
Si Te Inyectas Drogas.’, can be identified by the text (i.e., ‘1. 
Equipos Mas Seguros: Si Te Inyectas Drogas.’) and its 
neighbouring tag segments. These include the list of tags located 
before the text (i.e., &lt;/TITLE&gt;, &lt;/HEAD&gt;, &lt;BODY&gt;, &lt;HR&gt;, 
&lt;H3&gt;, &lt;B&gt; and &lt;I&gt;) and the neighbouring tags located after the 
text (i.e., &lt;/I&gt;, &lt;/B&gt;, &lt;/H3&gt;, &lt;I&gt; and &lt;B&gt;). Thus, this segment is 
then represented as (‘1. Equipos Mas Seguros: Si Te Inyectas 
Drogas.’, (&lt;/TITLE&gt;, &lt;/HEAD&gt;, &lt;BODY&gt;, &lt;HR&gt;, &lt;H3&gt;, &lt;B&gt; 
,&lt;I&gt;), (&lt;/I&gt;, &lt;/B&gt;, &lt;/H3&gt;, &lt;I&gt;, &lt;B&gt;)). Figure 5 shows the content
# Para 287 1
3
# Para 288 3
representation of the CHID document (given in Figure 3) 
generated based on TNATS. Given a sampled document, d, with n 
text segments, the content of d is then represented as: Content(d)
# Para 291 1
= {txs1, ..., txsn}, where txsi represents a text segment, 1 &lt;_ i &lt;_ n.
# Para 292 1
Figure 4. The source code for the CHID document.
# Para 293 2
Figure 5. The content representation of the CHID document
using TNATS.
# Para 295 1
3.2.2 Detect Templates
# Para 296 6
In the domain of Hidden Web databases, documents are often 
presented to users through one or more templates. Templates are 
typically employed in order to describe document contents or to 
assist users in navigation. For example, information contained in 
the document (as shown in Figure 3) can be classified into the two 
following categories:
# Para 302 7
(i) Template-Generated Information. This includes information 
such as navigation panels, search interfaces and 
advertisements. In addition, information may be given to 
describe the content of a document. Such information is 
irrelevant to a user’s query. For example, navigation links 
(such as ‘Next Doc’ and ‘Last Doc’) and headings (such 
‘Subfile’ and ‘Format’) are found in the document.
# Para 309 3
(ii) Query-Related Information. This information is retrieved in 
response to a user’s query, i.e., ‘1. Equipos Mas Seguros: 
Si Te Inyectas Drogas. ...’.
# Para 312 2
The 2PS technique detects Web page templates employed by 
databases to generate documents in order to extract information
# Para 314 3
that is relevant to queries. Figure 6 describes the algorithm that 
detects information contained in Web page templates from n 
sampled documents. di represents a sampled document from the
# Para 317 1
database D, di, e D, 1 &lt;_ i &lt;_ n. Content(di) denotes the content
# Para 318 1
representation of di.
# Para 319 2
Algorithm DetectTemplate 
For i = 1 to n
# Para 321 1
If T = 0
# Para 322 1
If S = 0
# Para 323 1
S = di
# Para 324 1
Else if S ;t� 0
# Para 325 1
While l &lt;= s AND T = 0
# Para 326 2
Compare (Content(di),Content(dl)) 
If Content(di) = Content(dl)
# Para 328 1
wptk = Content(di) n Content(dl),
# Para 329 1
Store wptk, T = wptk
# Para 330 1
Delete (Content(di) n Content(dl)) from
# Para 331 2
Content(di), Content(dl) 
Gk = di, Gk = dl
# Para 333 1
Delete dl from S
# Para 334 1
End if
# Para 335 1
End while
# Para 336 1
If T = 0
# Para 337 2
S = di 
End if
# Para 339 1
End if
# Para 340 1
Else if T ;t� 0
# Para 341 1
While k &lt;= r AND di o Gk
# Para 342 2
Compare (Content(wptk), Content(di)) 
If Content(wptk) = Content(di)
# Para 344 1
Delete (Content(wptk) n Content(di)) from
# Para 345 1
Content(di)
# Para 346 1
Gk = di
# Para 347 2
End if 
End while
# Para 349 1
If S ;t� 0 AND di o Gk
# Para 350 1
While l &lt;= s AND di o Gk
# Para 351 2
Compare (Content(di),Content(dl)) 
If Content(di) = Content(dl)
# Para 353 1
wptk = Content(di) n Content(dl)
# Para 354 1
Store wptk, T = wptk
# Para 355 1
Delete (Content(di) n Content(dl)) from
# Para 356 2
Content(di), Content(dl) 
Gk = di, Gk = dl
# Para 358 1
Delete dl from S
# Para 359 1
End if
# Para 360 1
End while
# Para 361 1
End if
# Para 362 1
If di o Gk
# Para 363 2
S = di 
End if
# Para 365 1
End if
# Para 366 1
End for
# Para 367 2
Figure 6. The algorithm for detecting and eliminating the
information contained in Web page templates.
# Para 369 1
...
# Para 370 2
&lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;CHID Document 
&lt;/TITLE&gt;&lt;/HEAD&gt;
# Para 372 1
&lt;BODY&gt;
# Para 373 2
&lt;HR&gt;&lt;H3&gt;&lt;B&gt;&lt;I&gt; 1. Equipos Mas Seguros: Si Te Inyectas 
Drogas.
# Para 375 1
&lt;/I&gt;&lt;/B&gt;&lt;/H3&gt;
# Para 376 1
&lt;I&gt;&lt;B&gt;Subfile: &lt;/B&gt;&lt;/I&gt;
# Para 377 1
AIDS Education&lt;BR&gt;
# Para 378 1
&lt;I&gt;&lt;B&gt;Format (FM): &lt;/B&gt;&lt;/I&gt;
# Para 379 1
08 - Brochure.
# Para 380 1
&lt;BR&gt;
# Para 381 1
...
# Para 382 1
...
# Para 383 3
‘CHID Document’, (&lt;HTML&gt;, &lt;HEAD&gt;, &lt;TITLE&gt;), 
(&lt;/TITLE&gt;, &lt;/HEAD&gt;, &lt;BODY&gt;, &lt;HR&gt;, &lt;H3&gt;, &lt;B&gt;, 
&lt;I&gt;);
# Para 386 3
‘1. Equipos Mas Seguros: Si Te Inyectas Drogas.’, 
(&lt;/TITLE&gt;, &lt;/HEAD&gt;, &lt;BODY&gt;, &lt;HR&gt;, &lt;H3&gt;, &lt;B&gt;, 
&lt;I&gt;), (&lt;/I&gt;, &lt;/B&gt;, &lt;/H3&gt;, &lt;I&gt;, &lt;B&gt;);
# Para 389 2
‘Subfile:’, (&lt;/I&gt;, &lt;/B&gt;, &lt;/H3&gt;, &lt;I&gt;, &lt;B&gt;), (&lt;/B&gt;, &lt;/I&gt;); 
‘AIDS Education’, (&lt;/B&gt;, &lt;/I&gt;), (&lt;BR&gt;, &lt;I&gt;, &lt;B&gt;);
# Para 391 1
‘Format (FM):’, (&lt;BR&gt;, &lt;I&gt;, &lt;B&gt;), (&lt;/B&gt;, &lt;/I&gt;);
# Para 392 1
...
# Para 393 1
4
# Para 394 3
Similar to the representation for the contents of sampled 
documents, the content of a Web page template, wpt, is 
represented as Content(wpt) = {txs1, ..., txsq}, where q is the
# Para 397 1
number of text segments, txsj, 1 ≤ j ≤ q. T represents a set of
# Para 398 1
templates detected. T = {wpt1, ..., wptr}, where r is the distinct
# Para 399 1
number of templates, wptk, 1 ≤ k ≤ r. Gk represents a group of
# Para 400 3
documents generated from wptk. Furthermore, S represents the 
sampled documents from which no templates have yet been 
detected. Thus, S = {d1, ..., ds}, where s is the number of
# Para 403 1
temporarily stored document, dl, 1 ≤ l ≤ s.
# Para 404 9
The process of detecting templates is executed until all sampled 
documents are analysed. This results in the identification of one 
or more templates. For each template, two or more documents are 
assigned to a group associated with the template from which the 
documents are generated. Each document contains text segments 
that are not found in their respective template. These text 
segments are partially related to their queries. In addition to a set 
of templates, the content representations of zero or more 
documents in which no matched patterns are found are stored.
# Para 413 1
3.2.3 Extract Query-Related Information
# Para 414 4
This process analyses a group of documents associated with each 
template from which documents are generated. It further identifies 
any repeated patterns from the remaining text segments of the 
documents in order to extract query-related information.
# Para 418 6
We compute cosine similarity [14] given in (1) to determine the 
similarities between the text segments of different documents that 
are associated the template where the documents are generated. 
The textual content of each text segment is represented as a vector 
of terms with weights. The weight of a term is obtained by its 
occurrence in the segment.
# Para 424 2
from the document content (given in Figure 4) as a result of 
eliminating information contained in the Web page template.
# Para 426 1
3.2.4 Generate Content Summary
# Para 427 3
Frequencies are computed for the terms extracted from randomly 
sampled documents. These summarise the information content of 
a database, which we refer to as Content Summary.
# Para 430 1
Algorithm ExtractQueryInfo
# Para 431 1
For each (da ∈ Gk)
# Para 432 1
For each (db ∈ Gk), da ≠ db
# Para 433 2
Compare (Content(da),Content(db)) 
If Content(da) = Content(db)
# Para 435 1
Delete (Content(da) ∩ Content(db)) from
# Para 436 2
Content(da), Content(db) 
End if
# Para 438 1
End for
# Para 439 1
End for
# Para 440 1
For each (di ∈ Gk)
# Para 441 2
Extract txm of txsm from Content(di) 
End for
# Para 443 1
For each (dl ∈ S)
# Para 444 2
Extract txn of txsn from Content(dl) 
End for
# Para 446 2
Figure 7. The algorithm for extracting query-related
information from template-generated documents.
# Para 448 2
1. Equipos Mas Seguros: Si Te Inyectas Drogas. 
AIDS Education
# Para 450 1
...
# Para 451 1
t
# Para 452 1
(	,	)	(	)	(	) 2	(	)
# Para 453 1
txs txs	tw tw	tw	tw
# Para 454 1
i	j	ik	jk	ik
# Para 455 1
=	∗	∗
# Para 456 1
∑	∑	∑ jk
# Para 457 1
k=1	k=1
# Para 458 1
(1)	Figure 8. The query-related information extracted from the
# Para 459 1
CHID document.
# Para 460 1
COSINE
# Para 461 1
t
# Para 462 1
t
# Para 463 1
1
# Para 464 1
=
# Para 465 1
k
# Para 466 1
2
# Para 467 1
.
# Para 468 6
where txsi and txsj represent two text segments in a document; twik 
is the weight of term k in txsi, and twjk is the weight of term k in 
txsj . This is only applied to text segments with identical adjacent 
tag segments. Two segments are considered to be similar if their 
similarity exceeds a threshold value. The threshold value is 
determined experimentally.
# Para 474 2
The algorithm that extracts information relevant to queries is 
illustrated in Figure 7. da and db represent the sampled documents
# Para 476 1
from the database, D, da, db ∈ Gk, where Gk denotes a group of
# Para 477 2
documents associated with the template, wptk, from which the 
documents are generated. txm represents the textual content of a
# Para 479 2
text segment, txsm, contained in di, di ∈ Gk. txn represents the 
textual content of a text segment, txsn, contained in dl, dl ∈ S. S
# Para 481 2
represents the sampled documents from which no templates are 
detected.
# Para 483 4
The results of the above algorithm extract text segments with 
different tag structures. It also extracts text segments that have 
identical adjacent tag structures but are significantly different in 
their textual contents. Figure 8 shows the information extracted
# Para 487 3
Previous experiments in [2] demonstrate that a number of 
randomly sampled documents (i.e., 300 documents) sufficiently 
represent the information content of a database.
# Para 490 8
In the domain of Hidden Web databases, the inverse document 
frequency (idf), used in traditional information retrieval, is not 
applicable, since the total number of documents in a database is 
often unknown. Therefore, document frequency (df), collection 
term frequency (ctf) and average term frequency (avg_tf) initially 
used in [2] are applied in this paper. We consider the following 
frequencies to compute the content summary of a Hidden Web 
database.
# Para 498 1
•	Document frequency (df): the number of documents in the
# Para 499 2
collection of documents sampled that contain term t, 
where d is the document and f is the frequency
# Para 501 1
•	Collection term frequency (ctf): the occurrence of a term
# Para 502 2
in the collection of documents sampled, where c is the 
collection, t is the term and f is the frequency
# Para 504 1
•	Average term frequency (avg_tf): the average frequency
# Para 505 2
of a term obtained from dividing collection term 
frequency by document frequency (i.e., avg_tf = ctf / df)
# Para 507 1
5
# Para 508 1
Table 1. 3 Hidden Web databases used in the experiments
# Para 509 4
Database	URL	Subject	Content	Template
Help Site	www.help-site.com	Computer manuals	Homogeneous	Multiple templates
CHID	www.chid.nih.gov	Healthcare articles	Homogeneous	Single template
Wired News	www.wired.com	General news articles	Heterogeneous	Single template
# Para 513 13
The content summary of a document database is defined as 
follows. Assume that a Hidden Web database, D, is sampled with 
N documents. Each sampled document, d, is represented as a 
vector of terms and their associated weights [14]. Thus d = (w1, 
..., wm), where wi is the weight of term ti, and m is the number of 
distinct terms in d e D, 1 &lt;_ i ≤ m. Each wi is computed using term 
frequency metric, avg_tf (i. e., wi = ctfi/dfi). The content summary 
is then denoted as CS(D), which is generated from the vectors of 
sampled documents. Assume that n is the number of distinct terms 
in all sampled documents. CS(D) is, therefore, expressed as a 
vector of terms: CS(D)= {w1, ..., wn}, where wi is computed by 
adding the weights of ti in the documents sampled from D and 
dividing the sum by the number of sampled documents that
# Para 526 1
contain ti, 1 &lt;_ i ≤ n.
# Para 527 1
4. EXPERIMENTAL RESULTS
# Para 528 8
This section reports on a number of experiments conducted to 
assess the effectiveness of the 2PS technique in terms of: (i) 
detecting Web page templates, and (ii) extracting relevant 
information from the documents of a Hidden Web databases 
through sampling. The experimental results are compared with 
those from query-based sampling (abbreviated as QS). We 
compare 2PS with QS as it is a well-established technique and has 
also been widely adopted by other relevant studies [5, 10, 11, 15].
# Para 536 10
Experiments are carried out on three real-world Hidden Web 
document databases including Help Site, CHID and Wired News, 
which provide information about user manuals, healthcare 
archives and news articles, respectively. Table 1 summarises 
these databases in terms of their subjects, contents and templates 
employed. For instance, Help Site and CHID contain documents 
relating to subjects on computing and healthcare, respectively. 
Their information contents are homogeneous in nature. By 
contrast, Wired News contains articles that relate to different 
subjects of interest.
# Para 546 5
Where the number of templates is concerned, CHID and Wired 
News generate documents from one Web page template. Help 
Site maintains a collection of documents produced by other 
information sources. Subsequently, different Web page templates 
are found in Help Site sampled documents.
# Para 551 11
The experiment conducted using QS initiates the first query to a 
database with a frequently used term to obtain a set of sampled 
documents. Subsequent query terms are randomly selected from 
those contained in the sampled documents. It extracts terms 
(including terms contained in Web page templates) and updates 
the frequencies after each document is sampled. By contrast, 2PS 
initiates the sampling process with a term contained in the search 
interface pages of a database. In addition, 2PS analyses the 
sampled documents in the second phase in order to extract query- 
related information, from which terms and frequencies are 
generated.
# Para 562 7
Experimental results in [2] conclude that QS obtains 
approximately 80% of terms from a database, when 300 
documents are sampled and top 4 documents are retrieved for 
each query. These two parameters are used to obtain results for 
our experiments in which terms and frequencies are generated for 
QS and 2PS after 300 documents have been sampled. The results 
generated from QS provide the baseline for the experiments.
# Para 569 8
Three sets of samples are obtained for each database and 300 
documents are retrieved for each sample. First, we manually 
examine each set of sampled documents to obtain the number of 
Web page templates used to generate the documents. This is then 
compared with the number of templates detected by 2PS. The 
detection of Web page templates from the sampled documents is 
important as this determines whether irrelevant information is 
effectively eliminated.
# Para 577 9
Next, we compare the number of relevant terms (from top 50 
terms) retrieved using 2PS with the number obtained by QS. 
Terms are ranked according to their ctf frequencies to determine 
their relevancy to the queries. This frequency represents the 
occurrences of a term contained in the sampled documents. Ctf 
frequencies are used to demonstrate the effectiveness of 
extracting query-related information from sampled documents 
since the terms extracted from Web page templates are often 
ranked with high ctf frequencies.
# Para 586 2
Table 2. The number of templates employed by databases and
the number detected by 2PS
# Para 588 11
Databases		Number of templates	
		Employed	Detected
Help Site	Sample 1	17	15
	Sample 2	17	16
	Sample 3	19	17
CHID	Sample 1	1	1
	Sample 2	1	1
	Sample 3	1	1
Wired News	Sample 1	1	1
	Sample 2	1	1
	Sample 3	1	1
# Para 599 9
Experimental results for QS and 2PS are summarised as follows. 
Firstly, Table 2 gives the number of Web page templates 
employed by the databases and the number detected by 2PS. It 
shows that 2PS effectively identifies the number of templates 
found in the sampled documents. However, a small number of 
templates are not detected from Help Site. For instance, 2PS does 
not detect two of the templates from the first set of sampled 
documents, since the two templates are very similar in terms of 
content and structure.
# Para 608 1
6
# Para 609 10
Table 3 summarises the number of relevant terms (from top 50 
terms ranked according to their ctf frequencies) obtained for the 
three databases. These terms are retrieved using 2PS and QS. We 
determine the relevancy of a term by examining whether the term 
is found in Web page templates. Table 3 gives the number of 
retrieved terms that do not appear in Web page templates. The 
results show that 2PS obtains more relevant terms. For instance, 
in the first set of documents sampled from CHID using 2PS, the 
number of relevant terms retrieved is 47. By comparison, the 
number of terms obtained for QS is 20.
# Para 619 11
The results generated from CHID and Wired News demonstrate 
that 2PS retrieves more relevant terms, as a large number of terms 
contained in the templates have been successfully eliminated from 
the top 50 terms. However, the elimination of template terms is 
less noticeable for Help Site. Our observation is that template 
terms attain high frequencies since the CHID and Wired News 
databases generate documents using a single Web page template. 
By comparison, a larger number of Web page templates are found 
in the documents sampled from Help Site. As a result, terms 
contained in the templates do not attain high frequencies as those 
found in the templates employed by CHID and Wired News.
# Para 630 10
Table 4 and 5 show the results of the top 50 terms ranked 
according to their ctf frequencies retrieved from the first set of 
sampled documents of the CHID database. Table 4 shows the top 
50 terms retrieved for QS whereby terms contained in Web page 
templates are not excluded. As a result, a number of terms (such 
as ‘author’, ‘language’ and ‘format’) have attained much higher 
frequencies. By contrast, Table 5 lists the top 50 terms retrieved 
using 2PS. Our technique eliminates terms (such as ‘author’ and 
‘format’) and obtains terms (such as ‘treatment’, ‘disease’ and 
‘immunodeficiency’) in the higher rank.
# Para 640 2
Table 3. The number of relevant terms retrieved (from top 50
terms) according to ctf frequencies
# Para 642 11
Databases		Number of relevant terms	
		QS	2PS
Help Site	Sample 1	46	48
	Sample 2	47	48
	Sample 3	46	48
CHID	Sample 1	20	47
	Sample 2	19	47
	Sample 3	20	47
Wired News	Sample 1	14	42
	Sample 2	10	43
	Sample 3	11	39
# Para 653 1
5. CONCLUSION
# Para 654 11
This paper presents a sampling and extraction technique, 2PS, 
which utilises information that is contained in the search interface 
pages and documents of a database in the sampling process. This 
technique extracts information relevant to queries from the 
sampled documents in order to generate terms and frequencies 
with improved accuracy. Experimental results demonstrate that 
our technique effectively eliminates information contained in 
Web page templates, thus attaining terms and frequencies that are 
of a higher degree of relevancy. This can also enhance the 
effectiveness of categorisation in which such statistics are used to 
represent the information contents of underlying databases.
# Para 665 4
We obtain promising results by applying 2PS in the experiments 
on three databases that differ in nature. However, experiments on 
a larger number of Hidden Web databases are required in order to 
further assess the effectiveness of the proposed technique.
# Para 669 1
Table 4. Top 50 terms and frequencies ranked according to ctf generated from CHID when QS is applied
# Para 670 18
Rank	Term	Rank	Term	Rank	Term
1	hiv	18	document	35	lg
2	aids	19	disease	36	ve
3	information	20	published	37	yr
4	health	21	physical	38	ac
5	prevention	22	subfile	39	corporate
6	education	23	audience	40	mj
7	tb	24	update	41	description
8	accession	25	verification	42	www
9	number	26	major	43	cn
10	author	27	pamphlet	44	pd
11	persons	28	chid	45	english
12	language	29	human	46	national
13	sheet	30	date	47	public
14	format	31	abstract	48	immunodeficiency
15	treatment	32	code	49	virus
16	descriptors	33	ab	50	org
17	availability	34	fm		
# Para 688 1
7
# Para 689 1
Table 5. Top 50 terms and frequencies ranked according to ctf generated from CHID when 2PS is applied
# Para 690 18
Rank	Term	Rank	Term	Rank	Term
1	hiv	18	education	35	testing
2	aids	19	virus	36	programs
3	information	20	org	37	services
4	health	21	notes	38	clinical
5	prevention	22	nt	39	people
6	tb	23	cdc	40	hepatitis
7	persons	24	service	41	community
8	sheet	25	box	42	world
9	treatment	26	research	43	listed
10	disease	27	department	44	professionals
11	human	28	positive	45	training
12	pamphlet	29	tuberculosis	46	diseases
13	www	30	control	47	accession
14	http	31	drug	48	network
15	national	32	discusses	49	general
16	public	33	ill	50	std
17	immunodeficiency	34	organizations		
# Para 708 1
6. REFERENCES
# Para 709 3
[1] Arasu, A. and Garcia-Molina, H. Extracting Structured Data 
from Web Pages. In Proceedings of the 2003 ACM SIGMOD 
International Conference on Management, 2003, 337-348.
# Para 712 3
[2] Callan, J. and Connell, M. Query-Based Sampling of Text 
Databases. ACM Transactions on Information Systems 
(TOIS), Vol. 19, No. 2, 2001, 97-130.
# Para 715 3
[3] Caverlee, J., Buttler, D. and Liu, L. Discovering Objects in 
Dynamically-Generated Web Pages. Technical report, 
Georgia Institute of Technology, 2003.
# Para 718 4
[4] Crescenzi, V., Mecca, G. and Merialdo, P. ROADRUNNER: 
Towards Automatic Data Extraction from Large Web Sites, 
In Proceedings of the 27th International Conference on Very 
Large Data Bases (VLDB), 2001, 109-118.
# Para 722 4
[5] Gravano, L., Ipeirotis, P. G. and Sahami, M. QProber: A 
System for Automatic Classification of Hidden-Web 
Databases. ACM Transactions on Information Systems 
(TOIS), Vol. 21, No. 1, 2003.
# Para 726 3
[6] Heß, M. and Drobnik, O. Clustering Specialised Web- 
databases by Exploiting Hyperlinks. In Proceedings of the 
Second Asian Digital Library Conference, 1999.
# Para 729 4
[7] Hedley, Y.L., Younas, M., James, A. and Sanderson M. 
Query-Related Data Extraction of Hidden Web Documents. 
In Proceedings of the 27th Annual International ACM SIGIR 
Conference, 2004, 558-559.
# Para 733 2
[8] Lage, J. P., da Silva, A. S., Golgher, P. B. and Laender, A. 
H. F. Automatic Generation of Agents for Collecting Hidden
# Para 735 2
Web Pages for Data Extraction. Data &amp; Knowledge 
Engineering, Vol. 49, No. 2, 2004, 177-196.
# Para 737 4
[9] Liddle, S.W., Yau, S.H. and Embley, D. W. On the 
Automatic Extraction of Data from the Hidden Web. In 
Proceedings of the 20th International Conference on 
Conceptual Modeling, (ER) Workshops, 2001, 212-226.
# Para 741 4
[ 10] Lin, K.I. and Chen, H. Automatic Information Discovery 
from the Invisible Web. International Conference on 
Information Technology: Coding and Computing (ITCC), 
2002, 332-337.
# Para 745 2
[ 11 ] Meng, W., Wang, W., Sun, H. and Yu, C. Concept
Hierarchy Based Text Database Categorization.
# Para 747 2
International Journal on Knowledge and Information 
Systems, Vol. 4, No. 2, 2002, 132-150.
# Para 749 3
[ 12] Raghavan, S. and Garcia-Molina, H. Crawling the Hidden 
Web. In Proceedings of the 27th International Conference on 
Very Large Databases (VLDB), 2001, 129-138.
# Para 752 3
[ 13] Rahardjo, B. and Yap, R. Automatic Information Extraction 
from Web Pages, In Proceedings of the 24th Annual 
International ACM SIGIR Conference, 2001, 430-431.
# Para 755 2
[ 14] Salton, G. and McGill, M. Introduction to Modern 
Information Retrieval. New York, McCraw-Hill, 1983.
# Para 757 4
[ 15] Sugiura, A. and Etzioni, O. Query Routing for Web Search 
Engines: Architecture and Experiments. In Proceedings of 
the 9th International World Wide Web Conference: The 
Web: The Next Generation, 2000, 417-430.
# Para 761 1
8
