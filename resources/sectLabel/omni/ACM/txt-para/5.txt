# Para 0 1
A Dependability Perspective on Emerging Technologies
# Para 1 1
Lucian Prodan	Mihai Udrescu	Mircea Vladutiu
# Para 2 4
Advanced Computing Systems and Architectures (ACSA) Laboratory,
Computer Science and Engineering Department, “Politehnica” University of Timisoara,
2 V.Parvan Blvd, 300223 Timisoara, Romania
www.acsa.upt.ro
# Para 6 1
+40-722-664779	+40-723-154989	+40-256-403258
# Para 7 1
lprodan@cs.upt.ro	mudrescu@cs.upt.ro	mvlad@cs.upt.ro
# Para 8 1
ABSTRACT
# Para 9 16
Emerging technologies are set to provide further provisions for 
computing in times when the limits of current technology of 
microelectronics become an ever closer presence. A technology 
roadmap document lists biologically-inspired computing and 
quantum computing as two emerging technology vectors for novel 
computing architectures [43]. But the potential benefits that will 
come from entering the nanoelectronics era and from exploring 
novel nanotechnologies are foreseen to come at the cost of 
increased sensitivity to influences from the surrounding 
environment. This paper elaborates on a dependability perspective 
over these two emerging technology vectors from a designer’s 
standpoint. Maintaining or increasing the dependability of 
unconventional computational processes is discussed in two 
different contexts: one of a bio-inspired computing architecture 
(the Embryonics project) and another of a quantum computational 
architecture (the QUERIST project).
# Para 25 1
Categories and Subject Descriptors
# Para 26 2
B.8.1 [Performance and Reliability]: Reliability, Testing, and 
Fault-Tolerance.
# Para 28 2
C.4 [Performance of Systems]: Fault-Tolerance, Reliability, 
Availability, and Serviceability.
# Para 30 1
General Terms
# Para 31 1
Design, Reliability, Theory.
# Para 32 1
Keywords
# Para 33 3
Dependability, emerging technologies, evolvable hardware, bio-
inspired computing, bio-inspired digital design, Embryonics, 
reliability, quantum computing, fault-tolerance assessment.
# Para 36 1
1. INTRODUCTION
# Para 37 3
High-end computing has reached nearly every corner of our 
present day life, in a variety of forms taylored to accommodate 
either general purpose or specialized applications. Computers
# Para 40 6
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee.
# Para 46 1
CF’06, May 3–5, 2006, Ischia, Italy.
# Para 47 1
Copyright 2006 ACM 1-59593-302-6/06/0005...$5.00. 
# Para 48 3
may be considerred as fine exponents of the present days’ 
technological wave – if not their finest, they certainly do count as 
solid, indispensable support for the finest.
# Para 51 7
From the very beginning of the computing advent, the main target 
was squeezing out any additional performance. The inception 
period was not always trouble-free, accurate computation results 
being required at an ever faster pace on a road that has become 
manifold: some applications do require computational speed as a 
top priority; others are set for the highest possible dependability, 
while still delivering sufficient performance levels.
# Para 58 8
Several definitions for dependability have been proposed: “the 
ability of a system to avoid service failures that are more frequent 
or more severe than is acceptable” [2], or “the property of a 
computer system such that reliance can justifiably be placed on 
the service it delivers” [9][45]. Dependability is therefore a 
synthetic term specifying a qualitative system descriptor that can 
generally be quantified through a list of attributes including 
reliability, fault tolerance, availability, and others.
# Para 66 10
In real world, a dependable system would have to operate 
normally over extended periods of time before experiencing any 
fail (reliability, availability) and to recover quickly from errors 
(fault tolerance, self-test and self-repair). The term “acceptable” 
has an essential meaning within the dependability’s definition, 
setting the upper limits of the damages that can be supported by 
the system while still remaining functional or computationally 
accurate. A dependability analysis should take into consideration 
if not quantitative figures for the acceptable damage limit, at least 
a qualitative parameter representation for its attributes.
# Para 76 5
Dependable systems are therefore crucial for applications that 
prohibit or limit human interventions, such as long-term exposure 
to aggressive (or even hostile) environments. The best examples 
are long term operating machines as required by managing deep-
underwater/nuclear activities and outer space exploration.
# Para 81 2
There are three main concerns that should be posed through a 
system’s design in order to achieve high dependability [42]:
# Para 83 5
1. Specifying the dependability requirements: selecting the 
dependability requirements that have to be pursued in 
building the computing system, based on known or assumed 
goals for the part of the world that is directly affected by the 
computing system;
# Para 88 3
2. Designing and implementing the computing system so as to 
achieve the dependability required. However, this step is hard 
to implement since the system reliability cannot be satisfied
# Para 91 1
187
# Para 92 3
simply from careful design. Some techniques can be used to 
help to achieve this goal, such as using fault injection to 
evaluate the design process.
# Para 95 2
3. Validating a system: gaining confidence that a certain 
dependability requirement/goal has been attained.
# Para 97 4
This paper will address these main concerns through an attempt to 
provide an in-depth view over modern computing directions and 
paradigms, which we consider to be representative for the efforts 
involved in improving overall dependability.
# Para 101 1
1.1 Motivations
# Para 102 12
We have listed some of the applications of dependable computing 
systems as linked to activities that take place in special 
environments, such as deep underwater or outer space. At a very 
first sight, these applications would appear specific enough to not 
encourage a specific design for dependability approach in 
computing. However, evidence suggest this is hardly the case; on 
the contrary, it is difficult to imagine a domain left unconquered 
by computer systems during times when industrial, transport, 
financial services and others do rely heavily on accurate computer 
operation at any given moment. If computer innacuracies could be 
more easily overlooked at home, professional environments 
cannot accept such missbehaviors.
# Para 114 14
Yet the recent history of computing provides evidence that 
dependability is not a sine qua non feature. During their life 
cycle, electronic devices constantly suffer a number of influences 
that manifest predominantly over transient regimes, which in turn 
introduce a variety of errors unified in the literature under the 
name of transient faults, soft errors or single event upsets (SEUs). 
The rate electronic devices are affected with is known under the 
term of soft error rate or simply SER and is measured in fails per 
unit time. Because it relies on transient phenomena due to 
changing states and logical values, digital electronics makes up 
for a special category that is also affected by soft errors. No 
matter the name they are referred under, these errors affect the 
computing processes and are due to electromagnetic noise and/or 
external radiations rather than design or manufacturing flaws [28].
# Para 128 8
One cause at the origin of soft fails affecting digital devices is 
known to be due to radioactive decay processes. Radioactive 
isotopes, widely used for a range of purposes, might contaminate 
semiconductor materials leading to soft errors; evidence is 
available throughout the literature, both by empirical observations 
and experimental results [20]. Consequently, cosmic rays, 
containing a broad range of energized atomic/subatomic particles 
may lead to the appearance of soft fails.
# Para 136 13
Computers therefore are susceptive to soft errors, an issue that 
will potentially become essential with the advent of emerging 
technologies. As acknowledged by the International Technology 
Roadmap for Semiconductors (ITRS), issued at the end of 2004 
[43], the microelectronics industry faces a challenging task in 
going to and beyond 45nm scale in order to address “beyond 
CMOS” applications. Scaling down the technology will enable an 
extremely large number of devices to be integrated onto the same 
chip. However, the great challenge will be to ensure the new 
devices will be operational at this scale [6], since they will exhibit 
a sensitive behavior to soft fails. In order to address the negative 
effects brought by technology scaling, it is to be expected that 
significant control resources will need to be implemented [3].
# Para 149 10
Another challenging aspect concerning emerging technologies is 
to match the newly developed device technologies with new 
system architectures, a synergistic/collaborative development of 
the two being seen as likely to be very rewarding. The potential of 
biologically-inspired and quantum computing architectures is 
acknowledged by the ITRS report on emerging technologies [43] 
(see Figure 1). This paper will investigate the relevance of soft 
fails and attempt to provide means of harnessing their negative 
effects on modern computing in the context of biologically- 
inspired and quantum computing architectures.
# Para 159 3
Figure 1: Bio-inspired and quantum computing are
acknowledged as architectural technology vectors in emerging
technologies [43]
# Para 162 1
1.2 Paper Outline
# Para 163 8
This paper is structured as follows. Section 2 will address the first 
main concern, that is, specifying and selecting dependability 
requirements that will have to be pursued when building a 
computational platform. Parameters that describe and quantify 
dependability attributes, such as reliability, will be introduced, 
with a highlight on their accepted models and their issues. A 
particular consideration will be given to the failure rate parameter, 
which is the basis of all reliability analyses.
# Para 171 18
Section 3 will approach some of the means for design for 
dependability; it will therefore elaborate upon two emerging 
technology vectors, as seen by the ITRS report [43], which define 
two novel architectures, namely biologically-inspired (or bio-
inspired) and quantum computing. We will introduce two projects 
and their corresponding architectures, called Embryonics (as a 
biologically-inspired computing platform) and QUERIST (as a 
quantum computing platform designed to allow and study error 
injection). These two architectures are representative for the 
coming age of nano-computing, where computational processes 
take place as encoded at the very inner core level of matter, be it 
semiconductor material (for nanoelectronics, targetted here by the 
Embryonics project) or atomic scale dynamics (for quantum 
computing, targetted here by the QUERIST project). This section 
will then introduce dependability aspects within bio-inspired 
computing (the Embryonics project being investigated in 
SubSection 3.1) and within quantum computing (the QUERIST 
project being investigated in SubSection 3.2).
# Para 189 3
Finally, Section 4 will present the conclusions and prospects for 
designing emerging technology dependable computing systems, 
as we see them.
# Para 192 1
188
# Para 193 1
2. DEPENDABILITY ATTRIBUTES
# Para 194 11
An important dependability attribute for any given system lies in 
its capacity to operate reliably for a given time interval, knowing 
that normal operation was delivered at initial time [8]. Reliability 
functions are modelled as exponential functions of parameter A, 
which is the failure rate. The reliability of a system is the 
consequence of the reliability of all of its subsystems. The 
heterogeneity of the system leads to a difficult quantitative 
assessment of its overall reliability; moreover, estimating the 
reliability functions is further made difficult because formal 
rigour is not commercially available, this being kept under 
military mandate [44].
# Para 205 6
The failure rate for a given system can be modelled as a function 
of the failure rates of its individual subsystems, suggestions being 
present in the MIL-HDBC-217 document, which is publicly 
available [44]. However, this document has been strongly 
criticized for its failure rate estimations based on the Arrhenius 
model, which relates the failure rate to the operating temperature:
# Para 211 6
where K is a constant, KB is Boltzmann’s constant, T is the 
absolute temperature and E is the “activation energy” for the 
process [18]. Quantitative values for failure rates show significant 
differences between those predicted using MIL-HDBC-217 and 
those from testing real devices (see Figure 2). There are two 
conclusions that can be drawn from this:
# Para 217 4
1. quantitative estimations for failure rate values are strongly 
dependant on the quality of information used; unfortunately, 
current reliable information about electronic devices is known 
to be lacking [44];
# Para 221 4
2. despite differences between predicted and real values, the 
MIL-HDBC-217 methodology can be useful for qualitative 
analyses in order to take decisions regarding sub-system parts 
that should benefit from improved designs.
# Para 225 2
Figure 2. Predicted vs real failure rates plotted against
temperature [18]
# Para 227 7
So far the failure rate of digital devices has been considerred as 
due to internal causes. However, this is not always the case, soft 
fails being equally present due to the aggressive influences of the 
external environment, which also have to be modelled [22]. The 
external envirnment features highly dynamic changes in its 
parameters, which will eventually affect the normal operation of 
digital devices that lack sufficient protection or ability to adapt.
# Para 234 6
Ideally, computing devices would behave in a consistent and 
accurate manner regardless of fluctuations in environmental 
parameters. This is either a consequence of soft error mitigation 
techniques or due to flexible hardware/software functionality that 
allow the system as a whole to adapt to environamental changes 
and tolerate induced faults.
# Para 240 14
While certain soft error mitigation techniques are available, the 
technology scaling towards nanoelectronics affects their 
efficiency by integrating a larger number of devices per chip 
(which requires a larger amount of redundant/control logic or 
other measures), which feature, at the same time, smaller 
dimensions (which renders an electronic device much more 
senzitive to the influence of stray energetic particles that reach it 
as part of cosmic rays). Both aspects are involved in the 
development of the two emerging technology vectors mentioned 
in SubSection 1.1, although having slightly different motivations: 
while the nature of the quantum environment prohibits precise 
computation in the absence of fault tolerance techniques, such 
techniques are targetted by bio-inspired computing as means of 
improving the dependability of a computing platform.
# Para 254 1
2.1 Bio-Inspired Computing
# Para 255 11
If living beings may be considered to fulfill computational tasks, 
then Nature is the ultimate engineer: each of the living beings 
exhibit solutions that were successfully tested and refined in such 
ways human engineers will never afford. One reason is time: the 
testing period coinciding with the very existence of life itself. 
Another reason is variety and complexity: Nature has found and 
adapted a variety of solutions to address complex survivability 
issues in a dynamically changing environment. No matter how 
Nature approached the process of evolution, engineering could 
perhaps benefit most from drawing inspiration from its 
mechanisms rather from trying to develop particular techniques.
# Para 266 10
Bio-inspired computing is not a new idea. John von Neumann was 
preoccupied to design a machine that could replicate itself and 
was quite interested in the study of how the behavior of the 
human brain could be implemented by a computer [13][14]. He 
also pioneered the field of dependable computing by studying the 
possibility of building reliable machines out of unreliable 
components [15]. Unfortunately, the dream of implementing his 
self-reproducing automata could not become true until the 1990s, 
when massively programmable logic opened the new era of 
reconfigurable computing.
# Para 276 10
But when trying to adapt nature’s mechanisms in digital devices, 
it becomes most evident that biological organisms are rightfully 
the most intricate structures known to man. They continuously 
demonstrate a highly complex behavior due to massive, parallel 
cooperation between huge numbers of relatively simple elements, 
the cells. And considering uncountable variety of living beings, 
with a life span up to several hundreds (for the animal regnum) or 
even thousands (for the vegetal regnum) of years, it seems nature 
is the closest spring of inspiration for designing dependable, fault 
tolerant systems.
# Para 286 2
Investigating the particularities of natural systems, a taxonomy of 
three categories of processes can be identified [32]:
# Para 288 3
1. Phylogenetic processes constitute the first level of 
organization of the living matter. They are concerned with the 
temporal evolution of the genetic heritage of all individuals,
# Para 291 1
E
# Para 292 1
λ= Ke KBT	(1)
# Para 293 1
−
# Para 294 1
189
# Para 295 5
therefore mastering the evolution of all species. The 
phylogenetic processes rely on mechanisms such as 
recombination and mutation, which are essentially 
nondeterministic; the error rate ensures here nature’s 
diversity.
# Para 300 8
2. Ontogenetic processes represent the second level of 
organization of the living matter. They are also concerned 
with the temporal evolution of the genetic heritage of, in this 
case, a single, multicellular individual, therefore mastering an 
individual’s development from the stage of a single cell, the 
zygote, through succesive cellular division and specialization, 
to the adult stage. These processes rely on deterministic 
mechanisms; any error at this level results in malformations.
# Para 308 4
3. Epigenetic processes represent the third level of organization 
of the living matter. They are concerned with the integration 
of interactions with the surrounding environment therefore 
resulting in what we call learning systems.
# Para 312 5
This taxonomy is important in that it provides a model called POE 
(from Phylogeny, Ontogeny and Epigenesis) that inspires the 
combination of processes in order to create novel bio-inspired 
hardware (see Figure 3). We believe this is also important from a 
dependability engineering perspective, for the following reasons:
# Para 317 16
1. Phylogenetic processes were assimilated by modern 
computing as evolutionary computation, including genetic 
algorithms and genetic programming. The essence of any 
genetic algorithm is the derivation of a solution space based 
on recombination, crossover and mutation processes that 
spawn a population of individuals, each encoding a possible 
solution. One may consider that each such step, with the 
exception of discovering the solution, is equivalent to a 
process of error injection, which in turn leads to wandering 
from the optimal solution (or class of solutions). However, 
genetic algorithms prove to be successful despite this error 
injection, the fitness function being responsible for the 
successful quantification of the significance of the “error”. 
Therefore genetic computation is intrinsicaly resilient to 
faults and errors, largely due to the fact that they are part of 
the very process that generates the solutions.
# Para 333 9
2. Ontogenetic processes have been implemented in digital 
hardware with modular and uniform architectures. Such an 
architecture enables the implementation of mechanisms 
similar to the cellular division and cellular differentiation that 
take place in living beings [31]. These mechanisms bring the 
advantage of distributed and hierarchical fault tolerance 
strategies: the uniformity of the architecture also makes any 
module to be universal, that is, to be able to take over the role 
of any other damaged module.
# Para 342 9
3. Epigenetic processes were assimilated by modern computing 
mainly as artificial neural networks (or ANNs) as inspired by 
the nervous system, and much less as inspired by the immune 
or endocrine systems from superior multicellular living 
beings. ANNs are known to have a generalization capacity, 
that is, to respond well even if the input patterns are not part 
of the patterns used during the learning phase. This means that 
ANNs possess a certain ability to tolerante faults, whether 
they manifest at the inputs or inside their intenal architecture.
# Para 351 11
With the advent of field programmable logic (of which the most 
salient representative are the FPGAs) it is now possible to change 
hardware functionality through software, thus allowing 
information to govern matter in digital electronics. This is not 
dissimilar to what happens in nature: information coded in DNA 
affects the development of an organism. A special kind of such 
digital devices that change dynamically their behavior are known 
as evolvable or adaptive hardware; they are bio-inspired 
computing systems whose behaviors may change according to 
computational targets, or, if harsh or unknown environments are 
to be explored, for the purpose of maximizing dependability.
# Para 362 1
Figure 3. The POE model of bio-inspired systems [32]
# Para 363 1
2.2 Quantum Computing
# Para 364 8
Error detection and correction techniques are vital in quantum 
computing due to the destructive effect of the environment, which 
therefore acts as an omnipresent error generator. Error detection 
and correction must provide a safe recovery process within 
quantum computing processes through keeping error propagation 
under control. Without such dependability techniques there could 
be no realistic prospect of an operational quantum computational 
device [19].
# Para 372 5
There are two main sources of errors: the first is due to the 
erroneous behavior of the quantum gate, producing the so-called 
processing errors; the second is due to the macroscopic 
environment that interacts with the quantum state, producing the 
storing and transmitting errors.
# Para 377 5
The consistency of any quantum computation process can be 
destroyed by innacuracies and errors if the error probability in the 
basic components (qubits, quantum gates) excedes an accuracy 
threshold. This is a critical aspect since the microscopic quantum 
states are prone to frequent errors.
# Para 382 15
The main error source is the decoherence effect [16]. The 
environment is constantly attempting to measure the sensitive 
quantum superposition state, a phenomenon that cannot be 
avoided technologically since it is not (yet) possible to isolate 
them perfectly. The superposition state will decay through 
measuring and will therefore become a projection of the state 
vector onto a basis vector (or eigenstate). The most insidious 
error, however, appears when decoherence affects the quantum 
amplitudes without destroying them; this is similar to small 
analog errors. Issues stated above are solved, on one hand, 
through intrinsic fault tolerance by technological implementation 
(topological interactions [1]) and, on the other hand, by error 
correcting techniques at the unitary (gate network) level. We will 
focus on the error detecting and correcting techniques, which are 
difficult to approach due to quantum constraints: the useful state
# Para 397 1
190
# Para 398 2
can neither be observed (otherwise it will decohere), nor can it be 
cloned.
# Para 400 1
2.2.1 Background
# Para 401 1
As expressed in bra-ket notation [16], the qubit is a normalized
# Para 402 1
vector in some Hilbert space H2 , { 0 , 1 } being the orthonormal
# Para 403 3
basis: ψ = a0 0 + a1 1 ( a0, a1 ∈ C are the so-called quantum
amplitudes, representing the square root of the associated 
measurement probabilities for the eigenstates
# Para 406 1
respectively, with a0 2 + a1 2 =1). Therefore, the qubit can be
# Para 407 1
affected by 3 types of errors:
# Para 408 3
Bit flip errors are somewhat similar to classical bit flip errors. For 
a single qubit things are exactly the same as in classical 
computation: 0 H 1 , 1 H 0 . For 2 or more qubits, flip errors
# Para 411 2
affecting the state may modify it or leave it unchanged. For 
instance,	if we consider the so-called cat state
# Para 413 1
ψ Cat = 2 ( 00 + 11 ) [19], and the first qubit is affected by a
# Para 414 1
bit flip error, the resulting state will be yr Cat H 2 ( 10 + 01) .
# Para 415 1
But, if both qubits are affected by bit flips, there will be no
# Para 416 1
change in the state: V Cat H 2 ( 11 + 00 ) = ψ Cat
# Para 417 1
Phase errors affect the phase of one of the qubit&apos;s amplitudes and
# Para 418 1
is expressed as 0 H 0 , 1 H − 1 . This type of error is very
# Para 419 3
dangerous, due to its propagation behavior but it only makes 
sense when dealing with superposition states. If we consider an 
equally weighted qubit superposition state and inject a phase
# Para 422 1
error, this results in 2 ( 0 + 1 )H 2 ( 0 − 1 ) .
# Para 423 3
There is a strict correspondence between bit flip and phase error 
types due to the way they map onto Hilbert spaces with the same 
dimension but different basis. The bit flip is an error from the
# Para 426 1
{ 0 , 1 } , whereas the phase error appears in the
# Para 427 1
same space with basis r	0 + 1, �(0 − 1 )⎫⎬⎭ or{ +
# Para 428 3
The space basis conversion, in this case, is made by applying the 
Hadamard transform; Figure 4 shows an example of transforming 
a bit flip error into a phase error (A, and vice versa (B.
# Para 431 1
Small amplitude errors: amplitudes a0 and a1 of the quantum bit
# Para 432 7
can be affected by small errors, similar to analog errors. Even if 
such an error does not destroy the superposition and conserves the 
value of the superposed states, small amplitude errors could 
accumulate over time, eventually ruining the computation. In 
order to avoid this situation, specific methodologies for digitizing 
small errors are used to reduce them to a non-fault or a bit-flip 
[19].
# Para 439 2
Due to the quantum physics laws, fault tolerance techniques have 
to comply with the following computational constraints:
# Para 441 3
– The observation destroys the state. Since observation is 
equivalent to measurement, this leads to destroying the 
useful state superposition.
# Para 444 5
– Information copying is impossible. Quantum physics renders 
the cloning of a quantum state impossible, meaning that a 
quantum state cannot be copied correctly. Therefore 
quantum error correction must address the following 
problems:
# Para 449 5
Non-destructive measurement. Despite the first constraint it is 
necessary to find a way to measure the encoded information 
without destroying it. Because the encoded state cannot be 
measured directly, one needs to properly prepare some scratch 
(ancilla) qubits, which can then be measured.
# Para 454 4
Fault-tolerant recovery. Due to the high error rate in quantum 
computational devices, it is likely that the error recovery itself 
will be affected by errors. If the recovery process is not fault- 
tolerant, then any error coding becomes useless.
# Para 458 5
Phase error backward propagation. If we consider the XOR gate 
from Figure 5(A, a flip error affecting the target qubit (b) will 
propagate backwards and also affect the source qubit. This is due 
to the gate network equivalence from Figure 5(B and the basis 
transformation described by Figure 4.
# Para 463 1
Figure 4. Correspondence between bit flip and phase errors
# Para 464 2
Figure 5. (A The backward propagation of a phase error for
the XOR gate; (B Gate network equivalence
# Para 466 2
In order to deal with the problems described the next strategies 
have to be followed:
# Para 468 3
Digitizing small errors. The presence of small errors is not a 
major concern, as they can be digitized using a special technique 
based on measuring auxiliary (ancilla) qubits [19].
# Para 471 11
Ancilla usage. Since qubit cloning is impossible, a majority 
voting strategy is difficult to implement. However, by using 
ancilla qubits, the eigenstate information can be duplicated inside 
the existing superposition, resulting in the entanglement of the 
ancilla with the useful data. Because any measurement performed 
on the ancilla could have repercussions on the useful qubits, the 
appropriate strategy will employ special coding for both data 
qubits and ancilla (data errors only will be copied onto the 
ancilla), followed by the computation of an error syndrome, 
which has to be obtained through measuring the ancilla (see 
Figure 6).
# Para 482 4
Avoiding massive spreading of phase errors. As shown 
previously, a phase error on the target qubit will propagate on all 
source qubits. The solution is to use more ancilla qubits as targets, 
so that no ancilla qubit is used more than once.
# Para 486 1
0 and 1
# Para 487 1
	2
# Para 488 1
space with basis
# Para 489 1
191
# Para 490 1
1
# Para 491 1
Figure 6. Fault-tolerant procedure with ancilla qubits
# Para 492 7
Ancilla and syndrome accuracy. Setting the ancilla code to some 
known quantum state could be an erroneous process. Computing 
the syndrome is also prone to errors. Hence, on one hand, one has 
to make sure that the ancilla qubits are in the right state by 
verifying and recovering them if needed; on the other hand, in 
order to have a reliable syndrome, it must be computed 
repeatedly.
# Para 499 4
Error recovery. As the small errors can be digitized (therefore, 
they are either corrected or transformed into bit flip errors), the 
recovery must deal only with bit flip and phase errors. A state that 
needs to be recovered is described by:
# Para 503 1
Correcting a bit flip error 1means applying the negation unitarytransformation UN = ux = ° Oj to the affected qubit. To
# Para 504 2
correct phase and combined errors, the following unitary 
operators	will	have	to	be	applied	respectively:
# Para 506 3
⎡1	0 ⎤	⎡0	−i ⎤
UZ=⎢0	−1], UY = UN⋅UZ	⎣i	.
			0 ⎥⎦
# Para 509 1
2.2.2 Quantum Error Correcting Codes
# Para 510 5
Quantum error coding and correcting (QECC) is performed with 
special coding techniques inspired from the classic Hamming 
codes. The classical error coding is adapted so that it becomes 
suitable for the quantum strategy, allowing only the ancilla qubits 
to be measured.
# Para 515 7
The state-of-the-art in QECC is represented by the stabilizer 
encoding, a particular case being the Steane codes (the Shor codes 
may also be used [29]). Steane&apos;s 7-qubit code is a single error 
correcting code inspired from classical Hamming coding and can 
be adapted for ancilla coding as well. Therefore it cannot recover 
from two identical qubit faults, but it can recover from a bit flip a 
phase flip. The Steane 7-qubit coding of 0 and 1 consists of
# Para 522 2
an equally weighted superposition of all the valid Hamming 7-bit 
words with an even and odd number of 1s, respectively:
# Para 524 1
= 1
# Para 525 1
S	3	odd⎞u0u1u2u3c0c1c2
# Para 526 1
⎜	⎟
# Para 527 1
⎝	⎠
# Para 528 1
=1 1111111 + 1101000 + 1010001 + 1000110
# Para 529 1
232
# Para 530 1
+ 0110100 + 0100011 + 0011010 + 0001101
# Para 531 1
Applying the Steane coding on an arbitrary given quantum state
# Para 532 1
ψ = a0 0 +a1 1 transforms it into V S = a0 0 S + a1 1 S. This
# Para 533 3
code was designed to correct bit-flip errors, but by changing the 
basis (through a Hadamard transform) the phase error transforms 
into a bit flip error, which can then be corrected:
# Para 536 1
2.2.3 Fault Tolerance Methodologies
# Para 537 2
Quantum error-correcting codes exist for r errors, r ∈ ICY, r ≥ 1. 
Therefore a non-correctable error occurs if a number of r +1
# Para 539 1
errors occur simultaneously before the recovery process.
# Para 540 1
If the probability of a quantum gate error or storage error in the
# Para 541 1
time unit is of orderξ , then the probability of an error affecting
# Para 542 3
the processed data block becomes of order �r+1 , which is
negligible if r is sufficiently large. However, by increasing r the 
safe recovery also becomes more complex and hence prone to
# Para 545 1
errors: it is possible that r +1 errors accumulate in the block
# Para 546 1
before the recovery is performed.
# Para 547 2
Considering the relationship between r and the number of 
computational steps required for computing the syndrome is
# Para 549 2
polynomial of the order rp . It was proven that in order to reduce 
as much as possible the error probability r must be chosen so that
# Para 551 1
1
# Para 552 1
−
# Para 553 1
r — e 1K p [7][19]. By consequence, if attempting to execute N
# Para 554 1
cycles Sof error correction without any r+1 errors accumulating
# Para 555 1
before the recovery ends, then N — exp⎜ξp
# Para 556 1
⎛ − 1
# Para 557 1
. Therefore the
# Para 558 1
accuracy degree will be of the form � —(logN)−p , which is
# Para 559 1
better than the accuracy degree corresponding to the no-coding
# Para 560 1
case, � — N-1. However, there exists a Nmax so that if N &gt; Nmax
# Para 561 3
then non-correctable error becomes likely, which limits the length 
of the recovery process. Given the extremely large number of 
gates employed by a quantum algorithm implementation, Nmax
# Para 564 1
also has to be very large; for Shor&apos;s algorithm Nmax must be
# Para 565 1
higher than 3⋅109 [30].
# Para 566 4
As shown in Figure 7, the required accuracy degree approaches 
today&apos;s technological limits (tipically 10-3 for p=4) after N=105. 
For a fault tolerant encoding solution for Shor algorithm 
implementation this should have happened after N=109 [19][34].
# Para 570 1
+	(2)	Additional fault tolerance must be employed in order to preserve
# Para 571 1
⎯⎯⎯→⎨error ⎧ ⎪ ⎪ ⎪ ⎪ ⎩
# Para 572 1
a
# Para 573 1
a1 0 + a0 1 for a flip error
# Para 574 1
.
# Para 575 1
a0 0 −a1 1 for a phase error
# Para 576 1
a1 0 − a0 1 for both flip and phase errors
# Para 577 1
0 0 +a1 1 if no error occurs
# Para 578 1
a00+a11
# Para 579 1
= 1 0000000 + 0010111 + 0101110 + 0111001
# Para 580 1
+ 1001011 + 1011100 + 1100101 + 1110010
# Para 581 1
232
# Para 582 1
(
# Para 583 1
)
# Para 584 1
1
# Para 585 1
=
# Para 586 1
S	232	evelu0`2u3c01c2)
# Para 587 1
0
# Para 588 1
u0u1u2u3c0c1c2
# Para 589 1
+	(3)
# Para 590 1
)
# Para 591 1
0 S =H⋅ 0S= 1
# Para 592 1
1 S =H⋅ 1 S = 1
# Para 593 1
2
# Para 594 1
2
# Para 595 1
(0S+1S)
# Para 596 1
(0S−1S)
# Para 597 1
(4)
# Para 598 3
reliable quantum computation over an arbitrary number of 
computational steps. Concatenated coding represents one such 
technique, which improves the reliability by shaping the size of
# Para 601 1
192
# Para 602 2
the code blocks and the number of code levels. It is also resource 
demanding and vulnerable to correlated errors [19][37].
# Para 604 9
Another approach, replacing the concatenated codes, is based on 
Reconfigurable Quantum Gate Arrays (RQGAs) [34][37], which 
are used for configuring ECC circuits based on stabilizer codes 
[7][33]. By using a quantum configuration register for the RQGA 
(i.e. a superposition of classical configurations), the 
reconfigurable circuit is brought to a state where it represents a 
simultaneous superposition of distinct ECC circuits. After 
measuring the configuration register, only one ECC circuit is 
selected and used; if k distinct ECC circuits were superposed and
# Para 613 1
the gate error rate is � , then the overall gate error probability
# Para 614 3
becomes �k (see Figure 8). As a result, the accuracy threshold 
value for the RQGA solution clearly dominates the technological 
accuracy limit, as shown in Figure 9 [37].
# Para 617 3
Figure 7. Accuracy plots: p=3 for xi1, p=4 for xi2, p=5 for xi3;
xi4 for no-coding, ref is the reference accuracy (i.e. the
accuracy allowed by today&apos;s state of the art technology)
# Para 620 3
Figure 8. A quantum configuration register acts as a
superposition of k distinct circuits sharing the same input
state and the same output qubits
# Para 623 1
3. DEPENDABLE SYSTEM DESIGN
# Para 624 10
In order to model the erroneous behavior of a device of system it 
is necessary to understand the causality of phenomena concerned. 
A defect affecting a device from a physical point of view is called 
a fault, or a fail. Faults may be put in evidence through logical 
misbehavior, in which case they transform into errors. Finally, 
errors accumulating can lead to system failure [8]. The fault-
error-failure causal chain is essential to developping techniques 
that reduce the risk of error occurrence, even in the presence of 
faults, in order to minimize the probability of a system failure, 
and can be architecture specific. We will elaborate next on
# Para 634 2
techniques used by a bio-inspired and by a quantum computing 
platform.
# Para 636 3
Figure 9. Evolution of accuracy threshold value for RQHW
stabilizer codes (xir); the technological accuracy limit (dim) is
also provided for a relevant comparison
# Para 639 1
3.1 The Embryonics Approach
# Para 640 9
Several years before his untimely death John von Neumann began 
developping a theory of automata, which was to contain a 
systematic theory of mixed mathematical and logical forms, 
aimed to a better understanding of both natural systems and 
computers [14]. The essence of von Neumann’s message appears 
to entail the formula “genotype + ribotype = phenotype”. He 
provided the foundations of a self-replicating machine (the 
phenotype), consisting of its complete description (the genotype), 
which is interpreted by a ribosome (the ribotype).
# Para 649 9
Embryonics (a contraction for embryonic electronics) is a long 
term research project launched by the Logic Systems Laboratory 
at the Swiss Federal Institute of Technology, Lausanne, 
Switzerland. Its aim is to explore the potential of biologically- 
inspired mechanisms by borrowing and adapting them from 
nature into digital devices for the purpose of endowing them with 
the remarkable robustness present in biological entities [39]. 
Though perhaps fuzzy at a first glance, analogies between biology 
and electronics are presented in Table 1 [12][31].
# Para 658 7
But if we consider that the function of a living cell is determined 
by the genome, and that a computer’s functionality is determined 
by the operating program, then the two worlds may be regarded as 
sharing a certain degree of similarity. Three fundamental features 
shared by living entities are required to be targetted by 
Embryonics in order to embody the formula “genotype + ribotype 
= phenotype” into digital hardware:
# Para 665 3
–multicellular organisms are made of a finite number of cells, 
which in turn are made of a finite number of chemically 
bonded molecules;
# Para 668 4
–each cell (beginning with the original cell, the zygote) may 
generate one or several daughter cell(s) through a process 
called cellular division; both the parent and the daughter 
cell(s) share the same genetic information, called the genome;
# Para 672 3
–different types of cells may exist due to cellular 
differentiation, a process through which only a part of the 
genome is executed.
# Para 675 5
These fundamental features led the Embryonics project to settle 
for an architectural hierarchy of four levels (see Figure 10). We 
will not delve very deep inside the Embryonics’phylosophy, as 
such details were broadly covered by literature [12][20][23][24] 
[25][40]; we will, however, introduce each of the four levels in
# Para 680 1
193
# Para 681 2
order to be able to see how this bio-inspired platform fits modern 
design for dependability efforts.
# Para 683 1
Table 1. Analogies present in Embryonics [12]
# Para 684 4
Biology	Electronics
Multicellular organism	Parallel computer systems
Cell	Processor
Molecule	FPGA Element
# Para 688 1
Figure 10. Structural hierarchy in Embryonics [12]
# Para 689 12
The upmost level in Embryonics, bearing a certain similarity to 
what is found in nature, is the population level, composed of a 
number of organisms. One level down the hierarchy constitutes 
the organismic level, and corresponds to individual entities in a 
variety of functionalities and sizes. Each of the organisms may be 
further decomposed into smaller, simpler parts, called cells, which 
in turn may be decomposed in molecules. According to 
Embryonics, a biological organism corresponds in the world of 
digital systems to a complete computer, a biological cell is 
equivalent to a processor, and the smallest part in biology, the 
molecule, may be seen as the smallest, programmable element in 
digital electronics (see Table 1).
# Para 701 16
An extremely valuable consequence of the Embryonics 
architecture is that each cell is &quot;universal&quot;, containing a copy of 
the whole of the organism’s genetic material, the genome. This 
enables very flexible redundancy strategies, the living organisms 
being capable of self-repair (healing) or self-replication (cloning) 
[12]. Self-replication may be of great interest in the 
nanoelectronics era, where extremely large areas of 
programmable logic will probably render any centralized control 
very inefficient. Instead, the self-replication mechanism 
implemented in Embryonics will allow the initial colonization of 
the entire programmable array in a decentralized and distributed 
manner. Figure 11 presents an example of such colonization. At 
initial time the configuration bitstream (containing the genome) 
enters the bottom left corner of a programmable array and, at each 
clock cycle, the genome is pushed through and partitions the 
programmable space accordingly.
# Para 717 2
From a dependability standpoint, the Embryonics hierarchical 
architecture offers incentives for an also hierarchical self-repair
# Para 719 5
strategy. Because the target applications are those in which the 
failure frequency must be very low to be “acceptable”, two levels 
of self-repair are offered: at the molecular level (programmable 
logic is susceptible to soft fail occurrences) and at the cellular 
level (soft fails manifest at this level as soft errors).
# Para 724 5
Let us consider an example of a simple cell made of 3 lines and 3 
columns of molecules, of which one column contains spare 
molecules. If a fault occurs inside an active cell, it can be repaired 
through transferring its functionality toward the appropriate spare 
molecule, which will become active (see Figure 12).
# Para 729 1
Figure 11. Space colonization in Embryonics [11]
# Para 730 2
Figure 12. Self-repair at the molecular level: faulty molecule 
E is replaced by spare molecule H, which becomes active [39]
# Para 732 10
The self-repair process at molecular level ensures the fault 
recovery as long as there are spare molecules left for repair. 
However, it is possible for a cell to experience a multiple error, in 
which case the self-repair mechanism at the molecular level can 
no longer reconfigure the inside of the cell successfully. If such a 
situation arises, then a second self-repair strategy is trigerred at a 
higher level. The cell will “die”, therefore trigerring the self- 
repair at the cellular level, the entire column containing the faulty 
cell (cell C in this example) being deactivated, its role being taken 
by the nearest spare column to the right (see Figure 13).
# Para 742 6
A critique that could be addressed to the current Embryonics 
design would be its strategy of self-repair at the higher, cellular 
level: in case of a faulty cell, an entire column containing that cell 
will be deactivated, its role being transferred to the first available 
column of spares to the right (see Figure 13). There are two points 
in which this strategy could benefit:
# Para 748 1
194
# Para 749 12
1. Instead of deactivating a whole column of cells, it would be 
more efficient to only deactivate the faulty cell only (see 
Figure 14). The resources affected by the role transfer would 
be greatly reduced (one cell versus an entire column), 
coupled with the fact that particle flux generating soft fails is 
unlikely to be homogeneous and isotrope. This means 
regions assimilable more likely to cells rather than entire 
column of cells would be more affected by soft fails, not to 
mention that during genetic data transfer (required by taking 
over the role of the faulty cell) there is a greater risk of 
enduring a new soft fail (moving data is much more sensitive 
to soft fails than static data) [5][10].
# Para 761 6
2. Such a strategy would be consistent with that used for the 
self-repair at the molecular level, which would simplify a 
thorough reliability analysis. Concatenated coding would 
also seem easier to be implemented and the strategy 
consistency would mean that concatenated coding would not 
be limited to a two-level hierarchy [20][21].
# Para 767 2
Figure 13. Molecular self-repair failure: the cell “dies” 
(bottom), triggering the cellular self-repair (top) [39]
# Para 769 13
We consider a cell of M lines and N columns, being composed of 
modules of M lines and n+s columns (for instance, the cell 
presented in Figure 12 consists of a single such module of two 
active columns and one spare column), of which s are spares. In 
order to meet certain reliability criteria, it is necessary to know 
what is the number s of spare columns of molecules that 
correspond to n columns of active molecules, that is, the 
horizontal dimensions for such a module. We will not provide a 
thorough reliability analysis, as this has been done previously 
[4][17][20][21]; instead, we will analyze the influences of the 
proposed consistent self-repair strategy at both molecular and 
cellular levels through the use of logic molecules. Therefore 
Equation (5) holds:
# Para 782 1
k
# Para 783 1
RModRow (t)=Prob{ no fails} (t) + ∑ Prob{ i fails} (t)
# Para 784 1
i=1	(5)
# Para 785 1
N=k(n+s)
# Para 786 2
where RModRow(t) represents the reliability function for a row 
within a module. Then, considering the failure rate for one
# Para 788 1
molecule λ, the probability of all molecules (both active and
# Para 789 1
spare) to operate normally in a module’s row becomes:
# Para 790 1
Prob{ no fails} (t) = e−&quot;n+s)t	(6)
# Para 791 5
The probability of a row enduring i fails in the active molecules 
part is the conditional probability of having n-i active molecules 
operating normally, while a number of s-i spare molecules are 
ready to activate (that is, they are not affected by errors 
themselves):
# Para 796 1
Prob{ i fails} (t) = Prob{ i fails active} (t)
# Para 797 1
⋅Prob{i spares ok}(t)
# Para 798 1
Prob{ i fails active} (t) = (n Je λ(n-i)t (1 − eλ(n-i)t)	(8)
# Para 799 1
i	l
# Para 800 1
Prob{ i spares ok} (t) = (k )e−λit (1 − a λ(k−i)t
# Para 801 1
i
# Para 802 2
Then the reliability function for an entire cell is the cummulated 
reliability functions for the total number of modules:
# Para 804 1
RCell(t) = [RModRow(t)]MN`	(10)
# Para 805 2
Figure 14. Proposed reconfiguration strategy at the cellular
level
# Para 807 11
A self-repair strategy that conserves the consistency between the 
molecular and the cellular level would allow for a more 
straightforward reliability analysis. Basically, it would be 
sufficient to substitute dimension parameters in Equations (5)- 
(10) with those approapriate to the analysis of an organism 
instead of a cell. To illustrate this, we will consider an organism 
of M* lines and N* columns, being composed of modules of M* 
lines and n*+s* columns, of which s* are spares; we will also use 
the organism partitioning into modules, similar to the partitioning 
of cells used before. Therefore Equation (5) transforms into 
Equation (11):
# Para 818 1
k
# Para 819 1
RCellMR (t)=Prob*{nofails}(t)+∑Prob* { ifails} (t) i = 1	(11)
# Para 820 1
N*=k*(n +s )
# Para 821 1
where RCellMR (t) represents the reliability function for a row of
# Para 822 2
cells within an organism module. In this case, the significance of 
the terms will be as follows:
# Para 824 1
Prob {no fails} (t) = [RCell ( tj +s	(12)
# Para 825 1
⋅
# Para 826 1
(7)
# Para 827 1
(9)
# Para 828 1
195
# Para 829 3
While Equation (7) continues to hold under the form of Equation 
(13), the significance of its terms will change according to the 
dimensions at the cellular level:
# Para 832 1
⎛ ⎞ −
# Para 833 1
Prob*{ifailsactive}(t)=⎜
# Para 834 1
 i
# Para 835 1
(t)(1−R
# Para 836 1
L
# Para 837 1
ll(t
# Para 838 1
⎛ ⎞	−
# Para 839 1
Prob*{isparesok}(t)=⎜k*
# Para 840 1
J
# Para 841 1
RCiell(t)(1−RCkelli(t))	(15) i
# Para 842 3
t0, t1,..., tm−1 will be given by 
The outputs ofthe firstcycle, whichare also inputs forthe 
The used FTAMs are only valid if the relationship between the
# Para 845 1
experimental ξsim and the assumed singular error rateξ is of the
# Para 846 1
order gsim _ � 2 [19].
# Para 847 1
) )(14)
# Para 848 1
Finally, similar to Equation(10), the reliabilityfunctionforan
# Para 849 1
entire organism is the cummulated re
# Para 850 1
liability functions for the
# Para 851 1
total number of its modules:
# Para 852 1
ROrg(t)=
# Para 853 1
 [
# Para 854 1
RCellMR(t)
# Para 855 1
]
# Para 856 1
MN
# Para 857 1
 �
# Para 858 1
+
# Para 859 1
s	(16)
# Para 860 1
Equations (5) to (16) provide the basics forathorough reliability
# Para 861 1
analysis for the proposed, uniformstrategy ofhierarchical
# Para 862 1
reconfiguration, as opposedto the analysis providedby [21],
# Para 863 1
whichspecifically targetted the currentEmbryonics architecture.
# Para 864 1
Despite having settled the reliabilitymodel, bothanalyses are
# Para 865 1
incomplete, inthatthe failure rate parameteris missing, which
# Para 866 1
makes aprecise, quantitative dependability targetdifficultto
# Para 867 1
meet. However, areliability analysis is still valuable from a
# Para 868 1
qualitative pointofview, allowingadirectcomparison of
# Para 869 1
differentsystems.
# Para 870 1
3.2 The QUERISTApproach
# Para 871 1
Inorderto deal with errors inducedby the constantinfluence of
# Para 872 1
the external environmentuponcomputational processes, the
# Para 873 1
following assumptions were made: errors appear randomly, are
# Para 874 1
uncorrelated (neitherinspace, norintime), there are no storage
# Para 875 1
errors, andthere are no leakage phenomenainvolved[19].
# Para 876 1
Classical HDL-based faultinjectionmethodologies can be
# Para 877 1
mappedto simulatingquantumcircuits withoutintervention
# Para 878 1
providedthatthe new errorand faultmodels are takeninto
# Para 879 1
account[35]. Ofcourse, efficiencycriteriarequire thatthey be
# Para 880 1
adaptedto one ofthe available efficientsimulationframeworks
# Para 881 1
[36][38][41]. QUERIST(from QUantum ERrorInjection
# Para 882 1
Simulation Tool) is the name ofsucha project, fostering
# Para 883 1
simulated faultinjectiontechniques inquantum circuits [34].
# Para 884 1
Similarto classical computation, simulatedfaultinjection is used
# Para 885 1
in orderto evaluate the employed FTAMS (FaultTolerance
# Para 886 1
Algorithms andMethodologies) [26][27].
# Para 887 1
Anoverview ofthe QUERISTprojectis presented in Figure 15.
# Para 888 1
The three cycles ofinitialization, simulation, anddata
# Para 889 1
computationare commonto bothclassical andquantum
# Para 890 1
approaches. The firstcycle takes the quantumcircuitHDL
# Para 891 1
description as aninput. Two abstractinputs are considered, the
# Para 892 1
HDL model andthe assumederrormodel; the firstinfluences how
# Para 893 1
the HDLdescription is presented, while the secondone dictates
# Para 894 1
the testscenario by definingthe start/stop simulationstates (since
# Para 895 1
qubits are equallyprone to error, all the signals mustbe
# Para 896 1
observed). HDLmodelingofquantumcircuits inorderto attain
# Para 897 1
effi
# Para 898 1
cient simulation is discussed in [34][35][36][38].
# Para 899 1
196
# Para 900 10
secondcycle consists oftime diagrams forall qubits, from the 
startto the stop states. Useful information, extracted fromthe raw, 
bubble-bit-represented, qubittraces are comparedto correctqubit 
values, the resultbeingthe probabilistic accuracy thresholdvalue, 
inthe thirdcycle. The initialization andsimulation cycles depend 
on specific aspects ofquantum circuitsimulation [35]. The data 
processing cycle is independentfromthe specific simulation 
framework andis aimedatdeterminingthe accuracythresholdas 
the mainreliability measure thatalso defines the feasibility ofthe 
quantum circuitimplementations.
# Para 910 1
Suppose that, atsimulationtime twe observe signals
# Para 911 1
{s0,s1,...,sn}
# Para 912 1
 . In ouranalysis,
# Para 913 1
si
# Para 914 3
 is the state observedduringnon- 
faulty simulation, so forthe same state ina faulty environmentwe 
will have the state
# Para 917 1
si* .
# Para 918 1
Forvalidationofthe quantum FTAMs, we needto compare
# Para 919 1
si
# Para 920 1
with
# Para 921 1
si*.
# Para 922 1
 This can be done by using operator
# Para 923 1
 d
# Para 924 1
if
# Para 925 1
(si,s
# Para 926 1
;
# Para 927 1
)
# Para 928 1
 . This
# Para 929 1
mean
# Para 930 1
s that the total number of overall state errors at simulation
# Para 931 1
n − 1
# Para 932 1
time tis
# Para 933 1
. et=∑d
# Para 934 1
if
# Para 935 1
(si,s
# Para 936 1
;
# Para 937 1
)The error rate on the overall observed
# Para 938 1
4. CONCLUSIONS
# Para 939 19
This paper presented arguments in favor of two novel computing 
architectures for the purpose of addressing the challenges raised 
by the forthcoming nanoelectronics era. Distributed self-testing 
and self-repairing will probably become a must in the next years 
as centralized control logic is expected to become unable to 
harness the extremely large number of devices, all equally prone 
to errors, that will be integrated onto the same chip. Bio-inspired 
computing brings valuable techniques that explore the potential of 
massively parallel, distributed computation and fault-tolerance 
that will likely provide an essential help to jumpstart new 
nanoelectronic architectures. As one of the representatives of bio-
inspired computing, the Embryonics project presents a 
hierarchical architecture that achieves fault tolerance through 
implementing an also hierarchical reconfiguration. A similar 
approach for maximizing fault tolerance is present in quantum 
computing, the QUERIST project; even if bio-inspired and 
quantum computing may seem dissimilar at a first glance, they 
both achieve fault tolerance by adapting the same techniques from 
classical computing and using essentially the same error model.
# Para 958 4
Nanoelectronics will potentially change the way computing 
systems are designed, not only because of the sheer number of 
devices that will coexist onto the same chip, but also because of 
the sensitivity of these devices.
# Para 962 1
Prob*
# Para 963 1
{ i fails} (t) = Prob* { i fails active} (t)
# Para 964 1
⋅
# Para 965 1
⋅ Prob* { i spares ok} (t)
# Para 966 1
(13)
# Para 967 1
i=0
# Para 968 3
simulation cycle, consistofatestscenario and an executable HDL 
model withthe correspondingentanglementanalysis, dictatedby 
the bubble-bitencoded quantum states [36][38]. The outputofthe
# Para 971 1
states at moments
# Para 972 1
1 m−1
# Para 973 1
ξsim	∑ et •
# Para 974 1
m
# Para 975 1
j=0 �
# Para 976 1
Figure 15. An overview of the QUERIST project
# Para 977 8
Therefore, if nanoelectronics is to be employed to build 
dependable computing machines (a certain contradiction 
notwithstanding), valuable expertise in design can be drawn from 
natural sciences. While biology provides countless examples of 
successfully implemented fault tolerance strategies, physics offers 
theoretical foundations, both of which were found to share 
common ground. It is perhaps a coincidence worth exploring in 
digital computing.
# Para 985 1
5. REFERENCES
# Para 986 4
[1] Aharonov, D., Ben-Or, M. Fault Tolerant Quantum 
Computation with Constant Error. Proc. ACM 29th Ann. 
Symposium on Theory of Computing, El Paso, Texas, May 
1997, pp. 176-188.
# Para 990 4
[2] Avižienis, A., Laprie, J.C., Randell, B., Landwehr, C. Basic 
Concepts and Taxonomy of Dependable and Secure 
Computing. IEEE Transactions on Dependable and Secure 
Computing, 1, 1 (Jan-Mar 2004), 11-33.
# Para 994 4
[3] Butts, M., DeHon, A., Golstein, S.C. Molecular Electronics: 
Devices, Systems and Tools for Gigagate, Gigabit Chips. 
Proc. Intl. Conference on CAD (ICCAD’02), 2002, pp. 433- 
440.
# Para 998 4
[4] Canham, R., Tyrrell, A. An Embryonic Array with Improved 
Efficiency and Fault Tolerance. Proc. IEEE NASA/DoD 
Conference on Evolvable Hardware, Chicago Il, 2003, 275- 
282.
# Para 1002 4
[5] Gaisler, J. Evaluation of a 32-Bit Microprocessor with Built- 
In Concurrent Error Detection. Proc. 27th Annual Intl. 
Symposium on Fault-Tolerant Computing (FTCS-27), 1997, 
pp. 42-46.
# Para 1006 4
[6] Goldstein, S.C. The Challenges and Opportunities of 
Nanoelectronics. Proc. Government Microcircuit Applica-
tions and Critical Technology Conference (GOMAC Tech - 
04), Monterey, CA, March 2004.
# Para 1010 3
[7] Gottesman, D. Class of quantum error-correcting codes 
saturating the quantum Hamming bound. Phys. Rev. A 54, 
1996, pp. 1862-1868.
# Para 1013 2
[8] Johnson, B.W. Design and Analysis of Fault-Tolerant 
Digital Systems. Addison-Wesley, 1989.
# Para 1015 3
[9] Laprie, J.-C. (Ed.). Dependability: Basic Concepts and 
Terminology. Dependable Computing and Fault-Tolerant 
Systems Series, Vol. 5, Springer-Verlag, Vienna, 1992.
# Para 1018 4
[10] Liden, P., Dahlgren, P., Johansson, R., Karlsson, J. On 
Latching Probability of Particle Induced Transients in 
Combinational Networks. Proc. Intl. Symposium on Fault- 
Tolerant Computing (FTCS-24), 1994, pp.340-349.
# Para 1022 3
[11] Mange, D., Sipper, M., Stauffer, A., Tempesti, G. Toward 
Robust Integrated Circuits: The Embryonics Approach. Proc. 
of the IEEE, vol. 88, No. 4, April 2000, pp. 516-541.
# Para 1025 4
[12] Mange, D. and Tomassini, M. eds. Bio -Inspired Computing 
Machines: Towards Novel Computational Architectures. 
Presses Polytechniques et Universitaires Romandes, 
Lausanne, Switzerland, 1998.
# Para 1029 2
[13] Von Neumann, J. The Computer and the Brain (2nd edition). 
Physical Science, 2000.
# Para 1031 3
[14] Von Neumann, J. The Theory of Self-Reproducing 
Automata. A. W. Burks, ed. University of Illinois Press, 
Urbana, IL, 1966.
# Para 1034 5
[15] Von Neumann, J. Probabilistic Logic and the Synthesis of 
Reliable Organisms from Unreliable Components. In C.E. 
Shannon, J. McCarthy (eds.) Automata Studies, Annals of 
Mathematical Studies 34, Princeton University Press, 1956, 
43-98.
# Para 1039 2
[16] Nielsen, M.A., Chuang, I.L. Quantum Computation and 
Quantum Information. Cambridge University Press, 2000.
# Para 1041 3
[17] Ortega, C., Tyrrell, A. Reliability Analysis in Self-Repairing 
Embryonic Systems. Proc. 1st NASA/DoD Workshop on 
Evolvable Hardware, Pasadena CA, 1999, 120-128.
# Para 1044 2
[18] O’Connor, P.D.T. Practical Reliability Engineering. John 
Wiley &amp; Sons, 4th edition, 2002.
# Para 1046 4
[19] Preskill, J. Fault Tolerant Quantum Computation. In H.K. 
Lo, S. Popescu and T.P. Spiller, eds. Introduction to 
Quantum Computation, World Scientific Publishing Co., 
1998.
# Para 1050 1
197
# Para 1051 3
[20] Prodan, L. Self-Repairing Memory Arrays Inspired by 
Biological Processes. Ph.D. Thesis, “Politehnica” University 
of Timisoara, Romania, October 14, 2005.
# Para 1054 4
[21] Prodan, L., Udrescu, M., Vladutiu, M. Survivability Analysis 
in Embryonics: A New Perspective. Proc. IEEE NASA/DoD 
Conference on Evolvable Hardware, Washington DC, 2005, 
280-289.
# Para 1058 4
[22] Prodan, L., Udrescu, M., Vladutiu, M. Self-Repairing 
Embryonic Memory Arrays. Proc. IEEE NASA/DoD 
Conference on Evolvable Hardware, Seattle WA, 2004, 130- 
137.
# Para 1062 3
[23] Prodan, L., Tempesti, G., Mange, D., and Stauffer, A. 
Embryonics: Electronic Stem Cells. Proc. Artificial Life VIII, 
The MIT Press, Cambridge MA, 2003, 101-105.
# Para 1065 5
[24] Prodan, L., Tempesti, G., Mange, D., and Stauffer, A. 
Embryonics: Artificial Cells Driven by Artificial DNA. 
Proc. 4th International Conference on Evolvable Systems 
(ICES2001), Tokyo, Japan, LNCS vol. 2210, Springer, 
Berlin, 2001, 100-111.
# Para 1070 5
[25] Prodan, L., Tempesti, G., Mange, D., and Stauffer, A. 
Biology Meets Electronics: The Path to a Bio-Inspired 
FPGA. In Proc. 3rd International Conference on Evolvable 
Systems (ICES2000), Edinburgh, Scotland, LNCS 1801, 
Springer, Berlin, 2000, 187-196.
# Para 1075 4
[26] Rimen, M., Ohlsson, J., Karlsson, J., Jenn, E., Arlat, J. 
Validation of fault tolerance by fault injection in VHDL 
simulation models. Rapport LAAS No. 92469, December 
1992.
# Para 1079 4
[27] Rimen, M., Ohlsson, J., Karlsson, J., Jenn, E., Arlat, J. 
Design guidelines of a VHDL-based simulation tool for the 
validation of fault tolerance. Rapport LAAS No931 70, Esprit 
Basic Research Action No. 6362, May 1993.
# Para 1083 5
[28] Shivakumar, P., Kistler, M., Keckler, S.W., Burger, D., 
Alvisi, L. Modelling the Effect of Technology Trends on the 
Soft Error Rate of Combinational Logic. Proc. Intl. 
Conference on Dependable Systems and Networks (DSN), 
June 2002, pp. 389-398.
# Para 1088 1
[29] Shor, P. Fault-tolerant quantum computation.
# Para 1089 1
arXiv.org:quant-ph/9605011, 1996.
# Para 1090 3
[30] Shor, P. Algorithms for Quantum Computation: Discrete 
Logarithms and Factoring. Proc. 35th Symp. on Foundations 
of Computer Science, 1994, pp. 124-134.
# Para 1093 2
[31] Sipper, M., Mange, D., Stauffer, A. Ontogenetic Hardware. 
BioSystems, 44, 3, 1997, 193-207.
# Para 1095 5
[32] Sipper, M., Sanchez, E., Mange, D., Tomassini, M., Perez-
Uribe, A., Stauffer, A. A Phylogenetic, Ontogenetic and 
Epigenetic View of Bio-Inspired Hardware Systems. IEEE 
Transactions on Evolutionary Computation, 1, 1, April 1997, 
83-97.
# Para 1100 2
[33] Steane, A. Multiple Particle Interference and Quantum Error 
Correction. Proc. Roy. Soc. Lond. A 452, 1996, pp. 2551.
# Para 1102 4
[34] Udrescu, M. Quantum Circuits Engineering: Efficient 
Simulation and Reconfigurable Quantum Hardware. Ph.D. 
Thesis, “Politehnica” University of Timisoara, Romania, 
November 25, 2005.
# Para 1106 4
[35] Udrescu, M., Prodan, L., Vladutiu, M. Simulated Fault 
Injection in Quantum Circuits with the Bubble Bit 
Technique. Proc. International Conference &quot;Adaptive and 
Natural Computing Algorithms&quot;, pp. 276-279.
# Para 1110 4
[36] Udrescu, M., Prodan, L., Vladutiu, M. The Bubble Bit 
Technique as Improvement of HDL-Based Quantum Circuits 
Simulation. IEEE 38th Annual Simulation Symposium, San 
Diego CA, USA, 2005, pp. 217-224.
# Para 1114 4
[37] Udrescu, M., Prodan, L., Vladutiu, M. Improving Quantum 
Circuit Dependability with Reconfigurable Quantum Gate 
Arrays. 2nd ACM International Conference on Computing 
Frontiers, Ischia, Italy, 2005, pp. 133-144.
# Para 1118 4
[38] Udrescu, M., Prodan, L., Vladutiu, M. Using HDLs for 
describing quantum circuits: a framework for efficient 
quantum algorithm simulation. Proc. 1st ACM Conference 
on Computing Frontiers, Ischia, Italy, 2004, 96-110.
# Para 1122 4
[39] Tempesti, G. A Self-Repairing Multiplexer-Based FPGA 
Inspired by Biological Processes. Ph.D. Thesis No. 1827, 
Logic Systems Laboratory, The Swiss Federal Institute of 
Technology, Lausanne, 1998.
# Para 1126 4
[40] Tempesti, G., Mange, D., Petraglio, E., Stauffer, A., Thoma 
Y. Developmental Processes in Silicon: An Engineering 
Perspective. Proc. IEEE NASA/DoD Conference on 
Evolvable Hardware, Chicago Il, 2003, 265-274.
# Para 1130 4
[41] Viamontes, G., Markov, I., Hayes, J.P. High-performance 
QuIDD-based Simulation of Quantum Circuits. Proc. Design 
Autom. and Test in Europe (DATE), Paris, France, 2004, pp. 
1354-1359.
# Para 1134 3
[42] Yu, Y., Johnson, B.W. A Perspective on the State of 
Research on Fault Injection Techniques. Technical Report 
UVA-CSCS-FIT-001, University of Virginia, May 20, 2002.
# Para 1137 3
[43] ***. ITRS – International Technology Roadmap for Semic-
onductors, Emerging Research Devices, 2004, http://www. 
itrs.net/Common/2004Update/2004_05_ERD.pdf
# Para 1140 2
[44] ***. Society of Reliability Engineers, http://www.sre.org/ 
pubs/
# Para 1142 1
[45] ***. http://www.dependability.org/wg10.4/
# Para 1143 1
198
