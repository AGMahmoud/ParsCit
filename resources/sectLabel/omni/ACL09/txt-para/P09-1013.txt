# Para 0 1
Knowing the Unseen: Estimating Vocabulary Size over Unseen Samples
# Para 1 1
Suma Bhat	Richard Sproat
# Para 2 1
Department of ECE	Center for Spoken Language Understanding
# Para 3 1
University of Illinois	Oregon Health &amp; Science University
# Para 4 1
spbhat2@illinois.edu	rws@xoba.com
# Para 5 1
Abstract
# Para 6 26
Empirical studies on corpora involve mak-
ing measurements of several quantities for 
the purpose of comparing corpora, creat-
ing language models or to make general-
izations about specific linguistic phenom-
ena in a language. Quantities such as av-
erage word length are stable across sam-
ple sizes and hence can be reliably esti-
mated from large enough samples. How-
ever, quantities such as vocabulary size 
change with sample size. Thus measure-
ments based on a given sample will need 
to be extrapolated to obtain their estimates 
over larger unseen samples. In this work, 
we propose a novel nonparametric estima-
tor of vocabulary size. Our main result is 
to show the statistical consistency of the 
estimator – the first of its kind in the lit-
erature. Finally, we compare our proposal 
with the state of the art estimators (both 
parametric and nonparametric) on large 
standard corpora; apart from showing the 
favorable performance of our estimator, 
we also see that the classical Good-Turing 
estimator consistently underestimates the 
vocabulary size.
# Para 32 1
1 Introduction
# Para 33 11
Empirical studies on corpora involve making mea-
surements of several quantities for the purpose of 
comparing corpora, creating language models or 
to make generalizations about specific linguistic 
phenomena in a language. Quantities such as av-
erage word length or average sentence length are 
stable across sample sizes. Hence empirical mea-
surements from large enough samples tend to be 
reliable for even larger sample sizes. On the other 
hand, quantities associated with word frequencies, 
such as the number of hapax legomena or the num-
# Para 44 10
ber of distinct word types changes are strictly sam-
ple size dependent. Given a sample we can ob-
tain the seen vocabulary and the seen number of 
hapax legomena. However, for the purpose of 
comparison of corpora of different sizes or lin-
guistic phenomena based on samples of different 
sizes it is imperative that these quantities be com-
pared based on similar sample sizes. We thus need 
methods to extrapolate empirical measurements of 
these quantities to arbitrary sample sizes.
# Para 54 15
Our focus in this study will be estimators of 
vocabulary size for samples larger than the sam-
ple available. There is an abundance of estima-
tors of population size (in our case, vocabulary 
size) in existing literature. Excellent survey arti-
cles that summarize the state-of-the-art are avail-
able in (Bunge and Fitzpatrick, 1993) and (Gan-
dolfi and Sastri, 2004). Of particular interest to 
us is the set of estimators that have been shown 
to model word frequency distributions well. This 
study proposes a nonparametric estimator of vo-
cabulary size and evaluates its theoretical and em-
pirical performance. For comparison we consider 
some state-of-the-art parametric and nonparamet-
ric estimators of vocabulary size.
# Para 69 12
The proposed non-parametric estimator for the 
number of unseen elements assumes a regime 
characterizing word frequency distributions. This 
work is motivated by a scaling formulation to ad-
dress the problem of unlikely events proposed in 
(Baayen, 2001; Khmaladze, 1987; Khmaladze and 
Chitashvili, 1989; Wagner et al., 2006). We also 
demonstrate that the estimator is strongly consis-
tent under the natural scaling formulation. While 
compared with other vocabulary size estimates, 
we see that our estimator performs at least as well 
as some of the state of the art estimators.
# Para 81 1
2 Previous Work
# Para 82 2
Many estimators of vocabulary size are available 
in the literature and a comparison of several non
# Para 84 1
109
# Para 85 2
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 109–117,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
# Para 87 13
parametric estimators of population size occurs in 
(Gandolfi and Sastri, 2004). While a definite com-
parison including parametric estimators is lacking, 
there is also no known work comparing methods 
of extrapolation of vocabulary size. Baroni and 
Evert, in (Baroni and Evert, 2005), evaluate the 
performance of some estimators in extrapolating 
vocabulary size for arbitrary sample sizes but limit 
the study to parametric estimators. Since we con-
sider both parametric and nonparametric estima-
tors here, we consider this to be the first study 
comparing a set of estimators for extrapolating vo-
cabulary size.
# Para 100 2
Estimators of vocabulary size that we compare 
can be broadly classified into two types:
# Para 102 8
1. Nonparametric estimators- here word fre-
quency information from the given sample 
alone is used to estimate the vocabulary size. 
A good survey of the state of the art is avail-
able in (Gandolfi and Sastri, 2004). In this 
paper, we compare our proposed estimator 
with the canonical estimators available in 
(Gandolfi and Sastri, 2004).
# Para 110 10
2. Parametric estimators- here a probabilistic 
model capturing the relation between ex-
pected vocabulary size and sample size is the 
estimator. Given a sample of size n, the 
sample serves to calculate the parameters of 
the model. The expected vocabulary for a 
given sample size is then determined using 
the explicit relation. The parametric esti-
mators considered in this study are (Baayen, 
2001; Baroni and Evert, 2005),
# Para 120 1
(a) Zipf-Mandelbrot estimator (ZM);
# Para 121 1
(b) finite Zipf-Mandelbrot estimator (fZM).
# Para 122 5
In addition to the above estimators we consider 
a novel non parametric estimator. It is the nonpara-
metric estimator that we propose, taking into ac-
count the characteristic feature of word frequency 
distributions, to which we will turn next.
# Para 127 1
3 Novel Estimator of Vocabulary size
# Para 128 7
We observe (Xi, ... , Xn), an i.i.d. sequence 
drawn according to a probability distribution P 
from a large, but finite, vocabulary Q. Our goal 
is in estimating the “essential” size of the vocabu-
lary Q using only the observations. In other words, 
having seen a sample of size n we wish to know, 
given another sample from the same population,
# Para 135 8
how many unseen elements we would expect to 
see. Our nonparametric estimator for the number 
of unseen elements is motivated by the character-
istic property of word frequency distributions, the 
Large Number of Rare Events (LNRE) (Baayen, 
2001). We also demonstrate that the estimator is 
strongly consistent under a natural scaling formu-
lation described in (Khmaladze, 1987).
# Para 143 1
3.1 A Scaling Formulation
# Para 144 19
Our main interest is in probability distributions P 
with the property that a large number of words in 
the vocabulary Q are unlikely, i.e., the chance any 
word appears eventually in an arbitrarily long ob-
servation is strictly between 0 and 1. The authors 
in (Baayen, 2001; Khmaladze and Chitashvili, 
1989; Wagner et al., 2006) propose a natural scal-
ing formulation to study this problem; specifically, 
(Baayen, 2001) has a tutorial-like summary of the 
theoretical work in (Khmaladze, 1987; Khmaladze 
and Chitashvili, 1989). In particular, the authors 
consider a sequence of vocabulary sets and prob-
ability distributions, indexed by the observation 
size n. Specifically, the observation (Xi, ... , Xn) 
is drawn i.i.d. from a vocabulary Qn according to 
probability Pn. If the probability of a word, say 
ω E Qn is p, then the probability that this specific 
word ω does not occur in an observation of size n 
is
# Para 163 1
(1 — p)n .
# Para 164 3
For ω to be an unlikely word, we would like this 
probability for large n to remain strictly between 
0 and 1. This implies that
# Para 167 1
, (1)
# Para 168 5
for some strictly positive constants 0 &lt; c� &lt; c� &lt; 
oc. We will assume throughout this paper that c� 
and c� are the same for every word ω E Qn. This 
implies that the vocabulary size is growing lin-
early with the observation size:
# Para 173 1
&lt; lQnl &lt; n
# Para 174 1
c� .
# Para 175 3
This model is called the LNRE zone and its appli-
cability in natural language corpora is studied in 
detail in (Baayen, 2001).
# Para 178 1
3.2 Shadows
# Para 179 2
Consider the observation string (Xi, ... , Xn) and 
let us denote the quantity of interest – the number
# Para 181 1
c�	c�
# Para 182 1
n &lt;p&lt; n
# Para 183 2
n 
c�
# Para 185 1
110
# Para 186 11
of word types in the vocabulary Stn that are not 
observed – by On. This quantity is random since 
the observation string itself is. However, we note 
that the distribution of On is unaffected if one re-
labels the words in Stn. This motivates studying 
of the probabilities assigned by Pn without refer-
ence to the labeling of the word; this is done in 
(Khmaladze and Chitashvili, 1989) via the struc-
tural distribution function and in (Wagner et al., 
2006) via the shadow. Here we focus on the latter 
description:
# Para 197 4
Definition 1 Let Xn be a random variable on Stn 
with distribution Pn. The shadow of Pn is de-
fined to be the distribution of the random variable 
Pn(I Xn}).
# Para 201 4
For the finite vocabulary situation we are con-
sidering, specifying the shadow is exactly equiv-
alent to specifying the unordered components of 
Pn, viewed as a probability vector.
# Para 205 1
3.3 Scaled Shadows Converge
# Para 206 8
We will follow (Wagner et al., 2006) and sup-
pose that the scaled shadows, the distribution of 
n • Pn (Xn), denoted by Qn converge to a distribu-
tion Q. As an example, if Pn is a uniform distribu-
tion over a vocabulary of size cn, then n • Pn (Xn) 
equals 1 c almost surely for each n (and hence it 
converges in distribution). From this convergence 
assumption we can, further, infer the following:
# Para 214 4
1. Since the probability of each word ω is lower 
and upper bounded as in Equation (1), we 
know that the distribution Qn is non-zero 
only in the range [�c, �c] .
# Para 218 4
2. The “essential” size of the vocabulary, i.e., 
the number of words of Stn on which Pn 
puts non-zero probability can be evaluated di-
rectly from the scaled shadow, scaled by n1 as
# Para 222 3
Zc�1 dQn(y).	(2)
y 
c�
# Para 225 4
Using the dominated convergence theorem, 
we can conclude that the convergence of the 
scaled shadows guarantees that the size of the 
vocabulary, scaled by 1/n, converges as well:
# Para 229 1
f y dQ(y).	(3)
# Para 230 1
3.4 Profiles and their Limits
# Para 231 2
Our goal in this paper is to estimate the size of the 
underlying vocabulary, i.e., the expression in (2),
# Para 233 3
Zc�n dQn (y) ,	(4)
y 
c�
# Para 236 8
from the observations (X1, ... , Xn). We observe 
that since the scaled shadow Qn does not de-
pend on the labeling of the words in Stn, a suf-
ficient statistic to estimate (4) from the observa-
tion (X1, ... , Xn) is theprofile of the observation: 
(ϕn1 , ... , ϕnn), defined as follows. ϕnk is the num-
ber of word types that appear exactly k times in 
the observation, for k = 1, ... , n. Observe that
# Para 244 1
Xn k=1	kn — cpk — n,
# Para 245 1
and that
# Para 246 1
V def = Xn ϕnk	(5)
# Para 247 1
k=1
# Para 248 2
is the number of observed words. Thus, the object 
of our interest is,
# Para 250 1
On = JStnJ — V.	(6)
# Para 251 1
3.5 Convergence of Scaled Profiles
# Para 252 5
One of the main results of (Wagner et al., 2006) is 
that the scaled profiles converge to a deterministic 
probability vector under the scaling model intro-
duced in Section 3.3. Specifically, we have from 
Proposition 1 of (Wagner et al., 2006):
# Para 257 1
—�0, almost surely, (7)
# Para 258 1
c�yk exp(—y) dQ(y) k = 0, 1, 2, ... .
# Para 259 1
k!
# Para 260 3
(8) 
This convergence result suggests a natural estima-
tor for On, expressed in Equation (6).
# Para 263 1
3.6 A Consistent Estimator of On
# Para 264 7
We start with the limiting expression for scaled 
profiles in Equation (7) and come up with a natu-
ral estimator for On. Our development leading to 
the estimator is somewhat heuristic and is aimed 
at motivating the structure of the estimator for the 
number of unseen words, On. We formally state 
and prove its consistency at the end of this section.
# Para 271 1
JStnJ
# Para 272 1
n
# Para 273 1
����
# Para 274 1
λk-1
# Para 275 2
kϕk
n
# Para 277 1
i
# Para 278 2
Xn 
k=1
# Para 280 1
where
# Para 281 1
λk �= I&lt;
# Para 282 1
111
# Para 283 2
It helps to write the truncated geometric series as 
a power series in y:
# Para 285 1
3.6.1 A Heuristic Derivation
# Para 286 2
Starting from (7), let us first make the approxima-
tion that
# Para 288 2
kϕk	Nλk—1, k = 1,...,n.	(9)	1 c�	�M	(1 — V
n			ℓ=0	
# Para 290 1
We now have the formal calculation
# Para 291 1
λk—1	(10)
# Para 292 1
k
# Para 293 4
�n p6e—yyk-1 
J 	dQ(y)
c	k!
N fc e—y n yk
# Para 297 2
Jk=1dQ(y) (11) 
c y k!
# Para 299 1
k=1
# Para 300 1
N
# Para 301 2
ϕnk 
n
# Para 303 1
�n
# Para 304 1
k=1
# Para 305 1
�n
# Para 306 1
k=1
# Para 307 1
1
# Para 308 1
c�
# Para 309 1
�M
# Para 310 1
ℓ=0
# Para 311 1
�M
# Para 312 2
k=0 
M
# Para 314 1
E
# Para 315 1
k=0
# Para 316 1
ℓ	ℓ
# Para 317 1
1 ( k) (—1)k (y)k c�
# Para 318 1
k=0
# Para 319 1
XM� ℓ )� (—1)k(y)k
# Para 320 1
	k	c�
# Para 321 1
ℓ=k
# Para 322 1
	(—1)k aMk yk,	(17)
# Para 323 2
1 
c�
# Para 325 1
Ic e—y
# Para 326 1
y	(ey — 1) dQ(y) (12)
# Para 327 1
�c e—y dQ(y).	(13)
# Para 328 1
Jc y
# Para 329 6
Here the approximation in Equation (10) follows 
from the approximation in Equation (9), the ap-
proximation in Equation (11) involves swapping 
the outer discrete summation with integration and 
is justified formally later in the section, the ap-
proximation in Equation (12) follows because
# Para 335 1
�n
# Para 336 1
k=1
# Para 337 5
as n —* oc, and the approximation in Equa-
tion (13) is justified from the convergence in Equa-
tion (3). Now, comparing Equation (13) with 
Equation (6), we arrive at an approximation for 
our quantity of interest:
# Para 342 3
On n	N
	c e—y dQ(y).	(14)
	6 y
# Para 345 1
The geometric series allows us to write
# Para 346 4
(1 VP 
— ,	, by E (0, 6).	(15)
c 
Approximating this infinite series by a finite sum-
# Para 350 1
mation, we have for all y E (�c, �c),
# Para 351 1
XM(k)
# Para 352 1
ℓ=k
# Para 353 5
Substituting the finite summation approximation 
in Equation 16 and its power series expression in 
Equation (17) into Equation (14) and swapping the 
discrete summation with the integral, we can con-
tinue
# Para 358 1
c
# Para 359 2
(—1)k aM f e—yyk dQ(y) 
(—1)k aMk k!λk.	(18)
# Para 361 3
Here, in Equation (18), we used the definition of 
λk from Equation (8). From the convergence in 
Equation (7), we finally arrive at our estimate:
# Para 364 1
On N �M (—1)k aMk (k + 1)! ϕk+1.	(19)
# Para 365 1
k=0
# Para 366 1
3.6.2 Consistency
# Para 367 2
Our main result is the demonstration of the consis-
tency of the estimator in Equation (19).
# Para 369 1
Theorem 1 For any ǫ &gt; 0,
# Para 370 1
���On — PM���
# Para 371 1
k=0 (—1)k aMk (k + 1)! ϕk+1
# Para 372 1
n
# Para 373 1
N JQnJ
# Para 374 1
n
# Para 375 1
yk
# Para 376 1
k!
# Para 377 1
—*ey—1,
# Para 378 1
1
# Para 379 1
=
# Para 380 1
c�
# Para 381 2
1 
y
# Para 383 1
�oo
# Para 384 1
ℓ=0
# Para 385 5
On n	N	M
		E
		k=0
		�M
		k=0
# Para 390 1
lim
# Para 391 1
n—&gt;oo
# Para 392 1
Gǫ
# Para 393 1
where we have written
# Para 394 2
aM _ 1 
k	�ck+1
# Para 396 2
1 
y
# Para 398 1
1
# Para 399 1
—c�
# Para 400 1
�M
# Para 401 1
ℓ=0
# Para 402 1
�M
# Para 403 1
(	�1 — y
# Para 404 1
1 — y )ℓ	c� 
# Para 405 2
= 
c�	y
# Para 407 1
(1 — c)M
# Para 408 1
G 	
# Para 409 1
c�
# Para 410 1
almost surely, as long as
# Para 411 3
. (16)	M&gt;	c�l�92 e + l�92 (ǫ�c)
		(20)
		l092 (C — C) — 1 — l092 (C)
# Para 414 1
112
# Para 415 1
Proof: From Equation (6), we have	Combining Equations (22) and (24), we have that,
# Para 416 1
almost surely,
# Para 417 1
On — PMk=0 (—1)k aMk (k + 1)! ϕk+1
# Para 418 3
(26) 
ForMthatsatisfyEquation(20)thistermislessthan ǫ. The proof concludes. 
—* 0, n —* oc,
# Para 421 1
—* 0, n —* oc.
# Para 422 1
This simple calculation suggests that we can re-
# Para 423 1
=
# Para 424 1
n
# Para 425 1
r. e—y	— 1:(—1)k aMk yk dQ(y). (25) )
# Para 426 2
y k=0 
1 M
# Para 428 4
On	=	JQnJ	�n	ϕk n
n	=	n	k=1	λk—1
		JQnJ	�n	k
		n	k=1	
# Para 432 1
lim
# Para 433 1
n—&gt;oo
# Para 434 2
�n	1k Ck�k — λk—1 I . (21)	Combining Equation (16) with Equation (17), we have
k=1		
# Para 436 6
The first term in the right hand side (RHS) of 
Equation (21) converges as seen in Equation (3). 
The third term in the RHS of Equation (21) con-
verges to zero, almost surely, as seen from Equa-
tion (7). The second term in the RHS of Equa-
tion (21), on the other hand,
# Para 442 1
�c e—y n yk )
# Para 443 1
	JC y	k! dQ(y) k=1
# Para 444 5
Ifc —y
	ey	(ey — 1) dQ(y), n —* oc,
c q 1&apos;dQ(y) — Ic e—y dQ(y). 
.Y	y
The monotone convergence theorem justifies the
# Para 449 2
convergence in the second step above. Thus we 
conclude that
# Para 451 1
Ic e—y dQ(y)	(22)
# Para 452 1
	=	y
# Para 453 2
almost surely. Coming to the estimator, we can 
write it as the sum of two terms:
# Para 455 1
	�M (—1)k aMk k!λk	(23)
# Para 456 1
k=0
# Para 457 6
The second term in Equation (23) above is seen to 
converge to zero almost surely as n —* oc, using 
Equation (7) and noting that M is a constant not 
depending on n. The first term in Equation (23) 
can be written as, using the definition of λk from 
Equation (8),
# Para 463 1
c	M
# Para 464 1
fe—y	(—1)k aM yk dQ(y).	(24)
# Para 465 1
k=0 
# Para 466 1
(1 — c��M
# Para 467 1
(—1)k a	k 	c 
# Para 468 1
k y —
# Para 469 2
The quantity in Equation (25) can now be upper 
bounded by, using Equation (26),
# Para 471 1
e—�c �1 — c� �M
# Para 472 1
c�	.
# Para 473 1
c�
# Para 474 1
3.7 Uniform Consistent Estimation
# Para 475 1
Oneofthemainissueswithactuallyemployingtheestimatorforthenumberofunseenelements(cf.Equation(19))isthatitinvolvesknowingtheparameter
# Para 476 1
 c
# Para 477 1
.Inpractice,thereisnonaturalwaytoobtainanyestimateonthisparameter
# Para 478 1
 c
# Para 479 2
.Itwouldbemostusefuliftherewereawaytomodifytheestimatorinawaythatitdoesnotdependontheunobservablequantity 
 c
# Para 481 1
.Inthissectionweseethatsuchamodificationispossible,whilestillretain-ingthemaintheoreticalperformanceresultofcon-sistency(cf.Theorem1).Thefirststeptoseethemodificationisinob-servingwheretheneedfor
# Para 482 1
 c
# Para 483 1
arises:itisinwritingthegeometricseriesforthefunction1y(cf.Equa-tions(15)and(16)).Ifwecouldlet
# Para 484 1
 c
# Para 485 1
alongwiththenumberofelementsMitselfdependonthesamplesizen,thenwecouldstillhavethegeo-metricseries formula. More precisely, we have
# Para 486 2
1 
y
# Para 488 1
as long as
# Para 489 1
Mn
# Para 490 1
�cn 	(27)
# Para 491 1
place
# Para 492 1
c�and
# Para 493 2
 M in the formula for the estimator (cf. 
Equation (19)) by terms that depend on n an
# Para 495 2
d sat-
isfy the condition expressed by Equation (27).
# Para 497 1
�n
# Para 498 1
k=1
# Para 499 1
λk—1
# Para 500 1
k
# Para 501 1
lim On
# Para 502 1
n—&gt;oo n
# Para 503 1
�M
# Para 504 1
+
# Para 505 1
k=0
# Para 506 1
(—1)k aM 	n k! C (k + 1) ϕk+1	/
# Para 507 1
— λk
# Para 508 1
.
# Para 509 2
1 
y
# Para 511 1
—
# Para 512 1
�M
# Para 513 1
k=0
# Para 514 1
0&lt;
# Para 515 1
c�
# Para 516 1
—
# Para 517 1
�cn
# Para 518 1
1
# Para 519 1
Mn
# Para 520 1
C1 — y
# Para 521 1
� cn
# Para 522 1
ℓ=0
# Para 523 1
�ℓ
# Para 524 1
(	l Mn
# Para 525 1
\
# Para 526 1
1y1y
# Para 527 1
 /
# Para 528 1
cn
# Para 529 1
113
# Para 530 1
4 Experiments
# Para 531 1
4.1 Corpora
# Para 532 1
In our experiments we used the following corpora:
# Para 533 1
1. The British National Corpus (BNC): A cor-
# Para 534 1
pus of about 100 million words of written and
# Para 535 1
spoken British English from the years 1975-
# Para 536 1
1994.
# Para 537 2
2. The New York Times Corpus (NYT): A cor-
pus of about 5 million words.
# Para 539 4
3. The Malayalam Corpus (MAL): A collection 
of about 2.5 million words from varied ar-
ticles in the Malayalam language from the 
Central Institute of Indian Languages.
# Para 543 4
4. The Hindi Corpus (HIN): A collection of 
about 3 million words from varied articles in 
the Hindi language also from the Central In-
stitute of Indian Languages.
# Para 547 1
4.2 Methodology
# Para 548 13
We would like to see how well our estimator per-
forms in terms of estimating the number of unseen 
elements. A natural way to study this is to ex-
pose only half of an existing corpus to be observed 
and estimate the number of unseen elements (as-
suming the the actual corpus is twice the observed 
size). We can then check numerically how well 
our estimator performs with respect to the “true” 
value. We use a subset (the first 10%, 20%, 30%, 
40% and 50%) of the corpus as the observed sam-
ple to estimate the vocabulary over twice the sam-
ple size. The following estimators have been com-
pared.
# Para 561 4
Nonparametric: Along with our proposed esti-
mator (in Section 3), the following canonical es-
timators available in (Gandolfi and Sastri, 2004) 
and (Baayen, 2001) are studied.
# Para 565 8
1. Our proposed estimator On (cf. Section 3): 
since the estimator is rather involved we con-
sider only small values of M (we see empir-
ically that the estimator converges for very 
small values of M itself) and choose c� = M. 
This allows our estimator for the number of 
unseen elements to be of the following form, 
for different values of M:
# Para 573 4
M		On
1		2 (ϕ1 — ϕ2)
2	32	(ϕ1 — ϕ2) + 4ϕ3
3	43(ϕ1	—ϕ2)+9(ϕ3—3)
# Para 577 2
Using this, the estimator of the true vocabu-
lary size is simply,
# Para 579 1
	On + V.	(28)
# Para 580 1
Here (cf. Equation (5))
# Para 581 1
	V= �n ϕnk.	(29)
# Para 582 1
k=1
# Para 583 1
In the simulations below, we have considered
# Para 584 4
M large enough until we see numerical con-
vergence of the estimators: in all the cases, 
no more than a value of 4 is needed for M. 
For the English corpora, very small values of
# Para 588 5
M suffice – in particular, we have considered 
the average of the first three different estima-
tors (corresponding to the first three values 
of M). For the non-English corpora, we have 
needed to consider M = 4.
# Para 593 1
2. Gandolfi-Sastri estimator,
# Para 594 1
VGS def 	n (V + ϕ1γ2) ,	(30)
# Para 595 1
n—ϕ1
# Para 596 1
where
# Para 597 1
γ2 = ϕ1 —n—V +
# Para 598 1
2n
# Para 599 1
V/5n2 + 2n(V — 3ϕ1) + (V — ϕ1)2
# Para 600 1
	2n	;
# Para 601 1
3. Chao estimator,
# Para 602 1
2
# Para 603 1
VCh. def V + ϕ2
# Para 604 1
	;2(31)
# Para 605 1
4. Good-Turing estimator,
# Para 606 3
VGTdef	V
= 
(
# Para 609 1
5. “Simplistic” estimator,
# Para 610 1
def	nnew 
# Para 611 1
VSMpl def = V	;(33)
# Para 612 1
n
# Para 613 3
here the supposition is that the vocabulary 
size scales linearly with the sample size (here 
nnew is the new sample size);
# Para 616 1
6. Baayen estimator,
# Para 617 1
VByn def V + l r ϕn11
# Para 618 1
J
# Para 619 5
here the supposition is that the vocabulary 
growth rate at the observed sample size is 
given by the ratio of the number of hapax 
legomena to the sample size (cf. (Baayen, 
2001) pp. 50).
# Para 624 1
1 — ϕ1);	(32)
# Para 625 1
n
# Para 626 1
nnew;	(34)
# Para 627 1
114
# Para 628 1
BNC	NYT	Malayalam	Hindi
# Para 629 1
Our GT ZM	Our GT ZM	Our GT ZM	Our GT ZM
# Para 630 1
% error of top 2 and Good−Turing estimates compared
# Para 631 8
Figure 1: Comparison of error estimates of the 2 
best estimators-ours and the ZM, with the Good- 
Turing estimator using 10% sample size of all the 
corpora. A bar with a positive height indicates 
and overestimate and that with a negative height 
indicates and underestimate. Our estimator out-
performs ZM. Good-Turing estimator widely un-
derestimates vocabulary size.
# Para 639 11
Parametric: Parametric estimators use the ob-
servations to first estimate the parameters. Then 
the corresponding models are used to estimate the 
vocabulary size over the larger sample. Thus the 
frequency spectra of the observations are only in-
directly used in extrapolating the vocabulary size. 
In this study we consider state of the art paramet-
ric estimators, as surveyed by (Baroni and Evert, 
2005). We are aided in this study by the availabil-
ity of the implementations provided by the Z i p f R 
package and their default settings.
# Para 650 1
5 Results and Discussion
# Para 651 4
The performance of the different estimators as per-
centage errors of the true vocabulary size using 
different corpora are tabulated in tables 1-4. We 
now summarize some important observations.
# Para 655 6
•	From the Figure 1, we see that our estima-
tor compares quite favorably with the best of 
the state of the art estimators. The best of the 
state of the art estimator is a parametric one 
(ZM), while ours is a nonparametric estima-
tor.
# Para 661 5
•	In table 1 and table 2 we see that our esti-
mate is quite close to the true vocabulary, at 
all sample sizes. Further, it compares very fa-
vorably to the state of the art estimators (both 
parametric and nonparametric).
# Para 666 2
•	Again, on the two non-English corpora (ta-
bles 3 and 4) we see that our estimator com-
# Para 668 3
pares favorably with the best estimator of vo-
cabulary size and at some sample sizes even 
surpasses it.
# Para 671 6
•	Our estimator has theoretical performance 
guarantees and its empirical performance is 
comparable to that of the state of the art es-
timators. However, this performance comes 
at a very small fraction of the computational 
cost of the parametric estimators.
# Para 677 4
•	The state of the art nonparametric Good- 
Turing estimator wildly underestimates the 
vocabulary; this is true in each of the four 
corpora studied and at all sample sizes.
# Para 681 1
6 Conclusion
# Para 682 15
In this paper, we have proposed a new nonpara-
metric estimator of vocabulary size that takes into 
account the LNRE property of word frequency 
distributions and have shown that it is statistically 
consistent. We then compared the performance of 
the proposed estimator with that of the state of the 
art estimators on large corpora. While the perfor-
mance of our estimator seems favorable, we also 
see that the widely used classical Good-Turing 
estimator consistently underestimates the vocabu-
lary size. Although as yet untested, with its com-
putational simplicity and favorable performance, 
our estimator may serve as a more reliable alter-
native to the Good-Turing estimator for estimating 
vocabulary sizes.
# Para 697 1
Acknowledgments
# Para 698 3
This research was partially supported by Award 
IIS-0623805 from the National Science Founda-
tion.
# Para 701 1
References
# Para 702 2
R. H. Baayen. 2001. Word Frequency Distributions, 
Kluwer Academic Publishers.
# Para 704 5
Marco Baroni and Stefan Evert. 2001. “Testing the ex-
trapolation quality of word frequency models”, Pro-
ceedings of Corpus Linguistics , volume 1 of The 
Corpus Linguistics Conference Series, P. Danielsson 
and M. Wagenmakers (eds.).
# Para 709 4
J. Bunge and M. Fitzpatrick. 1993. “Estimating the 
number of species: a review”, Journal of the Amer-
ican Statistical Association, Vol. 88(421), pp. 364- 
373.
# Para 713 1
115
# Para 714 8
Sample	True value	% error w.r.t the true value							
(% of corpus)									
		Our	GT	ZM	fZM	Smpl	Byn	Chao	GS
10	153912	1	-27	-4	-8	46	23	8	-11
20	220847	-3	-30	-9	-12	39	19	4	-15
30	265813	-2	-30	-9	-11	39	20	6	-15
40	310351	1	-29	-7	-9	42	23	9	-13
50	340890	2	-28	-6	-8	43	24	10	-12
# Para 722 3
Table 1: Comparison of estimates of vocabulary size for the BNC corpus as percentage errors w.r.t the 
true value. A negative value indicates an underestimate. Our estimator outperforms the other estimators 
at all sample sizes.
# Para 725 8
Sample	True value	% error w.r.t the true value							
(% of corpus)									
		Our	GT	ZM	fZM	Smpl	Byn	Chao	GS
10	37346	1	-24	5	-8	48	28	4	-8
20	51200	-3	-26	0	-11	46	22	-1	-11
30	60829	-2	-25	1	-10	48	23	1	-10
40	68774	-3	-25	0	-10	49	21	-1	-11
50	75526	-2	-25	0	-10	50	21	0	-10
# Para 733 3
Table 2: Comparison of estimates of vocabulary size for the NYT corpus as percentage errors w.r.t the 
true value. A negative value indicates an underestimate. Our estimator compares favorably with ZM and 
Chao.
# Para 736 8
Sample	True value	% error w.r.t the true value							
(% of corpus)									
		Our	GT	ZM	fZM	Smpl	Byn	Chao	GS
10	146547	-2	-27	-5	-10	9	34	82	-2
20	246723	8	-23	4	-2	19	47	105	5
30	339196	4	-27	0	-5	16	42	93	-1
40	422010	5	-28	1	-4	17	43	95	-1
50	500166	5	-28	1	-4	18	44	94	-2
# Para 744 3
Table 3: Comparison of estimates of vocabulary size for the Malayalam corpus as percentage errors 
w.r.t the true value. A negative value indicates an underestimate. Our estimator compares favorably with 
ZM and GS.
# Para 747 8
Sample	True value	% error w.r.t the true value							
(% of corpus)									
		Our	GT	ZM	fZM	Smpl	Byn	Chao	GS
10	47639	-2	-34	-4	-9	25	32	31	-12
20	71320	7	-30	2	-1	34	43	51	-7
30	93259	2	-33	-1	-5	30	38	42	-10
40	113186	0	-35	-5	-7	26	34	39	-13
50	131715	-1	-36	-6	-8	24	33	40	-14
# Para 755 3
Table 4: Comparison of estimates of vocabulary size for the Hindi corpus as percentage errors w.r.t the 
true value. A negative value indicates an underestimate. Our estimator outperforms the other estimators 
at certain sample sizes.
# Para 758 1
116
# Para 759 4
A. Gandolfi and C. C. A. Sastri. 2004. “Nonparamet-
ric Estimations about Species not Observed in a 
Random Sample”, Milan Journal of Mathematics, 
Vol. 72, pp. 81-105.
# Para 763 4
E. V. Khmaladze. 1987. “The statistical analysis of 
large number of rare events”, Technical Report, De-
partment ofMathematics and Statistics., CWI, Am-
sterdam, MS-R8804.
# Para 767 4
E. V. Khmaladze and R. J. Chitashvili. 1989. “Statis-
tical analysis of large number of rate events and re-
lated problems”, Probability theory and mathemati-
cal statistics (Russian), Vol. 92, pp. 196-245.
# Para 771 4
.P. Santhanam, A. Orlitsky, and K. Viswanathan, “New 
tricks for old dogs: Large alphabet probability es-
timation”, in Proc. 2007 IEEE Information Theory 
Workshop, Sept. 2007, pp. 638–643.
# Para 775 3
A. B. Wagner, P. Viswanath and S. R. Kulkarni. 2006. 
“Strong Consistency of the Good-Turing estimator”, 
IEEE Symposium on Information Theory, 2006.
# Para 778 1
117
