# Para 0 1 title
Heterogeneous Transfer Learning for Image Clustering via the Social Web
# Para 1 3 author
Qiang Yang
Hong Kong University of Science and Technology, Clearway Bay, Kowloon, Hong Kong
qyang@cs.ust.hk
# Para 4 1 author
Yuqiang Chen	Gui-Rong Xue	Wenyuan Dai	Yong Yu
# Para 5 1 affiliation
Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200240, China
# Para 6 1 email
{yuqiangchen,grxue,dwyak,yyu}@apex.sjtu.edu.cn
# Para 7 1 sectionHeader
Abstract
# Para 8 26 bodyText
In this paper, we present a new learning 
scenario, heterogeneous transfer learn-
ing, which improves learning performance 
when the data can be in different feature 
spaces and where no correspondence be-
tween data instances in these spaces is pro-
vided. In the past, we have classified Chi-
nese text documents using English train-
ing data under the heterogeneous trans-
fer learning framework. In this paper, 
we present image clustering as an exam-
ple to illustrate how unsupervised learning 
can be improved by transferring knowl-
edge from auxiliary heterogeneous data 
obtained from the social Web. Image 
clustering is useful for image sense dis-
ambiguation in query-based image search, 
but its quality is often low due to image- 
data sparsity problem. We extend PLSA 
to help transfer the knowledge from social 
Web data, which have mixed feature repre-
sentations. Experiments on image-object 
clustering and scene clustering tasks show 
that our approach in heterogeneous trans-
fer learning based on the auxiliary data is 
indeed effective and promising.
# Para 34 1 sectionHeader
1 Introduction
# Para 35 11 bodyText
Traditional machine learning relies on the avail-
ability of a large amount of data to train a model, 
which is then applied to test data in the same 
feature space. However, labeled data are often 
scarce and expensive to obtain. Various machine 
learning strategies have been proposed to address 
this problem, including semi-supervised learning 
(Zhu, 2007), domain adaptation (Wu and Diet-
terich, 2004; Blitzer et al., 2006; Blitzer et al., 
2007; Arnold et al., 2007; Chan and Ng, 2007; 
Daume, 2007; Jiang and Zhai, 2007; Reichart
# Para 46 15 bodyText
and Rappoport, 2007; Andreevskaia and Bergler, 
2008), multi-task learning (Caruana, 1997; Re-
ichart et al., 2008; Arnold et al., 2008), self-taught 
learning (Raina et al., 2007), etc. A commonality 
among these methods is that they all require the 
training data and test data to be in the same fea-
ture space. In addition, most of them are designed 
for supervised learning. However, in practice, we 
often face the problem where the labeled data are 
scarce in their own feature space, whereas there 
may be a large amount of labeled heterogeneous 
data in another feature space. In such situations, it 
would be desirable to transfer the knowledge from 
heterogeneous data to domains where we have rel-
atively little training data available.
# Para 61 26 bodyText
To learn from heterogeneous data, researchers 
have previously proposed multi-view learning 
(Blum and Mitchell, 1998; Nigam and Ghani, 
2000) in which each instance has multiple views in 
different feature spaces. Different from previous 
works, we focus on the problem of heterogeneous 
transfer learning, which is designed for situation 
when the training data are in one feature space 
(such as text), and the test data are in another (such 
as images), and there may be no correspondence 
between instances in these spaces. The type of 
heterogeneous data can be very different, as in the 
case of text and image. To consider how hetero-
geneous transfer learning relates to other types of 
learning, Figure 1 presents an intuitive illustration 
of four learning strategies, including traditional 
machine learning, transfer learning across differ-
ent distributions, multi-view learning and hetero-
geneous transfer learning. As we can see, an 
important distinguishing feature of heterogeneous 
transfer learning, as compared to other types of 
learning, is that more constraints on the problem 
are relaxed, such that data instances do not need to 
correspond anymore. This allows, for example, a 
collection of Chinese text documents to be classi-
fied using another collection of English text as the
# Para 87 1 page
1
# Para 88 1 note
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1–9,
# Para 89 1 note
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
# Para 90 2 bodyText
training data (c.f. (Ling et al., 2008) and Section 
2.1).
# Para 92 30 bodyText
In this paper, we will give an illustrative exam-
ple of heterogeneous transfer learning to demon-
strate how the task of image clustering can ben-
efit from learning from the heterogeneous social 
Web data. A major motivation of our work is 
Web-based image search, where users submit tex-
tual queries and browse through the returned result 
pages. One problem is that the user queries are of-
ten ambiguous. An ambiguous keyword such as 
“Apple” might retrieve images of Apple comput-
ers and mobile phones, or images of fruits. Im-
age clustering is an effective method for improv-
ing the accessibility of image search result. Loeff 
et al. (2006) addressed the image clustering prob-
lem with a focus on image sense discrimination. 
In their approach, images associated with textual 
features are used for clustering, so that the text 
and images are clustered at the same time. Specif-
ically, spectral clustering is applied to the distance 
matrix built from a multimodal feature set associ-
ated with the images to get a better feature repre-
sentation. This new representation contains both 
image and text information, with which the per-
formance of image clustering is shown to be im-
proved. A problem with this approach is that when 
images contained in the Web search results are 
very scarce and when the textual data associated 
with the images are very few, clustering on the im-
ages and their associated text may not be very ef-
fective.
# Para 122 19 bodyText
Different from these previous works, in this pa-
per, we address the image clustering problem as 
a heterogeneous transfer learning problem. We 
aim to leverage heterogeneous auxiliary data, so-
cial annotations, etc. to enhance image cluster-
ing performance. We observe that the World Wide 
Web has many annotated images in Web sites such 
as Flickr (http : / /www. flickr . com), which 
can be used as auxiliary information source for 
our clustering task. In this work, our objective 
is to cluster a small collection of images that we 
are interested in, where these images are not suf-
ficient for traditional clustering algorithms to per-
form well due to data sparsity and the low level of 
image features. We investigate how to utilize the 
readily available socially annotated image data on 
the Web to improve image clustering. Although 
these auxiliary data may be irrelevant to the im-
ages to be clustered and cannot be directly used 
# Para 141 4 bodyText
to solve the data sparsity problem, we show that 
they can still be used to estimate a good latentfea-
ture representation, which can be used to improve 
image clustering.
# Para 145 1 sectionHeader
2 Related Works
# Para 146 2 subsectionHeader
2.1 Heterogeneous Transfer Learning 
Between Languages
# Para 148 6 bodyText
In this section, we summarize our previous work 
on cross-language classification as an example of 
heterogeneous transfer learning. This example 
is related to our image clustering problem be-
cause they both rely on data from different feature 
spaces.
# Para 154 11 bodyText
As the World Wide Web in China grows rapidly, 
it has become an increasingly important prob-
lem to be able to accurately classify Chinese Web 
pages. However, because the labeled Chinese Web 
pages are still not sufficient, we often find it diffi-
cult to achieve high accuracy by applying tradi-
tional machine learning algorithms to the Chinese 
Web pages directly. Would it be possible to make 
the best use of the relatively abundant labeled En-
glish Web pages for classifying the Chinese Web 
pages?
# Para 165 11 bodyText
To answer this question, in (Ling et al., 2008), 
we developed a novel approach for classifying the 
Web pages in Chinese using the training docu-
ments in English. In this subsection, we give a 
brief summary of this work. The problem to be 
solved is: we are given a collection of labeled 
English documents and a large number of unla-
beled Chinese documents. The English and Chi-
nese texts are not aligned. Our objective is to clas-
sify the Chinese documents into the same label 
space as the English data.
# Para 176 15 bodyText
Our key observation is that even though the data 
use different text features, they may still share 
many of the same semantic information. What we 
need to do is to uncover this latent semantic in-
formation by finding out what is common among 
them. We did this in (Ling et al., 2008) by us-
ing the information bottleneck theory (Tishby et 
al., 1999). In our work, we first translated the 
Chinese document into English automatically us-
ing some available translation software, such as 
Google translate. Then, we encoded the training 
text as well as the translated target text together, 
in terms of the information theory. We allowed all 
the information to be put through a ‘bottleneck’ 
and be represented by a limited number of code-
# Para 191 1 page
2
# Para 192 2 figureCaption
Figure 1: An intuitive illustration of different kinds learning strategies using classification/clustering of 
image apple and banana as the example.
# Para 194 14 bodyText
words (i.e. labels in the classification problem). 
Finally, information bottleneck was used to main-
tain most of the common information between the 
two data sources, and discard the remaining irrel-
evant information. In this way, we can approxi-
mate the ideal situation where similar training and 
translated test pages shared in the common part are 
encoded into the same codewords, and are thus as-
signed the correct labels. In (Ling et al., 2008), we 
experimentally showed that heterogeneous trans-
fer learning can indeed improve the performance 
of cross-language text classification as compared 
to directly training learning models (e.g., Naive 
Bayes or SVM) and testing on the translated texts.
# Para 208 1 subsectionHeader
2.2 Other Works in Transfer Learning
# Para 209 15 bodyText
In the past, several other works made use of trans-
fer learning for cross-feature-space learning. Wu 
and Oard (2008) proposed to handle the cross- 
language learning problem by translating the data 
into a same language and applying kNN on the 
latent topic space for classification. Most learning 
algorithms for dealing with cross-language hetero-
geneous data require a translator to convert the 
data to the same feature space. For those data that 
are in different feature spaces where no transla-
tor is available, Davis and Domingos (2008) pro-
posed a Markov-logic-based transfer learning al-
gorithm, which is called deep transfer, for trans-
ferring knowledge between biological domains 
and Web domains. Dai et al. (2008a) proposed
# Para 224 5 bodyText
a novel learning paradigm, known as translated 
learning, to deal with the problem of learning het-
erogeneous data that belong to quite different fea-
ture spaces by using a risk minimization frame-
work.
# Para 229 1 subsectionHeader
2.3 Relation to PLSA
# Para 230 12 bodyText
Our work makes use of PLSA. Probabilistic la-
tent semantic analysis (PLSA) is a widely used 
probabilistic model (Hofmann, 1999), and could 
be considered as a probabilistic implementation of 
latent semantic analysis (LSA) (Deerwester et al., 
1990). An extension to PLSA was proposed in 
(Cohn and Hofmann, 2000), which incorporated 
the hyperlink connectivity in the PLSA model by 
using a joint probabilistic model for connectivity 
and content. Moreover, PLSA has shown a lot 
of applications ranging from text clustering (Hof-
mann, 2001) to image analysis (Sivic et al., 2005).
# Para 242 1 subsectionHeader
2.4 Relation to Clustering
# Para 243 10 bodyText
Compared to many previous works on image clus-
tering, we note that traditional image cluster-
ing is generally based on techniques such as K- 
means (MacQueen, 1967) and hierarchical clus-
tering (Kaufman and Rousseeuw, 1990). How-
ever, when the data are sparse, traditional clus-
tering algorithms may have difficulties in obtain-
ing high-quality image clusters. Recently, several 
researchers have investigated how to leverage the 
auxiliary information to improve target clustering
# Para 253 1 page
3
# Para 254 1 equation
P(zIv)	P(fIz)
# Para 255 4 bodyText
performance, such as supervised clustering (Fin-
ley and Joachims, 2005), semi-supervised cluster-
ing (Basu et al., 2004), self-taught clustering (Dai 
et al., 2008b), etc.
# Para 259 2 sectionHeader
3 Image Clustering with Annotated 
Auxiliary Data
# Para 261 18 bodyText
In this section, we present our annotation-based 
probabilistic latent semantic analysis algorithm 
(aPLSA), which extends the traditional PLSA 
model by incorporating annotated auxiliary im-
age data. Intuitively, our algorithm aPLSA per-
forms PLSA analysis on the target images, which 
are converted to an image instance-to-feature co- 
occurrence matrix. At the same time, PLSA is 
also applied to the annotated image data from so-
cial Web, which is converted into a text-to-image-
feature co-occurrence matrix. In order to unify 
those two separate PLSA models, these two steps 
are done simultaneously with common latent vari-
ables used as a bridge linking them. Through 
these common latent variables, which are now 
constrained by both target image data and auxil-
iary annotation data, a better clustering result is 
expected for the target data.
# Para 279 2 subsectionHeader
3.1 Probabilistic Latent Semantic Analysis 
Let F = { fi}!Fi be an image feature space, and
# Para 281 3 bodyText
V = {vi}iv11 be the image data set. Each image 
vi E V is represented by a bag-of-features {f I f E 
vi A f E F}.
# Para 284 6 bodyText
Based on the image data set V, we can esti-
mate an image instance-to-feature co-occurrence 
matrix AIVI x I FI E RIVIx IFI, where each element 
AiT (1 G i G IV I and 1 G j G IF I) in the matrix 
A is the frequency of the feature fT appearing in 
the instance vi.
# Para 290 7 bodyText
Let W = {wi } I WI i��be a text feature space. The 
annotated image data allow us to obtain the co- 
occurrence information between images v and text 
features w E W. An example of annotated im-
age data is the Flickr (http : / /www. f l ickr . 
com), which is a social Web site containing a large 
number of annotated images.
# Para 297 7 bodyText
By extracting image features from the annotated 
images v, we can estimate a text-to-image fea-
ture co-occurrence matrix B I WIxIFI E RIW IxIFI, 
where each element BiT (1 G i G I W I and 
1 G j G IFI) in the matrix B is the frequency 
of the text feature wi and the image feature fT oc-
curring together in the annotated image data set.
# Para 304 2 bodyText
Figure 2: Graphical model representation of PLSA 
model.
# Para 306 3 bodyText
Let Z = {ziffl be the latent variable set in our 
aPLSA model. In clustering, each latent variable 
zi E Z corresponds to a certain cluster.
# Para 309 3 bodyText
Our objective is to estimate a clustering func-
tion g : V H Z with the help of the two co- 
occurrence matrices A and B as defined above.
# Para 312 10 bodyText
To formally introduce the aPLSA model, we 
start from the probabilistic latent semantic anal-
ysis (PLSA) (Hofmann, 1999) model. PLSA is 
a probabilistic implementation of latent seman-
tic analysis (LSA) (Deerwester et al., 1990). In 
our image clustering task, PLSA decomposes the 
instance-feature co-occurrence matrix A under the 
assumption of conditional independence of image 
instances V and image features F, given the latent 
variables Z.
# Para 322 1 equation
P(f Iv) = 1: P(f I z)P(zI v).	(1)
# Para 323 1 equation
ZEZ
# Para 324 2 bodyText
The graphical model representation of PLSA is 
shown in Figure 2.
# Para 326 2 bodyText
Based on the PLSA model, the log-likelihood can 
be defined as:
# Para 328 1 equation
1: L =
# Para 329 1 equation
i 1:AiT
# Para 330 1 equation
T ET, AiT, log P(fT I vi)	(2)
# Para 331 1 bodyText
where AIVI x IFI E RIVI xIFI is the image instance-
# Para 332 1 bodyText
feature co-occurrence matrix. The term : A�j
# Para 333 1 bodyText
j, Azj/
# Para 334 3 bodyText
in Equation (2) is a normalization term ensuring 
each image is giving the same weight in the log- 
likelihood.
# Para 337 5 bodyText
Using EM algorithm (Dempster et al., 1977), 
which locally maximizes the log-likelihood of 
the PLSA model (Equation (2)), the probabilities 
P(f I z) and P(zI v) can be estimated. Then, the 
clustering function is derived as
# Para 342 1 equation
g(v) = argmax P(zIv).	(3)
# Para 343 1 equation
ZEZ
# Para 344 3 bodyText
Due to space limitation, we omit the details for the 
PLSA model, which can be found in (Hofmann, 
1999).
# Para 347 1 subsectionHeader
3.2 aPLSA: Annotation-based PLSA
# Para 348 2 bodyText
In this section, we consider how to incorporate 
a large number of socially annotated images in a
# Para 350 1 equation
V	Z	F
# Para 351 1 page
4
# Para 352 2 figureCaption
Figure 3: Graphical model representation of 
aPLSA model.
# Para 354 6 bodyText
unified PLSA model for the purpose of utilizing 
the correlation between text features and image 
features. In the auxiliary data, each image has cer-
tain textual tags that are attached by users. The 
correlation between text features and image fea-
tures can be formulated as follows.
# Para 360 1 equation
P(f Iw) = X P(f I z)P(zI w). (4)
# Para 361 1 equation
ZEZ
# Para 362 12 bodyText
It is clear that Equations (1) and (4) share a same 
term P(f I z). So we design a new PLSA model by 
joining the probabilistic model in Equation (1) and 
the probabilistic model in Equation (4) into a uni-
fied model, as shown in Figure 3. In Figure 3, the 
latent variables Z depend not only on the corre-
lation between image instances V and image fea-
tures F, but also the correlation between text fea-
tures W and image features F. Therefore, the aux-
iliary socially-annotated image data can be used 
to help the target image clustering performance by 
estimating good set of latent variables Z.
# Para 374 4 bodyText
Based on the graphical model representation in 
Figure 3, we derive the log-likelihood objective 
function, in a similar way as in (Cohn and Hof-
mann, 2000), as follows 
# Para 378 4 bodyText
text-to-image occurrence matrix B. In this case, 
the aPLSA model degenerates to the traditional 
PLSA model. Therefore, aPLSA is an extension 
to the PLSA model.
# Para 382 6 bodyText
Now, the objective is to maximize the log- 
likelihood L of the aPLSA model in Equation (5). 
Then we apply the EM algorithm (Dempster et 
al., 1977) to estimate the conditional probabilities 
P(f I z), P(zI w) and P(zI v) with respect to each 
dependence in Figure 3 as follows.
# Para 388 5 listItem
•	E-Step: calculate the posterior probability of 
each latent variable z given the observation 
of image features f, image instances v and 
text features w based on the old estimate of 
P(f I z), P(zI w) and P(zI v):
# Para 393 1 equation
PIvi,fj) =	P(fjIzk)P(zkIvi)
# Para 394 1 equation
(zk
# Para 395 1 equation
P(zkI wl, fj) =	P(fjIzk)P(zkIwl)
# Para 396 1 equation
Pk, P(fjIzk,)P(zk,Iwl)
# Para 397 1 equation
(7)
# Para 398 2 listItem
•	M-Step: re-estimates conditional probabili-
ties P(zkIvi) and P(zkI wl):
# Para 400 1 equation
PAAP(zkIvi,fj) (8)
# Para 401 1 equation
PBlBlj, P(zk I wl , fj) (9)
# Para 402 3 bodyText
and conditional probability P(fj I zk ), which 
is a mixture portion of posterior probability 
of latent variables
# Para 405 1 equation
W
# Para 406 1 equation
V
# Para 407 1 equation
Z	F
# Para 408 1 equation
P(fIz)
# Para 409 1 equation
P
# Para 410 1 equation
k, P(fj Izk,)P(zk, I vi)
# Para 411 1 equation
(6)
# Para 412 1 equation
XP(zkIvi) =
# Para 413 1 equation
j
# Para 414 1 equation
XP(zkIwl) =
# Para 415 1 equation
j
# Para 416 1 equation
P(fjIzk) a AX
# Para 417 1 equation
i
# Para 418 1 equation
L=X
# Para 419 1 equation
j
# Para 420 1 equation
P
# Para 421 1 equation
j, Blj,
# Para 422 1 equation
X
# Para 423 1 equation
+(1 — A)
# Para 424 1 equation
l
# Para 425 1 equation
+(1—A)X
# Para 426 1 equation
l
# Para 427 2 equation
Aij  P( zkI vi, fj) 
Pj,Aij,
# Para 429 1 equation
Blj P(zkIwl,fj)
# Para 430 1 equation
(10)
# Para 431 1 equation
�
# Para 432 1 equation
XAij
# Para 433 1 equation
A i Pj, Aij, log P(fj I vi)
# Para 434 1 equation
�
# Para 435 1 equation
Blj	log P(fj I wl )
# Para 436 1 equation
Pj, Blj,
# Para 437 1 equation
(5)
# Para 438 5 bodyText
where AIVIxIFI E RIVIxIFI is the image instance- 
feature co-occurrence matrix, and BIW I xIFI E 
RIWIxIFI is the text-to-image feature-level co- 
occurrence matrix. Similar to Equation (2), 
PAij and PB`j in Equation (5) are the nor-
# Para 443 1 bodyText
ij,
# Para 444 1 bodyText
malization terms to prevent imbalanced cases.
# Para 445 4 bodyText
Furthermore, A acts as a trade-off parameter be-
tween the co-occurrence matrices A and B. In 
the extreme case when A = 1, the log-likelihood 
objective function ignores all the biases from the
# Para 449 2 bodyText
Finally, the clustering function for a certain im-
age v is
# Para 451 1 equation
g(v) = argmax P(zIv).	(11)
# Para 452 1 equation
ZEZ
# Para 453 7 bodyText
From the above equations, we can derive 
our annotation-based probabilistic latent semantic 
analysis (aPLSA) algorithm. As shown in Algo-
rithm 1, aPLSA iteratively performs the E-Step 
and the M-Step in order to seek local optimal 
points based on the objective function L in Equa-
tion (5).
# Para 460 1 page
5
# Para 461 2 construct
Algorithm 1 Annotation-based PLSA Algorithm 
(aPLSA)
# Para 463 2 construct
Input: The V-F co-occurrence matrix A and W- 
F co-occurrence matrix B.
# Para 465 3 construct
Output: A clustering (partition) function g : V H
i, which maps an image instance v E V to a latent 
variable z E i.
# Para 468 2 listItem
1: Initial i so that IiI equals the number clus-
ters desired.
# Para 470 1 listItem
2: Initialize P(zIv), P(zIw), P(f Iz) randomly.
# Para 471 3 listItem
3: while the change of L in Eq. (5) between two 
sequential iterations is greater than a prede-
fined threshold do
# Para 474 2 listItem
4: E-Step: Update P(zIv, f) and P(zIw, f) 
based on Eq. (6) and (7) respectively.
# Para 476 3 listItem
5: M-Step: Update P(zIv), P(zIw) and 
P(f Iz) based on Eq. (8), (9) and (10) re-
spectively.
# Para 479 1 listItem
6: end while
# Para 480 1 listItem
7: for all v in V do
# Para 481 1 listItem
8:	g(v) +— argmaxP(zIv).
# Para 482 1 listItem
9: end for
# Para 483 1 listItem
10: Return g.
# Para 484 1 sectionHeader
4 Experiments
# Para 485 5 bodyText
In this section, we empirically evaluate the aPLSA 
algorithm together with some state-of-art base-
line methods on two widely used image corpora, 
to demonstrate the effectiveness of our algorithm 
aPLSA.
# Para 490 1 subsectionHeader
4.1 Data Sets
# Para 491 18 bodyText
In order to evaluate the effectiveness of our algo-
rithm aPLSA, we conducted experiments on sev-
eral data sets generated from two image corpora, 
Caltech-256 (Griffin et al., 2007) and the fifteen- 
scene (Lazebnik et al., 2006). The Caltech-256 
data set has 256 image objective categories, rang-
ing from animals to buildings, from plants to au-
tomobiles, etc. The fifteen-scene data set con-
tains 15 scenes such as store and forest. 
From these two corpora, we randomly generated 
eleven image clustering tasks, including seven 2- 
way clustering tasks, two 4-way clustering task, 
one 5-way clustering task and one 8-way cluster-
ing task. The detailed descriptions for these clus-
tering tasks are given in Table 1. In these tasks, 
b i 7 and o c t 1 were generated from fifteen-scene 
data set, and the rest were from Caltech-256 data 
set.
# Para 509 1 table
DATA SET INVOLVED CLASSES	DATA SIZE
# Para 510 18 table
bi1	skateboard, airplanes	102,800
bi2	billiards, mars	278, 155
bi3	cd, greyhound	102,	94
bi4	electric-guitar, snake	122,112
bi5	calculator, dolphin	100,106
bi6	mushroom, teddy-bear	202,	99
bi7	MIThighway, livingroom	260,289
quad1	calculator,	diamond-ring,	dolphin,	100, 118, 106,116
	microscope	
quad2	bonsai, comet, frog, saddle	122, 120, 115, 110
quint1	frog, kayak, bear, jesus-christ,watch115,102,101,87,	201
oct1	MIThighway,	MITmountain,	260, 374, 210, 360,
	kitchen, MITcoast, PARoffice, MIT- tallbuilding, livingroom, bedroom	215, 356, 289, 216
tune1	coin, horse	123,270
tune2	socks, spider	111,106
tune3	galaxy, snowmobile	80,112
tune4	dice, fern	98,110
tune5	backpack, lightning, mandolin, swan	151, 136,	93,114
# Para 528 5 tableCaption
Table 1: The descriptions of all the image clus-
tering tasks used in our experiment. Among 
these data sets, b i 7 and o c t 1 were generated 
from fifteen-scene data set, and the rest were from 
Caltech-256 data set.
# Para 533 6 bodyText
To empirically investigate the parameter A and 
the convergence of our algorithm aPLSA, we gen-
erated five more date sets as the development sets. 
The detailed description of these five development 
sets, namely tune1 to tune5 is listed in Table 1 
as well.
# Para 539 12 bodyText
The auxiliary data were crawled from the Flickr 
(http://www.flickr.com/) web site dur-
ing August 2007. Flickr is an internet community 
where people share photos online and express their 
opinions as social tags (annotations) attached to 
each image. From Flicker, we collected 19,959 
images and 91,719 related annotations, among 
which 2,600 words are distinct. Based on the 
method described in Section 3, we estimated the 
co-occurrence matrix B between text features and 
image features. This co-occurrence matrix B was 
used by all the clustering tasks in our experiments.
# Para 551 12 bodyText
For data preprocessing, we adopted the bag-of-
features representation of images (Li and Perona, 
2005) in our experiments. Interesting points were 
found in the images and described via the SIFT 
descriptors (Lowe, 2004). Then, the interesting 
points were clustered to generate a codebook to 
form an image feature space. The size of code- 
book was set to 2, 000 in our experiments. Based 
on the codebook, which serves as the image fea-
ture space, each image can be represented as a cor-
responding feature vector to be used in the next 
step.
# Para 563 1 bodyText
To set our evaluation criterion, we used the
# Para 564 1 page
6
# Para 565 14 table
Data Set	KMeancombined		PLSA		STC	aPLSA
	separate		separate	combined		
bi1	0.645±0.064	0.548±0.031	0.544±0.074	0.537±0.033	0.586±0.139	0.482±0.062
bi2	0.687±0.003	0.662±0.014	0.464±0.074	0.692±0.001	0.577±0.016	0.455±0.096
bi3	1.294±0.060	1.300±0.015	1.085±0.073	1.126±0.036	1.103±0.108	1.029±0.074
bi4	1.227±0.080	1.164±0.053	0.976±0.051	1.038±0.068	1.024±0.089	0.919±0.065
bi5	1.450±0.058	1.417±0.045	1.426±0.025	1.405±0.040	1.411±0.043	1.377±0.040
bi6	1.969±0.078	1.852±0.051	1.514±0.039	1.709±0.028	1.589±0.121	1.503±0.030
bi7	0.686±0.006	0.683±0.004	0.643±0.058	0.632±0.037	0.651±0.012	0.624±0.066
quad1	0.591±0.094	0.675±0.017	0.488±0.071	0.662±0.013	0.580±0.115	0.432±0.085
quad2	0.648±0.036	0.646±0.045	0.614±0.062	0.626±0.026	0.591±0.087	0.515±0.098
quint1	0.557±0.021	0.508±0.104	0.547±0.060	0.539±0.051	0.538±0.100	0.502±0.067
oct1	0.659±0.031	0.680±0.012	0.340±0.147	0.691±0.002	0.411±0.089	0.306±0.101
average	0.947±0.029	0.922±0.017	0.786±0.009	0.878±0.006	0.824±0.036	0.741±0.018
# Para 579 1 tableCaption
Table 2: Experimental result in term of entropy for all data sets and evaluation methods.
# Para 580 7 bodyText
entropy to measure the quality of our clustering 
results. In information theory, entropy (Shan-
non, 1948) is a measure of the uncertainty as-
sociated with a random variable. In our prob-
lem, entropy serves as a measure of randomness 
of clustering result. The entropy of g on a sin-
gle latent variable z is defined to be H(g, z)
# Para 587 1 bodyText
- PcEC P(c�z)lo�2 P(cIz), where C is the class
# Para 588 1 bodyText
label set of V and P(clz) = ��v�s(v)=zAt(v)=c}� 
# Para 589 1 bodyText
11v1s(v)=z}1 ,
# Para 590 3 bodyText
in which t(v) is the true class label of image v. 
Lower entropy H(g, i) indicates less randomness 
and thus better clustering result.
# Para 593 1 subsectionHeader
4.2 Empirical Analysis
# Para 594 12 bodyText
We now empirically analyze the effectiveness of 
our aPLSA algorithm. Because, to our best of 
knowledge, few existing methods addressed the 
problem of image clustering with the help of so-
cial annotation image data, we can only compare 
our aPLSA with several state-of-the-art cluster-
ing algorithms that are not directly designed for 
our problem. The first baseline is the well-known 
KMeans algorithm (MacQueen, 1967). Since our 
algorithm is designed based on PLSA (Hofmann, 
1999), we also included PLSA for clustering as a 
baseline method in our experiments.
# Para 606 11 bodyText
For each of the above two baselines, we have 
two strategies: (1) separated: the baseline 
method was applied on the target image data only; 
(2) combined: the baseline method was applied 
to cluster the combined data consisting of both 
target image data and the annotated image data. 
Clustering results on target image data were used 
for evaluation. Note that, in the combined data, all 
the annotations were thrown away since baseline 
methods evaluated in this paper do not leverage 
annotation information.
# Para 617 1 bodyText
In addition, we compared our algorithm aPLSA
# Para 618 7 bodyText
to a state-of-the-art transfer clustering strategy, 
known as self-taught clustering (STC) (Dai et al., 
2008b). STC makes use of auxiliary data to esti-
mate a better feature representation to benefit the 
target clustering. In these experiments, the anno-
tated image data were used as auxiliary data in 
STC, which does not use the annotation text.
# Para 625 17 bodyText
In our experiments, the performance is in the 
form of the average entropy and variance of five 
repeats by randomly selecting 50 images from 
each of the categories. We selected only 50 im-
ages per category, since this paper is focused on 
clustering sparse data. Table 2 shows the perfor-
mance with respect to all comparison methods on 
each of the image clustering tasks measured by 
the entropy criterion. From the tables, we can see 
that our algorithm aPLSA outperforms the base-
line methods in all the data sets. We believe that is 
because aPLSA can effectively utilize the knowl-
edge from the socially annotated image data. On 
average, aPLSA gives rise to 21.8% of entropy re-
duction and as compared to KMeans, 5.7% of en-
tropy reduction as compared to PLSA, and 10.1% 
of entropy reduction as compared to S TC.
# Para 642 1 subsectionHeader
4.2.1 Varying Data Size
# Para 643 11 bodyText
We now show how the data size affects aPLSA, 
with two baseline methods KMeans and PLSA as 
reference. The experiments were conducted on 
different amounts of target image data, varying 
from 10 to 80. The corresponding experimental 
results in average entropy over all the 11 clustering 
tasks are shown in Figure 4(a). From this figure, 
we observe that aPLSA always yields a significant 
reduction in entropy as compared with two base-
line methods KMeans and PLSA, regardless of the 
size of target image data that we used.
# Para 654 1 page
7
# Para 655 7 figure
1 
0.95 
0.9 
0.85 
0.8 
0.75 
0.7
# Para 662 1 figure
10	20	30	40	50	60	70	80
# Para 663 3 figure
KMeans 
PLSA 
aPLSA
# Para 666 1 figure
0.75
# Para 667 3 figure
0.65 
0.6 
0.55
# Para 670 1 figure
0.45
# Para 671 1 figure
0.7
# Para 672 1 figure
0.5
# Para 673 1 figure
0.4
# Para 674 1 figure
0	0.2	0.4	0.6	0.8	1
# Para 675 1 figure
average over 5 development sets
# Para 676 1 figure
0.75
# Para 677 1 figure
0.65
# Para 678 1 figure
0.55
# Para 679 1 figure
0.7
# Para 680 1 figure
0.6
# Para 681 1 figure
0.5
# Para 682 1 figure
0	50	100	150	200	250 300
# Para 683 1 figure
average over 5 development sets
# Para 684 1 figure
Data size per category		Number of Iteration
# Para 685 1 figure
(a)	(b)	(c)
# Para 686 3 figureCaption
Figure 4: (a) The entropy curve as a function of different amounts of data per category. (b) The entropy 
curve as a function of different number of iterations. (c) The entropy curve as a function of different 
trade-off parameter A.
# Para 689 1 subsubsectionHeader
4.2.2 Parameter Sensitivity
# Para 690 15 bodyText
In aPLSA, there is a trade-off parameter A that af-
fects how the algorithm relies on auxiliary data. 
When A = 0, the aPLSA relies only on annotated 
image data B. When A = 1, aPLSA relies only 
on target image data A, in which case aPLSA de-
generates to PLSA. Smaller A indicates heavier re-
liance on the annotated image data. We have done 
some experiments on the development sets to in-
vestigate how different A affect the performance 
of aPLSA. We set the number of images per cate-
gory to 50, and tested the performance of aPLSA. 
The result in average entropy over all development 
sets is shown in Figure 4(b). In the experiments 
described in this paper, we set A to 0.2, which is 
the best point in Figure 4(b).
# Para 705 1 subsectionHeader
4.2.3 Convergence
# Para 706 8 bodyText
In our experiments, we tested the convergence 
property of our algorithm aPLSA as well. Fig-
ure 4(c) shows the average entropy curve given 
by aPLSA over all development sets. From this 
figure, we see that the entropy decreases very fast 
during the first 100 iterations and becomes stable 
after 150 iterations. We believe that 200 iterations 
is sufficient for aPLSA to converge.
# Para 714 1 sectionHeader
5 Conclusions
# Para 715 11 bodyText
In this paper, we proposed a new learning scenario 
called heterogeneous transfer learning and illus-
trated its application to image clustering. Image 
clustering, a vital component in organizing search 
results for query-based image search, was shown 
to be improved by transferring knowledge from 
unrelated images with annotations in a social Web. 
This is done by first learning the high-quality la-
tent variables in the auxiliary data, and then trans-
ferring this knowledge to help improve the cluster-
ing of the target image data. We conducted experi-
# Para 726 4 bodyText
ments on two image data sets, using the Flickr data 
as the annotated auxiliary image data, and showed 
that our aPLSA algorithm can greatly outperform 
several state-of-the-art clustering algorithms.
# Para 730 12 bodyText
In natural language processing, there are many 
future opportunities to apply heterogeneous trans-
fer learning. In (Ling et al., 2008) we have shown 
how to classify the Chinese text using English text 
as the training data. We may also consider cluster-
ing, topic modeling, question answering, etc., to 
be done using data in different feature spaces. We 
can consider data in different modalities, such as 
video, image and audio, as the training data. Fi-
nally, we will explore the theoretical foundations 
and limitations of heterogeneous transfer learning 
as well.
# Para 742 3 sectionHeader
Acknowledgement Qiang Yang thanks Hong 
Kong CERG grant 621307 for supporting the re-
search.
# Para 745 1 sectionHeader
References
# Para 746 4 reference
Alina Andreevskaia and Sabine Bergler. 2008. When spe-
cialists and generalists work together: Overcoming do-
main dependence in sentiment tagging. In ACL-08: HLT, 
pages 290–298, Columbus, Ohio, June.
# Para 750 4 reference
Andrew Arnold, Ramesh Nallapati, and William W. Cohen. 
2007. A comparative study of methods for transductive 
transfer learning. In ICDM 2007 Workshop on Mining 
and Management of Biological Data, pages 77-82.
# Para 754 3 reference
Andrew Arnold, Ramesh Nallapati, and William W. Cohen. 
2008. Exploiting feature hierarchy for transfer learning in 
named entity recognition. In ACL-08: HLT.
# Para 757 3 reference
Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney. 
2004. A probabilistic framework for semi-supervised 
clustering. In ACM SIGKDD 2004, pages 59–68.
# Para 760 3 reference
John Blitzer, Ryan Mcdonald, and Fernando Pereira. 2006. 
Domain adaptation with structural correspondence learn-
ing. In EMNLP 2006, pages 120–128, Sydney, Australia.
# Para 763 1 page
8
# Para 764 4 reference
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. 
Biographies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In ACL 2007, 
pages 440–447, Prague, Czech Republic.
# Para 768 3 reference
Avrim Blum and Tom Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. In COLT 1998, pages 
92–100, New York, NY, USA. ACM.
# Para 771 2 reference
Rich Caruana. 1997. Multitask learning. Machine Learning, 
28(1):41–75.
# Para 773 3 reference
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation 
with active learning for word sense disambiguation. In 
ACL 2007, Prague, Czech Republic.
# Para 776 3 reference
David A. Cohn and Thomas Hofmann. 2000. The missing 
link - a probabilistic model of document content and hy-
pertext connectivity. In NIPS 2000, pages 430–436.
# Para 779 4 reference
Wenyuan Dai, Yuqiang Chen, Gui-Rong Xue, Qiang Yang, 
and Yong Yu. 2008a. Translated learning: Transfer learn-
ing across different feature spaces. In NIPS 2008, pages 
353–360.
# Para 783 3 reference
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong Yu. 
2008b. Self-taught clustering. In ICML 2008, pages 200– 
207. Omnipress.
# Para 786 2 reference
Hal Daume,III. 2007. Frustratingly easy domain adaptation.
In ACL 2007, pages 256–263, Prague, Czech Republic.
# Para 788 3 reference
Jesse Davis and Pedro Domingos. 2008. Deep transfer via 
second-order markov logic. In AAAI 2008 Workshop on 
Transfer Learning, Chicago, USA.
# Para 791 4 reference
Scott Deerwester, Susan T. Dumais, George W. Furnas, 
Thomas K. L, and Richard Harshman. 1990. Indexing by 
latent semantic analysis. Journal of the American Society 
for Information Science, pages 391–407.
# Para 795 3 reference
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em algo-
rithm. J. of the Royal Statistical Society, 39:1–38.
# Para 798 3 reference
Thomas Finley and Thorsten Joachims. 2005. Supervised 
clustering with support vector machines. In ICML 2005, 
pages 217–224, New York, NY, USA. ACM.
# Para 801 3 reference
G. Griffin, A. Holub, and P. Perona. 2007. Caltech-256 ob-
ject category dataset. Technical Report 7694, California 
Institute of Technology.
# Para 804 3 reference
Thomas Hofmann. 1999 Probabilistic latent semantic anal-
ysis. In Proc. of Uncertainty in Artificial Intelligence, 
UAI99. Pages 289–296
# Para 807 4 reference
Thomas Hofmann. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning. vol-
ume 42, number 1-2, pages 177–196. Kluwer Academic 
Publishers.
# Para 811 3 reference
Jing Jiang and Chengxiang Zhai. 2007. Instance weighting 
for domain adaptation in NLP. In ACL 2007, pages 264– 
271, Prague, Czech Republic, June.
# Para 814 3 reference
Leonard Kaufman and Peter J. Rousseeuw. 1990. Finding 
groups in data: an introduction to cluster analysis. John 
Wiley and Sons, New York.
# Para 817 4 reference
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006. 
Beyond bags of features: Spatial pyramid matching for 
recognizing natural scene categories. In CVPR 2006, 
pages 2169–2178, Washington, DC, USA.
# Para 821 3 reference
Fei-Fei Li and Pietro Perona. 2005. A bayesian hierarchi-
cal model for learning natural scene categories. In CVPR 
2005, pages 524–531, Washington, DC, USA.
# Para 824 4 reference
Xiao Ling, Gui-Rong Xue, Wenyuan Dai, Yun Jiang, Qiang 
Yang, and Yong Yu. 2008. Can chinese web pages be 
classified with english data source? In WWW 2008, pages 
969–978, New York, NY, USA. ACM.
# Para 828 4 reference
Nicolas Loeff, Cecilia Ovesdotter Alm, and David A. 
Forsyth. 2006. Discriminating image senses by clustering 
with multimodal features. In COLING/ACL 2006 Main 
conference poster sessions, pages 547–554.
# Para 832 3 reference
David G. Lowe. 2004. Distinctive image features from scale- 
invariant keypoints. International Journal of Computer 
Vision (IJCV) 2004, volume 60, number 2, pages 91–110.
# Para 835 4 reference
J. B. MacQueen. 1967. Some methods for classification and 
analysis of multivariate observations. In Proceedings of 
Fifth Berkeley Symposium on Mathematical Statistics and 
Probability, pages 1:281–297, Berkeley, CA, USA.
# Para 839 4 reference
Kamal Nigam and Rayid Ghani. 2000. Analyzing the effec-
tiveness and applicability of co-training. In Proceedings 
of the Ninth International Conference on Information and 
Knowledge Management, pages 86–93, New York, USA.
# Para 843 4 reference
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, 
and Andrew Y. Ng. 2007. Self-taught learning: transfer 
learning from unlabeled data. In ICML 2007, pages 759– 
766, New York, NY, USA. ACM.
# Para 847 3 reference
Roi Reichart and Ari Rappoport. 2007. Self-training for 
enhancement and domain adaptation of statistical parsers 
trained on small datasets. In ACL 2007.
# Para 850 3 reference
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-
poport. 2008. Multi-task active learning for linguistic 
annotations. In ACL-08: HLT, pages 861–869.
# Para 853 2 reference
C. E. Shannon. 1948. A mathematical theory of communi-
cation. Bell system technicaljournal, 27.
# Para 855 3 reference
J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. 
Freeman. 2005. Discovering object categories in image 
collections. In ICCV 2005.
# Para 858 4 reference
Naftali Tishby, Fernando C. Pereira, and William Bialek. The 
information bottleneck method. 1999. In Proc. of the 37- 
th Annual Allerton Conference on Communication, Con-
trol and Computing, pages 368–377.
# Para 862 3 reference
Pengcheng Wu and Thomas G. Dietterich. 2004. Improving 
svm accuracy by training on auxiliary data sources. In 
ICML 2004, pages 110–117, New York, NY, USA.
# Para 865 3 reference
Yejun Wu and Douglas W. Oard. 2008. Bilingual topic as-
pect classification with a few training examples. In ACM 
SIGIR 2008, pages 203–210, New York, NY, USA.
# Para 868 3 reference
Xiaojin Zhu. 2007. Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences, Univer-
sity of Wisconsin-Madison.
# Para 871 1 page
9
