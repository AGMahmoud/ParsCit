title ||| Brutus: A Semantic Role Labeling System Incorporating CCG, CFG, and
title ||| Dependency Features
author ||| Stephen A. Boxwell, Dennis Mehay, and Chris Brew
affiliation ||| Department of Linguistics
affiliation ||| The Ohio State University
email ||| {boxwe11,mehay,cbrew}@1ing.ohio-state.edu
sectionHeader ||| Abstract
bodyText ||| We describe a semantic role labeling system
bodyText ||| that makes primary use of CCG-based fea-
bodyText ||| tures. Most previously developed systems
bodyText ||| are CFG-based and make extensive use of a
bodyText ||| treepath feature, which suffers from data spar-
bodyText ||| sity due to its use of explicit tree configura-
bodyText ||| tions. CCG affords ways to augment treepath-
bodyText ||| based features to overcome these data sparsity
bodyText ||| issues. By adding features over CCG word-
bodyText ||| word dependencies and lexicalized verbal sub-
bodyText ||| categorization frames (âsupertagsâ), we can
bodyText ||| obtain an F-score that is substantially better
bodyText ||| than a previous CCG-based SRL system and
bodyText ||| competitive with the current state of the art. A
bodyText ||| manual error analysis reveals that parser errors
bodyText ||| account for many of the errors of our system.
bodyText ||| This analysis also suggests that simultaneous
bodyText ||| incremental parsing and semantic role labeling
bodyText ||| may lead to performance gains in both tasks.
sectionHeader ||| 1 Introduction
bodyText ||| Semantic Role Labeling (SRL) is the process of assign-
bodyText ||| ing semantic roles to strings of words in a sentence ac-
bodyText ||| cording to their relationship to the semantic predicates
bodyText ||| expressed in the sentence. The task is difficult because
bodyText ||| the relationship between syntactic relations like âsub-
bodyText ||| jectâ and âobjectâ do not always correspond to seman-
bodyText ||| tic relations like âagentâ and âpatientâ. An effective
bodyText ||| semantic role labeling system must recognize the dif-
bodyText ||| ferences between different configurations:
listItem ||| (a) [The man]Arg0 opened [the door]Aï¿½g1 [for
listItem ||| him]Arg3 [today]ArgM-TMP.
listItem ||| (b) [The door]Aï¿½g1 opened.
listItem ||| (c) [The door]Aï¿½g1 was opened by [a man]Aï¿½g0.
bodyText ||| We use Propbank (Palmer et al., 2005), a corpus of
bodyText ||| newswire text annotated with verb predicate semantic
bodyText ||| role information that is widely used in the SRL litera-
bodyText ||| ture (M`arquez et al., 2008). Rather than describe se-
bodyText ||| mantic roles in terms of âagentâ or âpatientâ, Propbank
bodyText ||| defines semantic roles on a verb-by-verb basis. For ex-
bodyText ||| ample, the verb open encodes the OPENER as Arg0, the
bodyText ||| OPENEE as Arg1, and the beneficiary of the OPENING
bodyText ||| action as Arg3. Propbank also defines a set of adjunct
bodyText ||| roles, denoted by the letter M instead of a number. For
bodyText ||| example, ArgM-TMP denotes a temporal role, like âto-
bodyText ||| dayâ. By using verb-specific roles, Propbank avoids
bodyText ||| specific claims about parallels between the roles of dif-
bodyText ||| ferent verbs.
bodyText ||| We follow the approach in (Punyakanok et al., 2008)
bodyText ||| in framing the SRL problem as a two-stage pipeline:
bodyText ||| identification followed by labeling. During identifica-
bodyText ||| tion, every word in the sentence is labeled either as
bodyText ||| bearing some (as yet undetermined) semantic role or
bodyText ||| not . This is done for each verb. Next, during label-
bodyText ||| ing, the precise verb-specific roles for each word are
bodyText ||| determined. In contrast to the approach in (Punyakanok
bodyText ||| et al., 2008), which tags constituents directly, we tag
bodyText ||| headwords and then associate them with a constituent,
bodyText ||| as in a previous CCG-based approach (Gildea and
bodyText ||| Hockenmaier, 2003). Another difference is our choice
bodyText ||| of parsers. Brutus uses the CCG parser of (Clark and
bodyText ||| Curran, 2007, henceforth the C&amp;C parser), Charniakâs
bodyText ||| parser (Charniak, 2001) for additional CFG-based fea-
bodyText ||| tures, and MALT parser (Nivre et al., 2007) for de-
bodyText ||| pendency features, while (Punyakanok et al., 2008)
bodyText ||| use results from an ensemble of parses from Char-
bodyText ||| niakâs Parser and a Collins parser (Collins, 2003; Bikel,
bodyText ||| 2004). Finally, the system described in (Punyakanok et
bodyText ||| al., 2008) uses a joint inference model to resolve dis-
bodyText ||| crepancies between multiple automatic parses. We do
bodyText ||| not employ a similar strategy due to the differing no-
bodyText ||| tions of constituency represented in our parsers (CCG
bodyText ||| having a much more fluid notion of constituency and
bodyText ||| the MALT parser using a different approach entirely).
bodyText ||| For the identification and labeling steps, we train
bodyText ||| a maximum entropy classifier (Berger et al., 1996)
bodyText ||| over sections 02-21 of a version of the CCGbank cor-
bodyText ||| pus (Hockenmaier and Steedman, 2007) that has been
bodyText ||| augmented by projecting the Propbank semantic anno-
bodyText ||| tations (Boxwell and White, 2008). We evaluate our
bodyText ||| SRL systemâs argument predictions at the word string
bodyText ||| level, making our results directly comparable for each
bodyText ||| argument labeling.1
bodyText ||| In the following, we briefly introduce the CCG
bodyText ||| grammatical formalism and motivate its use in SRL
bodyText ||| (Sections 2â3). Our main contribution is to demon-
bodyText ||| strate that CCG â arguably a more expressive and lin-
footnote ||| 1This is guaranteed by our string-to-string mapping from
footnote ||| the original Propbank to the CCGbank.
page ||| 37
note ||| Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 37â45,
note ||| Suntec, Singapore, 2-7 August 2009. cï¿½2009 ACL and AFNLP
bodyText ||| guistically appealing syntactic framework than vanilla
bodyText ||| CFGs â is a viable basis for the SRL task. This is sup-
bodyText ||| ported by our experimental results, the setup and details
bodyText ||| of which we give in Sections 4â10. In particular, using
bodyText ||| CCG enables us to map semantic roles directly onto
bodyText ||| verbal categories, an innovation of our approach that
bodyText ||| leads to performance gains (Section 7). We conclude
bodyText ||| with an error analysis (Section 11), which motivates
bodyText ||| our discussion of future research for computational se-
bodyText ||| mantics with CCG (Section 12).
sectionHeader ||| 2 Combinatory Categorial Grammar
bodyText ||| Combinatory Categorial Grammar (Steedman, 2000)
bodyText ||| is a grammatical framework that describes syntactic
bodyText ||| structure in terms of the combinatory potential of the
bodyText ||| lexical (word-level) items. Rather than using standard
bodyText ||| part-of-speech tags and grammatical rules, CCG en-
bodyText ||| codes much of the combinatory potential of each word
bodyText ||| by assigning a syntactically informative category. For
bodyText ||| example, the verb loves has the category (s\np)/np,
bodyText ||| which could be read âthe kind of word that would be
bodyText ||| a sentence if it could combine with a noun phrase on
bodyText ||| the right and a noun phrase on the leftâ. Further, CCG
bodyText ||| has the advantage of a transparent interface between the
bodyText ||| way the words combine and their dependencies with
bodyText ||| other words. Word-word dependencies in the CCG-
bodyText ||| bank are encoded using predicate-argument (PARG)
bodyText ||| relations. PARG relations are defined by the functor
bodyText ||| word, the argument word, the category of the functor
bodyText ||| word and which argument slot of the functor category
bodyText ||| is being filled. For example, in the sentence John loves
bodyText ||| Mary (figure 1), there are two slots on the verbal cat-
bodyText ||| egory to be filled by NP arguments. The first argu-
bodyText ||| ment (the subject) fills slot 1. This can be encoded
bodyText ||| as &lt;loves,john,(s\np)/np,1&gt;, indicating the head of
bodyText ||| the functor, the head of the argument, the functor cat-
bodyText ||| egory and the argument slot. The second argument
bodyText ||| (the direct object) fills slot 2. This can be encoded as
bodyText ||| &lt;loves,mary,(s\np)/np,2&gt;. One of the potential ad-
bodyText ||| vantages to using CCGbank-style PARG relations is
bodyText ||| that they uniformly encode both local and long-range
bodyText ||| dependencies â e.g., the noun phrase the Mary that
bodyText ||| John loves expresses the same set of two dependencies.
bodyText ||| We will show this to be a valuable tool for semantic
bodyText ||| role prediction.
sectionHeader ||| 3 Potential Advantages to using CCG
bodyText ||| There are many potential advantages to using the CCG
bodyText ||| formalism in SRL. One is the uniformity with which
bodyText ||| CCG can express equivalence classes of local and long-
bodyText ||| range (including unbounded) dependencies. CFG-
bodyText ||| based approaches often rely on examining potentially
bodyText ||| long sequences of categories (or treepaths) between the
bodyText ||| verb and the target word. Because there are a number of
bodyText ||| different treepaths that correspond to a single relation
bodyText ||| (figure 2), this approach can suffer from data sparsity.
bodyText ||| CCG, however, can encode all treepath-distinct expres-
bodyText ||| sions of a single grammatical relation into a single
bodyText ||| predicate-argument relationship (figure 3). This fea-
bodyText ||| ture has been shown (Gildea and Hockenmaier, 2003)
bodyText ||| to be an effective substitute for treepath-based features.
bodyText ||| But while predicate-argument-based features are very
bodyText ||| effective, they are still vulnerable both to parser er-
bodyText ||| rors and to cases where the semantics of a sentence
bodyText ||| do not correspond directly to syntactic dependencies.
bodyText ||| To counteract this, we use both kinds of features with
bodyText ||| the expectation that the treepath feature will provide
bodyText ||| low-level detail to compensate for missed, incorrect or
bodyText ||| syntactically impossible dependencies.
bodyText ||| Another advantage of a CCG-based approach (and
bodyText ||| lexicalist approaches in general) is the ability to en-
bodyText ||| code verb-specific argument mappings. An argument
bodyText ||| mapping is a link between the CCG category and the
bodyText ||| semantic roles that are likely to go with each of its ar-
bodyText ||| guments. The projection of argument mappings onto
bodyText ||| CCG verbal categories is explored in (Boxwell and
bodyText ||| White, 2008). We describe this feature in more detail
bodyText ||| in section 7.
sectionHeader ||| 4 Identification and Labeling Models
bodyText ||| As in previous approaches to SRL, Brutus uses a two-
bodyText ||| stage pipeline of maximum entropy classifiers. In ad-
bodyText ||| dition, we train an argument mapping classifier (de-
bodyText ||| scribed in more detail below) whose predictions are
bodyText ||| used as features for the labeling model. The same
bodyText ||| features are extracted for both treebank and automatic
bodyText ||| parses. Automatic parses were generated using the
bodyText ||| C&amp;C CCG parser (Clark and Curran, 2007) with its
bodyText ||| derivation output format converted to resemble that of
bodyText ||| the CCGbank. This involved following the derivational
bodyText ||| bracketings of the C&amp;C parserâs output and recon-
bodyText ||| structing the backpointers to the lexical heads using an
bodyText ||| in-house implementation of the basic CCG combina-
bodyText ||| tory operations. All classifiers were trained to 500 iter-
bodyText ||| ations of L-BFGS training â a quasi-Newton method
bodyText ||| from the numerical optimization literature (Liu and No-
bodyText ||| cedal, 1989) â using Zhang Leâs maxent toolkit. 2 To
bodyText ||| prevent overfitting we used Gaussian priors with global
bodyText ||| variances of 1 and 5 for the identifier and labeler, re-
bodyText ||| spectively.3 The Gaussian priors were determined em-
bodyText ||| pirically by testing on the development set.
bodyText ||| Both the identifier and the labeler use the following
bodyText ||| features:
listItem ||| (1) Words. Words drawn from a 3 word window
listItem ||| around the target word ,4 with each word asso-
listItem ||| ciated with a binary indicator feature.
listItem ||| (2) Part of Speech. Part of Speech tags drawn
listItem ||| from a 3 word window around the target word,
footnote ||| 2Available for download at http://homepages.
footnote ||| inf.ed.ac.uk/s0450736/maxent_toolkit.
footnote ||| html.
footnote ||| 3Gaussian priors achieve a smoothing effect (to prevent
footnote ||| overfitting) by penalizing very large feature weights.
footnote ||| 4The size of the window was determined experimentally
footnote ||| on the development set â we use the same window sizes
footnote ||| throughout.
page ||| 38
figure ||| Robin	fixed	the car
figure ||| np	(s\np)/np np/n	n
figure ||| np
figure ||| s\np
figure ||| s
figure ||| John	loves	Mary
figure ||| &gt;
figure ||| &gt;
figure ||| ï¿½
figure ||| np	(s[dcl]\np)/np	np
figure ||| 		&gt;
figure ||| 		ï¿½
figure ||| 	s[dcl]\np
figure ||| 	s[dcl]
figureCaption ||| Figure 1:	This sentence has two depen-
figureCaption ||| dencies:	&lt;loves,mary,(s\np)/np,2&gt;	and
figureCaption ||| &lt;loves,john,(s\np)/np, 1 &gt;
figure ||| fixed
figureCaption ||| Figure 2: The semantic relation (Arg1) between âcarâ
figureCaption ||| and âfixedâ in both phrases is the same, but the
figureCaption ||| treepaths â traced with arrows above â are differ-
figureCaption ||| ent: (V&gt;VP&lt;NP&lt;N and V&gt;VP&gt;S&gt;RC&gt;N&lt;N, re-
figureCaption ||| spectively).
figure ||| the car	that	Robin	fixed
figure ||| np/n	n	(np\np)/(s/np)	np	(s\np)/np
figure ||| &gt;T
figure ||| s/(s\np)
figure ||| 	&gt; 	&gt;s
figure ||| np
figure ||| s/np
figure ||| &gt;
figure ||| ï¿½
figureCaption ||| Figure 3: CCG word-word dependencies are passed
figureCaption ||| up through subordinate clauses, encoding the rela-
figureCaption ||| tion between car and fixed the same in both cases:
figureCaption ||| (s\np)/np.2.â&gt; (Gildea and Hockenmaier, 2003)
bodyText ||| with each associated with a binary indicator
bodyText ||| feature.
listItem ||| (3) CCG Categories. CCG categories drawn from
bodyText ||| a 3 word window around the target word, with
bodyText ||| each associated with a binary indicator feature.
listItem ||| (4) Predicate. The lemma of the predicate we are
listItem ||| tagging. E.g. fix is the lemma offixed.
listItem ||| (5) Result Category Detail. The grammatical fea-
bodyText ||| ture on the category of the predicate (indicat-
bodyText ||| ing declarative, passive, progressive, etc). This
bodyText ||| can be read off the verb category: declarative
bodyText ||| for eats: (s[dcl]\np)/np or progressive for run-
bodyText ||| ning:s[ng]\np.
listItem ||| (6) Before/After. A binary indicator variable indi-
listItem ||| cating whether the target word is before or after
listItem ||| the verb.
listItem ||| (7) Treepath. The sequence of CCG categories
bodyText ||| representing the path through the derivation
bodyText ||| from the predicate to the target word. For
bodyText ||| the relationship between fixed and car in the
bodyText ||| first sentence of figure 3, the treepath is
bodyText ||| (s[dcl]\np)/np&gt;s[dcl]\np&lt;np&lt;n, with &gt; and
bodyText ||| &lt; indicating movement up and down the tree,
bodyText ||| respectively.
listItem ||| (8) Short Treepath. Similar to the above treepath
bodyText ||| feature, except the path stops at the highest
bodyText ||| node under the least common subsumer that
bodyText ||| is headed by the target word (this is the con-
bodyText ||| stituent that the role would be marked on if we
bodyText ||| identified this terminal as a role-bearing word).
bodyText ||| Again, for the relationship between fixed and
bodyText ||| car in the first sentence of figure 3, the short
bodyText ||| treepath is (s[dcl]\np)/np&gt;s[dcl]\np&lt;np.
listItem ||| (9) NP Modified. A binary indicator feature indi-
listItem ||| cating whether the target word is modified by
listItem ||| an NP modifier.5
footnote ||| 5This is easily read off of the CCG PARG relationships.
figure ||| S
figure ||| ï¿½ï¿½ ï¿½ ï¿½ï¿½ï¿½
figure ||| VP
figure ||| ï¿½ ï¿½ï¿½
figure ||| ï¿½
figure ||| V	NP
figure ||| ï¿½ ï¿½
figure ||| fixed Det N
figure ||| NP
figure ||| Robin
figure ||| the
figure ||| car
figure ||| N
figure ||| the
figure ||| RC
figure ||| car
figure ||| Rel
figure ||| ^S
figure ||| that
figure ||| NP
figure ||| VP
figure ||| Robin
figure ||| V
figure ||| NP
figure ||| ï¿½ï¿½ ï¿½ ï¿½ï¿½ï¿½
figure ||| Det
figure ||| N
figure ||| ï¿½ï¿½ ï¿½ ï¿½ï¿½ï¿½
figure ||| np\np
figure ||| np
page ||| 39
listItem ||| (10) Subcategorization. A sequence of the cate-
bodyText ||| gories that the verb combines with in the CCG
bodyText ||| derivation tree. For the first sentence in fig-
bodyText ||| ure 3, the correct subcategorization would be
bodyText ||| np,np. Notice that this is not necessarily a re-
bodyText ||| statement of the verbal category â in the second
bodyText ||| sentence of figure 3, the correct subcategoriza-
bodyText ||| tion is s/(s\np),(np\np)/(s[dcl]/np),np.
listItem ||| (11) PARG feature. We follow a previous CCG-
bodyText ||| based approach (Gildea and Hockenmaier,
bodyText ||| 2003) in using a feature to describe the PARG
bodyText ||| relationship between the two words, if one ex-
bodyText ||| ists. If there is a dependency in the PARG
bodyText ||| structure between the two words, then this fea-
bodyText ||| ture is defined as the conjunction of (1) the cat-
bodyText ||| egory of the functor, (2) the argument slot that
bodyText ||| is being filled in the functor category, and (3)
bodyText ||| an indication as to whether the functor (â&gt;) or
bodyText ||| the argument (+â) is the lexical head. For ex-
bodyText ||| ample, to indicate the relationship between car
bodyText ||| and fixed in both sentences of figure 3, the fea-
bodyText ||| ture is (s\np)/np.2.â&gt;.
bodyText ||| The labeler uses all of the previous features, plus the
bodyText ||| following:
listItem ||| (12) Headship. A binary indicator feature as to
bodyText ||| whether the functor or the argument is the lex-
bodyText ||| ical head of the dependency between the two
bodyText ||| words, if one exists.
listItem ||| (13) Predicate and Before/After. The conjunction
bodyText ||| of two earlier features: the predicate lemma
bodyText ||| and the Before/After feature.
listItem ||| (14) Rel Clause. Whether the path from predicate
bodyText ||| to target word passes through a relative clause
bodyText ||| (e.g., marked by the word âthatâ or any other
bodyText ||| word with a relativizer category).
listItem ||| (15) PP features. When the target word is a prepo-
bodyText ||| sition, we define binary indicator features for
bodyText ||| the word, POS, and CCG category of the head
bodyText ||| of the topmost NP in the prepositional phrase
bodyText ||| headed by a preposition (a.k.a. the âlexical
bodyText ||| headâ of the PP). So, if on heads the phrase âon
bodyText ||| the third Fridayâ, then we extract features re-
bodyText ||| lating to Friday for the preposition on. This is
bodyText ||| null when the target word is not a preposition.
listItem ||| (16) Argument Mappings. If there is a PARG rela-
bodyText ||| tion between the predicate and the target word,
bodyText ||| the argument mapping is the most likely pre-
bodyText ||| dicted role to go with that argument. These
bodyText ||| mappings are predicted using a separate classi-
bodyText ||| fier that is trained primarily on lexical informa-
bodyText ||| tion of the verb, its immediate string-level con-
bodyText ||| text, and its observed arguments in the train-
bodyText ||| ing data. This feature is null when there is
bodyText ||| no PARG relation between the predicate and
bodyText ||| the target word. The Argument Mapping fea-
bodyText ||| ture can be viewed as a simple prediction about
bodyText ||| some of the non-modifier semantic roles that a
bodyText ||| verb is likely to express. We use this informa-
bodyText ||| tion as a feature and not a hard constraint to
bodyText ||| allow other features to overrule the recommen-
bodyText ||| dation made by the argument mapping classi-
bodyText ||| fier. The features used in the argument map-
bodyText ||| ping classifier are described in detail in section
bodyText ||| 7.
sectionHeader ||| 5 CFG based Features
bodyText ||| In addition to CCG-based features, features can be
bodyText ||| drawn from a traditional CFG-style approach when
bodyText ||| they are available. Our motivation for this is twofold.
bodyText ||| First, others (Punyakanok et al., 2008, e.g.), have found
bodyText ||| that different parsers have different error patterns, and
bodyText ||| so using multiple parsers can yield complementary
bodyText ||| sources of correct information. Second, we noticed
bodyText ||| that, although the CCG-based system performed well
bodyText ||| on head word labeling, performance dropped when
bodyText ||| projecting these labels to the constituent level (see sec-
bodyText ||| tions 8 and 9 for more). This may have to do with the
bodyText ||| fact that CCG is not centered around a constituency-
bodyText ||| based analysis, as well as with inconsistencies between
bodyText ||| CCG and Penn Treebank-style bracketings (the latter
bodyText ||| being what was annotated in the original Propbank).
bodyText ||| Penn Treebank-derived features are used in the iden-
bodyText ||| tifier, labeler, and argument mapping classifiers. For
bodyText ||| automatic parses, we use Charniakâs parser (Charniak,
bodyText ||| 2001). For gold-standard parses, we remove func-
bodyText ||| tional tag and trace information from the Penn Tree-
bodyText ||| bank parses before we extract features over them, so as
bodyText ||| to simulate the conditions of an automatic parse. The
bodyText ||| Penn Treebank features are as follows:
listItem ||| (17) CFG Treepath. A sequence of traditional
listItem ||| CFG-style categories representing the path
listItem ||| from the verb to the target word.
listItem ||| (18) CFG Short Treepath. Analogous to the CCG-
listItem ||| based short treepath feature.
listItem ||| (19) CFG Subcategorization. Analogous to the
listItem ||| CCG-based subcategorization feature.
listItem ||| (20) CFG Least Common Subsumer. The cate-
listItem ||| gory of the root of the smallest tree that domi-
listItem ||| nates both the verb and the target word.
sectionHeader ||| 6 Dependency Parser Features
bodyText ||| Finally, several features can be extracted from a de-
bodyText ||| pendency representation of the same sentence. Au-
bodyText ||| tomatic dependency relations were produced by the
bodyText ||| MALT parser. We incorporate MALT into our col-
bodyText ||| lection of parses because it provides detailed informa-
bodyText ||| tion on the exact syntactic relations between word pairs
bodyText ||| (subject, object, adverb, etc) that is not found in other
bodyText ||| automatic parsers. The features used from the depen-
bodyText ||| dency parses are listed below:
page ||| 40
listItem ||| (21) DEP-Exists A binary indicator feature show-
listItem ||| ing whether or not there is a dependency be-
listItem ||| tween the target word and the predicate.
listItem ||| (22) DEP-Type If there is a dependency between
listItem ||| the target word and the predicate, what type of
listItem ||| dependency it is (SUBJ, OBJ, etc).
sectionHeader ||| 7 Argument Mapping Model
bodyText ||| An innovation in our approach is to use a separate clas-
bodyText ||| sifier to predict an argument mapping feature. An ar-
bodyText ||| gument mapping is a mapping from the syntactic argu-
bodyText ||| ments of a verbal category to the semantic arguments
bodyText ||| that should correspond to them (Boxwell and White,
bodyText ||| 2008). In order to generate examples of the argument
bodyText ||| mapping for training purposes, it is necessary to em-
bodyText ||| ploy the PARG relations for a given sentence to identify
bodyText ||| the headwords of each of the verbal arguments. That is,
bodyText ||| we use the PARG relations to identify the headwords of
bodyText ||| each of the constituents that are arguments of the verb.
bodyText ||| Next, the appropriate semantic role that corresponds to
bodyText ||| that headword (given by Propbank) is identified. This
bodyText ||| is done by climbing the CCG derivation tree towards
bodyText ||| the root until we find a semantic role corresponding to
bodyText ||| the verb in question â i.e., by finding the point where
bodyText ||| the constituent headed by the verbal category combines
bodyText ||| with the constituent headed by the argument in ques-
bodyText ||| tion. These semantic roles are then marked on the cor-
bodyText ||| responding syntactic argument of the verb.
bodyText ||| As an example, consider the sentence The boy loves
bodyText ||| a girl. (figure 4). By examining the arguments that the
bodyText ||| verbal category combines with in the treebank, we can
bodyText ||| identify the corresponding semantic role for each argu-
bodyText ||| ment that is marked on the verbal category. We then use
bodyText ||| these tags to train the Argument Mapping model, which
bodyText ||| will predict likely argument mappings for verbal cate-
bodyText ||| gories based on their local surroundings and the head-
bodyText ||| words of their arguments, similar to the supertagging
bodyText ||| approaches used to label the informative syntactic cat-
bodyText ||| egories of the verbs (Bangalore and Joshi, 1999; Clark,
bodyText ||| 2002), except tagging âone level aboveâ the syntax.
bodyText ||| The Argument Mapping Predictor uses the following
bodyText ||| features:
listItem ||| (23) Predicate. The lemma of the predicate, as be-
listItem ||| fore.
listItem ||| (24) Words. Words drawn from a 5 word window
listItem ||| around the target word, with each word associ-
listItem ||| ated with a binary indicator feature, as before.
listItem ||| (25) Parts of Speech. Part of Speech tags drawn
listItem ||| from a 5 word window around the target word,
listItem ||| with each tag associated with a binary indicator
listItem ||| feature, as before.
listItem ||| (26) CCG Categories. CCG categories drawn from
bodyText ||| a 5 word window around the target word, with
bodyText ||| each category associated with a binary indica-
bodyText ||| tor feature, as before.
figure ||| the boy	loves	a	girl
figure ||| np/n	n	(s[dcl]\npArg0)/npArg1 np/n	n
figure ||| np â Arga	np â Argl
figure ||| ï¿½
figure ||| s[dcl]
figureCaption ||| Figure 4: By looking at the constituents that the verb
figureCaption ||| combines with, we can identify the semantic roles cor-
figureCaption ||| responding to the arguments marked on the verbal cat-
figureCaption ||| egory.
listItem ||| (27) Argument Data. The word, POS, and CCG
listItem ||| category, and treepath of the headwords of each
listItem ||| of the verbal arguments (i.e., PARG depen-
listItem ||| dents), each encoded as a separate binary in-
listItem ||| dicator feature.
listItem ||| (28) Number of arguments. The number of argu-
listItem ||| ments marked on the verb.
listItem ||| (29) Words of Arguments. The head words of each
listItem ||| of the verbâs arguments.
listItem ||| (30) Subcategorization. The CCG categories that
listItem ||| combine with this verb. This includes syntactic
listItem ||| adjuncts as well as arguments.
listItem ||| (31) CFG-Sisters. The POS categories of the sis-
listItem ||| ters of this predicate in the CFG representation.
listItem ||| (32) DEP-dependencies. The individual depen-
bodyText ||| dency types of each of the dependencies re-
bodyText ||| lating to the verb (SBJ, OBJ, ADV, etc) taken
bodyText ||| from the dependency parse. We also incorpo-
bodyText ||| rate a single feature representing the entire set
bodyText ||| of dependency types associated with this verb
bodyText ||| into a single feature, representing the set of de-
bodyText ||| pendencies as a whole.
bodyText ||| Given these features with gold standard parses, our
bodyText ||| argument mapping model can predict entire argument
bodyText ||| mappings with an accuracy rate of 87.96% on the test
bodyText ||| set, and 87.70% on the development set. We found the
bodyText ||| features generated by this model to be very useful for
bodyText ||| semantic role prediction, as they enable us to make de-
bodyText ||| cisions about entire sets of semantic roles associated
bodyText ||| with individual lemmas, rather than choosing them in-
bodyText ||| dependently of each other.
sectionHeader ||| 8 Enabling Cross-System Comparison
bodyText ||| The Brutus system is designed to label headwords of
bodyText ||| semantic roles, rather than entire constituents. How-
bodyText ||| ever, because most SRL systems are designed to label
bodyText ||| constituents rather than headwords, it is necessary to
bodyText ||| project the roles up the derivation to the correct con-
bodyText ||| stituent in order to make a meaningful comparison of
bodyText ||| the systemâs performance. This introduces the poten-
bodyText ||| tial for further error, so we report results on the ac-
bodyText ||| curacy of headwords as well as the correct string of
bodyText ||| words. We deterministically move the role to the high-
bodyText ||| est constituent in the derivation that is headed by the
bodyText ||| s[dcl]\np
page ||| 41
table ||| 	P	R	F
table ||| P. et al (treebank)	86.22%	87.40%	86.81%
table ||| Brutus (treebank)	88.29%	86.39%	87.33%
table ||| P. et al (automatic)	77.09%	75.51%	76.29%
table ||| Brutus (automatic)	76.73%	70.45%	73.45%
figure ||| a man	with	glasses spoke
figure ||| np/n	n	(np\np)/np	np	s\np
figure ||| ï¿½np
figure ||| np\np
figure ||| np - speak.Arg0
figure ||| s
figureCaption ||| Figure 5: The role is moved towards the root until the
figureCaption ||| original node is no longer the head of the marked con-
figureCaption ||| stituent.
table ||| 	P	R	F
table ||| G&amp;H (treebank)	67.5%	60.0%	63.5%
table ||| Brutus (treebank)	88.18%	85.00%	86.56%
table ||| G&amp;H (automatic)	55.7%	49.5%	52.4%
table ||| Brutus (automatic)	76.06%	70.15%	72.99%
tableCaption ||| Table 1: Accuracy of semantic role prediction using
tableCaption ||| only CCG based features.
bodyText ||| originally tagged terminal. In most cases, this corre-
bodyText ||| sponds to the node immediately dominated by the low-
bodyText ||| est common subsuming node of the the target word and
bodyText ||| the verb (figure 5). In some cases, the highest con-
bodyText ||| stituent that is headed by the target word is not imme-
bodyText ||| diately dominated by the lowest common subsuming
bodyText ||| node (figure 6).
sectionHeader ||| 9 Results
bodyText ||| Using a version of Brutus incorporating only the CCG-
bodyText ||| based features described above, we achieve better re-
bodyText ||| sults than a previous CCG based system (Gildea and
bodyText ||| Hockenmaier, 2003, henceforth G&amp;H). This could be
bodyText ||| due to a number of factors, including the fact that our
bodyText ||| system employs a different CCG parser, uses a more
bodyText ||| complete mapping of the Propbank onto the CCGbank,
bodyText ||| uses a different machine learning approach,6 and has a
bodyText ||| richer feature set. The results for constituent tagging
bodyText ||| accuracy are shown in table 1.
bodyText ||| As expected, by incorporating Penn Treebank-based
bodyText ||| features and dependency features, we obtain better re-
bodyText ||| sults than with the CCG-only system. The results for
bodyText ||| gold standard parses are comparable to the winning
bodyText ||| system of the CoNLL 2005 shared task on semantic
bodyText ||| role labeling (Punyakanok et al., 2008). Other systems
bodyText ||| (Toutanova et al., 2008; Surdeanu et al., 2007; Johans-
bodyText ||| son and Nugues, 2008) have also achieved comparable
bodyText ||| results â we compare our system to (Punyakanok et
bodyText ||| al., 2008) due to the similarities in our approaches. The
bodyText ||| performance of the full system is shown in table 2.
bodyText ||| Table 3 shows the ability of the system to predict
bodyText ||| the correct headwords of semantic roles. This is a nec-
bodyText ||| essary condition for correctness of the full constituent,
bodyText ||| but not a sufficient one. In parser evaluation, Carroll,
bodyText ||| Minnen, and Briscoe (Carroll et al., 2003) have argued
bodyText ||| 6G&amp;H use a generative model with a back-off lattice,
bodyText ||| whereas we use a maximum entropy classifier.
tableCaption ||| Table 2: Accuracy of semantic role prediction using
tableCaption ||| CCG, CFG, and MALT based features.
table ||| 	P	R	F
table ||| Headword (treebank)	88.94%	86.98%	87.95%
table ||| Boundary (treebank)	88.29%	86.39%	87.33%
table ||| Headword (automatic)	82.36%	75.97%	79.04%
table ||| Boundary (automatic)	76.33%	70.59%	73.35%
tableCaption ||| Table 3: Accuracy of the system for labeling semantic
tableCaption ||| roles on both constituent boundaries and headwords.
tableCaption ||| Headwords are easier to predict than boundaries, re-
tableCaption ||| flecting CCGâs focus on word-word relations rather
tableCaption ||| than constituency.
bodyText ||| for dependencies as a more appropriate means of eval-
bodyText ||| uation, reflecting the focus on headwords from con-
bodyText ||| stituent boundaries. We argue that, especially in the
bodyText ||| heavily lexicalized CCG framework, headword evalu-
bodyText ||| ation is more appropriate, reflecting the emphasis on
bodyText ||| headword combinatorics in the CCG formalism.
sectionHeader ||| 10 The Contribution of the New Features
bodyText ||| Two features which are less frequently used in SRL
bodyText ||| research play a major role in the Brutus system: The
bodyText ||| PARG feature (Gildea and Hockenmaier, 2003) and
bodyText ||| the argument mapping feature. Removing them has
bodyText ||| a strong effect on accuracy when labeling treebank
bodyText ||| parses, as shown in our feature ablation results in ta-
bodyText ||| ble 4. We do not report results including the Argu-
bodyText ||| ment Mapping feature but not the PARG feature, be-
bodyText ||| cause some predicate-argument relation information is
bodyText ||| assumed in generating the Argument Mapping feature.
table ||| 	P	R	F
table ||| +PARG +AM	88.77%	86.15%	87.44%
table ||| +PARG -AM	88.42%	85.78%	87.08%
table ||| -PARG -AM	87.92%	84.65%	86.26%
tableCaption ||| Table 4: The effects of removing key features from the
tableCaption ||| system on gold standard parses.
bodyText ||| The same is true for automatic parses, as shown in ta-
bodyText ||| ble 5.
sectionHeader ||| 11 Error Analysis
bodyText ||| Many of the errors made by the Brutus system can be
bodyText ||| traced directly to erroneous parses, either in the auto-
bodyText ||| matic or treebank parse. In some cases, PP attachment
page ||| 42
figure ||| with	even brief exposures	causing	symptoms
figure ||| (((vp\vp)/vp[ng])/np	n/n	n/n	n	(s[ng]\np)/np	np
figure ||| 	ï¿½ 	ï¿½
figure ||| n
figure ||| s[ng]\np
figure ||| n
figure ||| np â cause.Arg0
figure ||| 		ï¿½
figure ||| (vp\vp)/vp[ng]			ï¿½
figure ||| 	vp\vp
figureCaption ||| Figure 6: In this case, with is the head of with even brief exposures, so the role is correctly marked on even brief
figureCaption ||| exposures (based on wsj 0003.2).
table ||| ï¿½
table ||| 	P	R	F
table ||| +PARG +AM	74.14%	62.09%	67.58%
table ||| +PARG -AM	70.02%	64.68%	67.25%
table ||| -PARG -AM	73.90%	61.15%	66.93%
table ||| a form	of	asbestos used to make filters
table ||| np	(np\np)/np	np	np\np
table ||| 		ï¿½
table ||| 		ï¿½
table ||| 	np\np
table ||| 	np â Arg1
table ||| 	ï¿½
table ||| np
tableCaption ||| Table 5: The effects of removing key features from the
tableCaption ||| system on automatic parses.
bodyText ||| ambiguities cause a role to be marked too high in the
bodyText ||| derivation. In the sentence the company stopped using
bodyText ||| asbestos in 1956 (figure 7), the correct Arg 1 of stopped
bodyText ||| is using asbestos. However, because in 1956 is erro-
bodyText ||| neously modifying the verb using rather than the verb
bodyText ||| stopped in the treebank parse, the system trusts the syn-
bodyText ||| tactic analysis and places Arg1 of stopped on using as-
bodyText ||| bestos in 1956. This particular problem is caused by an
bodyText ||| annotation error in the original Penn Treebank that was
bodyText ||| carried through in the conversion to CCGbank.
bodyText ||| Another common error deals with genitive construc-
bodyText ||| tions. Consider the phrase a form of asbestos used
bodyText ||| to make filters. By CCG combinatorics, the relative
bodyText ||| clause could either attach to asbestos or to a form of
bodyText ||| asbestos. The gold standard CCG parse attaches the
bodyText ||| relative clause to a form of asbestos (figure 8). Prop-
bodyText ||| bank agrees with this analysis, assigning Arg1 of use
bodyText ||| to the constituent a form of asbestos. The automatic
bodyText ||| parser, however, attaches the relative clause low â to
bodyText ||| asbestos (figure 9). When the system is given the au-
bodyText ||| tomatically generated parse, it incorrectly assigns the
bodyText ||| semantic role to asbestos. In cases where the parser at-
bodyText ||| taches the relative clause correctly, the system is much
bodyText ||| more likely to assign the role correctly.
bodyText ||| Problems with relative clause attachment to genitives
bodyText ||| are not limited to automatic parses â errors in gold-
bodyText ||| standard treebank parses cause similar problems when
bodyText ||| Treebank parses disagree with Propbank annotator in-
bodyText ||| tuitions. In the phrase a group of workers exposed to
bodyText ||| asbestos (figure 10), the gold standard CCG parse at-
bodyText ||| taches the relative clause to workers. Propbank, how-
bodyText ||| ever, annotates a group of workers as Arg 1 of exposed,
bodyText ||| rather than following the parse and assigning the role
bodyText ||| only to workers. The system again follows the parse
bodyText ||| and incorrectly assigns the role to workers instead of a
bodyText ||| group of workers. Interestingly, the C&amp;C parser opts
bodyText ||| for high attachment in this instance, resulting in the
figureCaption ||| Figure 8: CCGbank gold-standard parse of a relative
figureCaption ||| clause attachment. The system correctly identifies a
figureCaption ||| form of asbestos as Arg1 of used. (wsj 0003.1)
figure ||| a form	of	asbestos used to make filters
figure ||| np	(np\np)/np np â Arg1	np\np
figure ||| ï¿½
figure ||| 	ï¿½
figure ||| np\np
figure ||| ï¿½
figureCaption ||| Figure 9: Automatic parse of the noun phrase in fig-
figureCaption ||| ure 8. Incorrect relative clause attachment causes the
figureCaption ||| misidentification of asbestos as a semantic role bearing
figureCaption ||| unit. (wsj 0003.1)
bodyText ||| correct prediction of a group of workers as Arg1 of ex-
bodyText ||| posed in the automatic parse.
sectionHeader ||| 12 Future Work
bodyText ||| As described in the error analysis section, a large num-
bodyText ||| ber of errors in the system are attributable to errors in
bodyText ||| the CCG derivation, either in the gold standard or in
bodyText ||| automatically generated parses. Potential future work
bodyText ||| may focus on developing an improved CCG parser us-
bodyText ||| ing the revised (syntactic) adjunct-argument distinc-
bodyText ||| tions (guided by the Propbank annotation) described in
bodyText ||| (Boxwell and White, 2008). This resource, together
bodyText ||| with the reasonable accuracy (,: 90%) with which ar-
bodyText ||| gument mappings can be predicted, suggests the possi-
bodyText ||| bility of an integrated, simultaneous syntactic-semantic
bodyText ||| parsing process, similar to that of (Musillo and Merlo,
bodyText ||| 2006; Merlo and Musillo, 2008). We expect this would
bodyText ||| improve the reliability and accuracy of both the syntac-
bodyText ||| tic and semantic analysis components.
sectionHeader ||| 13 Acknowledgments
bodyText ||| This research was funded by NSF grant IIS-0347799.
bodyText ||| We are deeply indebted to Julia Hockenmaier for the
figure ||| np
figure ||| np
page ||| 43
figure ||| the company	stopped	using	asbestos	in 1956
figure ||| np	((s[dcl]\np)/(s[ng]\np)) (s[ng]\np)/np	np	(s\np)\(s\np)
figure ||| 	ï¿½
figure ||| s[ng]\np
figure ||| s[ng]\np - stop.Arg1
figure ||| s[dcl]\np
figure ||| s[dcl]
figure ||| ï¿½
figure ||| ï¿½
figure ||| ï¿½
figureCaption ||| Figure 7: An example of how incorrect PP attachment can cause an incorrect labeling. Stop.Arg1 should cover us-
figureCaption ||| ing asbestos rather than using asbestos in 1956. This sentence is based on wsj 0003.3, with the structure simplified
figureCaption ||| for clarity.
figure ||| a group	of	workers	exposed to asbestos
figure ||| np	(np\np)/np np - exposed.Arg1	np\np
figure ||| 					ï¿½
figure ||| 					ï¿½
figure ||| 					ï¿½
figure ||| 				np
figure ||| 			np\np
figure ||| 		np
figureCaption ||| Figure 10: Propbank annotates a group of workers as Arg1 of exposed, while CCGbank attaches the relative clause
figureCaption ||| low. The system incorrectly labels workers as a role bearing unit. (Gold standard â wsj 0003.1)
bodyText ||| use of her PARG generation tool.
sectionHeader ||| References
reference ||| Srinivas Bangalore and Aravind Joshi. 1999. Su-
reference ||| pertagging: An approach to almost parsing. Com-
reference ||| putational Linguistics, 25(2):237â265.
reference ||| Adam L. Berger, S. Della Pietra, and V. Della Pietra.
reference ||| 1996. A maximum entropy approach to natural
reference ||| language processing. Computational Linguistics,
reference ||| 22(1):39â71.
reference ||| D.M. Bikel. 2004. Intricacies of Collinsâ parsing
reference ||| model. Computational Linguistics, 30(4):479â511.
reference ||| Stephen A. Boxwell and Michael White. 2008.
reference ||| Projecting propbank roles onto the ccgbank. In
reference ||| Proceedings of the Sixth International Language
reference ||| Resources and Evaluation Conference (LREC-08),
reference ||| Marrakech, Morocco.
reference ||| J. Carroll, G. Minnen, and T. Briscoe. 2003. Parser
reference ||| evaluation. Treebanks: Building and Using Parsed
reference ||| Corpora, pages 299â316.
reference ||| E. Charniak. 2001. Immediate-head parsing for lan-
reference ||| guage models. In Proc. ACL-01, volume 39, pages
reference ||| 116â123.
reference ||| Stephen Clark and James R. Curran. 2007. Wide-
reference ||| coverage Efficient Statistical Parsing with CCG and
reference ||| Log-linear Models. Computational Linguistics,
reference ||| 33(4):493â552.
reference ||| Stephen Clark. 2002. Supertagging for combinatory
reference ||| categorial grammar. In Proceedings of the 6th In-
reference ||| ternational Workshop on Tree Adjoining Grammars
reference ||| and Related Frameworks (TAG+6), pages 19â24,
reference ||| Venice, Italy.
reference ||| M. Collins. 2003. Head-driven statistical models for
reference ||| natural language parsing. Computational Linguis-
reference ||| tics, 29(4):589â637.
reference ||| Daniel Gildea and Julia Hockenmaier. 2003. Identi-
reference ||| fying semantic roles using Combinatory Categorial
reference ||| Grammar. In Proc. EMNLP-03.
reference ||| Julia Hockenmaier and Mark Steedman. 2007. CCG-
reference ||| bank: A Corpus of CCG Derivations and Depen-
reference ||| dency Structures Extracted from the Penn Treebank.
reference ||| Computational Linguistics, 33(3):355â396.
reference ||| R. Johansson and P. Nugues. 2008. Dependency-
reference ||| based syntacticâsemantic analysis with PropBank
reference ||| and NomBank. Proceedings of CoNLL â2008.
reference ||| D C Liu and Jorge Nocedal. 1989. On the limited
reference ||| memory method for large scale optimization. Math-
reference ||| ematical Programming B, 45(3).
reference ||| Lluis M`arquez, Xavier Carreras, Kenneth C. Litowski,
reference ||| and Suzanne Stevenson. 2008. Semantic Role La-
reference ||| beling: An Introduction to the Special Issue. Com-
reference ||| putational Linguistics, 34(2):145â159.
reference ||| Paola Merlo and Gabrile Musillo. 2008. Semantic
reference ||| parsing for high-precision semantic role labelling. In
reference ||| Proceedings of CONLL-08, Manchester, UK.
reference ||| Gabriele Musillo and Paola Merlo. 2006. Robust pars-
reference ||| ing of the proposition bank. In Proceedings of the
reference ||| EACL 2006 Workshop ROMAND, Trento.
reference ||| J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
reference ||| S. KÂ¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
reference ||| Parser: A language-independent system for data-
reference ||| driven dependency parsing. Natural Language En-
reference ||| gineering, 13(02):95â135.
reference ||| Martha Palmer, Daniel Gildea, and Paul Kingsbury.
reference ||| 2005. The Proposition Bank: An Annotated Cor-
reference ||| pus of Semantic Roles. Computational Linguistics,
reference ||| 31(1):71â106.
page ||| 44
reference ||| Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
reference ||| The Importance of Syntactic Parsing and Inference
reference ||| in Semantic Role Labeling. Computational Linguis-
reference ||| tics, 34(2):257â287.
reference ||| Mark Steedman. 2000. The Syntactic Process. MIT
reference ||| Press.
reference ||| M. Surdeanu, L. M`arquez, X. Carreras, and P. Comas.
reference ||| 2007. Combination strategies for semantic role la-
reference ||| beling. Journal of Artificial Intelligence Research,
reference ||| 29:105â151.
reference ||| K. Toutanova, A. Haghighi, and C.D. Manning. 2008.
reference ||| A global joint model for semantic role labeling.
reference ||| Computational Linguistics, 34(2):161â191.
page ||| 45
