title ||| Unsupervised Argument Identification for Semantic Role Labeling
author ||| Omri Abend&apos; Roi Reichart2 Ari Rappoport&apos;
affiliation ||| &apos;Institute of Computer Science, 2ICNC
affiliation ||| Hebrew University of Jerusalem
email ||| {omria01|roiri|arir}@cs.huji.ac.il
sectionHeader ||| Abstract
bodyText ||| The task of Semantic Role Labeling
bodyText ||| (SRL) is often divided into two sub-tasks:
bodyText ||| verb argument identification, and argu-
bodyText ||| ment classification. Current SRL algo-
bodyText ||| rithms show lower results on the identifi-
bodyText ||| cation sub-task. Moreover, most SRL al-
bodyText ||| gorithms are supervised, relying on large
bodyText ||| amounts of manually created data. In
bodyText ||| this paper we present an unsupervised al-
bodyText ||| gorithm for identifying verb arguments,
bodyText ||| where the only type of annotation required
bodyText ||| is POS tagging. The algorithm makes use
bodyText ||| of a fully unsupervised syntactic parser,
bodyText ||| using its output in order to detect clauses
bodyText ||| and gather candidate argument colloca-
bodyText ||| tion statistics. We evaluate our algorithm
bodyText ||| on PropBank10, achieving a precision of
bodyText ||| 56%, as opposed to 47% of a strong base-
bodyText ||| line. We also obtain an 8% increase in
bodyText ||| precision for a Spanish corpus. This is
bodyText ||| the first paper that tackles unsupervised
bodyText ||| verb argument identification without using
bodyText ||| manually encoded rules or extensive lexi-
bodyText ||| cal or syntactic resources.
sectionHeader ||| 1 Introduction
bodyText ||| Semantic Role Labeling (SRL) is a major NLP
bodyText ||| task, providing a shallow sentence-level semantic
bodyText ||| analysis. SRL aims at identifying the relations be-
bodyText ||| tween the predicates (usually, verbs) in the sen-
bodyText ||| tence and their associated arguments.
bodyText ||| The SRL task is often viewed as consisting of
bodyText ||| two parts: argument identification (ARGID) and ar-
bodyText ||| gument classification. The former aims at identi-
bodyText ||| fying the arguments of a given predicate present
bodyText ||| in the sentence, while the latter determines the
bodyText ||| type of relation that holds between the identi-
bodyText ||| fied arguments and their corresponding predicates.
bodyText ||| The division into two sub-tasks is justified by
bodyText ||| the fact that they are best addressed using differ-
bodyText ||| ent feature sets (Pradhan et al., 2005). Perfor-
bodyText ||| mance in the ARGID stage is a serious bottleneck
bodyText ||| for general SRL performance, since only about
bodyText ||| 81% of the arguments are identified, while about
bodyText ||| 95% of the identified arguments are labeled cor-
bodyText ||| rectly (M`arquez et al., 2008).
bodyText ||| SRL is a complex task, which is reflected by the
bodyText ||| algorithms used to address it. A standard SRL al-
bodyText ||| gorithm requires thousands to dozens of thousands
bodyText ||| sentences annotated with POS tags, syntactic an-
bodyText ||| notation and SRL annotation. Current algorithms
bodyText ||| show impressive results but only for languages and
bodyText ||| domains where plenty of annotated data is avail-
bodyText ||| able, e.g., English newspaper texts (see Section 2).
bodyText ||| Results are markedly lower when testing is on a
bodyText ||| domain wider than the training one, even in En-
bodyText ||| glish (see the WSJ-Brown results in (Pradhan et
bodyText ||| al., 2008)).
bodyText ||| Only a small number of works that do not re-
bodyText ||| quire manually labeled SRL training data have
bodyText ||| been done (Swier and Stevenson, 2004; Swier and
bodyText ||| Stevenson, 2005; Grenager and Manning, 2006).
bodyText ||| These papers have replaced this data with the
bodyText ||| VerbNet (Kipper et al., 2000) lexical resource or
bodyText ||| a set of manually written rules and supervised
bodyText ||| parsers.
bodyText ||| A potential answer to the SRL training data bot-
bodyText ||| tleneck are unsupervised SRL models that require
bodyText ||| little to no manual effort for their training. Their
bodyText ||| output can be used either by itself, or as training
bodyText ||| material for modern supervised SRL algorithms.
bodyText ||| In this paper we present an algorithm for unsu-
bodyText ||| pervised argument identification. The only type of
bodyText ||| annotation required by our algorithm is POS tag-
page ||| 28
note ||| Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 28â36,
note ||| Suntec, Singapore, 2-7 August 2009. cï¿½2009 ACL and AFNLP
bodyText ||| ging, which needs relatively little manual effort.
bodyText ||| The algorithm consists of two stages. As pre-
bodyText ||| processing, we use a fully unsupervised parser to
bodyText ||| parse each sentence. Initially, the set of possi-
bodyText ||| ble arguments for a given verb consists of all the
bodyText ||| constituents in the parse tree that do not contain
bodyText ||| that predicate. The first stage of the algorithm
bodyText ||| attempts to detect the minimal clause in the sen-
bodyText ||| tence that contains the predicate in question. Us-
bodyText ||| ing this information, it further reduces the possible
bodyText ||| arguments only to those contained in the minimal
bodyText ||| clause, and further prunes them according to their
bodyText ||| position in the parse tree. In the second stage we
bodyText ||| use pointwise mutual information to estimate the
bodyText ||| collocation strength between the arguments and
bodyText ||| the predicate, and use it to filter out instances of
bodyText ||| weakly collocating predicate argument pairs.
bodyText ||| We use two measures to evaluate the perfor-
bodyText ||| mance of our algorithm, precision and F-score.
bodyText ||| Precision reflects the algorithmâs applicability for
bodyText ||| creating training data to be used by supervised
bodyText ||| SRL models, while the standard SRL F-score mea-
bodyText ||| sures the modelâs performance when used by it-
bodyText ||| self. The first stage of our algorithm is shown to
bodyText ||| outperform a strong baseline both in terms of F-
bodyText ||| score and of precision. The second stage is shown
bodyText ||| to increase precision while maintaining a reason-
bodyText ||| able recall.
bodyText ||| We evaluated our model on sections 2-21 of
bodyText ||| Propbank. As is customary in unsupervised pars-
bodyText ||| ing work (e.g. (Seginer, 2007)), we bounded sen-
bodyText ||| tence length by 10 (excluding punctuation). Our
bodyText ||| first stage obtained a precision of 52.8%, which is
bodyText ||| more than 6% improvement over the baseline. Our
bodyText ||| second stage improved precision to nearly 56%, a
bodyText ||| 9.3% improvement over the baseline. In addition,
bodyText ||| we carried out experiments on Spanish (on sen-
bodyText ||| tences of length bounded by 15, excluding punctu-
bodyText ||| ation), achieving an increase of over 7.5% in pre-
bodyText ||| cision over the baseline. Our algorithm increases
bodyText ||| Fâscore as well, showing an 1.8% improvement
bodyText ||| over the baseline in English and a 2.2% improve-
bodyText ||| ment in Spanish.
bodyText ||| Section 2 reviews related work. In Section 3 we
bodyText ||| detail our algorithm. Sections 4 and 5 describe the
bodyText ||| experimental setup and results.
sectionHeader ||| 2 Related Work
bodyText ||| The advance of machine learning based ap-
bodyText ||| proaches in this field owes to the usage of large
bodyText ||| scale annotated corpora. English is the most stud-
bodyText ||| ied language, using the FrameNet (FN) (Baker et
bodyText ||| al., 1998) and PropBank (PB) (Palmer et al., 2005)
bodyText ||| resources. PB is a corpus well suited for evalu-
bodyText ||| ation, since it annotates every non-auxiliary verb
bodyText ||| in a real corpus (the WSJ sections of the Penn
bodyText ||| Treebank). PB is a standard corpus for SRL eval-
bodyText ||| uation and was used in the CoNLL SRL shared
bodyText ||| tasks of 2004 (Carreras and M`arquez, 2004) and
bodyText ||| 2005 (Carreras and M`arquez, 2005).
bodyText ||| Most work on SRL has been supervised, requir-
bodyText ||| ing dozens of thousands of SRL annotated train-
bodyText ||| ing sentences. In addition, most models assume
bodyText ||| that a syntactic representation of the sentence is
bodyText ||| given, commonly in the form of a parse tree, a de-
bodyText ||| pendency structure or a shallow parse. Obtaining
bodyText ||| these is quite costly in terms of required human
bodyText ||| annotation.
bodyText ||| The first work to tackle SRL as an indepen-
bodyText ||| dent task is (Gildea and Jurafsky, 2002), which
bodyText ||| presented a supervised model trained and evalu-
bodyText ||| ated on FrameNet. The CoNLL shared tasks of
bodyText ||| 2004 and 2005 were devoted to SRL, and stud-
bodyText ||| ied the influence of different syntactic annotations
bodyText ||| and domain changes on SRL results. Computa-
bodyText ||| tional Linguistics has recently published a special
bodyText ||| issue on the task (M`arquez et al., 2008), which
bodyText ||| presents state-of-the-art results and surveys the lat-
bodyText ||| est achievements and challenges in the field.
bodyText ||| Most approaches to the task use a multi-level
bodyText ||| approach, separating the task to an ARGID and an
bodyText ||| argument classification sub-tasks. They then use
bodyText ||| the unlabeled argument structure (without the se-
bodyText ||| mantic roles) as training data for the ARGID stage
bodyText ||| and the entire data (perhaps with other features)
bodyText ||| for the classification stage. Better performance
bodyText ||| is achieved on the classification, where state-
bodyText ||| of-the-art supervised approaches achieve about
bodyText ||| 81% F-score on the in-domain identification task,
bodyText ||| of which about 95% are later labeled correctly
bodyText ||| (M`arquez et al., 2008).
bodyText ||| There have been several exceptions to the stan-
bodyText ||| dard architecture described in the last paragraph.
bodyText ||| One suggestion poses the problem of SRL as a se-
bodyText ||| quential tagging of words, training an SVM clas-
bodyText ||| sifier to determine for each word whether it is in-
bodyText ||| side, outside or in the beginning of an argument
bodyText ||| (Hacioglu and Ward, 2003). Other works have in-
bodyText ||| tegrated argument classification and identification
bodyText ||| into one step (Collobert and Weston, 2007), while
bodyText ||| others went further and combined the former two
bodyText ||| along with parsing into a single model (Musillo
page ||| 29
bodyText ||| and Merlo, 2006).
bodyText ||| Work on less supervised methods has been
bodyText ||| scarce. Swier and Stevenson (2004) and Swier
bodyText ||| and Stevenson (2005) presented the first model
bodyText ||| that does not use an SRL annotated corpus. How-
bodyText ||| ever, they utilize the extensive verb lexicon Verb-
bodyText ||| Net, which lists the possible argument structures
bodyText ||| allowable for each verb, and supervised syntac-
bodyText ||| tic tools. Using VerbNet along with the output of
bodyText ||| a rule-based chunker (in 2004) and a supervised
bodyText ||| syntactic parser (in 2005), they spot instances in
bodyText ||| the corpus that are very similar to the syntactic
bodyText ||| patterns listed in VerbNet. They then use these as
bodyText ||| seed for a bootstrapping algorithm, which conse-
bodyText ||| quently identifies the verb arguments in the corpus
bodyText ||| and assigns their semantic roles.
bodyText ||| Another less supervised work is that
bodyText ||| of (Grenager and Manning, 2006), which presents
bodyText ||| a Bayesian network model for the argument
bodyText ||| structure of a sentence. They use EM to learn
bodyText ||| the modelâs parameters from unannotated data,
bodyText ||| and use this model to tag a test corpus. However,
bodyText ||| ARGID was not the task of that work, which dealt
bodyText ||| solely with argument classification. ARGID was
bodyText ||| performed by manually-created rules, requiring a
bodyText ||| supervised or manual syntactic annotation of the
bodyText ||| corpus to be annotated.
bodyText ||| The three works above are relevant but incom-
bodyText ||| parable to our work, due to the extensive amount
bodyText ||| of supervision (namely, VerbNet and a rule-based
bodyText ||| or supervised syntactic system) they used, both in
bodyText ||| detecting the syntactic structure and in detecting
bodyText ||| the arguments.
bodyText ||| Work has been carried out in a few other lan-
bodyText ||| guages besides English. Chinese has been studied
bodyText ||| in (Xue, 2008). Experiments on Catalan and Span-
bodyText ||| ish were done in SemEval 2007 (M`arquez et al.,
bodyText ||| 2007) with two participating systems. Attempts
bodyText ||| to compile corpora for German (Burdchardt et al.,
bodyText ||| 2006) and Arabic (Diab et al., 2008) are also un-
bodyText ||| derway. The small number of languages for which
bodyText ||| extensive SRL annotated data exists reflects the
bodyText ||| considerable human effort required for such en-
bodyText ||| deavors.
bodyText ||| Some SRL works have tried to use unannotated
bodyText ||| data to improve the performance of a base su-
bodyText ||| pervised model. Methods used include bootstrap-
bodyText ||| ping approaches (Gildea and Jurafsky, 2002; Kate
bodyText ||| and Mooney, 2007), where large unannotated cor-
bodyText ||| pora were tagged with SRL annotation, later to
bodyText ||| be used to retrain the SRL model. Another ap-
bodyText |||  proach used similarity measures either between
bodyText ||| verbs (Gordon and Swanson, 2007) or between
bodyText ||| nouns (Gildea and Jurafsky, 2002) to overcome
bodyText ||| lexical sparsity. These measures were estimated
bodyText ||| using statistics gathered from corpora augmenting
bodyText ||| the modelâs training data, and were then utilized
bodyText ||| to generalize across similar verbs or similar argu-
bodyText ||| ments.
bodyText ||| Attempts to substitute full constituency pars-
bodyText ||| ing by other sources of syntactic information have
bodyText ||| been carried out in the SRL community. Sugges-
bodyText ||| tions include posing SRL as a sequence labeling
bodyText ||| problem (M`arquez et al., 2005) or as an edge tag-
bodyText ||| ging problem in a dependency representation (Ha-
bodyText ||| cioglu, 2004). Punyakanok et al. (2008) provide
bodyText ||| a detailed comparison between the impact of us-
bodyText ||| ing shallow vs. full constituency syntactic infor-
bodyText ||| mation in an English SRL system. Their results
bodyText ||| clearly demonstrate the advantage of using full an-
bodyText ||| notation.
bodyText ||| The identification of arguments has also been
bodyText ||| carried out in the context of automatic subcatego-
bodyText ||| rization frame acquisition. Notable examples in-
bodyText ||| clude (Manning, 1993; Briscoe and Carroll, 1997;
bodyText ||| Korhonen, 2002) who all used statistical hypothe-
bodyText ||| sis testing to filter a parserâs output for arguments,
bodyText ||| with the goal of compiling verb subcategorization
bodyText ||| lexicons. However, these works differ from ours
bodyText ||| as they attempt to characterize the behavior of a
bodyText ||| verb type, by collecting statistics from various in-
bodyText ||| stances of that verb, and not to determine which
bodyText ||| are the arguments of specific verb instances.
bodyText ||| The algorithm presented in this paper performs
bodyText ||| unsupervised clause detection as an intermedi-
bodyText ||| ate step towards argument identification. Super-
bodyText ||| vised clause detection was also tackled as a sepa-
bodyText ||| rate task, notably in the CoNLL 2001 shared task
bodyText ||| (Tjong Kim Sang and D`ejean, 2001). Clause in-
bodyText ||| formation has been applied to accelerating a syn-
bodyText ||| tactic parser (Glaysher and Moldovan, 2006).
sectionHeader ||| 3 Algorithm
bodyText ||| In this section we describe our algorithm. It con-
bodyText ||| sists of two stages, each of which reduces the set
bodyText ||| of argument candidates, which a-priori contains all
bodyText ||| consecutive sequences of words that do not con-
bodyText ||| tain the predicate in question.
subsectionHeader ||| 3.1 Algorithm overview
bodyText ||| As pre-processing, we use an unsupervised parser
bodyText ||| that generates an unlabeled parse tree for each sen-
page ||| 30
bodyText ||| tence (Seginer, 2007). This parser is unique in that
bodyText ||| it is able to induce a bracketing (unlabeled pars-
bodyText ||| ing) from raw text (without even using POS tags)
bodyText ||| achieving state-of-the-art results. Since our algo-
bodyText ||| rithm uses millions to tens of millions sentences,
bodyText ||| we must use very fast tools. The parserâs high
bodyText ||| speed (thousands of words per second) enables us
bodyText ||| to process these large amounts of data.
bodyText ||| The only type of supervised annotation we
bodyText ||| use is POS tagging. We use the taggers MX-
bodyText ||| POST (Ratnaparkhi, 1996) for English and Tree-
bodyText ||| Tagger (Schmid, 1994) for Spanish, to obtain POS
bodyText ||| tags for our model.
bodyText ||| The first stage of our algorithm uses linguisti-
bodyText ||| cally motivated considerations to reduce the set of
bodyText ||| possible arguments. It does so by confining the set
bodyText ||| of argument candidates only to those constituents
bodyText ||| which obey the following two restrictions. First,
bodyText ||| they should be contained in the minimal clause
bodyText ||| containing the predicate. Second, they should be
bodyText ||| k-th degree cousins of the predicate in the parse
bodyText ||| tree. We propose a novel algorithm for clause de-
bodyText ||| tection and use its output to determine which of
bodyText ||| the constituents obey these two restrictions.
bodyText ||| The second stage of the algorithm uses point-
bodyText ||| wise mutual information to rule out constituents
bodyText ||| that appear to be weakly collocating with the pred-
bodyText ||| icate in question. Since a predicate greatly re-
bodyText ||| stricts the type of arguments with which it may
bodyText ||| appear (this is often referred to as âselectional re-
bodyText ||| strictionsâ), we expect it to have certain character-
bodyText ||| istic arguments with which it is likely to collocate.
subsectionHeader ||| 3.2 Clause detection stage
bodyText ||| The main idea behind this stage is the observation
bodyText ||| that most of the arguments of a predicate are con-
bodyText ||| tained within the minimal clause that contains the
bodyText ||| predicate. We tested this on our development data
bodyText ||| â section 24 of the WSJ PTB, where we saw that
bodyText ||| 86% of the arguments that are also constituents
bodyText ||| (in the gold standard parse) were indeed contained
bodyText ||| in that minimal clause (as defined by the tree la-
bodyText ||| bel types in the gold standard parse that denote
bodyText ||| a clause, e.g., S, SBAR). Since we are not pro-
bodyText ||| vided with clause annotation (or any label), we at-
bodyText ||| tempted to detect them in an unsupervised manner.
bodyText ||| Our algorithm attempts to find sub-trees within the
bodyText ||| parse tree, whose structure resembles the structure
bodyText ||| of a full sentence. This approximates the notion of
bodyText ||| a clause.
figure ||| VBP L
figure ||| L
figure ||| VBP L
figureCaption ||| Figure 1: An example of an unlabeled POS tagged
figureCaption ||| parse tree. The middle tree is the ST of âreachâ
figureCaption ||| with the root as the encoded ancestor. The bot-
figureCaption ||| tom one is the ST with its parent as the encoded
figureCaption ||| ancestor.
bodyText ||| Statistics gathering. In order to detect which
bodyText ||| of the verbâs ancestors is the minimal clause, we
bodyText ||| score each of the ancestors and select the one that
bodyText ||| maximizes the score. We represent each ancestor
bodyText ||| using its Spinal Tree (ST). The ST of a given
bodyText ||| verbâs ancestor is obtained by replacing all the
bodyText ||| constituents that do not contain the verb by a leaf
bodyText ||| having a label. This effectively encodes all the k-
bodyText ||| th degree cousins of the verb (for every k). The
bodyText ||| leaf labels are either the wordâs POS in case the
bodyText ||| constituent is a leaf, or the generic label âLâ de-
bodyText ||| noting a non-leaf. See Figure 1 for an example.
bodyText ||| In this stage we collect statistics of the occur-
bodyText ||| rences of STs in a large corpus. For every ST in
bodyText ||| the corpus, we count the number of times it oc-
bodyText ||| curs in a form we consider to be a clause (positive
bodyText ||| examples), and the number of times it appears in
bodyText ||| other forms (negative examples).
bodyText ||| Positive examples are divided into two main
bodyText ||| types. First, when the ST encodes the root an-
bodyText ||| cestor (as in the middle tree of Figure 1); second,
bodyText ||| when the ancestor complies to a clause lexico-
bodyText ||| syntactic pattern. In many languages there is a
bodyText ||| small set of lexico-syntactic patterns that mark a
bodyText ||| clause, e.g. the English âthatâ, the German âdassâ
bodyText ||| and the Spanish âqueâ. The patterns which were
bodyText ||| used in our experiments are shown in Figure 2.
bodyText ||| For each verb instance, we traverse over its an-
figure ||| L
figure ||| L
figure ||| L
figure ||| L
figure ||| IN
figure ||| DT
figure ||| NNS
figure ||| The
figure ||| materials
figure ||| L
figure ||| L
figure ||| VBP
figure ||| L
figure ||| in	DT NN
figure ||| NNS
figure ||| IN
figure ||| students
figure ||| CD
figure ||| about	90
figure ||| each	set
figure ||| reach
figure ||| L
figure ||| L
figure ||| L	L
figure ||| L	L
page ||| 31
figure ||| English
figure ||| TO + VB. The constituent starts with âtoâ followed by a verb in infinitive form.
figure ||| WP. The constituent is preceded by a Wh-pronoun.
figure ||| That. The constituent is preceded by a âthatâ marked by an âINâ POS tag indicating that it is a subordinating conjunction.
figure ||| Spanish
figure ||| CQUE. The constituent is preceded by a word with the POS âCQUEâ which denotes the word âqueâ as a con-junction.
figure ||| INT. The constituent is preceded by a word with the POS âINTâ which denotes an interrogative pronoun.
figure ||| CSUB. The constituent is preceded by a word with one of the POSs âCSUBFâ, âCSUBIâ or âCSUBXâ, which denote a subordinating conjunction.
figureCaption ||| Figure 2: The set of lexico-syntactic patterns that
figureCaption ||| mark clauses which were used by our model.
bodyText ||| cestors from top to bottom. For each of them we
bodyText ||| update the following counters: sentence(5T) for
bodyText ||| the root ancestorâs 5T, patternz (5T) for the ones
bodyText ||| complying to the i-th lexico-syntactic pattern and
bodyText ||| negative(5T) for the other ancestors1.
bodyText ||| Clause detection. At test time, when detecting
bodyText ||| the minimal clause of a verb instance, we use
bodyText ||| the statistics collected in the previous stage. De-
bodyText ||| note the ancestors of the verb with A1 ... Am.
bodyText ||| For each of them, we calculate clause(5TA, )
bodyText ||| and total (5TA, ). clause(5TA,) is the sum
bodyText ||| of sentence(5TA,) and patternz (5TA,) if this
bodyText ||| ancestor complies to the i-th pattern (if there
bodyText ||| is no such pattern, clause(5TA,) is equal to
bodyText ||| sentence (5TA, )). total (5TA,) is the sum of
bodyText ||| clause(5TA,) and negative(5TA, ).
bodyText ||| The selected ancestor is given by:
bodyText ||| clause(STA, )
equation ||| (1) Amax = argmaxA,  total(STA,)
bodyText ||| An 5T whose total(5T) is less than a small
bodyText ||| threshold2 is not considered a candidate to be the
bodyText ||| minimal clause, since its statistics may be un-
bodyText ||| reliable. In case of a tie, we choose the low-
bodyText ||| est constituent that obtained the maximal score.
footnote ||| 1If while traversing the tree, we encounter an ancestor
footnote ||| whose first word is preceded by a coordinating conjunction
footnote ||| (marked by the POS tag âCCâ), we refrain from performing
footnote ||| any additional counter updates. Structures containing coor-
footnote ||| dinating conjunctions tend not to obey our lexico-syntactic
footnote ||| rules.
footnote ||| 2We used 4 per million sentences, derived from develop-
footnote ||| ment data.
bodyText ||| If there is only one verb in the sentence3 or if
bodyText ||| clause(5TA,) = 0 for every 1 G j G m, we
bodyText ||| choose the top level constituent by default to be
bodyText ||| the minimal clause containing the verb. Other-
bodyText ||| wise, the minimal clause is defined to be the yield
bodyText ||| of the selected ancestor.
bodyText ||| Argument identification. For each predicate in
bodyText ||| the corpus, its argument candidates are now de-
bodyText ||| fined to be the constituents contained in the min-
bodyText ||| imal clause containing the predicate. However,
bodyText ||| these constituents may be (and are) nested within
bodyText ||| each other, violating a major restriction on SRL
bodyText ||| arguments. Hence we now prune our set, by keep-
bodyText ||| ing only the siblings of all of the verbâs ancestors,
bodyText ||| as is common in supervised SRL (Xue and Palmer,
bodyText ||| 2004).
subsectionHeader ||| 3.3 Using collocations
bodyText ||| We use the following observation to filter out some
bodyText ||| superfluous argument candidates: since the argu-
bodyText ||| ments of a predicate many times bear a semantic
bodyText ||| connection with that predicate, they consequently
bodyText ||| tend to collocate with it.
bodyText ||| We collect collocation statistics from a large
bodyText ||| corpus, which we annotate with parse trees and
bodyText ||| POS tags. We mark arguments using the argu-
bodyText ||| ment detection algorithm described in the previous
bodyText ||| two sections, and extract all (predicate, argument)
bodyText ||| pairs appearing in the corpus. Recall that for each
bodyText ||| sentence, the arguments are a subset of the con-
bodyText ||| stituents in the parse tree.
bodyText ||| We use two representations of an argument: one
bodyText ||| is the POS tag sequence of the terminals contained
bodyText ||| in the argument, the other is its head word4. The
bodyText ||| predicate is represented as the conjunction of its
bodyText ||| lemma with its POS tag.
bodyText ||| Denote the number of times a predicate x
bodyText ||| appeared with an argument y by nxy. Denote
bodyText ||| the total number of (predicate, argument) pairs
bodyText ||| by N. Using these notations, we define the
bodyText ||| following quantities: nx = Eynxy, ny = Exnxy,
bodyText ||| p(x) = nï¿½N , p(y) = nï¿½N and p(x, y) = nx N . The
bodyText ||| pointwise mutual information of x and y is then
bodyText ||| given by:
footnote ||| 3In this case, every argument in the sentence must be re-
footnote ||| lated to that verb.
footnote ||| 4Since we do not have syntactic labels, we use an approx-
footnote ||| imate notion. For English we use the Bikel parser default
footnote ||| head word rules (Bikel, 2004). For Spanish, we use the left-
footnote ||| most word.
page ||| 32
equation ||| (2) PMI(x, y) = log  p( x) P(y) =log nï¿½y
equation ||| (nï¿½ ï¿½ny)/N
bodyText ||| PMI effectively measures the ratio between
bodyText ||| the number of times x and y appeared together and
bodyText ||| the number of times they were expected to appear,
bodyText ||| had they been independent.
bodyText ||| At test time, when an (x, y) pair is observed, we
bodyText ||| check if PMI (x, y), computed on the large cor-
bodyText ||| pus, is lower than a threshold a for either of xâs
bodyText ||| representations. If this holds, for at least one rep-
bodyText ||| resentation, we prune all instances of that (x, y)
bodyText ||| pair. The parameter a may be selected differently
bodyText ||| for each of the argument representations.
bodyText ||| In order to avoid using unreliable statistics,
bodyText ||| we apply this for a given pair only if n .ny N&gt;
bodyText ||| r, for some parameter r. That is, we consider
bodyText ||| PMI (x, y) to be reliable, only if the denomina-
bodyText ||| tor in equation (2) is sufficiently large.
sectionHeader ||| 4 Experimental Setup
bodyText ||| Corpora. We used the PropBank corpus for de-
bodyText ||| velopment and for evaluation on English. Section
bodyText ||| 24 was used for the development of our model,
bodyText ||| and sections 2 to 21 were used as our test data.
bodyText ||| The free parameters of the collocation extraction
bodyText ||| phase were tuned on the development data. Fol-
bodyText ||| lowing the unsupervised parsing literature, multi-
bodyText ||| ple brackets and brackets covering a single word
bodyText ||| are omitted. We exclude punctuation according
bodyText ||| to the scheme of (Klein, 2005). As is customary
bodyText ||| in unsupervised parsing (e.g. (Seginer, 2007)), we
bodyText ||| bounded the lengths of the sentences in the cor-
bodyText ||| pus to be at most 10 (excluding punctuation). This
bodyText ||| results in 207 sentences in the development data,
bodyText ||| containing a total of 132 different verbs and 173
bodyText ||| verb instances (of the non-auxiliary verbs in the
bodyText ||| SRL task, see âevaluationâ below) having 403 ar-
bodyText ||| guments. The test data has 6007 sentences con-
bodyText ||| taining 1008 different verbs and 5130 verb in-
bodyText ||| stances (as above) having 12436 arguments.
bodyText ||| Our algorithm requires large amounts of data
bodyText ||| to gather argument structure and collocation pat-
bodyText ||| terns. For the statistics gathering phase of the
bodyText ||| clause detection algorithm, we used 4.5M sen-
bodyText ||| tences of the NANC (Graff, 1995) corpus, bound-
bodyText ||| ing their length in the same manner. In order
bodyText ||| to extract collocations, we used 2M sentences
bodyText ||| from the British National Corpus (Burnard, 2000)
bodyText ||| and about 29M sentences from the Dmoz cor-
bodyText ||| pus (Gabrilovich and Markovitch, 2005). Dmoz
bodyText ||| is a web corpus obtained by crawling and clean-
bodyText ||| ing the URLs in the Open Directory Project
bodyText ||| (dmoz.org). All of the above corpora were parsed
bodyText ||| using Seginerâs parser and POS-tagged by MX-
bodyText ||| POST (Ratnaparkhi, 1996).
bodyText ||| For our experiments on Spanish, we used 3.3M
bodyText ||| sentences of length at most 15 (excluding punctua-
bodyText ||| tion) extracted from the Spanish Wikipedia. Here
bodyText ||| we chose to bound the length by 15 due to the
bodyText ||| smaller size of the available test corpus. The
bodyText ||| same data was used both for the first and the sec-
bodyText ||| ond stages. Our development and test data were
bodyText ||| taken from the training data released for the Se-
bodyText ||| mEval 2007 task on semantic annotation of Span-
bodyText ||| ish (M`arquez et al., 2007). This data consisted
bodyText ||| of 1048 sentences of length up to 15, from which
bodyText ||| 200 were randomly selected as our development
bodyText ||| data and 848 as our test data. The development
bodyText ||| data included 313 verb instances while the test
bodyText ||| data included 1279. All corpora were parsed us-
bodyText ||| ing the Seginer parser and tagged by the âTree-
bodyText ||| Taggerâ (Schmid, 1994).
bodyText ||| Baselines. Since this is the first paper, to our
bodyText ||| knowledge, which addresses the problem of unsu-
bodyText ||| pervised argument identification, we do not have
bodyText ||| any previous results to compare to. We instead
bodyText ||| compare to a baseline which marks all k-th degree
bodyText ||| cousins of the predicate (for every k) as arguments
bodyText ||| (this is the second pruning we use in the clause
bodyText ||| detection stage). We name this baseline the ALL
bodyText ||| COUSINS baseline. We note that a random base-
bodyText ||| line would score very poorly since any sequence of
bodyText ||| terminals which does not contain the predicate is
bodyText ||| a possible candidate. Therefore, beating this ran-
bodyText ||| dom baseline is trivial.
bodyText ||| Evaluation. Evaluation is carried out using
bodyText ||| standard SRL evaluation software5. The algorithm
bodyText ||| is provided with a list of predicates, whose argu-
bodyText ||| ments it needs to annotate. For the task addressed
bodyText ||| in this paper, non-consecutive parts of arguments
bodyText ||| are treated as full arguments. A match is consid-
bodyText ||| ered each time an argument in the gold standard
bodyText ||| data matches a marked argument in our modelâs
bodyText ||| output. An unmatched argument is an argument
bodyText ||| which appears in the gold standard data, and fails
bodyText ||| to appear in our modelâs output, and an exces-
bodyText ||| sive argument is an argument which appears in
bodyText ||| our modelâs output but does not appear in the gold
bodyText ||| standard. Precision and recall are defined accord-
bodyText ||| ingly. We report an F-score as well (the harmonic
bodyText ||| mean of precision and recall). We do not attempt
footnote ||| 5http://www.lsi.upc.edu/âsrlconll/soft.html#software.
page ||| 33
bodyText ||| to identify multi-word verbs, and therefore do not
bodyText ||| report the modelâs performance in identifying verb
bodyText ||| boundaries.
bodyText ||| Since our model detects clauses as an interme-
bodyText ||| diate product, we provide a separate evaluation
bodyText ||| of this task for the English corpus. We show re-
bodyText ||| sults on our development data. We use the stan-
bodyText ||| dard parsing F-score evaluation measure. As a
bodyText ||| gold standard in this evaluation, we mark for each
bodyText ||| of the verbs in our development data the minimal
bodyText ||| clause containing it. A minimal clause is the low-
bodyText ||| est ancestor of the verb in the parse tree that has
bodyText ||| a syntactic label of a clause according to the gold
bodyText ||| standard parse of the PTB. A verb is any terminal
bodyText ||| marked by one of the POS tags of type verb ac-
bodyText ||| cording to the gold standard POS tags of the PTB.
sectionHeader ||| 5 Results
bodyText ||| Our results are shown in Table 1. The left section
bodyText ||| presents results on English and the right section
bodyText ||| presents results on Spanish. The top line lists re-
bodyText ||| sults of the clause detection stage alone. The next
bodyText ||| two lines list results of the full algorithm (clause
bodyText ||| detection + collocations) in two different settings
bodyText ||| of the collocation stage. The bottom line presents
bodyText ||| the performance of the ALL COUSINS baseline.
bodyText ||| In the âCollocation Maximum Precisionâ set-
bodyText ||| ting the parameters of the collocation stage (a and
bodyText ||| r) were generally tuned such that maximal preci-
bodyText ||| sion is achieved while preserving a minimal recall
bodyText ||| level (40% for English, 20% for Spanish on the de-
bodyText ||| velopment data). In the âCollocation Maximum F-
bodyText ||| scoreâ the collocation parameters were generally
bodyText ||| tuned such that the maximum possible F-score for
bodyText ||| the collocation algorithm is achieved.
bodyText ||| The best or close to best F-score is achieved
bodyText ||| when using the clause detection algorithm alone
bodyText ||| (59.14% for English, 23.34% for Spanish). Note
bodyText ||| that for both English and Spanish F-score im-
bodyText ||| provements are achieved via a precision improve-
bodyText ||| ment that is more significant than the recall degra-
bodyText ||| dation. F-score maximization would be the aim of
bodyText ||| a system that uses the output of our unsupervised
bodyText ||| ARGID by itself.
bodyText ||| The âCollocation Maximum Precisionâ
bodyText ||| achieves the best precision level (55.97% for
bodyText ||| English, 21.8% for Spanish) but at the expense
bodyText ||| of the largest recall loss. Still, it maintains a
bodyText ||| reasonable level of recall. The âCollocation
bodyText ||| Maximum F-scoreâ is an example of a model that
bodyText ||| provides a precision improvement (over both the
bodyText ||| baseline and the clause detection stage) with a
bodyText ||| relatively small recall degradation. In the Spanish
bodyText ||| experiments its F-score (23.87%) is even a bit
bodyText ||| higher than that of the clause detection stage
bodyText ||| (23.34%).
bodyText ||| The full twoâstage algorithm (clause detection
bodyText ||| + collocations) should thus be used when we in-
bodyText ||| tend to use the modelâs output as training data for
bodyText ||| supervised SRL engines or supervised ARGID al-
bodyText ||| gorithms.
bodyText ||| In our algorithm, the initial set of potential ar-
bodyText ||| guments consists of constituents in the Seginer
bodyText ||| parserâs parse tree. Consequently the fraction
bodyText ||| of arguments that are also constituents (81.87%
bodyText ||| for English and 51.83% for Spanish) poses an
bodyText ||| upper bound on our algorithmâs recall. Note
bodyText ||| that the recall of the ALL COUSINS baseline is
bodyText ||| 74.27% (45.75%) for English (Spanish). This
bodyText ||| score emphasizes the baselineâs strength, and jus-
bodyText ||| tifies the restriction that the arguments should be
bodyText ||| k-th cousins of the predicate. The difference be-
bodyText ||| tween these bounds for the two languages provides
bodyText ||| a partial explanation for the corresponding gap in
bodyText ||| the algorithmâs performance.
bodyText ||| Figure 3 shows the precision of the collocation
bodyText ||| model (on development data) as a function of the
bodyText ||| amount of data it was given. We can see that
bodyText ||| the algorithm reaches saturation at about 5M sen-
bodyText ||| tences. It achieves this precision while maintain-
bodyText ||| ing a reasonable recall (an average recall of 43.1%
bodyText ||| after saturation). The parameters of the colloca-
bodyText ||| tion model were separately tuned for each corpus
bodyText ||| size, and the graph displays the maximum which
bodyText ||| was obtained for each of the corpus sizes.
bodyText ||| To better understand our modelâs performance,
bodyText ||| we performed experiments on the English cor-
bodyText ||| pus to test how well its first stage detects clauses.
bodyText ||| Clause detection is used by our algorithm as a step
bodyText ||| towards argument identification, but it can be of
bodyText ||| potential benefit for other purposes as well (see
bodyText ||| Section 2). The results are 23.88% recall and 40%
bodyText ||| precision. As in the ARGID task, a random se-
bodyText ||| lection of arguments would have yielded an ex-
bodyText ||| tremely poor result.
sectionHeader ||| 6 Conclusion
bodyText ||| In this work we presented the first algorithm for ar-
bodyText ||| gument identification that uses neither supervised
bodyText ||| syntactic annotation nor SRL tagged data. We
bodyText ||| have experimented on two languages: English and
bodyText ||| Spanish. The straightforward adaptability of un-
page ||| 34
table ||| 	English (Test Data)			Spanish (Test Data)
table ||| 	Precision	Recall	F1	Precision	Recall	F1
table ||| Clause Detection	52.84	67.14	59.14	18.00	33.19	23.34
table ||| Collocation Maximum Fâscore	54.11	63.53	58.44	20.22	29.13	23.87
table ||| Collocation Maximum Precision	55.97	40.02	46.67	21.80	18.47	20.00
table ||| ALL COUSINS baseline	46.71	74.27	57.35	14.16	45.75	21.62
tableCaption ||| Table 1: Precision, Recall and F 1 score for the different stages of our algorithm. Results are given for English (PTB, sentences
tableCaption ||| length bounded by 10, left part of the table) and Spanish (SemEval 2007 Spanish SRL task, right part of the table). The results
tableCaption ||| of the collocation (second) stage are given in two configurations, Collocation Maximum F-score and Collocation Maximum
tableCaption ||| Precision (see text). The upper bounds on Recall, obtained by taking all arguments output by our unsupervised parser, are
tableCaption ||| 81.87% for English and 51.83% for Spanish.
figure ||| Number of Sentences (Millions)
figureCaption ||| Figure 3: The performance of the second stage on English
figureCaption ||| (squares) vs. corpus size. The precision of the baseline (trian-
figureCaption ||| gles) and of the first stage (circles) is displayed for reference.
figureCaption ||| The graph indicates the maximum precision obtained for each
figureCaption ||| corpus size. The graph reaches saturation at about 5M sen-
figureCaption ||| tences. The average recall of the sampled points from there
figureCaption ||| on is 43.1%. Experiments were performed on the English
figureCaption ||| development data.
bodyText ||| supervised models to different languages is one
bodyText ||| of their most appealing characteristics. The re-
bodyText ||| cent availability of unsupervised syntactic parsers
bodyText ||| has offered an opportunity to conduct research on
bodyText ||| SRL, without reliance on supervised syntactic an-
bodyText ||| notation. This work is the first to address the ap-
bodyText ||| plication of unsupervised parses to an SRL related
bodyText ||| task.
bodyText ||| Our model displayed an increase in precision of
bodyText ||| 9% in English and 8% in Spanish over a strong
bodyText ||| baseline. Precision is of particular interest in this
bodyText ||| context, as instances tagged by high quality an-
bodyText ||| notation could be later used as training data for
bodyText ||| supervised SRL algorithms. In terms of Fâscore,
bodyText ||| our model showed an increase of 1.8% in English
bodyText ||| and of 2.2% in Spanish over the baseline.
bodyText ||| Although the quality of unsupervised parses is
bodyText ||| currently low (compared to that of supervised ap-
bodyText ||| proaches), using great amounts of data in identi-
bodyText ||| fying recurring structures may reduce noise and
bodyText ||| in addition address sparsity. The techniques pre-
bodyText ||| sented in this paper are based on this observation,
bodyText ||| using around 35M sentences in total for English
bodyText ||| and 3.3M sentences for Spanish.
bodyText ||| As this is the first work which addressed un-
bodyText ||| supervised ARGID, many questions remain to be
bodyText ||| explored. Interesting issues to address include as-
bodyText ||| sessing the utility of the proposed methods when
bodyText ||| supervised parses are given, comparing our model
bodyText ||| to systems with no access to unsupervised parses
bodyText ||| and conducting evaluation using more relaxed
bodyText ||| measures.
bodyText ||| Unsupervised methods for syntactic tasks have
bodyText ||| matured substantially in the last few years. No-
bodyText ||| table examples are (Clark, 2003) for unsupervised
bodyText ||| POS tagging and (Smith and Eisner, 2006) for un-
bodyText ||| supervised dependency parsing. Adapting our al-
bodyText ||| gorithm to use the output of these models, either to
bodyText ||| reduce the little supervision our algorithm requires
bodyText ||| (POS tagging) or to provide complementary syn-
bodyText ||| tactic information, is an interesting challenge for
bodyText ||| future work.
sectionHeader ||| References
reference ||| Collin F. Baker, Charles J. Fillmore and John B. Lowe,
reference ||| 1998. The Berkeley FrameNet Project. ACL-
reference ||| COLING â98.
reference ||| Daniel M. Bikel, 2004. Intricacies of Collinsâ Parsing
reference ||| Model. Computational Linguistics, 30(4):479â511.
reference ||| Ted Briscoe, John Carroll, 1997. Automatic Extraction
reference ||| of Subcategorization from Corpora. Applied NLP
reference ||| 1997.
reference ||| Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
reference ||| Kowalski, Sebastian Pad and Manfred Pinkal, 2006
reference ||| The SALSA Corpus: a German Corpus Resource for
reference ||| Lexical Semantics. LREC â06.
reference ||| Lou Burnard, 2000. User Reference Guide for the
reference ||| British National Corpus. Technical report, Oxford
reference ||| University.
reference ||| Xavier Carreras and Lluis M`arquez, 2004. Intro-
reference ||| duction to the CoNLLâ2004 Shared Task: Semantic
reference ||| Role Labeling. CoNLL â04.
figure ||| 0	2	4	6	8	10
figure ||| 48
figure ||| 46
figure ||| 44
figure ||| 42
figure ||| 52
figure ||| 50
figure ||| Second Stage
figure ||| First Stage
figure ||| Baseline
page ||| 35
reference ||| Xavier Carreras and Lluis M`arquez, 2005. Intro-
reference ||| duction to the CoNLL â2005 Shared Task: Semantic
reference ||| Role Labeling. CoNLL â05.
reference ||| Alexander Clark, 2003. Combining Distributional and
reference ||| Morphological Information for Part of Speech In-
reference ||| duction. EACL â03.
reference ||| Ronan Collobert and Jason Weston, 2007. Fast Se-
reference ||| mantic Extraction Using a Novel Neural Network
reference ||| Architecture. ACL â07.
reference ||| Mona Diab, Aous Mansouri, Martha Palmer, Olga
reference ||| Babko-Malaya, Wajdi Zaghouani, Ann Bies and
reference ||| Mohammed Maamouri, 2008. A pilot Arabic Prop-
reference ||| Bank. LREC â08.
reference ||| Evgeniy Gabrilovich and Shaul Markovitch, 2005.
reference ||| Feature Generation for Text Categorization using
reference ||| World Knowledge. IJCAI â05.
reference ||| Daniel Gildea and Daniel Jurafsky, 2002. Automatic
reference ||| Labeling of Semantic Roles. Computational Lin-
reference ||| guistics, 28(3):245â288.
reference ||| Elliot Glaysher and Dan Moldovan, 2006. Speed-
reference ||| ing Up Full Syntactic Parsing by Leveraging Partial
reference ||| Parsing Decisions. COLING/ACL â06 poster ses-
reference ||| sion.
reference ||| Andrew Gordon and Reid Swanson, 2007. Generaliz-
reference ||| ing Semantic Role Annotations across Syntactically
reference ||| Similar Verbs. ACL â07.
reference ||| David Graff, 1995. North American News Text Cor-
reference ||| pus. Linguistic Data Consortium. LDC95T21.
reference ||| Trond Grenager and Christopher D. Manning, 2006.
reference ||| Unsupervised Discovery of a Statistical Verb Lexi-
reference ||| con. EMNLP â06.
reference ||| Kadri Hacioglu, 2004. Semantic Role Labeling using
reference ||| Dependency Trees. COLINGâ04.
reference ||| Kadri Hacioglu and Wayne Ward, 2003. Target Word
reference ||| Detection and Semantic Role Chunking using Sup-
reference ||| port Vector Machines. HLT-NAACL â03.
reference ||| Rohit J. Kate and Raymond J. Mooney, 2007. Semi-
reference ||| Supervised Learning for Semantic Parsing using
reference ||| Support Vector Machines. HLTâNAACL â07.
reference ||| Karin Kipper, Hoa Trang Dang and Martha Palmer,
reference ||| 2000. Class-Based Construction of a Verb Lexicon.
reference ||| AAAI â00.
reference ||| Dan Klein, 2005. The Unsupervised Learning ofNatu-
reference ||| ral Language Structure. Ph.D. thesis, Stanford Uni-
reference ||| versity.
reference ||| Anna Korhonen, 2002. Subcategorization Acquisition.
reference ||| Ph.D. thesis, University of Cambridge.
reference ||| Christopher D. Manning, 1993. Automatic Acquisition
reference ||| of a Large Subcategorization Dictionary. ACL â93.
reference ||| Lluis M`arquez, Xavier Carreras, Kenneth C. Lit-
reference ||| tkowski and Suzanne Stevenson, 2008. Semantic
reference ||| Role Labeling: An introdution to the Special Issue.
reference ||| Computational Linguistics, 34(2):145â159
reference ||| Lluis M`arquez, Jesus Gim`enez Pere Comas and Neus
reference ||| Catal`a, 2005. Semantic Role Labeling as Sequential
reference ||| Tagging. CoNLLâ05.
reference ||| Lluis M`arquez, Lluis Villarejo, M. A. Marti and Mar-
reference ||| iona Taul`e, 2007. SemEvalâ2007 Task 09: Multi-
reference ||| level Semantic Annotation of Catalan and Spanish.
reference ||| The 4th international workshop on Semantic Evalu-
reference ||| ations (SemEval â07).
reference ||| Gabriele Musillo and Paula Merlo, 2006. Accurate
reference ||| Parsing of the proposition bank. HLT-NAACL â06.
reference ||| Martha Palmer, Daniel Gildea and Paul Kingsbury,
reference ||| 2005. The Proposition Bank: A Corpus Annotated
reference ||| with Semantic Roles. Computational Linguistics,
reference ||| 31(1):71â106.
reference ||| Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
reference ||| Wayne Ward, James H. Martin and Daniel Jurafsky,
reference ||| 2005. Support Vector Learning for Semantic Argu-
reference ||| ment Classification. Machine Learning, 60(1):11â
reference ||| 39.
reference ||| Sameer Pradhan, Wayne Ward, James H. Martin, 2008.
reference ||| Towards Robust Semantic Role Labeling. Computa-
reference ||| tional Linguistics, 34(2):289â310.
reference ||| Adwait Ratnaparkhi, 1996. Maximum Entropy Part-
reference ||| Of-Speech Tagger. EMNLP â96.
reference ||| Helmut Schmid, 1994. Probabilistic Part-of-Speech
reference ||| Tagging Using Decision Trees International Confer-
reference ||| ence on New Methods in Language Processing.
reference ||| Yoav Seginer, 2007. Fast Unsupervised Incremental
reference ||| Parsing. ACL â07.
reference ||| Noah A. Smith and Jason Eisner, 2006. Annealing
reference ||| Structural Bias in Multilingual Weighted Grammar
reference ||| Induction. ACL â06.
reference ||| Robert S. Swier and Suzanne Stevenson, 2004. Unsu-
reference ||| pervised Semantic Role Labeling. EMNLP â04.
reference ||| Robert S. Swier and Suzanne Stevenson, 2005. Ex-
reference ||| ploiting a Verb Lexicon in Automatic Semantic Role
reference ||| Labelling. EMNLP â05.
reference ||| Erik F. Tjong Kim Sang and HervÂ´e DÂ´ejean, 2001. In-
reference ||| troduction to the CoNLL-2001 Shared Task: Clause
reference ||| Identification. CoNLL â01.
reference ||| Nianwen Xue and Martha Palmer, 2004. Calibrating
reference ||| Features for Semantic Role Labeling. EMNLP â04.
reference ||| Nianwen Xue, 2008. Labeling Chinese Predicates
reference ||| with Semantic Roles. Computational Linguistics,
reference ||| 34(2):225â255.
page ||| 36
