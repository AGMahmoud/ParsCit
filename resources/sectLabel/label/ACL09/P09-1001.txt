title ||| Heterogeneous Transfer Learning for Image Clustering via the Social Web
author ||| Qiang Yang
affiliation ||| Hong Kong University of Science and Technology, Clearway Bay, Kowloon, Hong Kong
email ||| qyang@cs.ust.hk
author ||| Yuqiang Chen	Gui-Rong Xue	Wenyuan Dai	Yong Yu
affiliation ||| Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200240, China
email ||| {yuqiangchen,grxue,dwyak,yyu}@apex.sjtu.edu.cn
sectionHeader ||| Abstract
bodyText ||| In this paper, we present a new learning
bodyText ||| scenario, heterogeneous transfer learn-
bodyText ||| ing, which improves learning performance
bodyText ||| when the data can be in different feature
bodyText ||| spaces and where no correspondence be-
bodyText ||| tween data instances in these spaces is pro-
bodyText ||| vided. In the past, we have classified Chi-
bodyText ||| nese text documents using English train-
bodyText ||| ing data under the heterogeneous trans-
bodyText ||| fer learning framework. In this paper,
bodyText ||| we present image clustering as an exam-
bodyText ||| ple to illustrate how unsupervised learning
bodyText ||| can be improved by transferring knowl-
bodyText ||| edge from auxiliary heterogeneous data
bodyText ||| obtained from the social Web. Image
bodyText ||| clustering is useful for image sense dis-
bodyText ||| ambiguation in query-based image search,
bodyText ||| but its quality is often low due to image-
bodyText ||| data sparsity problem. We extend PLSA
bodyText ||| to help transfer the knowledge from social
bodyText ||| Web data, which have mixed feature repre-
bodyText ||| sentations. Experiments on image-object
bodyText ||| clustering and scene clustering tasks show
bodyText ||| that our approach in heterogeneous trans-
bodyText ||| fer learning based on the auxiliary data is
bodyText ||| indeed effective and promising.
sectionHeader ||| 1 Introduction
bodyText ||| Traditional machine learning relies on the avail-
bodyText ||| ability of a large amount of data to train a model,
bodyText ||| which is then applied to test data in the same
bodyText ||| feature space. However, labeled data are often
bodyText ||| scarce and expensive to obtain. Various machine
bodyText ||| learning strategies have been proposed to address
bodyText ||| this problem, including semi-supervised learning
bodyText ||| (Zhu, 2007), domain adaptation (Wu and Diet-
bodyText ||| terich, 2004; Blitzer et al., 2006; Blitzer et al.,
bodyText ||| 2007; Arnold et al., 2007; Chan and Ng, 2007;
bodyText ||| Daume, 2007; Jiang and Zhai, 2007; Reichart
bodyText ||| and Rappoport, 2007; Andreevskaia and Bergler,
bodyText ||| 2008), multi-task learning (Caruana, 1997; Re-
bodyText ||| ichart et al., 2008; Arnold et al., 2008), self-taught
bodyText ||| learning (Raina et al., 2007), etc. A commonality
bodyText ||| among these methods is that they all require the
bodyText ||| training data and test data to be in the same fea-
bodyText ||| ture space. In addition, most of them are designed
bodyText ||| for supervised learning. However, in practice, we
bodyText ||| often face the problem where the labeled data are
bodyText ||| scarce in their own feature space, whereas there
bodyText ||| may be a large amount of labeled heterogeneous
bodyText ||| data in another feature space. In such situations, it
bodyText ||| would be desirable to transfer the knowledge from
bodyText ||| heterogeneous data to domains where we have rel-
bodyText ||| atively little training data available.
bodyText ||| To learn from heterogeneous data, researchers
bodyText ||| have previously proposed multi-view learning
bodyText ||| (Blum and Mitchell, 1998; Nigam and Ghani,
bodyText ||| 2000) in which each instance has multiple views in
bodyText ||| different feature spaces. Different from previous
bodyText ||| works, we focus on the problem of heterogeneous
bodyText ||| transfer learning, which is designed for situation
bodyText ||| when the training data are in one feature space
bodyText ||| (such as text), and the test data are in another (such
bodyText ||| as images), and there may be no correspondence
bodyText ||| between instances in these spaces. The type of
bodyText ||| heterogeneous data can be very different, as in the
bodyText ||| case of text and image. To consider how hetero-
bodyText ||| geneous transfer learning relates to other types of
bodyText ||| learning, Figure 1 presents an intuitive illustration
bodyText ||| of four learning strategies, including traditional
bodyText ||| machine learning, transfer learning across differ-
bodyText ||| ent distributions, multi-view learning and hetero-
bodyText ||| geneous transfer learning. As we can see, an
bodyText ||| important distinguishing feature of heterogeneous
bodyText ||| transfer learning, as compared to other types of
bodyText ||| learning, is that more constraints on the problem
bodyText ||| are relaxed, such that data instances do not need to
bodyText ||| correspond anymore. This allows, for example, a
bodyText ||| collection of Chinese text documents to be classi-
bodyText ||| fied using another collection of English text as the
page ||| 1
note ||| Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1â9,
note ||| Suntec, Singapore, 2-7 August 2009. cï¿½2009 ACL and AFNLP
bodyText ||| training data (c.f. (Ling et al., 2008) and Section
bodyText ||| 2.1).
bodyText ||| In this paper, we will give an illustrative exam-
bodyText ||| ple of heterogeneous transfer learning to demon-
bodyText ||| strate how the task of image clustering can ben-
bodyText ||| efit from learning from the heterogeneous social
bodyText ||| Web data. A major motivation of our work is
bodyText ||| Web-based image search, where users submit tex-
bodyText ||| tual queries and browse through the returned result
bodyText ||| pages. One problem is that the user queries are of-
bodyText ||| ten ambiguous. An ambiguous keyword such as
bodyText ||| âAppleâ might retrieve images of Apple comput-
bodyText ||| ers and mobile phones, or images of fruits. Im-
bodyText ||| age clustering is an effective method for improv-
bodyText ||| ing the accessibility of image search result. Loeff
bodyText ||| et al. (2006) addressed the image clustering prob-
bodyText ||| lem with a focus on image sense discrimination.
bodyText ||| In their approach, images associated with textual
bodyText ||| features are used for clustering, so that the text
bodyText ||| and images are clustered at the same time. Specif-
bodyText ||| ically, spectral clustering is applied to the distance
bodyText ||| matrix built from a multimodal feature set associ-
bodyText ||| ated with the images to get a better feature repre-
bodyText ||| sentation. This new representation contains both
bodyText ||| image and text information, with which the per-
bodyText ||| formance of image clustering is shown to be im-
bodyText ||| proved. A problem with this approach is that when
bodyText ||| images contained in the Web search results are
bodyText ||| very scarce and when the textual data associated
bodyText ||| with the images are very few, clustering on the im-
bodyText ||| ages and their associated text may not be very ef-
bodyText ||| fective.
bodyText ||| Different from these previous works, in this pa-
bodyText ||| per, we address the image clustering problem as
bodyText ||| a heterogeneous transfer learning problem. We
bodyText ||| aim to leverage heterogeneous auxiliary data, so-
bodyText ||| cial annotations, etc. to enhance image cluster-
bodyText ||| ing performance. We observe that the World Wide
bodyText ||| Web has many annotated images in Web sites such
bodyText ||| as Flickr (http : / /www. flickr . com), which
bodyText ||| can be used as auxiliary information source for
bodyText ||| our clustering task. In this work, our objective
bodyText ||| is to cluster a small collection of images that we
bodyText ||| are interested in, where these images are not suf-
bodyText ||| ficient for traditional clustering algorithms to per-
bodyText ||| form well due to data sparsity and the low level of
bodyText ||| image features. We investigate how to utilize the
bodyText ||| readily available socially annotated image data on
bodyText ||| the Web to improve image clustering. Although
bodyText ||| these auxiliary data may be irrelevant to the im-
bodyText ||| ages to be clustered and cannot be directly used
bodyText ||| to solve the data sparsity problem, we show that
bodyText ||| they can still be used to estimate a good latentfea-
bodyText ||| ture representation, which can be used to improve
bodyText ||| image clustering.
sectionHeader ||| 2 Related Works
subsectionHeader ||| 2.1 Heterogeneous Transfer Learning
subsectionHeader ||| Between Languages
bodyText ||| In this section, we summarize our previous work
bodyText ||| on cross-language classification as an example of
bodyText ||| heterogeneous transfer learning. This example
bodyText ||| is related to our image clustering problem be-
bodyText ||| cause they both rely on data from different feature
bodyText ||| spaces.
bodyText ||| As the World Wide Web in China grows rapidly,
bodyText ||| it has become an increasingly important prob-
bodyText ||| lem to be able to accurately classify Chinese Web
bodyText ||| pages. However, because the labeled Chinese Web
bodyText ||| pages are still not sufficient, we often find it diffi-
bodyText ||| cult to achieve high accuracy by applying tradi-
bodyText ||| tional machine learning algorithms to the Chinese
bodyText ||| Web pages directly. Would it be possible to make
bodyText ||| the best use of the relatively abundant labeled En-
bodyText ||| glish Web pages for classifying the Chinese Web
bodyText ||| pages?
bodyText ||| To answer this question, in (Ling et al., 2008),
bodyText ||| we developed a novel approach for classifying the
bodyText ||| Web pages in Chinese using the training docu-
bodyText ||| ments in English. In this subsection, we give a
bodyText ||| brief summary of this work. The problem to be
bodyText ||| solved is: we are given a collection of labeled
bodyText ||| English documents and a large number of unla-
bodyText ||| beled Chinese documents. The English and Chi-
bodyText ||| nese texts are not aligned. Our objective is to clas-
bodyText ||| sify the Chinese documents into the same label
bodyText ||| space as the English data.
bodyText ||| Our key observation is that even though the data
bodyText ||| use different text features, they may still share
bodyText ||| many of the same semantic information. What we
bodyText ||| need to do is to uncover this latent semantic in-
bodyText ||| formation by finding out what is common among
bodyText ||| them. We did this in (Ling et al., 2008) by us-
bodyText ||| ing the information bottleneck theory (Tishby et
bodyText ||| al., 1999). In our work, we first translated the
bodyText ||| Chinese document into English automatically us-
bodyText ||| ing some available translation software, such as
bodyText ||| Google translate. Then, we encoded the training
bodyText ||| text as well as the translated target text together,
bodyText ||| in terms of the information theory. We allowed all
bodyText ||| the information to be put through a âbottleneckâ
bodyText ||| and be represented by a limited number of code-
page ||| 2
figureCaption ||| Figure 1: An intuitive illustration of different kinds learning strategies using classification/clustering of
bodyText ||| image apple and banana as the example.
bodyText ||| words (i.e. labels in the classification problem).
bodyText ||| Finally, information bottleneck was used to main-
bodyText ||| tain most of the common information between the
bodyText ||| two data sources, and discard the remaining irrel-
bodyText ||| evant information. In this way, we can approxi-
bodyText ||| mate the ideal situation where similar training and
bodyText ||| translated test pages shared in the common part are
bodyText ||| encoded into the same codewords, and are thus as-
bodyText ||| signed the correct labels. In (Ling et al., 2008), we
bodyText ||| experimentally showed that heterogeneous trans-
bodyText ||| fer learning can indeed improve the performance
bodyText ||| of cross-language text classification as compared
bodyText ||| to directly training learning models (e.g., Naive
bodyText ||| Bayes or SVM) and testing on the translated texts.
subsectionHeader ||| 2.2 Other Works in Transfer Learning
bodyText ||| In the past, several other works made use of trans-
bodyText ||| fer learning for cross-feature-space learning. Wu
bodyText ||| and Oard (2008) proposed to handle the cross-
bodyText ||| language learning problem by translating the data
bodyText ||| into a same language and applying kNN on the
bodyText ||| latent topic space for classification. Most learning
bodyText ||| algorithms for dealing with cross-language hetero-
bodyText ||| geneous data require a translator to convert the
bodyText ||| data to the same feature space. For those data that
bodyText ||| are in different feature spaces where no transla-
bodyText ||| tor is available, Davis and Domingos (2008) pro-
bodyText ||| posed a Markov-logic-based transfer learning al-
bodyText ||| gorithm, which is called deep transfer, for trans-
bodyText ||| ferring knowledge between biological domains
bodyText ||| and Web domains. Dai et al. (2008a) proposed
bodyText ||| a novel learning paradigm, known as translated
bodyText ||| learning, to deal with the problem of learning het-
bodyText ||| erogeneous data that belong to quite different fea-
bodyText ||| ture spaces by using a risk minimization frame-
bodyText ||| work.
subsectionHeader ||| 2.3 Relation to PLSA
bodyText ||| Our work makes use of PLSA. Probabilistic la-
bodyText ||| tent semantic analysis (PLSA) is a widely used
bodyText ||| probabilistic model (Hofmann, 1999), and could
bodyText ||| be considered as a probabilistic implementation of
bodyText ||| latent semantic analysis (LSA) (Deerwester et al.,
bodyText ||| 1990). An extension to PLSA was proposed in
bodyText ||| (Cohn and Hofmann, 2000), which incorporated
bodyText ||| the hyperlink connectivity in the PLSA model by
bodyText ||| using a joint probabilistic model for connectivity
bodyText ||| and content. Moreover, PLSA has shown a lot
bodyText ||| of applications ranging from text clustering (Hof-
bodyText ||| mann, 2001) to image analysis (Sivic et al., 2005).
subsectionHeader ||| 2.4 Relation to Clustering
bodyText ||| Compared to many previous works on image clus-
bodyText ||| tering, we note that traditional image cluster-
bodyText ||| ing is generally based on techniques such as K-
bodyText ||| means (MacQueen, 1967) and hierarchical clus-
bodyText ||| tering (Kaufman and Rousseeuw, 1990). How-
bodyText ||| ever, when the data are sparse, traditional clus-
bodyText ||| tering algorithms may have difficulties in obtain-
bodyText ||| ing high-quality image clusters. Recently, several
bodyText ||| researchers have investigated how to leverage the
bodyText ||| auxiliary information to improve target clustering
page ||| 3
equation ||| P(zIv)	P(fIz)
bodyText ||| performance, such as supervised clustering (Fin-
bodyText ||| ley and Joachims, 2005), semi-supervised cluster-
bodyText ||| ing (Basu et al., 2004), self-taught clustering (Dai
bodyText ||| et al., 2008b), etc.
sectionHeader ||| 3 Image Clustering with Annotated
sectionHeader ||| Auxiliary Data
bodyText ||| In this section, we present our annotation-based
bodyText ||| probabilistic latent semantic analysis algorithm
bodyText ||| (aPLSA), which extends the traditional PLSA
bodyText ||| model by incorporating annotated auxiliary im-
bodyText ||| age data. Intuitively, our algorithm aPLSA per-
bodyText ||| forms PLSA analysis on the target images, which
bodyText ||| are converted to an image instance-to-feature co-
bodyText ||| occurrence matrix. At the same time, PLSA is
bodyText ||| also applied to the annotated image data from so-
bodyText ||| cial Web, which is converted into a text-to-image-
bodyText ||| feature co-occurrence matrix. In order to unify
bodyText ||| those two separate PLSA models, these two steps
bodyText ||| are done simultaneously with common latent vari-
bodyText ||| ables used as a bridge linking them. Through
bodyText ||| these common latent variables, which are now
bodyText ||| constrained by both target image data and auxil-
bodyText ||| iary annotation data, a better clustering result is
bodyText ||| expected for the target data.
subsectionHeader ||| 3.1 Probabilistic Latent Semantic Analysis
bodyText ||| Let F = { fi}!Fi be an image feature space, and
bodyText ||| V = {vi}iv11 be the image data set. Each image
bodyText ||| vi E V is represented by a bag-of-features {f I f E
bodyText ||| vi A f E F}.
bodyText ||| Based on the image data set V, we can esti-
bodyText ||| mate an image instance-to-feature co-occurrence
bodyText ||| matrix AIVI x I FI E RIVIx IFI, where each element
bodyText ||| AiT (1 G i G IV I and 1 G j G IF I) in the matrix
bodyText ||| A is the frequency of the feature fT appearing in
bodyText ||| the instance vi.
bodyText ||| Let W = {wi } I WI iï¿½ï¿½be a text feature space. The
bodyText ||| annotated image data allow us to obtain the co-
bodyText ||| occurrence information between images v and text
bodyText ||| features w E W. An example of annotated im-
bodyText ||| age data is the Flickr (http : / /www. f l ickr .
bodyText ||| com), which is a social Web site containing a large
bodyText ||| number of annotated images.
bodyText ||| By extracting image features from the annotated
bodyText ||| images v, we can estimate a text-to-image fea-
bodyText ||| ture co-occurrence matrix B I WIxIFI E RIW IxIFI,
bodyText ||| where each element BiT (1 G i G I W I and
bodyText ||| 1 G j G IFI) in the matrix B is the frequency
bodyText ||| of the text feature wi and the image feature fT oc-
bodyText ||| curring together in the annotated image data set.
bodyText ||| Figure 2: Graphical model representation of PLSA
bodyText ||| model.
bodyText ||| Let Z = {ziffl be the latent variable set in our
bodyText ||| aPLSA model. In clustering, each latent variable
bodyText ||| zi E Z corresponds to a certain cluster.
bodyText ||| Our objective is to estimate a clustering func-
bodyText ||| tion g : V H Z with the help of the two co-
bodyText ||| occurrence matrices A and B as defined above.
bodyText ||| To formally introduce the aPLSA model, we
bodyText ||| start from the probabilistic latent semantic anal-
bodyText ||| ysis (PLSA) (Hofmann, 1999) model. PLSA is
bodyText ||| a probabilistic implementation of latent seman-
bodyText ||| tic analysis (LSA) (Deerwester et al., 1990). In
bodyText ||| our image clustering task, PLSA decomposes the
bodyText ||| instance-feature co-occurrence matrix A under the
bodyText ||| assumption of conditional independence of image
bodyText ||| instances V and image features F, given the latent
bodyText ||| variables Z.
equation ||| P(f Iv) = 1: P(f I z)P(zI v).	(1)
equation ||| ZEZ
bodyText ||| The graphical model representation of PLSA is
bodyText ||| shown in Figure 2.
bodyText ||| Based on the PLSA model, the log-likelihood can
bodyText ||| be defined as:
equation ||| 1: L =
equation ||| i 1:AiT
equation ||| T ET, AiT, log P(fT I vi)	(2)
bodyText ||| where AIVI x IFI E RIVI xIFI is the image instance-
bodyText ||| feature co-occurrence matrix. The term : Aï¿½j
bodyText ||| j, Azj/
bodyText ||| in Equation (2) is a normalization term ensuring
bodyText ||| each image is giving the same weight in the log-
bodyText ||| likelihood.
bodyText ||| Using EM algorithm (Dempster et al., 1977),
bodyText ||| which locally maximizes the log-likelihood of
bodyText ||| the PLSA model (Equation (2)), the probabilities
bodyText ||| P(f I z) and P(zI v) can be estimated. Then, the
bodyText ||| clustering function is derived as
equation ||| g(v) = argmax P(zIv).	(3)
equation ||| ZEZ
bodyText ||| Due to space limitation, we omit the details for the
bodyText ||| PLSA model, which can be found in (Hofmann,
bodyText ||| 1999).
subsectionHeader ||| 3.2 aPLSA: Annotation-based PLSA
bodyText ||| In this section, we consider how to incorporate
bodyText ||| a large number of socially annotated images in a
equation ||| V	Z	F
page ||| 4
figureCaption ||| Figure 3: Graphical model representation of
figureCaption ||| aPLSA model.
bodyText ||| unified PLSA model for the purpose of utilizing
bodyText ||| the correlation between text features and image
bodyText ||| features. In the auxiliary data, each image has cer-
bodyText ||| tain textual tags that are attached by users. The
bodyText ||| correlation between text features and image fea-
bodyText ||| tures can be formulated as follows.
equation ||| P(f Iw) = X P(f I z)P(zI w). (4)
equation ||| ZEZ
bodyText ||| It is clear that Equations (1) and (4) share a same
bodyText ||| term P(f I z). So we design a new PLSA model by
bodyText ||| joining the probabilistic model in Equation (1) and
bodyText ||| the probabilistic model in Equation (4) into a uni-
bodyText ||| fied model, as shown in Figure 3. In Figure 3, the
bodyText ||| latent variables Z depend not only on the corre-
bodyText ||| lation between image instances V and image fea-
bodyText ||| tures F, but also the correlation between text fea-
bodyText ||| tures W and image features F. Therefore, the aux-
bodyText ||| iliary socially-annotated image data can be used
bodyText ||| to help the target image clustering performance by
bodyText ||| estimating good set of latent variables Z.
bodyText ||| Based on the graphical model representation in
bodyText ||| Figure 3, we derive the log-likelihood objective
bodyText ||| function, in a similar way as in (Cohn and Hof-
bodyText ||| mann, 2000), as follows
bodyText ||| text-to-image occurrence matrix B. In this case,
bodyText ||| the aPLSA model degenerates to the traditional
bodyText ||| PLSA model. Therefore, aPLSA is an extension
bodyText ||| to the PLSA model.
bodyText ||| Now, the objective is to maximize the log-
bodyText ||| likelihood L of the aPLSA model in Equation (5).
bodyText ||| Then we apply the EM algorithm (Dempster et
bodyText ||| al., 1977) to estimate the conditional probabilities
bodyText ||| P(f I z), P(zI w) and P(zI v) with respect to each
bodyText ||| dependence in Figure 3 as follows.
listItem ||| â¢	E-Step: calculate the posterior probability of
listItem ||| each latent variable z given the observation
listItem ||| of image features f, image instances v and
listItem ||| text features w based on the old estimate of
equation ||| P(f I z), P(zI w) and P(zI v):
equation ||| PIvi,fj) =	P(fjIzk)P(zkIvi)
equation ||| (zk
equation ||| P(zkI wl, fj) =	P(fjIzk)P(zkIwl)
equation ||| Pk, P(fjIzk,)P(zk,Iwl)
equation ||| (7)
listItem ||| â¢	M-Step: re-estimates conditional probabili-
listItem ||| ties P(zkIvi) and P(zkI wl):
equation ||| PAAP(zkIvi,fj) (8)
equation ||| PBlBlj, P(zk I wl , fj) (9)
bodyText ||| and conditional probability P(fj I zk ), which
bodyText ||| is a mixture portion of posterior probability
bodyText ||| of latent variables
equation ||| W
equation ||| V
equation ||| Z	F
equation ||| P(fIz)
equation ||| P
equation ||| k, P(fj Izk,)P(zk, I vi)
equation ||| (6)
equation ||| XP(zkIvi) =
equation ||| j
equation ||| XP(zkIwl) =
equation ||| j
equation ||| P(fjIzk) a AX
equation ||| i
equation ||| L=X
equation ||| j
equation ||| P
equation ||| j, Blj,
equation ||| X
equation ||| +(1 â A)
equation ||| l
equation ||| +(1âA)X
equation ||| l
equation ||| Aij  P( zkI vi, fj)
equation ||| Pj,Aij,
equation ||| Blj P(zkIwl,fj)
equation ||| (10)
equation ||| ï¿½
equation ||| XAij
equation ||| A i Pj, Aij, log P(fj I vi)
equation ||| ï¿½
equation ||| Blj	log P(fj I wl )
equation ||| Pj, Blj,
equation ||| (5)
bodyText ||| where AIVIxIFI E RIVIxIFI is the image instance-
bodyText ||| feature co-occurrence matrix, and BIW I xIFI E
bodyText ||| RIWIxIFI is the text-to-image feature-level co-
bodyText ||| occurrence matrix. Similar to Equation (2),
bodyText ||| PAij and PB`j in Equation (5) are the nor-
bodyText ||| ij,
bodyText ||| malization terms to prevent imbalanced cases.
bodyText ||| Furthermore, A acts as a trade-off parameter be-
bodyText ||| tween the co-occurrence matrices A and B. In
bodyText ||| the extreme case when A = 1, the log-likelihood
bodyText ||| objective function ignores all the biases from the
bodyText ||| Finally, the clustering function for a certain im-
bodyText ||| age v is
equation ||| g(v) = argmax P(zIv).	(11)
equation ||| ZEZ
bodyText ||| From the above equations, we can derive
bodyText ||| our annotation-based probabilistic latent semantic
bodyText ||| analysis (aPLSA) algorithm. As shown in Algo-
bodyText ||| rithm 1, aPLSA iteratively performs the E-Step
bodyText ||| and the M-Step in order to seek local optimal
bodyText ||| points based on the objective function L in Equa-
bodyText ||| tion (5).
page ||| 5
construct ||| Algorithm 1 Annotation-based PLSA Algorithm
construct ||| (aPLSA)
construct ||| Input: The V-F co-occurrence matrix A and W-
construct ||| F co-occurrence matrix B.
construct ||| Output: A clustering (partition) function g : V H
construct ||| i, which maps an image instance v E V to a latent
construct ||| variable z E i.
listItem ||| 1: Initial i so that IiI equals the number clus-
listItem ||| ters desired.
listItem ||| 2: Initialize P(zIv), P(zIw), P(f Iz) randomly.
listItem ||| 3: while the change of L in Eq. (5) between two
listItem ||| sequential iterations is greater than a prede-
listItem ||| fined threshold do
listItem ||| 4: E-Step: Update P(zIv, f) and P(zIw, f)
listItem ||| based on Eq. (6) and (7) respectively.
listItem ||| 5: M-Step: Update P(zIv), P(zIw) and
listItem ||| P(f Iz) based on Eq. (8), (9) and (10) re-
listItem ||| spectively.
listItem ||| 6: end while
listItem ||| 7: for all v in V do
listItem ||| 8:	g(v) +â argmaxP(zIv).
listItem ||| 9: end for
listItem ||| 10: Return g.
sectionHeader ||| 4 Experiments
bodyText ||| In this section, we empirically evaluate the aPLSA
bodyText ||| algorithm together with some state-of-art base-
bodyText ||| line methods on two widely used image corpora,
bodyText ||| to demonstrate the effectiveness of our algorithm
bodyText ||| aPLSA.
subsectionHeader ||| 4.1 Data Sets
bodyText ||| In order to evaluate the effectiveness of our algo-
bodyText ||| rithm aPLSA, we conducted experiments on sev-
bodyText ||| eral data sets generated from two image corpora,
bodyText ||| Caltech-256 (Griffin et al., 2007) and the fifteen-
bodyText ||| scene (Lazebnik et al., 2006). The Caltech-256
bodyText ||| data set has 256 image objective categories, rang-
bodyText ||| ing from animals to buildings, from plants to au-
bodyText ||| tomobiles, etc. The fifteen-scene data set con-
bodyText ||| tains 15 scenes such as store and forest.
bodyText ||| From these two corpora, we randomly generated
bodyText ||| eleven image clustering tasks, including seven 2-
bodyText ||| way clustering tasks, two 4-way clustering task,
bodyText ||| one 5-way clustering task and one 8-way cluster-
bodyText ||| ing task. The detailed descriptions for these clus-
bodyText ||| tering tasks are given in Table 1. In these tasks,
bodyText ||| b i 7 and o c t 1 were generated from fifteen-scene
bodyText ||| data set, and the rest were from Caltech-256 data
bodyText ||| set.
table ||| DATA SET INVOLVED CLASSES	DATA SIZE
table ||| bi1	skateboard, airplanes	102,800
table ||| bi2	billiards, mars	278, 155
table ||| bi3	cd, greyhound	102,	94
table ||| bi4	electric-guitar, snake	122,112
table ||| bi5	calculator, dolphin	100,106
table ||| bi6	mushroom, teddy-bear	202,	99
table ||| bi7	MIThighway, livingroom	260,289
table ||| quad1	calculator,	diamond-ring,	dolphin,	100, 118, 106,116
table ||| 	microscope
table ||| quad2	bonsai, comet, frog, saddle	122, 120, 115, 110
table ||| quint1	frog, kayak, bear, jesus-christ,watch115,102,101,87,	201
table ||| oct1	MIThighway,	MITmountain,	260, 374, 210, 360,
table ||| 	kitchen, MITcoast, PARoffice, MIT- tallbuilding, livingroom, bedroom	215, 356, 289, 216
table ||| tune1	coin, horse	123,270
table ||| tune2	socks, spider	111,106
table ||| tune3	galaxy, snowmobile	80,112
table ||| tune4	dice, fern	98,110
table ||| tune5	backpack, lightning, mandolin, swan	151, 136,	93,114
tableCaption ||| Table 1: The descriptions of all the image clus-
tableCaption ||| tering tasks used in our experiment. Among
tableCaption ||| these data sets, b i 7 and o c t 1 were generated
tableCaption ||| from fifteen-scene data set, and the rest were from
tableCaption ||| Caltech-256 data set.
bodyText ||| To empirically investigate the parameter A and
bodyText ||| the convergence of our algorithm aPLSA, we gen-
bodyText ||| erated five more date sets as the development sets.
bodyText ||| The detailed description of these five development
bodyText ||| sets, namely tune1 to tune5 is listed in Table 1
bodyText ||| as well.
bodyText ||| The auxiliary data were crawled from the Flickr
bodyText ||| (http://www.flickr.com/) web site dur-
bodyText ||| ing August 2007. Flickr is an internet community
bodyText ||| where people share photos online and express their
bodyText ||| opinions as social tags (annotations) attached to
bodyText ||| each image. From Flicker, we collected 19,959
bodyText ||| images and 91,719 related annotations, among
bodyText ||| which 2,600 words are distinct. Based on the
bodyText ||| method described in Section 3, we estimated the
bodyText ||| co-occurrence matrix B between text features and
bodyText ||| image features. This co-occurrence matrix B was
bodyText ||| used by all the clustering tasks in our experiments.
bodyText ||| For data preprocessing, we adopted the bag-of-
bodyText ||| features representation of images (Li and Perona,
bodyText ||| 2005) in our experiments. Interesting points were
bodyText ||| found in the images and described via the SIFT
bodyText ||| descriptors (Lowe, 2004). Then, the interesting
bodyText ||| points were clustered to generate a codebook to
bodyText ||| form an image feature space. The size of code-
bodyText ||| book was set to 2, 000 in our experiments. Based
bodyText ||| on the codebook, which serves as the image fea-
bodyText ||| ture space, each image can be represented as a cor-
bodyText ||| responding feature vector to be used in the next
bodyText ||| step.
bodyText ||| To set our evaluation criterion, we used the
page ||| 6
table ||| Data Set	KMeancombined		PLSA		STC	aPLSA
table ||| 	separate		separate	combined
table ||| bi1	0.645Â±0.064	0.548Â±0.031	0.544Â±0.074	0.537Â±0.033	0.586Â±0.139	0.482Â±0.062
table ||| bi2	0.687Â±0.003	0.662Â±0.014	0.464Â±0.074	0.692Â±0.001	0.577Â±0.016	0.455Â±0.096
table ||| bi3	1.294Â±0.060	1.300Â±0.015	1.085Â±0.073	1.126Â±0.036	1.103Â±0.108	1.029Â±0.074
table ||| bi4	1.227Â±0.080	1.164Â±0.053	0.976Â±0.051	1.038Â±0.068	1.024Â±0.089	0.919Â±0.065
table ||| bi5	1.450Â±0.058	1.417Â±0.045	1.426Â±0.025	1.405Â±0.040	1.411Â±0.043	1.377Â±0.040
table ||| bi6	1.969Â±0.078	1.852Â±0.051	1.514Â±0.039	1.709Â±0.028	1.589Â±0.121	1.503Â±0.030
table ||| bi7	0.686Â±0.006	0.683Â±0.004	0.643Â±0.058	0.632Â±0.037	0.651Â±0.012	0.624Â±0.066
table ||| quad1	0.591Â±0.094	0.675Â±0.017	0.488Â±0.071	0.662Â±0.013	0.580Â±0.115	0.432Â±0.085
table ||| quad2	0.648Â±0.036	0.646Â±0.045	0.614Â±0.062	0.626Â±0.026	0.591Â±0.087	0.515Â±0.098
table ||| quint1	0.557Â±0.021	0.508Â±0.104	0.547Â±0.060	0.539Â±0.051	0.538Â±0.100	0.502Â±0.067
table ||| oct1	0.659Â±0.031	0.680Â±0.012	0.340Â±0.147	0.691Â±0.002	0.411Â±0.089	0.306Â±0.101
table ||| average	0.947Â±0.029	0.922Â±0.017	0.786Â±0.009	0.878Â±0.006	0.824Â±0.036	0.741Â±0.018
tableCaption ||| Table 2: Experimental result in term of entropy for all data sets and evaluation methods.
bodyText ||| entropy to measure the quality of our clustering
bodyText ||| results. In information theory, entropy (Shan-
bodyText ||| non, 1948) is a measure of the uncertainty as-
bodyText ||| sociated with a random variable. In our prob-
bodyText ||| lem, entropy serves as a measure of randomness
bodyText ||| of clustering result. The entropy of g on a sin-
bodyText ||| gle latent variable z is defined to be H(g, z)
bodyText ||| - PcEC P(cï¿½z)loï¿½2 P(cIz), where C is the class
bodyText ||| label set of V and P(clz) = ï¿½ï¿½vï¿½s(v)=zAt(v)=c}ï¿½
bodyText ||| 11v1s(v)=z}1 ,
bodyText ||| in which t(v) is the true class label of image v.
bodyText ||| Lower entropy H(g, i) indicates less randomness
bodyText ||| and thus better clustering result.
subsectionHeader ||| 4.2 Empirical Analysis
bodyText ||| We now empirically analyze the effectiveness of
bodyText ||| our aPLSA algorithm. Because, to our best of
bodyText ||| knowledge, few existing methods addressed the
bodyText ||| problem of image clustering with the help of so-
bodyText ||| cial annotation image data, we can only compare
bodyText ||| our aPLSA with several state-of-the-art cluster-
bodyText ||| ing algorithms that are not directly designed for
bodyText ||| our problem. The first baseline is the well-known
bodyText ||| KMeans algorithm (MacQueen, 1967). Since our
bodyText ||| algorithm is designed based on PLSA (Hofmann,
bodyText ||| 1999), we also included PLSA for clustering as a
bodyText ||| baseline method in our experiments.
bodyText ||| For each of the above two baselines, we have
bodyText ||| two strategies: (1) separated: the baseline
bodyText ||| method was applied on the target image data only;
bodyText ||| (2) combined: the baseline method was applied
bodyText ||| to cluster the combined data consisting of both
bodyText ||| target image data and the annotated image data.
bodyText ||| Clustering results on target image data were used
bodyText ||| for evaluation. Note that, in the combined data, all
bodyText ||| the annotations were thrown away since baseline
bodyText ||| methods evaluated in this paper do not leverage
bodyText ||| annotation information.
bodyText ||| In addition, we compared our algorithm aPLSA
bodyText ||| to a state-of-the-art transfer clustering strategy,
bodyText ||| known as self-taught clustering (STC) (Dai et al.,
bodyText ||| 2008b). STC makes use of auxiliary data to esti-
bodyText ||| mate a better feature representation to benefit the
bodyText ||| target clustering. In these experiments, the anno-
bodyText ||| tated image data were used as auxiliary data in
bodyText ||| STC, which does not use the annotation text.
bodyText ||| In our experiments, the performance is in the
bodyText ||| form of the average entropy and variance of five
bodyText ||| repeats by randomly selecting 50 images from
bodyText ||| each of the categories. We selected only 50 im-
bodyText ||| ages per category, since this paper is focused on
bodyText ||| clustering sparse data. Table 2 shows the perfor-
bodyText ||| mance with respect to all comparison methods on
bodyText ||| each of the image clustering tasks measured by
bodyText ||| the entropy criterion. From the tables, we can see
bodyText ||| that our algorithm aPLSA outperforms the base-
bodyText ||| line methods in all the data sets. We believe that is
bodyText ||| because aPLSA can effectively utilize the knowl-
bodyText ||| edge from the socially annotated image data. On
bodyText ||| average, aPLSA gives rise to 21.8% of entropy re-
bodyText ||| duction and as compared to KMeans, 5.7% of en-
bodyText ||| tropy reduction as compared to PLSA, and 10.1%
bodyText ||| of entropy reduction as compared to S TC.
subsectionHeader ||| 4.2.1 Varying Data Size
bodyText ||| We now show how the data size affects aPLSA,
bodyText ||| with two baseline methods KMeans and PLSA as
bodyText ||| reference. The experiments were conducted on
bodyText ||| different amounts of target image data, varying
bodyText ||| from 10 to 80. The corresponding experimental
bodyText ||| results in average entropy over all the 11 clustering
bodyText ||| tasks are shown in Figure 4(a). From this figure,
bodyText ||| we observe that aPLSA always yields a significant
bodyText ||| reduction in entropy as compared with two base-
bodyText ||| line methods KMeans and PLSA, regardless of the
bodyText ||| size of target image data that we used.
page ||| 7
figure ||| 1
figure ||| 0.95
figure ||| 0.9
figure ||| 0.85
figure ||| 0.8
figure ||| 0.75
figure ||| 0.7
figure ||| 10	20	30	40	50	60	70	80
figure ||| KMeans
figure ||| PLSA
figure ||| aPLSA
figure ||| 0.75
figure ||| 0.65
figure ||| 0.6
figure ||| 0.55
figure ||| 0.45
figure ||| 0.7
figure ||| 0.5
figure ||| 0.4
figure ||| 0	0.2	0.4	0.6	0.8	1
figure ||| average over 5 development sets
figure ||| 0.75
figure ||| 0.65
figure ||| 0.55
figure ||| 0.7
figure ||| 0.6
figure ||| 0.5
figure ||| 0	50	100	150	200	250 300
figure ||| average over 5 development sets
figure ||| Data size per category		Number of Iteration
figure ||| (a)	(b)	(c)
figureCaption ||| Figure 4: (a) The entropy curve as a function of different amounts of data per category. (b) The entropy
figureCaption ||| curve as a function of different number of iterations. (c) The entropy curve as a function of different
figureCaption ||| trade-off parameter A.
subsubsectionHeader ||| 4.2.2 Parameter Sensitivity
bodyText ||| In aPLSA, there is a trade-off parameter A that af-
bodyText ||| fects how the algorithm relies on auxiliary data.
bodyText ||| When A = 0, the aPLSA relies only on annotated
bodyText ||| image data B. When A = 1, aPLSA relies only
bodyText ||| on target image data A, in which case aPLSA de-
bodyText ||| generates to PLSA. Smaller A indicates heavier re-
bodyText ||| liance on the annotated image data. We have done
bodyText ||| some experiments on the development sets to in-
bodyText ||| vestigate how different A affect the performance
bodyText ||| of aPLSA. We set the number of images per cate-
bodyText ||| gory to 50, and tested the performance of aPLSA.
bodyText ||| The result in average entropy over all development
bodyText ||| sets is shown in Figure 4(b). In the experiments
bodyText ||| described in this paper, we set A to 0.2, which is
bodyText ||| the best point in Figure 4(b).
subsectionHeader ||| 4.2.3 Convergence
bodyText ||| In our experiments, we tested the convergence
bodyText ||| property of our algorithm aPLSA as well. Fig-
bodyText ||| ure 4(c) shows the average entropy curve given
bodyText ||| by aPLSA over all development sets. From this
bodyText ||| figure, we see that the entropy decreases very fast
bodyText ||| during the first 100 iterations and becomes stable
bodyText ||| after 150 iterations. We believe that 200 iterations
bodyText ||| is sufficient for aPLSA to converge.
sectionHeader ||| 5 Conclusions
bodyText ||| In this paper, we proposed a new learning scenario
bodyText ||| called heterogeneous transfer learning and illus-
bodyText ||| trated its application to image clustering. Image
bodyText ||| clustering, a vital component in organizing search
bodyText ||| results for query-based image search, was shown
bodyText ||| to be improved by transferring knowledge from
bodyText ||| unrelated images with annotations in a social Web.
bodyText ||| This is done by first learning the high-quality la-
bodyText ||| tent variables in the auxiliary data, and then trans-
bodyText ||| ferring this knowledge to help improve the cluster-
bodyText ||| ing of the target image data. We conducted experi-
bodyText ||| ments on two image data sets, using the Flickr data
bodyText ||| as the annotated auxiliary image data, and showed
bodyText ||| that our aPLSA algorithm can greatly outperform
bodyText ||| several state-of-the-art clustering algorithms.
bodyText ||| In natural language processing, there are many
bodyText ||| future opportunities to apply heterogeneous trans-
bodyText ||| fer learning. In (Ling et al., 2008) we have shown
bodyText ||| how to classify the Chinese text using English text
bodyText ||| as the training data. We may also consider cluster-
bodyText ||| ing, topic modeling, question answering, etc., to
bodyText ||| be done using data in different feature spaces. We
bodyText ||| can consider data in different modalities, such as
bodyText ||| video, image and audio, as the training data. Fi-
bodyText ||| nally, we will explore the theoretical foundations
bodyText ||| and limitations of heterogeneous transfer learning
bodyText ||| as well.
sectionHeader ||| Acknowledgement Qiang Yang thanks Hong
bodyText ||| Kong CERG grant 621307 for supporting the re-
bodyText ||| search.
sectionHeader ||| References
reference ||| Alina Andreevskaia and Sabine Bergler. 2008. When spe-
reference ||| cialists and generalists work together: Overcoming do-
reference ||| main dependence in sentiment tagging. In ACL-08: HLT,
reference ||| pages 290â298, Columbus, Ohio, June.
reference ||| Andrew Arnold, Ramesh Nallapati, and William W. Cohen.
reference ||| 2007. A comparative study of methods for transductive
reference ||| transfer learning. In ICDM 2007 Workshop on Mining
reference ||| and Management of Biological Data, pages 77-82.
reference ||| Andrew Arnold, Ramesh Nallapati, and William W. Cohen.
reference ||| 2008. Exploiting feature hierarchy for transfer learning in
reference ||| named entity recognition. In ACL-08: HLT.
reference ||| Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney.
reference ||| 2004. A probabilistic framework for semi-supervised
reference ||| clustering. In ACM SIGKDD 2004, pages 59â68.
reference ||| John Blitzer, Ryan Mcdonald, and Fernando Pereira. 2006.
reference ||| Domain adaptation with structural correspondence learn-
reference ||| ing. In EMNLP 2006, pages 120â128, Sydney, Australia.
page ||| 8
reference ||| John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
reference ||| Biographies, bollywood, boom-boxes and blenders: Do-
reference ||| main adaptation for sentiment classification. In ACL 2007,
reference ||| pages 440â447, Prague, Czech Republic.
reference ||| Avrim Blum and Tom Mitchell. 1998. Combining labeled
reference ||| and unlabeled data with co-training. In COLT 1998, pages
reference ||| 92â100, New York, NY, USA. ACM.
reference ||| Rich Caruana. 1997. Multitask learning. Machine Learning,
reference ||| 28(1):41â75.
reference ||| Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation
reference ||| with active learning for word sense disambiguation. In
reference ||| ACL 2007, Prague, Czech Republic.
reference ||| David A. Cohn and Thomas Hofmann. 2000. The missing
reference ||| link - a probabilistic model of document content and hy-
reference ||| pertext connectivity. In NIPS 2000, pages 430â436.
reference ||| Wenyuan Dai, Yuqiang Chen, Gui-Rong Xue, Qiang Yang,
reference ||| and Yong Yu. 2008a. Translated learning: Transfer learn-
reference ||| ing across different feature spaces. In NIPS 2008, pages
reference ||| 353â360.
reference ||| Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong Yu.
reference ||| 2008b. Self-taught clustering. In ICML 2008, pages 200â
reference ||| 207. Omnipress.
reference ||| Hal Daume,III. 2007. Frustratingly easy domain adaptation.
reference ||| In ACL 2007, pages 256â263, Prague, Czech Republic.
reference ||| Jesse Davis and Pedro Domingos. 2008. Deep transfer via
reference ||| second-order markov logic. In AAAI 2008 Workshop on
reference ||| Transfer Learning, Chicago, USA.
reference ||| Scott Deerwester, Susan T. Dumais, George W. Furnas,
reference ||| Thomas K. L, and Richard Harshman. 1990. Indexing by
reference ||| latent semantic analysis. Journal of the American Society
reference ||| for Information Science, pages 391â407.
reference ||| A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Max-
reference ||| imum likelihood from incomplete data via the em algo-
reference ||| rithm. J. of the Royal Statistical Society, 39:1â38.
reference ||| Thomas Finley and Thorsten Joachims. 2005. Supervised
reference ||| clustering with support vector machines. In ICML 2005,
reference ||| pages 217â224, New York, NY, USA. ACM.
reference ||| G. Griffin, A. Holub, and P. Perona. 2007. Caltech-256 ob-
reference ||| ject category dataset. Technical Report 7694, California
reference ||| Institute of Technology.
reference ||| Thomas Hofmann. 1999 Probabilistic latent semantic anal-
reference ||| ysis. In Proc. of Uncertainty in Artificial Intelligence,
reference ||| UAI99. Pages 289â296
reference ||| Thomas Hofmann. 2001. Unsupervised learning by proba-
reference ||| bilistic latent semantic analysis. Machine Learning. vol-
reference ||| ume 42, number 1-2, pages 177â196. Kluwer Academic
reference ||| Publishers.
reference ||| Jing Jiang and Chengxiang Zhai. 2007. Instance weighting
reference ||| for domain adaptation in NLP. In ACL 2007, pages 264â
reference ||| 271, Prague, Czech Republic, June.
reference ||| Leonard Kaufman and Peter J. Rousseeuw. 1990. Finding
reference ||| groups in data: an introduction to cluster analysis. John
reference ||| Wiley and Sons, New York.
reference ||| Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006.
reference ||| Beyond bags of features: Spatial pyramid matching for
reference ||| recognizing natural scene categories. In CVPR 2006,
reference ||| pages 2169â2178, Washington, DC, USA.
reference ||| Fei-Fei Li and Pietro Perona. 2005. A bayesian hierarchi-
reference ||| cal model for learning natural scene categories. In CVPR
reference ||| 2005, pages 524â531, Washington, DC, USA.
reference ||| Xiao Ling, Gui-Rong Xue, Wenyuan Dai, Yun Jiang, Qiang
reference ||| Yang, and Yong Yu. 2008. Can chinese web pages be
reference ||| classified with english data source? In WWW 2008, pages
reference ||| 969â978, New York, NY, USA. ACM.
reference ||| Nicolas Loeff, Cecilia Ovesdotter Alm, and David A.
reference ||| Forsyth. 2006. Discriminating image senses by clustering
reference ||| with multimodal features. In COLING/ACL 2006 Main
reference ||| conference poster sessions, pages 547â554.
reference ||| David G. Lowe. 2004. Distinctive image features from scale-
reference ||| invariant keypoints. International Journal of Computer
reference ||| Vision (IJCV) 2004, volume 60, number 2, pages 91â110.
reference ||| J. B. MacQueen. 1967. Some methods for classification and
reference ||| analysis of multivariate observations. In Proceedings of
reference ||| Fifth Berkeley Symposium on Mathematical Statistics and
reference ||| Probability, pages 1:281â297, Berkeley, CA, USA.
reference ||| Kamal Nigam and Rayid Ghani. 2000. Analyzing the effec-
reference ||| tiveness and applicability of co-training. In Proceedings
reference ||| of the Ninth International Conference on Information and
reference ||| Knowledge Management, pages 86â93, New York, USA.
reference ||| Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer,
reference ||| and Andrew Y. Ng. 2007. Self-taught learning: transfer
reference ||| learning from unlabeled data. In ICML 2007, pages 759â
reference ||| 766, New York, NY, USA. ACM.
reference ||| Roi Reichart and Ari Rappoport. 2007. Self-training for
reference ||| enhancement and domain adaptation of statistical parsers
reference ||| trained on small datasets. In ACL 2007.
reference ||| Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-
reference ||| poport. 2008. Multi-task active learning for linguistic
reference ||| annotations. In ACL-08: HLT, pages 861â869.
reference ||| C. E. Shannon. 1948. A mathematical theory of communi-
reference ||| cation. Bell system technicaljournal, 27.
reference ||| J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T.
reference ||| Freeman. 2005. Discovering object categories in image
reference ||| collections. In ICCV 2005.
reference ||| Naftali Tishby, Fernando C. Pereira, and William Bialek. The
reference ||| information bottleneck method. 1999. In Proc. of the 37-
reference ||| th Annual Allerton Conference on Communication, Con-
reference ||| trol and Computing, pages 368â377.
reference ||| Pengcheng Wu and Thomas G. Dietterich. 2004. Improving
reference ||| svm accuracy by training on auxiliary data sources. In
reference ||| ICML 2004, pages 110â117, New York, NY, USA.
reference ||| Yejun Wu and Douglas W. Oard. 2008. Bilingual topic as-
reference ||| pect classification with a few training examples. In ACM
reference ||| SIGIR 2008, pages 203â210, New York, NY, USA.
reference ||| Xiaojin Zhu. 2007. Semi-supervised learning literature sur-
reference ||| vey. Technical Report 1530, Computer Sciences, Univer-
reference ||| sity of Wisconsin-Madison.
page ||| 9
