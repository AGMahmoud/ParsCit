title ||| Automated Rich Presentation of a Semantic Topic
author ||| Lie Lu and Zhiwei Li
affiliation ||| Microsoft Research Asia
email ||| {llu, zli}@microsoft.com
sectionHeader ||| ABSTRACT
bodyText ||| To have a rich presentation of a topic, it is not only expected that
bodyText ||| many relevant multimodal information, including images, text,
bodyText ||| audio and video, could be extracted; it is also important to
bodyText ||| organize and summarize the related information, and provide
bodyText ||| users a concise and informative storyboard about the target topic.
bodyText ||| It facilitates users to quickly grasp and better understand the
bodyText ||| content of a topic. In this paper, we present a novel approach to
bodyText ||| automatically generating a rich presentation of a given semantic
bodyText ||| topic. In our proposed approach, the related multimodal informa-
bodyText ||| tion of a given topic is first extracted from available multimedia
bodyText ||| databases or websites. Since each topic usually contains multiple
bodyText ||| events, a text-based event clustering algorithm is then performed
bodyText ||| with a generative model. Other media information, such as the
bodyText ||| representative images, possibly available video clips and flashes
bodyText ||| (interactive animates), are associated with each related event. A
bodyText ||| storyboard of the target topic is thus generated by integrating each
bodyText ||| event and its corresponding multimodal information. Finally, to
bodyText ||| make the storyboard more expressive and attractive, an incidental
bodyText ||| music is chosen as background and is aligned with the storyboard.
bodyText ||| A user study indicates that the presented system works quite well
bodyText ||| on our testing examples.
sectionHeader ||| Categories and Subject Descriptors
category ||| H.5.3 [Information Interfaces and Presentation]: Group and
category ||| Organization Interfaces - Organizational design; H.3.1 [Informa-
category ||| tion Storage and Retrieval]: Content Analysis and Indexing -
category ||| abstracting methods.
sectionHeader ||| General Terms
keyword ||| Algorithms, Design, Management, Experimentation, Theory
sectionHeader ||| Keywords
keyword ||| Rich presentation, multimodality, multimedia authoring, story-
keyword ||| board, events clustering, multimedia fusion
sectionHeader ||| 1. INTRODUCTION
bodyText ||| In the multimedia field, a major objective of content analysis is to
bodyText ||| discover the high-level semantics and structures from the low-
bodyText ||| level features, and thus to facilitate indexing, browsing, searching,
bodyText ||| and managing the multimedia database. In recent years, a lot of
copyright ||| Permission to make digital or hard copies of all or part of this work for
copyright ||| personal or classroom use is granted without fee provided that copies are
copyright ||| not made or distributed for profit or commercial advantage and that
copyright ||| copies bear this notice and the full citation on the first page. To copy
copyright ||| otherwise, or republish, to post on servers or to redistribute to lists,
copyright ||| requires prior specific permission and/or a fee.
note ||| MM’05, November 6–11, 2005, Singapore.
copyright ||| Copyright 2005 ACM 1-59593-044-2/05/0011...$5.00.
bodyText ||| technologies have been developed for various media types,
bodyText ||| including images, video, audio and etc. For example, various
bodyText ||| approaches and systems have been proposed in image content
bodyText ||| analysis, such as semantic classification [1], content-based image
bodyText ||| retrieval [2] and photo album management [3]. There are also a lot
bodyText ||| of research focuses on video analysis, such as video segmentation
bodyText ||| [4], highlight detection [5], video summarization [6][7], and video
bodyText ||| structure analysis [8], applied in various data including news
bodyText ||| video, movie and sports video. Since audio information is very
bodyText ||| helpful for video analysis, many research works on audio are also
bodyText ||| developed to enhance multimedia analysis, such as audio
bodyText ||| classification [9], and audio effect detection in different audio
bodyText ||| streams [10]. Most recently, there are more and more approaches
bodyText ||| and systems integrating multimodal information in order to
bodyText ||| improve analysis performance [11][12].
bodyText ||| The main efforts of the above mentioned research have focused on
bodyText ||| understanding the semantics (including a topic, an event or the
bodyText ||| similarity) from the multimodal information. That is, after the
bodyText ||| multimedia data is given, we want to detect the semantics implied
bodyText ||| in these data. In this paper, we propose a new task, Rich
bodyText ||| Presentation, which is an inverse problem of the traditional
bodyText ||| multimedia content analysis. That is, if we have a semantic topic,
bodyText ||| how can we integrate its relevant multimodal information,
bodyText ||| including image, text, audio and video, to richly present the target
bodyText ||| topic and to provide users a concise and informative storyboard?
bodyText ||| In this paper, the so-called “semantic topic” is a generic concept.
bodyText ||| It could be any keyword representing an event or events, a
bodyText ||| person’s name, or anything else. For example, “World Cup 2002”
bodyText ||| and “US election” could be topics, as well as “Halloween” and
bodyText ||| “Harry Potter”. In this paper, our task is to find sufficient
bodyText ||| information on these topics, extract the key points, fuse the
bodyText ||| information from different modalities, and then generate an
bodyText ||| expressive storyboard.
bodyText ||| Rich presentation can be very helpful to facilitate quickly
bodyText ||| grasping and better understanding the corresponding topic.
bodyText ||| People usually search information from (multimedia) database or
bodyText ||| the Internet. However, what they get is usually a bulk of
bodyText ||| unorganized information, with many duplicates and noise. It is
bodyText ||| tedious and costs a long time to get what they want by browsing
bodyText ||| the search results. If there is a tool to help summarize and
bodyText ||| integrate the multimodal information, and then produce a concise
bodyText ||| and informative storyboard, it will enable users to quickly figure
bodyText ||| out the overview contents of a topic that they want to understand.
bodyText ||| Rich presentation provides such a tool, and thus it could have
bodyText ||| many potential applications, such as education and learning,
bodyText ||| multimedia authoring, multimedia retrieval, documentary movie
bodyText ||| production, and information personalization.
bodyText ||| In this paper, we will present the approach to rich presentation. In
bodyText ||| order to produce a concise and informative storyboard to richly
bodyText ||| present a target topic, we need to answer the following questions.
bodyText ||| 1) How to extract the relevant information regarding the target
page ||| 745
bodyText ||| topic? 2) How to extract the key points from the relevant
bodyText ||| information and build a concise and informative storyboard? 3)
bodyText ||| How to fuse all the information from different modality? and 4)
bodyText ||| how to design the corresponding rendering interface?
figure ||| A Target Topic
figure ||| Rich Presentation
figureCaption ||| Fig. 1 The system framework of rich presentation of a target
figureCaption ||| semantic topic. It is mainly composed of three steps, relevant
figureCaption ||| multimodal information extraction, media analysis, and rich
figureCaption ||| presentation generation.
bodyText ||| In this paper, we propose a number of novel approaches to deal
bodyText ||| with the above issues and also present an example system. Fig. 1
bodyText ||| illustrates the proposed system framework of rich presentation. It
bodyText ||| is mainly composed of three steps, relevant multimodal informa-
bodyText ||| tion extraction, media analysis including multiple events cluster-
bodyText ||| ing, representative media detection and music rhythm analysis;
bodyText ||| and the final storyboard generation and music synchronization.
bodyText ||| In the proposed system, given the semantic topic, the relevant
bodyText ||| information, including text, image, video and music, is first
bodyText ||| extracted from the available multimedia database or the web data-
bodyText ||| base. User interaction is also allowed to provide extra relevant
bodyText ||| material or give relevant feedback. Then, the information is
bodyText ||| summarized, with an event clustering algorithm, to give a concise
bodyText ||| representation of the topic and figure out the overview of the
bodyText ||| contents. Other multimedia materials, such as representative
bodyText ||| images (or image sequences) and geographic information, are
bodyText ||| subsequently associated with each event. In the next step, all the
bodyText ||| above information is integrated to generate a storyboard, in which
bodyText ||| each event is presented as one or multiple slides. An incidental
bodyText ||| music, which is also possibly relevant to the topic, is finally
bodyText ||| synchronized with the storyboard to improve its expressiveness
bodyText ||| and attractiveness. Thus, with these steps, a concise and
bodyText ||| informative rich presentation regarding the target topic is gener-
bodyText ||| ated.
bodyText ||| The rest of the paper is organized as follows. Section 2 discusses
bodyText ||| the relevant information extraction corresponding to the target
bodyText ||| topic. Section 3 presents our approach to the topic representation,
bodyText ||| including multiple events clustering, event description, and
bodyText ||| representative media selection. Section 4 describes the approach
bodyText ||| to rich presentation generation, including storyboard generation,
bodyText ||| incidental music analysis and synchronization. Experiments and
bodyText ||| evaluations are presented in the Section 5. Conclusions are given
bodyText ||| in the Section 6.
sectionHeader ||| 2. OBTAINING RELEVANT INFORMATION
bodyText ||| To obtain the multimodal information which is relevant to the
bodyText ||| input topic (keyword), generally, we could search them from
bodyText ||| various databases which have been indexed with the “state-of-the-
bodyText ||| art” multimedia analysis techniques. However, in current stage,
bodyText ||| there is lack of such publicly available multimedia databases. The
bodyText ||| public search engine like MSN or Google indexes all the Internet
bodyText ||| web-pages and can return a lot of relevant information, but the
bodyText ||| search results usually contain much noise. We could also build a
bodyText ||| private database for this system to provide more relevant and
bodyText ||| clean results, but it will be too much expensive to collect and
bodyText ||| annotate sufficient multimedia data for various topics. In order to
bodyText ||| obtain relatively accurate and sufficient data for an arbitrary topic,
bodyText ||| in our system, we chose to collect the relevant multimodal
bodyText ||| information of the given topic from the news websites such as
bodyText ||| MSNBC, BBC and CNN, instead of building an available
bodyText ||| database from the scratch. These news websites are usually well
bodyText ||| organized and managed; and contain various kinds of high quality
bodyText ||| information including text, image and news video clips. Although
bodyText ||| the news websites are used as the information sources in our
bodyText ||| system, other various multimedia databases can be also easily
bodyText ||| incorporated into the system if they are available.
bodyText ||| Instead of directly submitting the topic as a query and getting the
bodyText ||| returned results by using the search function provided by the
bodyText ||| websites, in our system, we crawled the news documents from
bodyText ||| these websites in advance and then build a full-text index. It
bodyText ||| enables us to quickly obtain the relevant documents, and also en-
bodyText ||| able us to use some traditional information retrieval technologies,
bodyText ||| such as query expansion [13], to remove the query ambiguousness
bodyText ||| and get more relevant documents.
bodyText ||| In our approach, user interaction is also allowed to provide more
bodyText ||| materials relevant to the topic, or give relevant feedback on the
bodyText ||| returned results. For example, from the above websites, we can
bodyText ||| seldom find a music clip relevant to the target topic. In this case,
bodyText ||| users could provide the system a preferred music, which will be
bodyText ||| further used as incidental music to accompany with the storyboard
bodyText ||| presentation. Users could also give some feedbacks on the
bodyText ||| obtained documents. For example, if he gives a thumb-up to a
bodyText ||| document, the relevant information of the document needs to be
bodyText ||| presented in the final storyboard. On the other side, users could
bodyText ||| also thumb-down a document to remove the related information.
sectionHeader ||| 3. TOPIC REPRESENTATION
bodyText ||| A semantic topic is usually a quite broad concept and it usually
bodyText ||| contains multiple events. For example, in the topic “Harry Potter”,
bodyText ||| the publication of each book and the release of each movie could
bodyText ||| be considered as an event; while in the topic “World Cup 2002”,
bodyText ||| each match could also be taken as an event. For each event, there
bodyText ||| are usually many documents reporting it. Therefore, in order to
bodyText ||| generate an informative and expressive storyboard to present the
bodyText ||| topic, it would be better to decompose the obtained information
bodyText ||| and cluster the documents into different events.
bodyText ||| However, event definition is usually subjective, different
bodyText ||| individuals may have different opinions. It is also confusing in
bodyText ||| which scale an event should be defined. Also take “World Cup”
bodyText ||| as an example, in a larger scale, “World Cup 2002” and “World
bodyText ||| Cup 2006” could also be considered as a big event. Therefore,
bodyText ||| due to the above vagueness, in this paper, we do not strictly define
figure ||| Relevant multimodal information Retrieval
figure ||| User
figure ||| Interaction
figure ||| Music
figure ||| Text
figure ||| Relevant Media
figure ||| Rhythm Analysis
figure ||| •	Onset/Beat Sequence
figure ||| •	Strength confidence
figure ||| Multiple Events Clustering
figure ||| •	Event summary (4w + time)
figure ||| •	Geographic information
figure ||| Media Association
figure ||| •	Representative images
figure ||| •	Relevant video clips
figure ||| Storyboard Generation
figure ||| Event presentation, multimodal information fusion, layout design
figure ||| Storyboard
figure ||| Music and storyboard synchronization
page ||| 746
bodyText ||| each event of the target topic. Following our previous works on
bodyText ||| news event detection [14], an event is assumed as some similar
bodyText ||| information describing similar persons, similar keywords, similar
bodyText ||| places, and similar time duration. Therefore, in our system, an
bodyText ||| event is represented by four primary elements: who (persons),
bodyText ||| when (time), where (locations) and what (keywords); and event
bodyText ||| clustering is to group the documents reporting similar primary
bodyText ||| elements. As for the scale of event, in the paper, it could be
bodyText ||| adaptively determined by the time range of the obtained
bodyText ||| documents or the required event number.
bodyText ||| In this section, we present a novel clustering approach based on a
bodyText ||| generative model proposed in [14], instead of using traditional
bodyText ||| clustering methods such as K-means. After event clusters are
bodyText ||| obtained, the corresponding event summary is then extracted and
bodyText ||| other representative media is associated with each event.
subsectionHeader ||| 3.1 Multiple Event Clustering
bodyText ||| To group the documents into different events, essentially, we need
bodyText ||| to calculate p(ej I xi), which represents the probability that a docu-
bodyText ||| ment xi belongs to an event ej. Here, as mentioned above, an
bodyText ||| event ej (and thus the document xi describing the event) is
bodyText ||| represented by four primary elements: who (persons), when (time),
bodyText ||| where (locations) and what (keywords). That is,
equation ||| Event / Docment = {persons, locations, keywords, time}
bodyText ||| Assuming that a document is always caused by an event [14] and
bodyText ||| the four primary elements are independent, to calculate the
bodyText ||| probability p(ej I xi), in our approach, we first determine the likeli-
bodyText ||| hood that the document xi is generated from event ej, p(xi I ej)
bodyText ||| which could be further represented by the following generative
bodyText ||| model,
equation ||| p(xi | ej) =p(namei | ej)p(loci | ej)p(keyi | ej)p(timei | ej) (1)
bodyText ||| where namei, loci, keyi, and timei are the feature vectors
bodyText ||| representing persons, locations, keywords and time in the
bodyText ||| document xi, respectively. In our approach, the above entities are
bodyText ||| extracted by the BBN NLP tools [15]. The tool can extract seven
bodyText ||| types of entities, including persons, organizations, locations, date,
bodyText ||| time, money and percent. In our approach, the obtained organiza-
bodyText ||| tion entity is also considered as a person entity; and all the words
bodyText ||| except of persons, locations, and other stop-words are taken as
bodyText ||| keywords.
bodyText ||| In more detail, namei (similarly, loci and keyi) is a vector &lt;ci1,
bodyText ||| ci2, ..., ciNp&gt;, where cin is the occurrence frequency of the personn
bodyText ||| appears in the document xi, and personn is the nth person in the
bodyText ||| person vocabulary, which is composed of all the persons appeared
bodyText ||| in all the obtained documents (similarly, we can define keyword
bodyText ||| vocabulary and location vocabulary). Assuming Np is the size of
bodyText ||| person vocabulary, p(nameiI ej) could be further expressed by
equation ||| Np(namei | ej) = n p(personn | ej )cin (2)
equation ||| n=1
bodyText ||| Since the person, location and keyword are discrete variables
bodyText ||| represented by words, and the probability of the location and
bodyText ||| keyword can be also defined similarly as that of the person in (2),
bodyText ||| in the flowing sections, we will not discriminate them and
bodyText ||| uniformly represent the probability p(personn | ej) (correspond-
bodyText ||| ingly, the p(locationn | ej) and p(keywordn | ej)) as p(wn | ej), which
bodyText ||| denotes the probability that the word wn appears in the event ej
bodyText ||| On the other hand, the time of an event usually lasts a continuous
bodyText ||| duration. It is also observed, especially in the news domain, that
bodyText ||| the documents about an event usually increases at the beginning
bodyText ||| stage of the event and then decreases at the end. Therefore, in
bodyText ||| our approach, a Gaussian model N(uj, aj) is utilized to roughly
bodyText ||| represent the probability p(timei | ej), where uj and aj is the mean
bodyText ||| and standard deviation, respectively.
bodyText ||| To this end, in order to estimate the probability p(ej I xi), we need
bodyText ||| to estimate the parameters 6 = {p(wn | ej), uj, σj, 1!�j5K}, assuming
bodyText ||| K is the number of events (the selection of K is discussed in
bodyText ||| section 3.2). In our approach, the Maximum Likelihood is used to
bodyText ||| estimate the model parameters, as,
equation ||| θ* = argmaxθ log(p(X |θ)) =
equation ||| M	K
equation ||| =argmax θ ∑ log(∑ p(ej)p(xi | ej
equation ||| ia	j=1
equation ||| where X represents the corpus of the obtained documents; M and
equation ||| K are number of documents and events, respectively.
bodyText ||| Since it is difficult to derive a close formula to estimate the
bodyText ||| parameters, in our approach, an Expectation Maximization (EM)
bodyText ||| algorithm is applied to maximize the likelihood, by running E-step
bodyText ||| and M-step iteratively. A brief summary of these two steps is
bodyText ||| listed as follows, and more details can be found in [14].
listItem ||| •	In E-step, the posterior probability p(ej | xi) is estimated as:
equation ||| p(ej | xi)(t+1) =  p(xi | ej)(t)p(ej)(t)	(4)
equation ||| p( xi
equation ||| where the upper script (t) indicate the tth iteration.
listItem ||| •	In M-step, the model parameters are updated, as,
bodyText ||| where tf(i,n) is the term frequency of the word wn in the
bodyText ||| document xi and N is the corresponding vocabulary size. It
bodyText ||| is noted that, in (5), the Laplace smoothing [ 16] is applied to
bodyText ||| prevent zero probability for the infrequently occurring word.
bodyText ||| At last, the prior of each event is updated as:
equation ||| M
equation ||| ∑p (
equation ||| p(ej)(t+1) = i=1(8)
equation ||| M
bodyText ||| arg
bodyText ||| The algorithm can increase the log-likelihood consistently with
bodyText ||| the iterations; and then converge to a local maximum. Once the
bodyText ||| parameters are estimated, we can simply assign each document to
bodyText ||| an event, as following
equation ||| yi =argmaxj(p(ej |xi))	(9)
equation ||| where yi is the event label of the document xi.
equation ||| i=1
equation ||| M
equation ||| t+1)
equation ||| (
equation ||| (
equation ||| )
equation ||| (t
equation ||| u
equation ||| i=1
equation ||| ej
equation ||| +1)
equation ||| | xi
equation ||| (6)
equation ||| ∑p
equation ||| timei
equation ||| tf (i, n)
equation ||| p(wn | ej)(t+1) = 	Mi=1jN	(5)
equation ||| i=1	s=1
equation ||| 1+N+∑(p(e
equation |||  I x)	&apos;
equation ||| ∑s))
equation ||| tf 0
equation ||| ,
equation ||| M
equation ||| uj
equation ||| σ2(t+1) =  i=1 	/7)
equation ||| j	M
equation ||| l
equation ||| (
equation ||| ∑p
equation ||| )
equation ||| |xi
equation ||| i
equation ||| ej
equation ||| =1
equation ||| ∑ p(ej | xi)(t+1) ⋅ (timei −
equation ||| t+1) 2
equation ||| )
equation ||| (
equation ||| t+1)
equation ||| M
equation ||| |θ))
equation ||| maxθ log(∏ p(xi
equation ||| (3)
equation ||| ,θ))
equation ||| ej
equation ||| | xi
equation ||| ) (t+1)
page ||| 747
bodyText ||| The advantage of this generative approach is that it not only
bodyText ||| considers the temporal continuity of an event, it also can deal with
bodyText ||| the issue that some events overlap in some time durations. In this
bodyText ||| case, the Gaussian model of the event time can also be overlapped
bodyText ||| through this data-driven parameter estimation. From this view,
bodyText ||| the event clustering is also like a Gaussian mixture model (GMM)
bodyText ||| estimation in the timeline.
subsectionHeader ||| 3.2 Determining the Number of Events
bodyText ||| In the above approach to event clustering, the event number K is
bodyText ||| assumed known (as shown in (3)-(8)). However, the event number
bodyText ||| is usually very difficult to be determined a priori. In our approach,
bodyText ||| an intuitive way is adopted to roughly estimate the event number
bodyText ||| based on the document distribution along with the timeline.
bodyText ||| As mentioned above, it is assumed that each document is caused
bodyText ||| by an event, and the document number of an event changes with
bodyText ||| the development of the event. According to this property, each
bodyText ||| peak (or the corresponding contour) of the document distribution
bodyText ||| curve might indicate one event [14], as the Fig. 2 shows. Thus, we
bodyText ||| can roughly estimate the event number by simply counting the
bodyText ||| peak number. However, the curve is quite noisy and there
bodyText ||| inevitably exist some noisy peaks in the curve. In order to avoid
bodyText ||| the noisy peaks, in our approach, only the salient peaks are
bodyText ||| assumed to be relevant to the event number.
bodyText ||| To detect the salient peaks, we first smooth the document curve
bodyText ||| with a half-Hamming (raised-cosine) window, and then remove
bodyText ||| the very small peaks with a threshold. Fig.2 illustrates a
bodyText ||| smoothed document distribution with the corresponding threshold,
bodyText ||| collected on the topic “US Election” in four months. In
bodyText ||| experiments, the threshold is adaptively set as Yd-σd/2, where Yd
bodyText ||| and ad are the mean and standard deviation of the curve,
bodyText ||| respectively.
bodyText ||| After the smoothing and tiny peaks removal, we further detect the
bodyText ||| valleys between every two contingent peaks. Thus, the range of
bodyText ||| an event (which is correlated to the corresponding peak) can be
bodyText ||| considered as the envelope in the two valleys. As shown in Fig2,
bodyText ||| the duration denoted by Li+Ri is a rough range of the event
bodyText ||| correlated to the peak Pi. Assuming an important event usually
bodyText ||| has more documents and has effects in a longer duration, the
bodyText ||| saliency of each peak is defined as,
equation ||| Si =( P )(Li +Ri) (10)
equation ||| Pavr Davr
bodyText ||| where Pi is the ith peak, Li and Ri is the duration from the ith peak
bodyText ||| to the previous and next valley; Pavr is the average peak value and
bodyText ||| Davr is average duration between two valleys in the curve. Si is the
bodyText ||| saliency value of the peak Pi. It could also be considered as the
bodyText ||| normalized area under peak Pi, and thus, it roughly represents the
bodyText ||| document number of the corresponding event.
bodyText ||| In our approach, the top K salient peaks are selected to determine
bodyText ||| the event number:
equation ||| K=argmaxk{∑;1Si/∑N1S ≤η}	(11)
equation ||| where S; is the sorted saliency value from large to small, N is
bodyText ||| total number of detected peaks and ii is a threshold. In our
bodyText ||| experiments, ii is set as 0.9, which roughly means that at least
bodyText ||| 90% documents will be kept in the further initialization of event
bodyText ||| clustering. This selection scheme is designed to guarantee there is
bodyText ||| no important information is missed in presentation. After the
bodyText ||| event number and initial clusters (the most salient peaks with their
bodyText ||| corresponding range) are selected, the event parameters could be
bodyText ||| initialized and then updated iteratively.
figure ||| 0 20 40 60 80 100 120
figureCaption ||| Fig.2 Peak saliency definition. It also illustrates the smoothed
figureCaption ||| document distribution (document number per day) with the
figureCaption ||| corresponding threshold for tiny peak removal. Each peak Pi is
figureCaption ||| assumed to be correlated with each event.
bodyText ||| It is noted that some technology such as Bayesian Information
bodyText ||| Criteria (BIC) or minimum description length (MDL) [17] could
bodyText ||| be used to estimate the optimal event number, by searching
bodyText ||| through a reasonable range of the event number to find the one
bodyText ||| which maximizes the likelihood in (3). However, these algo-
bodyText ||| rithms take long time, and it is usually not necessary to estimate
bodyText ||| the exact event number in our scenario of rich presentation.
bodyText ||| Actually, in our system, the most important point of event cluster-
bodyText ||| ing is that the clustered documents ‘really’ represent the same
bodyText ||| event, rather than the event number, as observed in the experi-
bodyText ||| ments. Moreover, in the step of synchronization between the
bodyText ||| music and storyboard (in the section 4.2), the number of presented
bodyText ||| events may be further refined, based on the user’s preference, in
bodyText ||| order to match the presentation duration with the music duration.
subsectionHeader ||| 3.3 Event Description
bodyText ||| After obtaining the events and the corresponding documents, we
bodyText ||| not only need a concise event summary, but also need to extract
bodyText ||| some representative media to describe each event.
subsubsectionHeader ||| 3.3.1 Event Summary
bodyText ||| A simple way to summarize an event is to choose some
bodyText ||| representative words on the persons, locations and keywords of
bodyText ||| the event. For example, for the event ej, the ‘leading actor’ could
bodyText ||| be chosen as the person with the maximum p(personn | ej), while
bodyText ||| the major location could be selected based on p(locationn | ej).
bodyText ||| However, such brief description might have a bad readability.
bodyText ||| Therefore, in order to increase the readability of the summary, in
bodyText ||| our system, we also provide an alterative way. That is, we choose
bodyText ||| a candidate document to represent an event. For example, the
bodyText ||| document with the highest p(xi| ej) is a good candidate represen-
bodyText ||| tative of the event ej. However, a document might be too long to
bodyText ||| be shown on the storyboard. Therefore, in our system, only the
bodyText ||| “title-brow” (the text between the news title and news body) of
bodyText ||| the document, which usually exists and is usually a good
bodyText ||| overview (summary) of the document based on our observation
bodyText ||| (especially true in our case of news document), is selected to
bodyText ||| describe the event.
figure ||| 20
figure ||| 15
figure ||| 10
figure ||| 5
figure ||| 0
figure ||| Peaks relevant to event
figure ||| P;-1	P;+1
figure ||| L;	R;
figure ||| P;
figure ||| #Doc
figure ||| Threshold
figure ||| &apos;
figure ||| 748
figure ||| IV
figure ||| I
figure ||| III
figure ||| II
figureCaption ||| Fig. 3 The event template of the Storyboard, which illustrates (I) the representative media, (II)geographic information, (III) event summary,
figureCaption ||| and (IV) a film strip giving an overview of the events in the temporal order.
subsectionHeader ||| 3.3.2 Extracting Representative Media
bodyText ||| In the obtained documents describing an event, there are usually
bodyText ||| many illustrational images, with possible flashes and video clips.
bodyText ||| These media information is also a good representative of the
bodyText ||| corresponding event. However, since the obtained documents are
bodyText ||| directly crawled from the news websites, they usually contain
bodyText ||| many noisy multimedia resources, such as the advertisements.
bodyText ||| Moreover, there also possible exist some duplicate images in
bodyText ||| different documents describing the same event. Therefore, to
bodyText ||| extract the representative media from the documents, we need to
bodyText ||| remove noisy media and possible duplicate images. Before this,
bodyText ||| we also performed a pre-filtering to remove all the images smaller
bodyText ||| than 50 pixels in height or width.
listItem ||| •	Noisy Media Detection. In our approach, a simple but
listItem ||| efficient rule is used to remove the noisy media resources.
listItem ||| We find almost all advertisements are provided by other
listItem ||| agencies rather than these news websites themselves. That is,
listItem ||| the hosts of advertisement resources are from different
listItem ||| websites. Thus, in our approach, we extract the host names
listItem ||| from the URLs of all multimedia resources, and remove
listItem ||| those resources with different host name.
listItem ||| •	Duplicate Detection. A number of image signature schemes
bodyText ||| can be adopted here to accomplish duplicate detection. In
bodyText ||| our implementation, each image is converted into grayscale,
bodyText ||| and down-sampled to 8 × 8. That is, a 64-byte signature for
bodyText ||| each image is obtained. Then the Euclidean distance of the
bodyText ||| 64-byte signature are taken as the dissimilarity measure.
bodyText ||| Images have sufficiently small distance are considered as
bodyText ||| duplicates.
bodyText ||| Once removing the noisy resources and duplicate images, we
bodyText ||| simply select the 1-4 large images from the top representative
bodyText ||| documents (with the top largest p(xi|ej)), and take them as
bodyText ||| representative media of the corresponding event. The exact
bodyText ||| number of the selected images is dependent on the document
bodyText ||| number (i.e., the importance) of the event and the total image
bodyText ||| number the event has. It is noted that, in our current system, we
bodyText ||| only associates images with each event. However, other media
bodyText ||| like video and flashes can be chosen in a similar way.
sectionHeader ||| 4. RICH PRESENTATION GENERATION
bodyText ||| In the proposed system, the above obtained information, including
bodyText ||| event summary and representative media, are fused to generate a
bodyText ||| concise and informative storyboard, in order to richly present the
bodyText ||| target topic. In this section, we will first describe the storyboard
bodyText ||| generation for the target topic, by presenting each event with the
bodyText ||| multimodal information. Then, we present the approach to
bodyText ||| synchronizing the storyboard with an incidental music.
subsectionHeader ||| 4.1 Storyboard Generation
bodyText ||| In our approach, a storyboard of a target topic is generated by
bodyText ||| presenting each event of the topic slide by slide. To describe an
bodyText ||| event, we have obtained the corresponding information including
bodyText ||| the person, time, location, event summary and other relevant
bodyText ||| images. Therefore, to informatively present each event, we need
bodyText ||| first to design an event template (i.e., an interface) to integrate all
bodyText ||| the information.
bodyText ||| Fig. 3 illustrates the event template used in our proposed system,
bodyText ||| with an example event in the topic ‘US Election”. First, the
bodyText ||| template presents the representative images in the largest area
bodyText ||| (part I), since the pictures are more vivid than the words. As for
bodyText ||| each representative picture, the title and date of the document from
bodyText ||| which it is extracted is also illustrated. In the Fig.3, there are 4
bodyText ||| pictures extracted from 3 documents. Then, the corresponding
bodyText ||| event summaries of these three documents are presented (part III),
bodyText ||| where each paragraph refers to the summary of one document. If a
bodyText ||| user is interested in one document, he can click on the correspond-
bodyText ||| ing title to read more details. Moreover, the geographic informa-
bodyText ||| tion of the event is shown with a map in the top-left corner (part
bodyText ||| II), to give users a view of the event location. The map is obtained
bodyText ||| from “MapPoint Location” service [18], which can return a
page ||| 749
bodyText ||| corresponding map based on user’s location query. However, the
bodyText ||| mapping is usually difficult, especially when the event location is
bodyText ||| confusing so that the representative location is not accurately
bodyText ||| detected. For example, the event shown in the Fig 1 is mapped to
bodyText ||| Washington D.C. rather than New York where the republic
bodyText ||| convention is held, since Washington is the most frequently
bodyText ||| mentioned places in the documents. Finally, a film strip (part IV)
bodyText ||| is also presented, arranging each event in the temporal order,
bodyText ||| where each event is simply represented by a cluster of images,
bodyText ||| with the current event highlighted. It enables users to have a quick
bodyText ||| overview of the past and the future in the event sequence.
bodyText ||| By connecting various events slide by slide, we could get an
bodyText ||| informative storyboard regarding the target topic. In order to
bodyText ||| catch the development process of a topic, the events are ordered
bodyText ||| by their timestamps in the generated storyboard.
subsectionHeader ||| 4.2 Synchronizing with Music
bodyText ||| To make the storyboard more expressive and attractive, and to
bodyText ||| provide a more relaxing way to read information, in the proposed
bodyText ||| system, we will accompany the storyboard with an incidental
bodyText ||| music and align the transitions between event slides with the
bodyText ||| music beats, following the idea in music video generation [19][20].
bodyText ||| Sometimes, music could also provide extra information about the
bodyText ||| target topic. For example, when the target topic is a movie, the
bodyText ||| corresponding theme song could be chosen for the rich presenta-
bodyText ||| tion. In this sub-section, we will present our approach to music
bodyText ||| analysis and synchronization with the storyboard.
subsubsectionHeader ||| 4.2.1 Music Rhythm Analysis
bodyText ||| In the proposed system, we detect the onset sequences instead of
bodyText ||| the exact beat series to represent music rhythm. This is because
bodyText ||| the beat information is sometimes not obvious, especially in light
bodyText ||| music which is usually selected as incidental music. The strongest
bodyText ||| onset in a time window could be assumed as a “beat”. This is
bodyText ||| reasonable since there are some beat positions in a time window
bodyText ||| (for example, 5 seconds); thus, the most possible position of a beat
bodyText ||| is the position of the strongest onset.
bodyText ||| The process of onset estimation is illustrated in Fig. 4. After FFT
bodyText ||| is performed on each frame of 16ms-length, an octave-scale filter-
bodyText ||| bank is used to divide the frequency domain into six sub-bands,
bodyText ||| including [0, co0 /26), [co0 /26, co0 /25), ..., [co0 /22, co0 /2], where co0
bodyText ||| refers to the sampling rate.
figure ||| Onset Curve
figureCaption ||| Fig. 4 The process of onset sequence estimation
bodyText ||| After the amplitude envelope of each sub-band is extracted by
bodyText ||| using a half-Hamming window, a Canny operator is used for onset
bodyText ||| sequence detection by estimating its difference function,
equation ||| Di (n) = Ai (n) ⊗ C(n) (12)
bodyText ||| where Di(n) is the difference function in the ith sub-band, Ai(n) is
bodyText ||| the amplitude envelope of the ith sub-band, and C(n) is the Canny
bodyText ||| operator with a Gaussian kernel,
equation ||| C(n) = i e .2/2σ2 n∈
equation ||| σ 2 c c
bodyText ||| where Lc is the length of the Canny operator and a is used to
bodyText ||| control the operator’s shape, which are set as 12 and 4 in our
bodyText ||| implementation, respectively.
bodyText ||| Finally, the sum of the difference curves of these six sub-bands is
bodyText ||| used to extract onset sequence. Each peak is considered as an
bodyText ||| onset, and the peak value is considered as the onset strength.
bodyText ||| Based on the obtained onsets, an incidental music is further
bodyText ||| segmented into music sub-clips, where a strong onset is taken as
bodyText ||| the boundary of a music sub-clip. These music sub-clips are then
bodyText ||| used as the basic timeline for the synchronization in the next step.
bodyText ||| Thus, to satisfy the requirement that the event slide transitions of
bodyText ||| the storyboard should occur at the music beats, we just need to
bodyText ||| align the event slide boundaries and music sub-clip boundaries.
bodyText ||| To give a more pleasant perception, the music sub-clip should not
bodyText ||| be too short or too long, also it had better not always keep the
bodyText ||| same length. In our implementation, the length of music sub-clips
bodyText ||| is randomly selected in a range of [tmin, tmax] seconds. Thus, the
bodyText ||| music sub-clips can be extracted in the following way: given the
bodyText ||| previous boundary, the next boundary is selected as the strongest
bodyText ||| onset in the window which is [tmin, tmax] seconds away from the
bodyText ||| previous boundary. In the proposed system, users can manually
bodyText ||| specify the range of the length of the music sub-clip. The default
bodyText ||| range in the system is set as [12, 18] seconds, in order to let users
bodyText ||| have enough time to read all the information on each event slide.
subsubsectionHeader ||| 4.2.2 Alignment Scheme
bodyText ||| To synchronize the transitions between different event slides and
bodyText ||| the beats of the incidental music, as mentioned above, we actually
bodyText ||| need to align the slide boundaries and music sub-clip boundaries.
bodyText ||| To satisfy this requirement, a straightforward way is to set the
bodyText ||| length of each event slide be equal to the corresponding length of
bodyText ||| the sub-music clip.
bodyText ||| However, as Fig. 5 illustrates, the number of event slides is
bodyText ||| usually not equal to the number of music sub-clip. In this case, in
bodyText ||| our proposed system, we provide two schemes to solve this
bodyText ||| problem.
listItem ||| 1) Music Sub-clip Based. In this scheme, only the top N important
listItem ||| events of the target topic are adaptively chosen and used in the
listItem ||| rich presentation, where N is supposed as the number of music
listItem ||| sub-clip in the corresponding incidental music, as the Fig.5 shows.
listItem ||| Although a formal definition of event importance is usually hard
listItem ||| and subjective, in our approach, the importance score of an event
listItem ||| is simply measured by the number of documents reporting it,
listItem ||| assuming that the more important the event, the more the
listItem ||| corresponding documents. The assumption is quite similar as that
listItem ||| in the definition of (10).
figure ||| Acoustic Music Data
figure ||| FFT
figure ||| Difference curve
figure ||| Sub-Band 1
figure ||| Envelope
figure ||| Extractor
figure ||| ...	...	...
figure ||| .
figure ||| .
figure ||| .
figure ||| .
figure ||| .
figure ||| .
figure ||| Difference curve
figure ||| Sub-Band N
figure ||| Envelope
figure ||| Extractor
figure ||| ]
figure ||| (13)
page ||| 750
listItem ||| 2) Specified Event Number Based. In this scheme, users can
listItem ||| specify the number of the event he wants to learn. For example, a
listItem ||| user could choose to show the top 30 important events or all the
listItem ||| events. Thus, to accommodate all the events in the music duration,
listItem ||| we will repeat the incidental music if it is needed and then fade out
listItem ||| the music at the end.
bodyText ||| Fig. 5 Music and storyboard synchronization: a music sub-slip
bodyText ||| based scheme, that is, only the top important events are presented
bodyText ||| to match the number of music sub-clips.
subsubsectionHeader ||| 4.2.3 Rendering
bodyText ||| After the alignment between storyboard and incidental music, in
bodyText ||| our system, fifteen common transition effects, such as cross-fade,
bodyText ||| wipe and dissolve, are also randomly selected to connect the event
bodyText ||| slides, producing a better rich presentation in final rendering.
sectionHeader ||| 5. EVALUATIONS
bodyText ||| In this section, we evaluate the performance of the proposed
bodyText ||| approach to rich presentation and its key component, event
bodyText ||| clustering. In the experiments, we randomly select 8 topics of
bodyText ||| different types, including Earthquake, Halloween, Air Disaster,
bodyText ||| US Election, Nobel Prize, Britney Spears, David Beckham, and
bodyText ||| Harry Potter, from some hot news topics in the end of 2004 and
bodyText ||| beginning of 2005. Once the topic is selected, the topic name is
bodyText ||| used as a query and the relevant documents are collected from
bodyText ||| CNN, MSNBC and BBC. More details about the selected topics
bodyText ||| and the corresponding documents are shown in the Table 1, which
bodyText ||| lists the topic name, the time range of the collected documents,
bodyText ||| and the number of documents and its corresponding events.
tableCaption ||| Table 1. A list of testing topics in the rich presentation evaluations
table ||| No.	Topic	Time	#doc	#event
table ||| 1	Earthquake	1995-2004	976	17
table ||| 2	Halloween	1995-2004	762	9
table ||| 3	Air Disaster	1995-2004	210	13
table ||| 4	US Election	1995-2004	2486	—
table ||| 5	Britney Spears	2000-2004	1311	—
table ||| 6	Nobel Prize	1995-2004	186	—
table ||| 7	David Beckham	1995-2004	877	—
table ||| 8	Harry Potter	2000-2004	841	—
table ||| Total	——	——	7649	—
bodyText ||| It is noted that, in the table, only 3 topics have labeled events,
bodyText ||| while another 5 topics have not. This is because that, the labeling
bodyText ||| work of a topic is very subjective and usually hard for individuals
bodyText ||| to manually decide the event number of a given topic. Therefore,
bodyText ||| we only label the topics which are easily to be annotated based on
bodyText ||| the criterion in Topic Detection and Tracking (TDT) project [21].
bodyText ||| For example, Halloween is a topic which is reported once a year,
bodyText ||| thus, each year&apos;s documents can be regarded as an event; as for
bodyText ||| Earthquake and Air Disaster, their events lists could be found
bodyText ||| from corresponding official websites. In the annotation, we
bodyText ||| remove the events which do not have or have few (less than 4)
bodyText ||| relevant documents, and also remove the documents not belonging
bodyText ||| to any events.
bodyText ||| After parsing the obtained documents, for each topic, we usually
bodyText ||| can obtain 3.8 images per document in average. With further
bodyText ||| duplicate detection, only 1.6 images per document are remained.
bodyText ||| Moreover, from each document, we could also obtain about 3.0
bodyText ||| unique location entities and 2.8 unique name entities. Other words
bodyText ||| except of these entities are taken as keywords. Fig.6 shows a real
bodyText ||| representation of an example document with extracted entities in
bodyText ||| the XML format, from which the event clustering is performed.
figure ||| &lt;URL&gt;http://news.bbc.co.uk/1/hi/world/americas/4071845.stm &lt;/URL&gt;
figure ||| &lt;Abstract&gt;The US battleground state of Ohio has certified the victory
figure ||| of President George W Bush&apos;s in last month&apos;s poll. &lt;/Abstract&gt;
figure ||| &lt;Date&gt; 2004/12/6 &lt;/Date&gt;
figure ||| &lt;NLPRESULT&gt;
figure ||| &lt;LOCATION&gt;
figure ||| &lt;entity&gt; Ohio &lt;/entity&gt; &lt;freq&gt;4&lt;/freq&gt;
figure ||| &lt;entity&gt; US &lt;/entity&gt; &lt;freq&gt; 2 &lt;/freq&gt;
figure ||| &lt;/LOCATION&gt;
figure ||| &lt;PERSON&gt;
figure ||| &lt;entity&gt; Bush &lt;/entity&gt; &lt;freq&gt; 3 &lt;/freq&gt;
figure ||| &lt;entity&gt;David Cobb&lt;/entity&gt; &lt;freq&gt;1&lt;/freq&gt;
figure ||| ...
figure ||| &lt;/PERSON&gt;
figure ||| ...
figure ||| &lt;DATE&gt;
figure ||| &lt;entity&gt; 6 December, 200&lt;/entity&gt; &lt;freq&gt; 1 &lt;/freq&gt;
figure ||| &lt;entity&gt; Friday &lt;/entity&gt; &lt;freq&gt; 2 &lt;/freq&gt;
figure ||| ...
figure ||| &lt;/DATE&gt;
figure ||| &lt;KEYWORDS&gt;
figure ||| ...
figure ||| &lt;entity&gt; recount &lt;/entity&gt; &lt;freq&gt;7&lt;/freq&gt;
figure ||| &lt;entity&gt; elect &lt;/entity&gt; &lt;freq&gt;3&lt;/freq&gt;
figure ||| &lt;entity&gt; America &lt;/entity&gt; &lt;freq&gt;3&lt;/freq&gt;
figure ||| &lt;entity&gt; poll &lt;/entity&gt; &lt;freq&gt;3&lt;/freq&gt;
figure ||| ...
figure ||| &lt;/KEYWORDS&gt;
figure ||| &lt;/NLPRESULT&gt;
figureCaption ||| Fig. 6. XML representation of a document on “US Election” with
figureCaption ||| extracted entities
subsectionHeader ||| 5.1 Event Clustering
bodyText ||| As mentioned above, the evaluation of the approach to event
bodyText ||| clustering is evaluated on three topics, including Earthquake, Hal-
bodyText ||| loween, and Air Disaster, for which the corresponding event num-
bodyText ||| bers are determined and the documents are labeled using a similar
bodyText ||| method in the TDT project. However, in the proposed appraoch,
bodyText ||| we actually do not estimate the optimal event number, but use a
bodyText ||| much larger one. Therefore, in order to better evaluate the
bodyText ||| performance of the event clustering algorithm and compare with
bodyText ||| its counterpart, we use the event number in the ground truth to
bodyText ||| initialize the cluster number in the proposed clustering algorithm.
figure ||| Event
figure ||| Slide List
figure ||| Music
figure ||| Sub-Clip
figure ||| E1
figure ||| S1
figure ||| E2
figure ||| S2
figure ||| E3
figure ||| S3
figure ||| E4
figure ||| S4
figure ||| E5
figure ||| S5
figure ||| E6
figure ||| E7
figure ||| E8
page ||| 751
bodyText ||| In the experiments, K-means, which is another frequently used
bodyText ||| clustering algorithm (as well in TDT [22]), is adopted to compare
bodyText ||| with the proposed approach. The comparison results of two
bodyText ||| clustering approaches are illustrated in Table 2, with precision and
bodyText ||| recall for each topic.
tableCaption ||| Table 2. The performance comparison between our approach and
tableCaption ||| K-means on the event clustering
table ||| 	Precision		Recall	
table ||| 	K-means	Ours	K-means	Ours
table ||| Earthquake	0.74	0.87	0.63	0.74
table ||| Halloween	0.88	0.93	0.72	0.81
table ||| Air Disaster	0.57	0.68	0.55	0.61
table ||| Average	0.73	0.83	0.63	0.72
bodyText ||| From Table 2, it can be seen that the results of our approach are
bodyText ||| significantly better than those of K-means, both on precision and
bodyText ||| recall. On the three testing topics, the average precision of our
bodyText ||| approach is up to 0.83 and the average recall achieves 0.72, which
bodyText ||| is 10% and 9% higher than those of K-means, respectively. By
bodyText ||| tracing the process of K-means, we find that K-means usually
bodyText ||| assigns documents far away from each other on the timeline into
bodyText ||| the same cluster, since the time information affects little in K-
bodyText ||| means. It also indicates the advantages of our approach with time
bodyText ||| modeling.
bodyText ||| The algorithms also show different performance on different kind
bodyText ||| topics. As for the “Air disaster”, its performance is not as good as
bodyText ||| that of the other two, since the features (words and time) of its
bodyText ||| events are more complicated and intertwined in the feature space.
bodyText ||| As for the topics (4-8 in Table I) which could not have an
bodyText ||| objective evaluation, the clustering performance on these topics
bodyText ||| could be indirectly reflected by the subjective evaluation of the
bodyText ||| rich presentation presented in section 5.2. This is because users
bodyText ||| will be more satisfied when the grouped documents shown in each
bodyText ||| event slide really belong to the same event; while users are not
bodyText ||| satisfied if the documents from different events are mixed in one
bodyText ||| event slide.
subsectionHeader ||| 5.2 Rich Presentation
bodyText ||| It is usually difficult to find a quantitative measure for rich
bodyText ||| presentation, since the assessment of the goodness of rich presen-
bodyText ||| tation is a strong subjective task. In this paper, we carry out a pre-
bodyText ||| liminary user study to evaluate the performance of the proposed
bodyText ||| rich presentation schemes.
bodyText ||| To indicate the performance of rich presentation, we design two
bodyText ||| measures in the experiments, including ‘informativeness’ and
bodyText ||| ‘enjoyablity’, following the criteria used in the work [7]. Here, the
bodyText ||| informativeness measures whether the subjects satisfy with the
bodyText ||| information obtained from the rich presentation; while enjoyablity
bodyText ||| indicates if users feel comfortable and enjoyable when they are
bodyText ||| reading the rich presentation. In evaluating the informativeness,
bodyText ||| we also provide the documents from which the rich presentation is
bodyText ||| generated. They are used as baseline, based on which the subjects
bodyText ||| can more easily evaluate if the important overview information
bodyText ||| contained in the documents is conveyed by the rich presentation.
bodyText ||| Moreover, in order to reveal the subjects’ opinion on the design of
bodyText ||| the storyboard template, like the one shown in Fig 3, we also ask
bodyText ||| the subjects to evaluate the ‘interface design’.
bodyText ||| In the user study, 10 volunteered subjects including 8 males and 2
bodyText ||| females are invited. The subjects are around 20-35 years old, have
bodyText ||| much experience on computer manipulation, and usually read
bodyText ||| news on web in their leisure time. We ask them to give a
bodyText ||| subjective score between 1 and 5 for each measure of the rich
bodyText ||| presentation of each testing topic (an exception is ‘interface
bodyText ||| design’, which is the same for each rich presentation). Here, the
bodyText ||| score ‘1’ to ‘5’ stands for unsatisfied (1), somewhat unsatisfied (2),
bodyText ||| acceptable (3), satisfied (4) and very satisfied (5), respectively.
bodyText ||| In experiments, we first check with the ‘interface design’ measure.
bodyText ||| We find 7 out of 10 subjects satisfy with the event template design
bodyText ||| and the left three also think it is acceptable. The average score is
bodyText ||| up to 3.9. An interesting observation is that, some subjects like
bodyText ||| the template design very much at the first glance, but they feel a
bodyText ||| little boring after they finish all the user study since every slide in
bodyText ||| the rich presentation of each topic has the same appearance. It
bodyText ||| hints us that we had better design different templates for different
bodyText ||| topics to make the rich presentation more attractive.
bodyText ||| As for the other two measures, we average the score across all the
bodyText ||| subjects to represent the performance for each topic, and list the
bodyText ||| detailed results in Table 3. It can be seen that the average score of
bodyText ||| both enjoyablity and informativeness achieves 3.7, which indicates
bodyText ||| that most subjects satisfy the provided overview information of the
bodyText ||| target topic, and they enjoy themselves when reading these rich
bodyText ||| presentations.
tableCaption ||| Table 3. The evaluation results of rich presentation on each topic
table ||| No.	Topic	Informative	Enjoyable
table ||| 1	Earthquake	4.3	3.2
table ||| 2	Halloween	3.6	4.0
table ||| 3	Air Disaster	4.0	3.4
table ||| 4	US Election	4.1	4.0
table ||| 5	Britney Spears	3.6	4.1
table ||| 6	Nobel Prize	3.3	3.4
table ||| 7	David Beckham	3.4	4.0
table ||| 8	Harry Potter	3.3	3.4
table ||| Average		3.7	3.7
bodyText ||| In the experiments, we find informativeness is highly depended on
bodyText ||| the correlation between the presented documents and the target
bodyText ||| topic. If the presented information is consistent with the topic,
bodyText ||| subjects usually give a high score for informativeness, such as
bodyText ||| those on Earthquake and US Election; otherwise, they will give a
bodyText ||| low score, like those on David Beckham and Nobel Prize. It
bodyText ||| indicates that it is quite important to provide users clean
bodyText ||| information of the target topic with less noise. However, in
bodyText ||| current system, the documents are crawled from web and
bodyText ||| inevitably contain many noises. It affects much on the perform-
bodyText ||| ance of informativeness in the current system. We need to consider
bodyText ||| how to prone the information of the target topic in the future
bodyText ||| works.
bodyText ||| We also find that the enjoyablity score is usually related with
bodyText ||| informativeness. If the subjects do not get enough information
bodyText ||| from the rich presentation, they will be not enjoyable as well, such
bodyText ||| as the topics of Nobel Prize and Harry Potter. Enjoyablity is also
bodyText ||| topic-related, the subjects usually feel unconformable when they
bodyText ||| are facing with miserable topics, such as Earthquake and Air
bodyText ||| Disaster, although their informativeness is quite high. On the
page ||| 752
bodyText ||| contrary, users give a high score for enjoyablity on the interesting
bodyText ||| topics, such as Britney Spears and David Beckham, although their
bodyText ||| informative score is not high. This is because that there are
bodyText ||| usually many funny and interesting pictures in the presentation of
bodyText ||| these topics. Another finding is that users usually fell unenjoyable
bodyText ||| if the images and summaries in one event slide are not consistent
bodyText ||| with each other. From this view, the high enjoyablity score in our
bodyText ||| experiments also indicates that our event clustering algorithm
bodyText ||| works promisingly
sectionHeader ||| 6. CONCLUSIONS
bodyText ||| To facilitate users to quickly grasp and go through the content of a
bodyText ||| semantic topic, in this paper, we have proposed a novel approach
bodyText ||| to rich presentation to generate a concise and informative
bodyText ||| storyboard for the target topic, with many relevant multimodal
bodyText ||| information including image, text, audio and video. In this
bodyText ||| approach, the related multimodal information of a given topic is
bodyText ||| first extracted from news databases. Then, the events are clustered,
bodyText ||| and the corresponding information, such as representative images,
bodyText ||| geographic information, and event summary, is obtained. The
bodyText ||| information is composed into an attractive storyboard which is
bodyText ||| finally synchronized with incidental music. A user study indicates
bodyText ||| that the presented system works well on our testing examples.
bodyText ||| There is still some room for improving the proposed approach.
bodyText ||| First, the proposed approach could be extended to other
bodyText ||| multimedia databases or more general websites. For example,
bodyText ||| some standard multimedia database like NIST TRECVID could
bodyText ||| provide a nice platform for the implementation and evaluation of
bodyText ||| event detection and rich presentation. Second, to integrate more
bodyText ||| relevant multimedia information (such as video clips and flashes)
bodyText ||| and more accurate information regarding the target topic is highly
bodyText ||| expected by users. Thus, more advanced information retrieval/
bodyText ||| extraction techniques and other multimedia analysis techniques are
bodyText ||| needed to be exploited and integrated, such as relevance ranking,
bodyText ||| mapping schemes, important or representative video clips
bodyText ||| detection and video clip summarization. We also need to design a
bodyText ||| much natural way to incorporate video clips in the event template.
bodyText ||| Third, we also consider designing various storyboard templates for
bodyText ||| different kind of topics. For example, each topic may be belonging
bodyText ||| to different clusters such as politics, sports and entertainments,
bodyText ||| each of which can have a representative template. Forth,
bodyText ||| appropriate user interaction will be added to further make the
bodyText ||| storyboard more interactive and easy to control. Finally, a
bodyText ||| thorough evaluation will be implemented to evaluate the effect of
bodyText ||| each component in the framework and storyboard template.
sectionHeader ||| 7. REFERENCES
reference ||| [1] A. Vailaya, M.A.T. Figueiredo, A. K. Jain, and H.-J. Zhang.
reference ||| “Image classification for content-based indexing”. IEEE
reference ||| Transactions on Image Processing, Vol. 10, Iss.1, 2001
reference ||| [2] F. J., M.-J. Li, H.-J. Zhang, and B. Zhang. “An effective
reference ||| region-based image retrieval framework”. Proc. ACM
reference ||| Multimedia’02, pp. 456-465, 2002
reference ||| [3] J. Platt “AutoAlbum: Clustering Digital Photographs using
reference ||| Probabilistic Model Merging” Proc. IEEE Workshop on
reference ||| Content-Based Access of Image and Video Libraries, pp. 96–
reference ||| 100, 2000.
reference ||| [4] A. Hanjalic, R. L. Lagendijk, J. Biemond, “Automated high-
reference ||| level movie segmentation for advanced video-retrieval
reference ||| systems”, IEEE Trans on Circuits and Systems For Video
reference ||| Technology, Vol. 9, No. 4, pp. 580-588, 1999.
reference ||| [5] J. Assfalg and et al, “Semantic annotation of soccer videos:
reference ||| automatic highlights identification,&quot; CVIU&apos;03, vol. 92, pp.
reference ||| 285-305, 2003.
reference ||| [6] A. Ekin, A. M. Tekalp, and R. Mehrotra, &quot;Automatic soccer
reference ||| video analysis and summarization,&quot; IEEE Trans. on Image
reference ||| Processing, 12(7), pp. 796-807, 2003.
reference ||| [7] Y. -F. Ma, L. Lu, H. -J. Zhang, and M.-J Li. “A User
reference ||| Attention Model for Video Summarization”. ACM
reference ||| Multimeida’02, pp. 533-542, 2002.
reference ||| [8] L. Xie, P. Xu, S.F. Chang, A. Divakaran, and H. Sun,
reference ||| &quot;Structure analysis of soccer video with domain knowledge
reference ||| and hidden markov models,&quot; Pattern Recognition Letters,
reference ||| vol. 25(7), pp. 767-775, 2004.
reference ||| [9] L. Lu, H. Jiang, H. J. Zhang, “A Robust Audio Classification
reference ||| and Segmentation Method,” Proc. ACM Multimedia’01, pp.
reference ||| 203-211, 2001
reference ||| [10] R. Cai, L. Lu, H.-J. Zhang, and L.-H. Cai, “Highlight Sound
reference ||| Effects Detection in Audio Stream,” Proc. ICME’03 Vol.3,
reference ||| pp.37-40, 2003.
reference ||| [11] Y. Rui, A. Gupta, and A. Acero, “Automatically Extracting
reference ||| Highlights for TV Baseball Programs”, Proc. ACM Multi-
reference ||| media’00, pp. 105-115, 2000.
reference ||| [12] C. Snoek, and M. Worring. “Multimodal Video Indexing: A
reference ||| Review of the State-of-the-art”. Multimedia Tools and
reference ||| Applications, Vol. 25, No. 1 pp. 5 – 35, 2005
reference ||| [13] E.M. Voorhees, “Query expansion using lexical-semantic
reference ||| relations” Proc. ACM SIGIR Conference on Research and
reference ||| Development in Information Retrieval , pp 61 - 69, 1994
reference ||| [14] Z.-W. Li, M.-J. Li, and W.-Y. Ma. &quot;A Probabilistic Model for
reference ||| Retrospective News Event Detection”, Proc. SIGIR
reference ||| Conference on Research and Development in Information
reference ||| Retrieval, 2005
reference ||| [15] D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. “An
reference ||| Algorithm That Learns What’s in a Name”. Machine
reference ||| Learning, 34(1-3), 1999
reference ||| [16] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. “Text
reference ||| Classification from Labeled and Unlabeled Documents using
reference ||| EM”. Machine Learning, 39(2-3), 2000
reference ||| [17] T. Hastie, R. Tibshirani, and J. Friedman. “The Elements of
reference ||| Statistical Learning: Data Mining, Inference and Prediction”.
reference ||| Springer-Verlag, 2001
reference ||| [18] MapPoint Web Service http://www.microsoft.com/mappoint/
reference ||| products/ webservice/default.mspx
reference ||| [19] X.-S. Hua, L. Lu, H.-J. Zhang. &quot;Automated Home Video
reference ||| Editing&quot;, Proc. ACM Multimedia’03, pp. 490-497, 2003
reference ||| [20] J. Foote, M. Cooper, and A. Girgensohn. “Creating Music
reference ||| Videos Using Automatic Media Analysis”. ACM
reference ||| Multimedia’02, pp.553-560, 2002.
reference ||| [21] Topic Detection and Tracking (TDT) Project: http://www.
reference ||| nist.gov/speech/tests/tdt/
reference ||| [22] J. Allan, R. Papka, and V. Lavrenko. “On-line New Event
reference ||| Detection and Tracking”. Proc. SIGIR Conference on
reference ||| Research and Development in Information Retrieval 98,
reference ||| pp.37-45, 1998
page ||| 753
