title ||| A Frequency-based and a Poisson-based Definition of the
title ||| Probability of Being Informative
author ||| Thomas Roelleke
affiliation ||| Department of Computer Science
affiliation ||| Queen Mary University of London
email ||| thor@dcs.qmul.ac.uk
sectionHeader ||| ABSTRACT
bodyText ||| This paper reports on theoretical investigations about the
bodyText ||| assumptions underlying the inverse document frequency (idf ).
bodyText ||| We show that an intuitive idf-based probability function for
bodyText ||| the probability of a term being informative assumes disjoint
bodyText ||| document events. By assuming documents to be indepen-
bodyText ||| dent rather than disjoint, we arrive at a Poisson-based prob-
bodyText ||| ability of being informative. The framework is useful for
bodyText ||| understanding and deciding the parameter estimation and
bodyText ||| combination in probabilistic retrieval models.
sectionHeader ||| Categories and Subject Descriptors
category ||| H.3.3 [Information Search and Retrieval]: Retrieval
category ||| models
sectionHeader ||| General Terms
keyword ||| Theory
sectionHeader ||| Keywords
keyword ||| Probabilistic information retrieval, inverse document fre-
keyword ||| quency (idf), Poisson distribution, information theory, in-
keyword ||| dependence assumption
sectionHeader ||| 1. INTRODUCTION AND BACKGROUND
bodyText ||| The inverse document frequency (idf) is one of the most
bodyText ||| successful parameters for a relevance-based ranking of re-
bodyText ||| trieved objects. With N being the total number of docu-
bodyText ||| ments, and n(t) being the number of documents in which
bodyText ||| term t occurs, the idf is defined as follows:
equation ||| idf(t) := −log n(tt) , 0 &lt;= idf(t) &lt; ∞
bodyText ||| Ranking based on the sum of the idf-values of the query
bodyText ||| terms that occur in the retrieved documents works well, this
bodyText ||| has been shown in numerous applications. Also, it is well
bodyText ||| known that the combination of a document-specific term
copyright ||| Permission to make digital or hard copies of all or part of this work for
copyright ||| personal or classroom use is granted without fee provided that copies are
copyright ||| not made or distributed for profit or commercial advantage and that copies
copyright ||| bear this notice and the full citation on the first page. To copy otherwise, to
copyright ||| republish, to post on servers or to redistribute to lists, requires prior specific
copyright ||| permission and/or a fee.
note ||| SIGIR’03, July 28–August 1, 2003, Toronto, Canada.
copyright ||| Copyright 2003 ACM 1-58113-646-3/03/0007 ...$5.00.
bodyText ||| weight and idf works better than idf alone. This approach
bodyText ||| is known as tf-idf, where tf(t, d) (0 &lt;= tf(t, d) &lt;= 1) is
bodyText ||| the so-called term frequency of term t in document d. The
bodyText ||| idf reflects the discriminating power (informativeness) of a
bodyText ||| term, whereas the tf reflects the occurrence of a term.
bodyText ||| The idf alone works better than the tf alone does. An ex-
bodyText ||| planation might be the problem of tf with terms that occur
bodyText ||| in many documents; let us refer to those terms as “noisy”
bodyText ||| terms. We use the notion of “noisy” terms rather than “fre-
bodyText ||| quent” terms since frequent terms leaves open whether we
bodyText ||| refer to the document frequency of a term in a collection or
bodyText ||| to the so-called term frequency (also referred to as within-
bodyText ||| document frequency) of a term in a document. We asso-
bodyText ||| ciate “noise” with the document frequency of a term in a
bodyText ||| collection, and we associate “occurrence” with the within-
bodyText ||| document frequency of a term. The tf of a noisy term might
bodyText ||| be high in a document, but noisy terms are not good candi-
bodyText ||| dates for representing a document. Therefore, the removal
bodyText ||| of noisy terms (known as “stopword removal”) is essential
bodyText ||| when applying tf. In a tf-idf approach, the removal of stop-
bodyText ||| words is conceptually obsolete, if stopwords are just words
bodyText ||| with a low idf.
bodyText ||| From a probabilistic point of view, tf is a value with a
bodyText ||| frequency-based probabilistic interpretation whereas idf has
bodyText ||| an “informative” rather than a probabilistic interpretation.
bodyText ||| The missing probabilistic interpretation of idf is a problem
bodyText ||| in probabilistic retrieval models where we combine uncertain
bodyText ||| knowledge of different dimensions (e.g.: informativeness of
bodyText ||| terms, structure of documents, quality of documents, age
bodyText ||| of documents, etc.) such that a good estimate of the prob-
bodyText ||| ability of relevance is achieved. An intuitive solution is a
bodyText ||| normalisation of idf such that we obtain values in the inter-
bodyText ||| val [0; 1]. For example, consider a normalisation based on
bodyText ||| the maximal idf-value. Let T be the set of terms occurring
bodyText ||| in a collection.
equation ||| Pf,Q (t is informative) :=  idf(t) 
equation ||| maxidf
equation ||| maxidf := max({idf(t)|t ∈ T}), maxidf &lt;= −log(1/N)
equation ||| minidf := min({idf(t)|t ∈ T}), minidf &gt;= 0
equation ||| minidf &lt; Pf,Q (t is informative) ≤ 1.0
equation ||| maxidf —
bodyText ||| This frequency-based probability function covers the interval
bodyText ||| [0; 1] if the minimal idf is equal to zero, which is the case
bodyText ||| if we have at least one term that occurs in all documents.
bodyText ||| Can we interpret Pf� Q , the normalised idf, as the probability
bodyText ||| that the term is informative?
bodyText ||| When investigating the probabilistic interpretation of the
page ||| 227
bodyText ||| normalised idf, we made several observations related to dis-
bodyText ||| jointness and independence of document events. These ob-
bodyText ||| servations are reported in section 3. We show in section 3.1
bodyText ||| that the frequency-based noise probability n(t) Nused in the
bodyText ||| classic idf-definition can be explained by three assumptions:
bodyText ||| binary term occurrence, constant document containment and
bodyText ||| disjointness of document containment events. In section 3.2
bodyText ||| we show that by assuming independence of documents, we
bodyText ||| obtain 1 — e-1 Pz� 1 — 0.37 as the upper bound of the noise
bodyText ||| probability of a term. The value e−1 is related to the loga-
bodyText ||| rithm and we investigate in section 3.3 the link to informa-
bodyText ||| tion theory. In section 4, we link the results of the previous
bodyText ||| sections to probability theory. We show the steps from possi-
bodyText ||| ble worlds to binomial distribution and Poisson distribution.
bodyText ||| In section 5, we emphasise that the theoretical framework
bodyText ||| of this paper is applicable for both idf and tf. Finally, in
bodyText ||| section 6, we base the definition of the probability of be-
bodyText ||| ing informative on the results of the previous sections and
bodyText ||| compare frequency-based and Poisson-based definitions.
sectionHeader ||| 2. BACKGROUND
bodyText ||| The relationship between frequencies, probabilities and
bodyText ||| information theory (entropy) has been the focus of many
bodyText ||| researchers. In this background section, we focus on work
bodyText ||| that investigates the application of the Poisson distribution
bodyText ||| in IR since a main part of the work presented in this paper
bodyText ||| addresses the underlying assumptions of Poisson.
bodyText ||| [4] proposes a 2-Poisson model that takes into account
bodyText ||| the different nature of relevant and non-relevant documents,
bodyText ||| rare terms (content words) and frequent terms (noisy terms,
bodyText ||| function words, stopwords). [9] shows experimentally that
bodyText ||| most of the terms (words) in a collection are distributed
bodyText ||| according to a low dimension n-Poisson model. [10] uses a
bodyText ||| 2-Poisson model for including term frequency-based proba-
bodyText ||| bilities in the probabilistic retrieval model. The non-linear
bodyText ||| scaling of the Poisson function showed significant improve-
bodyText ||| ment compared to a linear frequency-based probability. The
bodyText ||| Poisson model was here applied to the term frequency of a
bodyText ||| term in a document. We will generalise the discussion by
bodyText ||| pointing out that document frequency and term frequency
bodyText ||| are dual parameters in the collection space and the docu-
bodyText ||| ment space, respectively. Our discussion of the Poisson dis-
bodyText ||| tribution focuses on the document frequency in a collection
bodyText ||| rather than on the term frequency in a document.
bodyText ||| [7] and [6] address the deviation of idf and Poisson, and
bodyText ||| apply Poisson mixtures to achieve better Poisson-based esti-
bodyText ||| mates. The results proved again experimentally that a one-
bodyText ||| dimensional Poisson does not work for rare terms, therefore
bodyText ||| Poisson mixtures and additional parameters are proposed.
bodyText ||| [3], section 3.3, illustrates and summarises comprehen-
bodyText ||| sively the relationships between frequencies, probabilities
bodyText ||| and Poisson. Different definitions of idf are put into con-
bodyText ||| text and a notion of “noise” is defined, where noise is viewed
bodyText ||| as the complement of idf. We use in our paper a different
bodyText ||| notion of noise: we consider a frequency-based noise that
bodyText ||| corresponds to the document frequency, and we consider a
bodyText ||| term noise that is based on the independence of document
bodyText ||| events.
bodyText ||| [11], [12], [8] and [1] link frequencies and probability esti-
bodyText ||| mation to information theory. [12] establishes a framework
bodyText ||| in which information retrieval models are formalised based
bodyText ||| on probabilistic inference. A key component is the use of a
bodyText ||| space of disjoint events, where the framework mainly uses
bodyText ||| terms as disjoint events. The probability of being informa-
bodyText ||| tive defined in our paper can be viewed as the probability
bodyText ||| of the disjoint terms in the term space of [12].
bodyText ||| [8] address entropy and bibliometric distributions. En-
bodyText ||| tropy is maximal if all events are equiprobable and the fre-
bodyText ||| quency-based Lotka law (N/iλ is the number of scientists
bodyText ||| that have written i publications, where N and λ are distri-
bodyText ||| bution parameters), Zipf and the Pareto distribution are re-
bodyText ||| lated. The Pareto distribution is the continuous case of the
bodyText ||| Lotka and Lotka and Zipf show equivalences. The Pareto
bodyText ||| distribution is used by [2] for term frequency normalisation.
bodyText ||| The Pareto distribution compares to the Poisson distribu-
bodyText ||| tion in the sense that Pareto is “fat-tailed”, i. e. Pareto as-
bodyText ||| signs larger probabilities to large numbers of events than
bodyText ||| Poisson distributions do. This makes Pareto interesting
bodyText ||| since Poisson is felt to be too radical on frequent events.
bodyText ||| We restrict in this paper to the discussion of Poisson, how-
bodyText ||| ever, our results show that indeed a smoother distribution
bodyText ||| than Poisson promises to be a good candidate for improving
bodyText ||| the estimation of probabilities in information retrieval.
bodyText ||| [1] establishes a theoretical link between tf-idf and infor-
bodyText ||| mation theory and the theoretical research on the meaning
bodyText ||| of tf-idf “clarifies the statistical model on which the different
bodyText ||| measures are commonly based”. This motivation matches
bodyText ||| the motivation of our paper: We investigate theoretically
bodyText ||| the assumptions of classical idf and Poisson for a better
bodyText ||| understanding of parameter estimation and combination.
sectionHeader ||| 3. FROM DISJOINT TO INDEPENDENT
bodyText ||| We define and discuss in this section three probabilities:
bodyText ||| The frequency-based noise probability (definition 1), the to-
bodyText ||| tal noise probability for disjoint documents (definition 2).
bodyText ||| and the noise probability for independent documents (defi-
bodyText ||| nition 3).
subsectionHeader ||| 3.1 Binary occurrence, constant containment
subsectionHeader ||| and disjointness of documents
bodyText ||| We show in this section, that the frequency-based noise
bodyText ||| probability nN(t) in the idf definition can be explained as
bodyText ||| a total probability with binary term occurrence, constant
bodyText ||| document containment and disjointness of document con-
bodyText ||| tainments.
bodyText ||| We refer to a probability function as binary if for all events
bodyText ||| the probability is either 1.0 or 0.0. The occurrence proba-
bodyText ||| bility P(t1d) is binary, if P(t1d) is equal to 1.0 if t E d, and
bodyText ||| P(t1d) is equal to 0.0, otherwise.
equation ||| P(t1d) is binary: P(t1d) = 1.0 V P(t1d) = 0.0
bodyText ||| We refer to a probability function as constant if for all
bodyText ||| events the probability is equal. The document containment
bodyText ||| probability reflect the chance that a document occurs in a
bodyText ||| collection. This containment probability is constant if we
bodyText ||| have no information about the document containment or
bodyText ||| we ignore that documents differ in containment. Contain-
bodyText ||| ment could be derived, for example, from the size, quality,
bodyText ||| age, links, etc. of a document. For a constant containment
bodyText ||| in a collection with N documents, N1 is often assumed as
bodyText ||| the containment probability. We generalise this definition
bodyText ||| and introduce the constant λ where 0 &lt; λ &lt; N. The con-
bodyText ||| tainment of a document d depends on the collection c, this
bodyText ||| is reflected by the notation P(d1c) used for the containment
page ||| 228
bodyText ||| of a document.
equation ||| P(d1c) is constant:	Vd : P(d1c) = λ
equation ||| N
bodyText ||| For disjoint documents that cover the whole event space,
bodyText ||| we set λ = 1 and obtain Ed P(d1c) = 1.0. Next, we define
bodyText ||| the frequency-based noise probability and the total noise
bodyText ||| probability for disjoint documents. We introduce the event
bodyText ||| notation t is noisy and t occurs for making the difference
bodyText ||| between the noise probability P(t is noisy1c) in a collection
bodyText ||| and the occurrence probability P(t occurs 1d) in a document
bodyText ||| more explicit, thereby keeping in mind that the noise prob-
bodyText ||| ability corresponds to the occurrence probability of a term
bodyText ||| in a collection.
construct ||| DefInItIOn 1. The frequency-based term noise prob-
construct ||| ability:
equation ||| Pfr q(t is noisy1c) := n(t)
equation ||| N
construct ||| DefInItIOn 2. The total term noise probability for
construct ||| disjoint documents:
equation ||| Pdi9(t is noisy1c) := E P(t occurs1d) • P(d1c)
equation ||| d
bodyText ||| Now, we can formulate a theorem that makes assumptions
bodyText ||| explicit that explain the classical idf.
construct ||| TheOrem 1. IDF assumptions: If the occurrence prob-
construct ||| ability P(t1d) of term t over documents d is binary, and
construct ||| the containment probability P(d1c) of documents d is con-
construct ||| stant, and document containments are disjoint events, then
construct ||| the noise probability for disjoint documents is equal to the
construct ||| frequency-based noise probability.
equation ||| Pdi9(t is noisy1c) = Pfr q(t is noisy1c)
bodyText ||| PrOOf. The assumptions are:
equation ||| Vd : (P(t occurs1d) = 1 V P(t occurs1d) = 0) n
equation ||| P(d1c) = N n
equation ||| E P(d1c) = 1.0
bodyText ||| d
bodyText ||| the containment for small documents tends to be smaller
bodyText ||| than for large documents. From that point of view, idf
bodyText ||| means that P(t n d1c) is constant for all d in which t occurs,
bodyText ||| and P(t n d1c) is zero otherwise. The occurrence and con-
bodyText ||| tainment can be term specific. For example, set P(t nd1c) =
bodyText ||| 1/ND(c) if t occurs in d, where ND(c) is the number of doc-
bodyText ||| uments in collection c (we used before just N). We choose a
bodyText ||| document-dependent occurrence P(t1d) := 1/NT(d), i. e. the
bodyText ||| occurrence probability is equal to the inverse of NT (d), which
bodyText ||| is the total number of terms in document d. Next, we choose
bodyText ||| the containment P(d1c) := NT(d)/NT(c)•NT(c)/ND(c) where
bodyText ||| NT(d)/NT(c) is a document length normalisation (number
bodyText ||| of terms in document d divided by the number of terms in
bodyText ||| collection c), and NT (c)/ND (c) is a constant factor of the
bodyText ||| collection (number of terms in collection c divided by the
bodyText ||| number of documents in collection c). We obtain P(tnd1c) =
bodyText ||| 1/ND (c).
bodyText ||| In a tf-idf-retrieval function, the tf-component reflects
bodyText ||| the occurrence probability of a term in a document. This is
bodyText ||| a further explanation why we can estimate the idf with a
bodyText ||| simple P(t1d), since the combined tf-idf contains the occur-
bodyText ||| rence probability. The containment probability corresponds
bodyText ||| to a document normalisation (document length normalisa-
bodyText ||| tion, pivoted document length) and is normally attached to
bodyText ||| the tf-component or the tf-idf-product.
bodyText ||| The disjointness assumption is typical for frequency-based
bodyText ||| probabilities. From a probability theory point of view, we
bodyText ||| can consider documents as disjoint events, in order to achieve
bodyText ||| a sound theoretical model for explaining the classical idf.
bodyText ||| But does disjointness reflect the real world where the con-
bodyText ||| tainment of a document appears to be independent of the
bodyText ||| containment of another document? In the next section, we
bodyText ||| replace the disjointness assumption by the independence as-
bodyText ||| sumption.
subsectionHeader ||| 3.2 The upper bound of the noise probability
subsectionHeader ||| for independent documents
bodyText ||| For independent documents, we compute the probability
bodyText ||| of a disjunction as usual, namely as the complement of the
bodyText ||| probability of the conjunction of the negated events:
equation ||| P(di V ... V dN) = 1 — P(-di n ... n �dN)
equation ||| (1 — P(d))
equation ||| = 1—rl
equation ||| d
equation ||| 1
equation ||| N
equation ||| =
equation ||| n(t)
equation ||| N
equation ||| = Pfr q(t is noisy1c)
equation ||| We obtain:
equation ||| Pdi9(t is noisy1c) = E
equation ||| dt∈d
bodyText ||| The noise probability can be considered as the conjunction
bodyText ||| of the term occurrence and the document containment.
bodyText ||| The above result is not a surprise but it is a mathemati-
bodyText ||| cal formulation of assumptions that can be used to explain
bodyText ||| the classical idf. The assumptions make explicit that the
bodyText ||| different types of term occurrence in documents (frequency
bodyText ||| of a term, importance of a term, position of a term, doc-
bodyText ||| ument part where the term occurs, etc.) and the different
bodyText ||| types of document containment (size, quality, age, etc.) are
bodyText ||| ignored, and document containments are considered as dis-
bodyText ||| joint events.
bodyText ||| From the assumptions, we can conclude that idf (frequency-
bodyText ||| based noise, respectively) is a relatively simple but strict
bodyText ||| estimate. Still, idf works well. This could be explained
bodyText ||| by a leverage effect that justifies the binary occurrence and
bodyText ||| constant containment: The term occurrence for small docu-
bodyText ||| ments tends to be larger than for large documents, whereas
equation ||| P(t is noisy1c) := P(t occurs n (dl V ... V dN)1c)
bodyText ||| For disjoint documents, this view of the noise probability
bodyText ||| led to definition 2. For independent documents, we use now
bodyText ||| the conjunction of negated events.
construct ||| DefInItIOn 3. The term noise probability for inde-
construct ||| pendent documents:
construct ||| Pin(t is noisy1c) := rl (1 — P(t occurs1d) • P(d1c))
construct ||| d
bodyText ||| With binary occurrence and a constant containment P(d1c) :=
bodyText ||| λ/N, we obtain the term noise of a term t that occurs in n(t)
bodyText ||| documents:
equation ||| Pin(t is noisy1c) = 1 — (1 — λN1n(t)
page ||| 229
bodyText ||| For binary occurrence and disjoint documents, the contain-
bodyText ||| ment probability was 1/N. Now, with independent docu-
bodyText ||| ments, we can use λ as a collection parameter that controls
bodyText ||| the average containment probability. We show through the
bodyText ||| next theorem that the upper bound of the noise probability
bodyText ||| depends on λ.
construct ||| TheOrem 2. The upper bound of being noisy: If the
construct ||| occurrence P(t|d) is binary, and the containment P(d|c)
construct ||| is constant, and document containments are independent
construct ||| events, then 1 − e−λ is the upper bound of the noise proba-
construct ||| bility.
equation ||| ∀t : Pin (t is noisy|c) &lt; 1 − e−λ
bodyText ||| PrOOf. The upper bound of the independent noise prob-
bodyText ||| ability follows from the limit limN→∞(1 + xN)N = ex (see
bodyText ||| any comprehensive math book, for example, [5], for the con-
bodyText ||| vergence equation of the Euler function). With x = −λ, we
bodyText ||| obtain:
equation ||| N
equation ||| lim C1 − λN) = e−λ
equation ||| N→
bodyText ||| For the term noise, we have:
equation ||| Pin(t is noisy|c) = 1 − C1 − λN)n(t)
bodyText ||| Pin (t is noisy |c) is strictly monotonous: The noise of a term
bodyText ||| tn is less than the noise of a term tn+1, where tn occurs in
bodyText ||| n documents and tn+1 occurs in n + 1 documents. There-
bodyText ||| fore, a term with n = N has the largest noise probability.
bodyText ||| For a collection with infinite many documents, the upper
bodyText ||| bound of the noise probability for terms tN that occur in all
bodyText ||| documents becomes:
equation ||| 1−C1−λN)N
equation ||| = 1−e−λ
bodyText ||| By applying an independence rather a disjointness assump-
bodyText ||| tion, we obtain the probability e−1 that a term is not noisy
bodyText ||| even if the term does occur in all documents. In the disjoint
bodyText ||| case, the noise probability is one for a term that occurs in
bodyText ||| all documents.
bodyText ||| If we view P(d|c) := λ/N as the average containment,
bodyText ||| then λ is large for a term that occurs mostly in large docu-
bodyText ||| ments, and λ is small for a term that occurs mostly in small
bodyText ||| documents. Thus, the noise of a term t is large if t occurs in
bodyText ||| n(t) large documents and the noise is smaller if t occurs in
bodyText ||| small documents. Alternatively, we can assume a constant
bodyText ||| containment and a term-dependent occurrence. If we as-
bodyText ||| sume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as
bodyText ||| the average probability that t represents a document. The
bodyText ||| common assumption is that the average containment or oc-
bodyText ||| currence probability is proportional to n(t). However, here
bodyText ||| is additional potential: The statistical laws (see [3] on Luhn
bodyText ||| and Zipf) indicate that the average probability could follow
bodyText ||| a normal distribution, i. e. small probabilities for small n(t)
bodyText ||| and large n(t), and larger probabilities for medium n(t).
bodyText ||| For the monotonous case we investigate here, the noise of
bodyText ||| a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and
bodyText ||| the noise of a term with n(t) = N is close to 1− e−λ. In the
bodyText ||| next section, we relate the value e−λ to information theory.
subsectionHeader ||| 3.3 The probability of a maximal informative
subsectionHeader ||| signal
bodyText ||| The probability e−1 is special in the sense that a signal
bodyText ||| with that probability is a signal with maximal information as
bodyText ||| derived from the entropy definition. Consider the definition
bodyText ||| of the entropy contribution H(t) of a signal t.
equation ||| H(t) := P(t) · − ln P(t)
bodyText ||| We form the first derivation for computing the optimum.
equation ||| − ln P(t) + P(t) · P(t)
equation ||| = −(1+lnP(t))
equation ||| For obtaining optima, we use:
equation ||| 0 = −(1 + ln P(t))
bodyText ||| The entropy contribution H(t) is maximal for P(t) = e−1.
bodyText ||| This result does not depend on the base of the logarithm as
bodyText ||| we see next:
equation ||| −1 
equation ||| P(t) · ln b ·P(t)
equation ||| 1	1 + ln P(t) 
equation ||| ln b + log P(t)	ln b	)
bodyText ||| We summarise this result in the following theorem:
bodyText ||| TheOrem 3. The probability of a maximal informa-
bodyText ||| tive signal: The probability Pmax = e−1 ≈ 0.37 is the prob-
bodyText ||| ability of a maximal informative signal. The entropy of a
bodyText ||| maximal informative signal is Hmax = e−1.
bodyText ||| PrOOf. The probability and entropy follow from the deriva-
bodyText ||| tion above.
bodyText ||| The complement of the maximal noise probability is e−λ
bodyText ||| and we are looking now for a generalisation of the entropy
bodyText ||| definition such that e−λ is the probability of a maximal in-
bodyText ||| formative signal. We can generalise the entropy definition
bodyText ||| by computing the integral of λ+ln P(t), i. e. this derivation
bodyText ||| is zero for e−λ. We obtain a generalised entropy:
equation ||| J −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t))
bodyText ||| The generalised entropy corresponds for λ = 1 to the classi-
bodyText ||| cal entropy. By moving from disjoint to independent docu-
bodyText ||| ments, we have established a link between the complement
bodyText ||| of the noise probability of a term that occurs in all docu-
bodyText ||| ments and information theory. Next, we link independent
bodyText ||| documents to probability theory.
sectionHeader ||| 4. THE LINK TO PROBABILITY THEORY
bodyText ||| We review for independent documents three concepts of
bodyText ||| probability theory: possible worlds, binomial distribution
bodyText ||| and Poisson distribution.
subsectionHeader ||| 4.1 Possible Worlds
bodyText ||| Each conjunction of document events (for each document,
bodyText ||| we consider two document events: the document can be
bodyText ||| true or false) is associated with a so-called possible world.
bodyText ||| For example, consider the eight possible worlds for three
bodyText ||| documents (N = 3).
equation ||| Pin (tN is noisy) = lim
equation ||| N→∞
equation ||| lim
equation ||| N→∞
equation ||| ∂H(t)
equation ||| ∂P(t)
equation ||| ∂H(t) 
equation ||| ∂P(t)
equation ||| = − logb P(t)+
equation ||| = −C
page ||| 230
table ||| world w	conjunction
table ||| w7	d1 ∧ d2 ∧ d3
table ||| ws	d1 ∧ d2 ∧ ¬d3
table ||| w5	d1 ∧ ¬d2 ∧ d3
table ||| w4	d1 ∧ ¬d2 ∧ ¬d3
table ||| w3	¬d1 ∧ d2 ∧ d3
table ||| w2	¬d1 ∧ d2 ∧ ¬d3
table ||| w1	¬d1 ∧ ¬d2 ∧ d3
table ||| w0	¬d1 ∧ ¬d2 ∧ ¬d3
bodyText ||| With each world w, we associate a probability µ(w), which
bodyText ||| is equal to the product of the single probabilities of the doc-
bodyText ||| ument events.
table ||| world w	µ(w)
table ||| 	probability
table ||| w7	(λ·N)0
table ||| 	(N)3
table ||| ws	·	(1 − N)1
table ||| 	rrN)2	
table ||| w5	\N)2·	(1 − N)1
table ||| w4	(N)1 ·	2
table ||| 		(1 − N)
table ||| w3	(N)2·	(1 − N)1
table ||| w2	(λ	(1 − N)2
table ||| w1	(N)1 ·	
table ||| w0	(λ	N)3
bodyText ||| The sum over the possible worlds in which k documents are
bodyText ||| true and N−k documents are false is equal to the probabil-
bodyText ||| ity function of the binomial distribution, since the binomial
bodyText ||| coefficient yields the number of possible worlds in which k
bodyText ||| documents are true.
subsectionHeader ||| 4.2 Binomial distribution
bodyText ||| The binomial probability function yields the probability
bodyText ||| that k of N events are true where each event is true with
bodyText ||| the single event probability p.
equation ||| P(k) := binom(N, k, p) := (N k / pk (1 − p)N− k
bodyText ||| The single event probability is usually defined as p := λ/N,
bodyText ||| i. e. p is inversely proportional to N, the total number of
bodyText ||| events. With this definition of p, we obtain for an infinite
bodyText ||| number of documents the following limit for the product of
bodyText ||| the binomial coefficient and p k :
equation ||| lim N k
equation ||| N→∞k p
equation ||| N · (N−1) · ... · (N−k +1)
equation ||| k!
bodyText ||| The limit is close to the actual value for k &lt;&lt; N. For large
bodyText ||| k, the actual value is smaller than the limit.
bodyText ||| The limit of (1−p)N−k follows from the limit limN→∞(1+ W=
bodyText ||| ex.
equation ||| N−k
equation ||| lim (1 −p)N −k = lim (1 − λN)
equation ||| N→∞	N→
bodyText ||| Again, the limit is close to the actual value for k &lt;&lt; N. For
bodyText ||| large k, the actual value is larger than the limit.
subsectionHeader ||| 4.3 Poisson distribution
bodyText ||| For an infinite number of events, the Poisson probability
bodyText ||| function is the limit of the binomial probability function.
equation ||| Ak
equation ||| binom(N, k, p) = k! · e−λ
equation ||| P(k) = poisson(k, λ) := k! · e−λ
bodyText ||| The probability poisson(0, 1) is equal to e−1, which is the
bodyText ||| probability of a maximal informative signal. This shows
bodyText ||| the relationship of the Poisson distribution and information
bodyText ||| theory.
bodyText ||| After seeing the convergence of the binomial distribution,
bodyText ||| we can choose the Poisson distribution as an approximation
bodyText ||| of the independent term noise probability. First, we define
bodyText ||| the Poisson noise probability:
construct ||| DefInItIOn 4. The Poisson term noise probability:
construct ||| P,oi(t is noisy|c) := e−λ ·
bodyText ||| For independent documents, the Poisson distribution ap-
bodyText ||| proximates the probability of the disjunction for large n(t),
bodyText ||| since the independent term noise probability is equal to the
bodyText ||| sum over the binomial probabilities where at least one of
bodyText ||| n(t) document containment events is true.
equation ||| Pin (t is noisy | c) = n(t) (n(t)1 pk (1 − p)N−k
equation ||| 1. k J
equation ||| 1
equation ||| Pin (t is noisy | c) ≈ P,oi (t is noisy | c)
bodyText ||| We have defined a frequency-based and a Poisson-based prob-
bodyText ||| ability of being noisy, where the latter is the limit of the
bodyText ||| independence-based probability of being noisy. Before we
bodyText ||| present in the final section the usage of the noise proba-
bodyText ||| bility for defining the probability of being informative, we
bodyText ||| emphasise in the next section that the results apply to the
bodyText ||| collection space as well as to the the document space.
equation ||| k
equation |||  N) = e—λ
sectionHeader ||| 5. THE COLLECTION SPACE AND THE
sectionHeader ||| DOCUMENT SPACE
bodyText ||| Consider the dual definitions of retrieval parameters in
bodyText ||| table 1. We associate a collection space D × T with a col-
bodyText ||| lection c where D is the set of documents and T is the set
bodyText ||| of terms in the collection. Let ND := |D| and NT := |T|
bodyText ||| be the number of documents and terms, respectively. We
bodyText ||| consider a document as a subset of T and a term as a subset
bodyText ||| of D. Let nT(d) := |{t|d ∈ t}| be the number of terms that
bodyText ||| occur in the document d, and let nD(t) := | {d|t ∈ d}| be the
bodyText ||| number of documents that contain the term t.
bodyText ||| In a dual way, we associate a document space L × T with
bodyText ||| a document d where L is the set of locations (also referred
bodyText ||| to as positions, however, we use the letters L and l and not
bodyText ||| P and p for avoiding confusion with probabilities) and T is
bodyText ||| the set of terms in the document. The document dimension
bodyText ||| in a collection space corresponds to the location (position)
bodyText ||| dimension in a document space.
bodyText ||| The definition makes explicit that the classical notion of
bodyText ||| term frequency of a term in a document (also referred to as
bodyText ||| the within-document term frequency) actually corresponds
bodyText ||| to the location frequency of a term in a document. For the
equation ||| lime
equation ||| —
equation ||| λ(1−
equation ||| N→
equation ||| =lim
equation ||| N→∞
equation ||| (λ)k = λk
equation ||| N	k!
equation ||| lim
equation ||| N→∞
equation ||| λk
equation ||| k!
equation ||| n(t)�
equation ||| k=1
page ||| 231
table ||| space	collection	document
table ||| dimensions	documents and terms	locations and terms
table ||| document/location frequency	nD(t, c): Number of documents in which term t occurs in collection c	nL(t, d): Number of locations (positions) at which term t occurs in document d
table ||| 	ND(c): Number of documents in collection c	NL(d): Number of locations (positions) in docu-ment d
table ||| term frequency	nT(d,c): Number of terms that document d con- tains in collection c	nT(l,d): Number of terms that location l contains in document d
table ||| 	NT(c): Number of terms in collection c	NT(d): Number of terms in document d
table ||| noise/occurrence containment	P(t1c) (term noise) P(d1c) (document)	P(t1d) (term occurrence) P(l1d) (location)
table ||| informativeness conciseness	— ln P(t1c) — ln P(d1c)	— ln P(t1d) — ln P(l 1d)
table ||| P(informative) P(concise)	ln(P(t1c))/ ln(P(tm in, c)) ln(P(d1c))/ ln(P(dmin1c))	ln(P(t1d))/ ln(P(tm in, d)) ln(P(l1d))/ln(P(lm in1d))
tableCaption ||| Table 1: Retrieval parameters
bodyText ||| actual term frequency value, it is common to use the max-
bodyText ||| imal occurrence (number of locations; let lf be the location
bodyText ||| frequency).
equation ||| tf(t, d) := lf(t, d) :=  Pf, , (t occurs 1d) =  nL (t, d)
equation ||| Pf,,(t. occurs1d) nL(t,, ,d)
bodyText ||| A further duality is between informativeness and concise-
bodyText ||| ness (shortness of documents or locations): informativeness
bodyText ||| is based on occurrence (noise), conciseness is based on con-
bodyText ||| tainment.
bodyText ||| We have highlighted in this section the duality between
bodyText ||| the collection space and the document space. We concen-
bodyText ||| trate in this paper on the probability of a term to be noisy
bodyText ||| and informative. Those probabilities are defined in the col-
bodyText ||| lection space. However, the results regarding the term noise
bodyText ||| and informativeness apply to their dual counterparts: term
bodyText ||| occurrence and informativeness in a document. Also, the
bodyText ||| results can be applied to containment of documents and lo-
bodyText ||| cations.
sectionHeader ||| 6. THE PROBABILITY OF BEING INFOR-
sectionHeader ||| MATIVE
bodyText ||| We showed in the previous sections that the disjointness
bodyText ||| assumption leads to frequency-based probabilities and that
bodyText ||| the independence assumption leads to Poisson probabilities.
bodyText ||| In this section, we formulate a frequency-based definition
bodyText ||| and a Poisson-based definition of the probability of being
bodyText ||| informative and then we compare the two definitions.
construct ||| DefInItIOn 5. The frequency-based probability of be-
construct ||| ing informative:
bodyText ||| We define the Poisson-based probability of being informa-
bodyText ||| tive analogously to the frequency-based probability of being
bodyText ||| informative (see definition 5).
construct ||| DefInItIOn 6. The Poisson-based probability of be-
construct ||| ing informative:
bodyText ||| For λ &gt;&gt; 1, we can alter the noise and informativeness Pois-
bodyText ||| son by starting the sum from 0, since eλ &gt;&gt; 1. Then, the
bodyText ||| minimal Poisson informativeness is poisson(0, λ) = e−λ. We
bodyText ||| obtain a simplified Poisson probability of being informative:
equation ||| λ — ln En (to \k 
equation ||| Ppoj(t is informative1c) �
equation ||| ln En(t) λk
equation ||| k=0 k! 
equation ||| λ
bodyText ||| The computation of the Poisson sum requires an optimi-
bodyText ||| sation for large n(t). The implementation for this paper
bodyText ||| exploits the nature of the Poisson density: The Poisson den-
bodyText ||| sity yields only values significantly greater than zero in an
bodyText ||| interval around λ.
bodyText ||| Consider the illustration of the noise and informative-
bodyText ||| ness definitions in figure 1. The probability functions dis-
bodyText ||| played are summarised in figure 2 where the simplified Pois-
bodyText ||| son is used in the noise and informativeness graphs. The
bodyText ||| frequency-based noise corresponds to the linear solid curve
bodyText ||| in the noise figure. With an independence assumption, we
bodyText ||| obtain the curve in the lower triangle of the noise figure. By
bodyText ||| changing the parameter p := λ/N of the independence prob-
bodyText ||| ability, we can lift or lower the independence curve. The
bodyText ||| noise figure shows the lifting for the value λ := ln N �
bodyText ||| 9.2. The setting λ = ln N is special in the sense that the
bodyText ||| frequency-based and the Poisson-based informativeness have
bodyText ||| the same denominator, namely ln N, and the Poisson sum
bodyText ||| converges to λ. Whether we can draw more conclusions from
bodyText ||| this setting is an open question.
bodyText ||| We can conclude, that the lifting is desirable if we know
bodyText ||| for a collection that terms that occur in relatively few doc-
equation ||| — ln (e−λ	n(t) λk I
equation ||| Ek=1 k!
equation ||| — ln(e−λ • λ)
equation ||| λ —ln Ek=1 kk 
equation ||| =
equation ||| λ — lnλ
bodyText ||| For the sum expression, the following limit holds:
bodyText ||| lim	n(t)�	λk	=eλ—1
bodyText ||| n(t)→∞	k=1	k!	
bodyText ||| Ppoj(t is informative1c) :=
bodyText ||| — ln n(t)
bodyText ||| N 
bodyText ||| —ln 1
bodyText ||| N
bodyText ||| — logN	N)= 1 — logN n(t) = 1 — ln In N
bodyText ||| N	N
bodyText ||| Pf, ,(t is informative1c) :=
bodyText ||| λ
bodyText ||| = 1
page ||| 232
figure ||| 0.8
figure ||| 0.6
figure ||| 0.4
figure ||| 0.2
figure ||| 0
figure ||| 1
figure ||| frequency
figure ||| independence: 1/N
figure ||| independence: ln(N)/N
figure ||| poisson: 1000
figure ||| poisson: 2000
figure ||| poisson: 1 000,2000
figure ||| frequency
figure ||| independence: 1/N
figure ||| 0.8	independence: ln(N)/N
figure ||| poisson: 1000
figure ||| poisson: 2000
figure ||| poisson: 1000,2000
figure ||| 0.6
figure ||| 0.4
figure ||| 0.2
figure ||| 0
figure ||| 1
figure ||| 0	2000 4000 6000 8000 10000
figure ||| n(t): Number of documents with term t
figure ||| 0	2000 4000 6000 8000 10000
figure ||| n(t): Number of documents with term t
figureCaption ||| Figure 1: Noise and Informativeness
figure ||| Probability function		Noise	Informativeness
figure ||| Frequency PfreQ	Def Interval	n(t)/N	ln(n(t)/N)/ln(1/N)
figure ||| 		1/N &lt; PfreQ &lt; 1.0	0.0 &lt; PfreQ &lt; 1.0
figure ||| Independence Pin	Def Interval	1 — (1 — p)n (t)	ln(1 — (1 — p)n(t))/ ln(p)
figure ||| 		p &lt; Pin &lt; 1 — e—λ	ln(p) &lt; Pin &lt; 1.0
figure ||| Poisson Ppoi	Def Interval Def	e—λEn(t) λk	(λ — ln En(t) \k )/(λ — ln λ)
figure ||| Poisson Ppoi simplified	Interval	k=1 k!	k=1
figure ||| 		e—λ • λ &lt; Ppoi &lt; 1 — e—λ	(λ — ln(eλ — 1))/(λ — ln λ) &lt; Ppoi &lt; 1.0
figure ||| 		e—λ Ek=0 kk	(λ — ln Ek=0 kk )/λ
figure ||| 		e—λ &lt; Ppoi &lt; 1.0	0.0 &lt; Ppoi &lt; 1.0
figureCaption ||| Figure 2: Probability functions
bodyText ||| uments are no guarantee for finding relevant documents,
bodyText ||| i. e. we assume that rare terms are still relatively noisy. On
bodyText ||| the opposite, we could lower the curve when assuming that
bodyText ||| frequent terms are not too noisy, i. e. they are considered as
bodyText ||| being still significantly discriminative.
bodyText ||| The Poisson probabilities approximate the independence
bodyText ||| probabilities for large n(t); the approximation is better for
bodyText ||| larger λ. For n(t) &lt; λ, the noise is zero whereas for n(t) &gt; λ
bodyText ||| the noise is one. This radical behaviour can be smoothened
bodyText ||| by using a multi-dimensional Poisson distribution. Figure 1
bodyText ||| shows a Poisson noise based on a two-dimensional Poisson:
equation ||| λkλk
equation ||| poisson(k, λ1, λ2) := π • e—λ1•k� +(1—π)•e- λ2 • 2
equation ||| k!
bodyText ||| The two dimensional Poisson shows a plateau between λ1 =
bodyText ||| 1000 and λ2 = 2000, we used here π = 0.5. The idea be-
bodyText ||| hind this setting is that terms that occur in less than 1000
bodyText ||| documents are considered to be not noisy (i.e. they are in-
bodyText ||| formative), that terms between 1000 and 2000 are half noisy,
bodyText ||| and that terms with more than 2000 are definitely noisy.
bodyText ||| For the informativeness, we observe that the radical be-
bodyText ||| haviour of Poisson is preserved. The plateau here is ap-
bodyText ||| proximately at 1/6, and it is important to realise that this
bodyText ||| plateau is not obtained with the multi-dimensional Poisson
bodyText ||| noise using π = 0.5. The logarithm of the noise is nor-
bodyText ||| malised by the logarithm of a very small number, namely
bodyText ||| 0.5 • e—1000 + 0.5 • e—2000. That is why the informativeness
bodyText ||| will be only close to one for very little noise, whereas for a
bodyText ||| bit of noise, informativeness will drop to zero. This effect
bodyText ||| can be controlled by using small values for π such that the
bodyText ||| noise in the interval [λ1; λ2] is still very little. The setting
bodyText ||| π = e—2000/6 leads to noise values of approximately e —2000/6
bodyText ||| in the interval [λ1; λ2], the logarithms lead then to 1/6 for
bodyText ||| the informativeness.
bodyText ||| The indepence-based and frequency-based informativeness
bodyText ||| functions do not differ as much as the noise functions do.
bodyText ||| However, for the indepence-based probability of being infor-
bodyText ||| mative, we can control the average informativeness by the
bodyText ||| definition p := λ/N whereas the control on the frequency-
bodyText ||| based is limited as we address next.
bodyText ||| For the frequency-based idf, the gradient is monotonously
bodyText ||| decreasing and we obtain for different collections the same
bodyText ||| distances of idf-values, i. e. the parameter N does not affect
bodyText ||| the distance. For an illustration, consider the distance be-
bodyText ||| tween the value idf(tn+1) of a term tn+1 that occurs in n+1
bodyText ||| documents, and the value idf(tn) of a term tn that occurs in
bodyText ||| n documents.
equation ||| idf(tn+1) — idf(tn) = ln  n
equation ||| n+ 1
bodyText ||| The first three values of the distance function are:
equation ||| idf(t2) — idf(t1) = ln(1/(1	+ 1))	=	0.69
equation ||| idf(t3) — idf(t2) = ln(1/(2	+ 1))	=	0.41
equation ||| idf(t4) — idf(t3) = ln(1/(3	+ 1))	=	0.29
bodyText ||| For the Poisson-based informativeness, the gradient decreases
bodyText ||| first slowly for small n(t), then rapidly near n(t) R� λ and
bodyText ||| then it grows again slowly for large n(t).
bodyText ||| In conclusion, we have seen that the Poisson-based defini-
bodyText ||| tion provides more control and parameter possibilities than
page ||| 233
bodyText ||| the frequency-based definition does. Whereas more control
bodyText ||| and parameter promises to be positive for the personalisa-
bodyText ||| tion of retrieval systems, it bears at the same time the dan-
bodyText ||| ger of just too many parameters. The framework presented
bodyText ||| in this paper raises the awareness about the probabilistic
bodyText ||| and information-theoretic meanings of the parameters. The
bodyText ||| parallel definitions of the frequency-based probability and
bodyText ||| the Poisson-based probability of being informative made
bodyText ||| the underlying assumptions explicit. The frequency-based
bodyText ||| probability can be explained by binary occurrence, constant
bodyText ||| containment and disjointness of documents. Independence
bodyText ||| of documents leads to Poisson, where we have to be aware
bodyText ||| that Poisson approximates the probability of a disjunction
bodyText ||| for a large number of events, but not for a small number.
bodyText ||| This theoretical result explains why experimental investiga-
bodyText ||| tions on Poisson (see [7]) show that a Poisson estimation
bodyText ||| does work better for frequent (bad, noisy) terms than for
bodyText ||| rare (good, informative) terms.
bodyText ||| In addition to the collection-wide parameter setting, the
bodyText ||| framework presented here allows for document-dependent
bodyText ||| settings, as explained for the independence probability. This
bodyText ||| is in particular interesting for heterogeneous and structured
bodyText ||| collections, since documents are different in nature (size,
bodyText ||| quality, root document, sub document), and therefore, bi-
bodyText ||| nary occurrence and constant containment are less appro-
bodyText ||| priate than in relatively homogeneous collections.
sectionHeader ||| 7. SUMMARY
bodyText ||| The definition of the probability of being informative trans-
bodyText ||| forms the informative interpretation of the idf into a proba-
bodyText ||| bilistic interpretation, and we can use the idf -based proba-
bodyText ||| bility in probabilistic retrieval approaches. We showed that
bodyText ||| the classical definition of the noise (document frequency) in
bodyText ||| the inverse document frequency can be explained by three
bodyText ||| assumptions: the term within-document occurrence prob-
bodyText ||| ability is binary, the document containment probability is
bodyText ||| constant, and the document containment events are disjoint.
bodyText ||| By explicitly and mathematically formulating the assump-
bodyText ||| tions, we showed that the classical definition of idf does not
bodyText ||| take into account parameters such as the different nature
bodyText ||| (size, quality, structure, etc.) of documents in a collection,
bodyText ||| or the different nature of terms (coverage, importance, po-
bodyText ||| sition, etc.) in a document. We discussed that the absence
bodyText ||| of those parameters is compensated by a leverage effect of
bodyText ||| the within-document term occurrence probability and the
bodyText ||| document containment probability.
bodyText ||| By applying an independence rather a disjointness as-
bodyText ||| sumption for the document containment, we could estab-
bodyText ||| lish a link between the noise probability (term occurrence
bodyText ||| in a collection), information theory and Poisson. From the
bodyText ||| frequency-based and the Poisson-based probabilities of be-
bodyText ||| ing noisy, we derived the frequency-based and Poisson-based
bodyText ||| probabilities of being informative. The frequency-based prob-
bodyText ||| ability is relatively smooth whereas the Poisson probability
bodyText ||| is radical in distinguishing between noisy or not noisy, and
bodyText ||| informative or not informative, respectively. We showed how
bodyText ||| to smoothen the radical behaviour of Poisson with a multi-
bodyText ||| dimensional Poisson.
bodyText ||| The explicit and mathematical formulation of idf- and
bodyText ||| Poisson-assumptions is the main result of this paper. Also,
bodyText ||| the paper emphasises the duality of idf and tf, collection
bodyText ||| space and document space, respectively. Thus, the result
bodyText ||| applies to term occurrence and document containment in a
bodyText ||| collection, and it applies to term occurrence and position
bodyText ||| containment in a document. This theoretical framework is
bodyText ||| useful for understanding and deciding the parameter estima-
bodyText ||| tion and combination in probabilistic retrieval models. The
bodyText ||| links between indepence-based noise as document frequency,
bodyText ||| probabilistic interpretation of idf, information theory and
bodyText ||| Poisson described in this paper may lead to variable proba-
bodyText ||| bilistic idf and tf definitions and combinations as required
bodyText ||| in advanced and personalised information retrieval systems.
bodyText ||| Acknowledgment: I would like to thank Mounia Lalmas,
bodyText ||| Gabriella Kazai and Theodora Tsikrika for their comments
bodyText ||| on the as they said “heavy” pieces. My thanks also go to the
bodyText ||| meta-reviewer who advised me to improve the presentation
bodyText ||| to make it less “formidable” and more accessible for those
bodyText ||| “without a theoretic bent”. This work was funded by a
bodyText ||| research fellowship from Queen Mary University of London.
sectionHeader ||| 8. REFERENCES
reference ||| [1] A. Aizawa. An information-theoretic perspective of
reference ||| tf-idf measures. Information Processing and
reference ||| Management, 39:45–65, January 2003.
reference ||| [2] G. Amati and C. J. Rijsbergen. Term frequency
reference ||| normalization via Pareto distributions. In 24th
reference ||| BCS-IRSG European Colloquium on IR Research,
reference ||| Glasgow, Scotland, 2002.
reference ||| [3] R. K. Belew. Finding out about. Cambridge University
reference ||| Press, 2000.
reference ||| [4] A. Bookstein and D. Swanson. Probabilistic models
reference ||| for automatic indexing. Journal of the American
reference ||| Society for Information Science, 25:312–318, 1974.
reference ||| [5] I. N. Bronstein. Taschenbuch der Mathematik. Harri
reference ||| Deutsch, Thun, Frankfurt am Main, 1987.
reference ||| [6] K. Church and W. Gale. Poisson mixtures. Natural
reference ||| Language Engineering, 1(2):163–190, 1995.
reference ||| [7] K. W. Church and W. A. Gale. Inverse document
reference ||| frequency: A measure of deviations from poisson. In
reference ||| Third Workshop on Very Large Corpora, ACL
reference ||| Anthology, 1995.
reference ||| [8] T. Lafouge and C. Michel. Links between information
reference ||| construction and information gain: Entropy and
reference ||| bibliometric distribution. Journal of Information
reference ||| Science, 27(1):39–49, 2001.
reference ||| [9] E. Margulis. N-poisson document modelling. In
reference ||| Proceedings of the 15th Annual International ACM
reference ||| SIGIR Conference on Research and Development in
reference ||| Information Retrieval, pages 177–189, 1992.
reference ||| [10] S. E. Robertson and S. Walker. Some simple effective
reference ||| approximations to the 2-poisson model for
reference ||| probabilistic weighted retrieval. In Proceedings of the
reference ||| 17th Annual International ACM SIGIR Conference on
reference ||| Research and Development in Information Retrieval,
reference ||| pages 232–241, London, et al., 1994. Springer-Verlag.
reference ||| [11] S. Wong and Y. Yao. An information-theoric measure
reference ||| of term specificity. Journal of the American Society
reference ||| for Information Science, 43(1):54–61, 1992.
reference ||| [12] S. Wong and Y. Yao. On modeling information
reference ||| retrieval with probabilistic inference. ACM
reference ||| Transactions on Information Systems, 13(1):38–68,
reference ||| 1995.
page ||| 234
