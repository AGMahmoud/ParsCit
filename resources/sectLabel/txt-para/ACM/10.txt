# Para 0 1
A Frequency-based and a Poisson-based Definition of the
# Para 1 1
Probability of Being Informative
# Para 2 1
Thomas Roelleke
# Para 3 2
Department of Computer Science 
Queen Mary University of London
# Para 5 1
thor@dcs.qmul.ac.uk
# Para 6 1
ABSTRACT
# Para 7 9
This paper reports on theoretical investigations about the 
assumptions underlying the inverse document frequency (idf ). 
We show that an intuitive idf-based probability function for 
the probability of a term being informative assumes disjoint 
document events. By assuming documents to be indepen-
dent rather than disjoint, we arrive at a Poisson-based prob-
ability of being informative. The framework is useful for 
understanding and deciding the parameter estimation and 
combination in probabilistic retrieval models.
# Para 16 1
Categories and Subject Descriptors
# Para 17 2
H.3.3 [Information Search and Retrieval]: Retrieval 
models
# Para 19 1
General Terms
# Para 20 1
Theory
# Para 21 1
Keywords
# Para 22 3
Probabilistic information retrieval, inverse document fre-
quency (idf), Poisson distribution, information theory, in-
dependence assumption
# Para 25 1
1. INTRODUCTION AND BACKGROUND
# Para 26 5
The inverse document frequency (idf) is one of the most 
successful parameters for a relevance-based ranking of re-
trieved objects. With N being the total number of docu-
ments, and n(t) being the number of documents in which 
term t occurs, the idf is defined as follows:
# Para 31 1
idf(t) := −log n(tt) , 0 &lt;= idf(t) &lt; ∞
# Para 32 4
Ranking based on the sum of the idf-values of the query 
terms that occur in the retrieved documents works well, this 
has been shown in numerous applications. Also, it is well 
known that the combination of a document-specific term
# Para 36 6
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, to 
republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee.
# Para 42 1
SIGIR’03, July 28–August 1, 2003, Toronto, Canada.
# Para 43 1
Copyright 2003 ACM 1-58113-646-3/03/0007 ...$5.00.
# Para 44 5
weight and idf works better than idf alone. This approach 
is known as tf-idf, where tf(t, d) (0 &lt;= tf(t, d) &lt;= 1) is 
the so-called term frequency of term t in document d. The 
idf reflects the discriminating power (informativeness) of a 
term, whereas the tf reflects the occurrence of a term.
# Para 49 17
The idf alone works better than the tf alone does. An ex-
planation might be the problem of tf with terms that occur 
in many documents; let us refer to those terms as “noisy” 
terms. We use the notion of “noisy” terms rather than “fre-
quent” terms since frequent terms leaves open whether we 
refer to the document frequency of a term in a collection or 
to the so-called term frequency (also referred to as within- 
document frequency) of a term in a document. We asso-
ciate “noise” with the document frequency of a term in a 
collection, and we associate “occurrence” with the within- 
document frequency of a term. The tf of a noisy term might 
be high in a document, but noisy terms are not good candi-
dates for representing a document. Therefore, the removal 
of noisy terms (known as “stopword removal”) is essential 
when applying tf. In a tf-idf approach, the removal of stop- 
words is conceptually obsolete, if stopwords are just words 
with a low idf.
# Para 66 13
From a probabilistic point of view, tf is a value with a 
frequency-based probabilistic interpretation whereas idf has 
an “informative” rather than a probabilistic interpretation. 
The missing probabilistic interpretation of idf is a problem 
in probabilistic retrieval models where we combine uncertain 
knowledge of different dimensions (e.g.: informativeness of 
terms, structure of documents, quality of documents, age 
of documents, etc.) such that a good estimate of the prob-
ability of relevance is achieved. An intuitive solution is a 
normalisation of idf such that we obtain values in the inter-
val [0; 1]. For example, consider a normalisation based on 
the maximal idf-value. Let T be the set of terms occurring 
in a collection.
# Para 79 1
Pf,Q (t is informative) :=  idf(t) 
# Para 80 1
maxidf
# Para 81 2
maxidf := max({idf(t)|t ∈ T}), maxidf &lt;= −log(1/N) 
minidf := min({idf(t)|t ∈ T}), minidf &gt;= 0
# Para 83 2
minidf &lt; Pf,Q (t is informative) ≤ 1.0 
maxidf —
# Para 85 5
This frequency-based probability function covers the interval 
[0; 1] if the minimal idf is equal to zero, which is the case 
if we have at least one term that occurs in all documents. 
Can we interpret Pf� Q , the normalised idf, as the probability 
that the term is informative?
# Para 90 1
When investigating the probabilistic interpretation of the
# Para 91 1
227
# Para 92 19
normalised idf, we made several observations related to dis-
jointness and independence of document events. These ob-
servations are reported in section 3. We show in section 3.1 
that the frequency-based noise probability n(t) Nused in the 
classic idf-definition can be explained by three assumptions: 
binary term occurrence, constant document containment and 
disjointness of document containment events. In section 3.2 
we show that by assuming independence of documents, we 
obtain 1 — e-1 Pz� 1 — 0.37 as the upper bound of the noise 
probability of a term. The value e−1 is related to the loga-
rithm and we investigate in section 3.3 the link to informa-
tion theory. In section 4, we link the results of the previous 
sections to probability theory. We show the steps from possi-
ble worlds to binomial distribution and Poisson distribution. 
In section 5, we emphasise that the theoretical framework 
of this paper is applicable for both idf and tf. Finally, in 
section 6, we base the definition of the probability of be-
ing informative on the results of the previous sections and 
compare frequency-based and Poisson-based definitions.
# Para 111 1
2. BACKGROUND
# Para 112 6
The relationship between frequencies, probabilities and 
information theory (entropy) has been the focus of many 
researchers. In this background section, we focus on work 
that investigates the application of the Poisson distribution 
in IR since a main part of the work presented in this paper 
addresses the underlying assumptions of Poisson.
# Para 118 17
[4] proposes a 2-Poisson model that takes into account 
the different nature of relevant and non-relevant documents, 
rare terms (content words) and frequent terms (noisy terms, 
function words, stopwords). [9] shows experimentally that 
most of the terms (words) in a collection are distributed 
according to a low dimension n-Poisson model. [10] uses a 
2-Poisson model for including term frequency-based proba-
bilities in the probabilistic retrieval model. The non-linear 
scaling of the Poisson function showed significant improve-
ment compared to a linear frequency-based probability. The 
Poisson model was here applied to the term frequency of a 
term in a document. We will generalise the discussion by 
pointing out that document frequency and term frequency 
are dual parameters in the collection space and the docu-
ment space, respectively. Our discussion of the Poisson dis-
tribution focuses on the document frequency in a collection 
rather than on the term frequency in a document.
# Para 135 5
[7] and [6] address the deviation of idf and Poisson, and 
apply Poisson mixtures to achieve better Poisson-based esti-
mates. The results proved again experimentally that a one- 
dimensional Poisson does not work for rare terms, therefore 
Poisson mixtures and additional parameters are proposed.
# Para 140 9
[3], section 3.3, illustrates and summarises comprehen-
sively the relationships between frequencies, probabilities 
and Poisson. Different definitions of idf are put into con-
text and a notion of “noise” is defined, where noise is viewed 
as the complement of idf. We use in our paper a different 
notion of noise: we consider a frequency-based noise that 
corresponds to the document frequency, and we consider a 
term noise that is based on the independence of document 
events.
# Para 149 5
[11], [12], [8] and [1] link frequencies and probability esti-
mation to information theory. [12] establishes a framework 
in which information retrieval models are formalised based 
on probabilistic inference. A key component is the use of a 
space of disjoint events, where the framework mainly uses 
# Para 154 3
terms as disjoint events. The probability of being informa-
tive defined in our paper can be viewed as the probability 
of the disjoint terms in the term space of [12].
# Para 157 17
[8] address entropy and bibliometric distributions. En-
tropy is maximal if all events are equiprobable and the fre-
quency-based Lotka law (N/iλ is the number of scientists 
that have written i publications, where N and λ are distri-
bution parameters), Zipf and the Pareto distribution are re-
lated. The Pareto distribution is the continuous case of the 
Lotka and Lotka and Zipf show equivalences. The Pareto 
distribution is used by [2] for term frequency normalisation. 
The Pareto distribution compares to the Poisson distribu-
tion in the sense that Pareto is “fat-tailed”, i. e. Pareto as-
signs larger probabilities to large numbers of events than 
Poisson distributions do. This makes Pareto interesting 
since Poisson is felt to be too radical on frequent events. 
We restrict in this paper to the discussion of Poisson, how-
ever, our results show that indeed a smoother distribution 
than Poisson promises to be a good candidate for improving 
the estimation of probabilities in information retrieval.
# Para 174 7
[1] establishes a theoretical link between tf-idf and infor-
mation theory and the theoretical research on the meaning 
of tf-idf “clarifies the statistical model on which the different 
measures are commonly based”. This motivation matches 
the motivation of our paper: We investigate theoretically 
the assumptions of classical idf and Poisson for a better 
understanding of parameter estimation and combination.
# Para 181 1
3. FROM DISJOINT TO INDEPENDENT
# Para 182 5
We define and discuss in this section three probabilities: 
The frequency-based noise probability (definition 1), the to-
tal noise probability for disjoint documents (definition 2). 
and the noise probability for independent documents (defi-
nition 3).
# Para 187 2
3.1 Binary occurrence, constant containment 
and disjointness of documents
# Para 189 5
We show in this section, that the frequency-based noise 
probability nN(t) in the idf definition can be explained as 
a total probability with binary term occurrence, constant 
document containment and disjointness of document con-
tainments.
# Para 194 4
We refer to a probability function as binary if for all events 
the probability is either 1.0 or 0.0. The occurrence proba-
bility P(t1d) is binary, if P(t1d) is equal to 1.0 if t E d, and 
P(t1d) is equal to 0.0, otherwise.
# Para 198 1
P(t1d) is binary: P(t1d) = 1.0 V P(t1d) = 0.0
# Para 199 13
We refer to a probability function as constant if for all 
events the probability is equal. The document containment 
probability reflect the chance that a document occurs in a 
collection. This containment probability is constant if we 
have no information about the document containment or 
we ignore that documents differ in containment. Contain-
ment could be derived, for example, from the size, quality, 
age, links, etc. of a document. For a constant containment 
in a collection with N documents, N1 is often assumed as 
the containment probability. We generalise this definition 
and introduce the constant λ where 0 &lt; λ &lt; N. The con-
tainment of a document d depends on the collection c, this 
is reflected by the notation P(d1c) used for the containment
# Para 212 1
228
# Para 213 1
of a document.
# Para 214 1
P(d1c) is constant:	Vd : P(d1c) = λ
# Para 215 1
N
# Para 216 10
For disjoint documents that cover the whole event space, 
we set λ = 1 and obtain Ed P(d1c) = 1.0. Next, we define 
the frequency-based noise probability and the total noise 
probability for disjoint documents. We introduce the event 
notation t is noisy and t occurs for making the difference 
between the noise probability P(t is noisy1c) in a collection 
and the occurrence probability P(t occurs 1d) in a document 
more explicit, thereby keeping in mind that the noise prob-
ability corresponds to the occurrence probability of a term 
in a collection.
# Para 226 2
DefInItIOn 1. The frequency-based term noise prob-
ability:
# Para 228 1
Pfr q(t is noisy1c) := n(t)
# Para 229 1
N
# Para 230 2
DefInItIOn 2. The total term noise probability for 
disjoint documents:
# Para 232 1
Pdi9(t is noisy1c) := E P(t occurs1d) • P(d1c)
# Para 233 1
d
# Para 234 2
Now, we can formulate a theorem that makes assumptions 
explicit that explain the classical idf.
# Para 236 6
TheOrem 1. IDF assumptions: If the occurrence prob-
ability P(t1d) of term t over documents d is binary, and 
the containment probability P(d1c) of documents d is con-
stant, and document containments are disjoint events, then 
the noise probability for disjoint documents is equal to the 
frequency-based noise probability.
# Para 242 1
Pdi9(t is noisy1c) = Pfr q(t is noisy1c)
# Para 243 1
PrOOf. The assumptions are:
# Para 244 2
Vd : (P(t occurs1d) = 1 V P(t occurs1d) = 0) n 
P(d1c) = N n
# Para 246 1
E P(d1c) = 1.0
# Para 247 1
d 
# Para 248 17
the containment for small documents tends to be smaller 
than for large documents. From that point of view, idf 
means that P(t n d1c) is constant for all d in which t occurs, 
and P(t n d1c) is zero otherwise. The occurrence and con-
tainment can be term specific. For example, set P(t nd1c) = 
1/ND(c) if t occurs in d, where ND(c) is the number of doc-
uments in collection c (we used before just N). We choose a 
document-dependent occurrence P(t1d) := 1/NT(d), i. e. the 
occurrence probability is equal to the inverse of NT (d), which 
is the total number of terms in document d. Next, we choose 
the containment P(d1c) := NT(d)/NT(c)•NT(c)/ND(c) where 
NT(d)/NT(c) is a document length normalisation (number 
of terms in document d divided by the number of terms in 
collection c), and NT (c)/ND (c) is a constant factor of the 
collection (number of terms in collection c divided by the 
number of documents in collection c). We obtain P(tnd1c) = 
1/ND (c).
# Para 265 8
In a tf-idf-retrieval function, the tf-component reflects 
the occurrence probability of a term in a document. This is 
a further explanation why we can estimate the idf with a 
simple P(t1d), since the combined tf-idf contains the occur-
rence probability. The containment probability corresponds 
to a document normalisation (document length normalisa-
tion, pivoted document length) and is normally attached to 
the tf-component or the tf-idf-product.
# Para 273 9
The disjointness assumption is typical for frequency-based 
probabilities. From a probability theory point of view, we 
can consider documents as disjoint events, in order to achieve 
a sound theoretical model for explaining the classical idf. 
But does disjointness reflect the real world where the con-
tainment of a document appears to be independent of the 
containment of another document? In the next section, we 
replace the disjointness assumption by the independence as-
sumption.
# Para 282 2
3.2 The upper bound of the noise probability 
for independent documents
# Para 284 3
For independent documents, we compute the probability 
of a disjunction as usual, namely as the complement of the 
probability of the conjunction of the negated events:
# Para 287 1
P(di V ... V dN) = 1 — P(-di n ... n �dN)
# Para 288 1
(1 — P(d))
# Para 289 1
= 1—rl
# Para 290 1
d
# Para 291 2
1 
N
# Para 293 1
=
# Para 294 1
n(t)
# Para 295 1
N
# Para 296 1
= Pfr q(t is noisy1c)
# Para 297 1
We obtain:
# Para 298 1
Pdi9(t is noisy1c) = E
# Para 299 1
dt∈d
# Para 300 2
The noise probability can be considered as the conjunction 
of the term occurrence and the document containment.
# Para 302 9
The above result is not a surprise but it is a mathemati-
cal formulation of assumptions that can be used to explain 
the classical idf. The assumptions make explicit that the 
different types of term occurrence in documents (frequency 
of a term, importance of a term, position of a term, doc-
ument part where the term occurs, etc.) and the different 
types of document containment (size, quality, age, etc.) are 
ignored, and document containments are considered as dis-
joint events.
# Para 311 6
From the assumptions, we can conclude that idf (frequency- 
based noise, respectively) is a relatively simple but strict 
estimate. Still, idf works well. This could be explained 
by a leverage effect that justifies the binary occurrence and 
constant containment: The term occurrence for small docu-
ments tends to be larger than for large documents, whereas
# Para 317 1
P(t is noisy1c) := P(t occurs n (dl V ... V dN)1c)
# Para 318 3
For disjoint documents, this view of the noise probability 
led to definition 2. For independent documents, we use now 
the conjunction of negated events.
# Para 321 2
DefInItIOn 3. The term noise probability for inde-
pendent documents:
# Para 323 1
Pin(t is noisy1c) := rl (1 — P(t occurs1d) • P(d1c))
# Para 324 1
d
# Para 325 3
With binary occurrence and a constant containment P(d1c) := 
λ/N, we obtain the term noise of a term t that occurs in n(t) 
documents:
# Para 328 1
Pin(t is noisy1c) = 1 — (1 — λN1n(t)
# Para 329 1
229
# Para 330 6
For binary occurrence and disjoint documents, the contain-
ment probability was 1/N. Now, with independent docu-
ments, we can use λ as a collection parameter that controls 
the average containment probability. We show through the 
next theorem that the upper bound of the noise probability 
depends on λ.
# Para 336 5
TheOrem 2. The upper bound of being noisy: If the 
occurrence P(t|d) is binary, and the containment P(d|c) 
is constant, and document containments are independent 
events, then 1 − e−λ is the upper bound of the noise proba-
bility.
# Para 341 1
∀t : Pin (t is noisy|c) &lt; 1 − e−λ
# Para 342 5
PrOOf. The upper bound of the independent noise prob-
ability follows from the limit limN→∞(1 + xN)N = ex (see 
any comprehensive math book, for example, [5], for the con-
vergence equation of the Euler function). With x = −λ, we 
obtain:
# Para 347 1
N
# Para 348 1
lim C1 − λN) = e−λ
# Para 349 1
N→
# Para 350 1
For the term noise, we have:
# Para 351 1
Pin(t is noisy|c) = 1 − C1 − λN)n(t)
# Para 352 7
Pin (t is noisy |c) is strictly monotonous: The noise of a term 
tn is less than the noise of a term tn+1, where tn occurs in 
n documents and tn+1 occurs in n + 1 documents. There-
fore, a term with n = N has the largest noise probability. 
For a collection with infinite many documents, the upper 
bound of the noise probability for terms tN that occur in all 
documents becomes:
# Para 359 1
1−C1−λN)N
# Para 360 1
= 1−e−λ
# Para 361 5
By applying an independence rather a disjointness assump-
tion, we obtain the probability e−1 that a term is not noisy 
even if the term does occur in all documents. In the disjoint 
case, the noise probability is one for a term that occurs in 
all documents.
# Para 366 15
If we view P(d|c) := λ/N as the average containment, 
then λ is large for a term that occurs mostly in large docu-
ments, and λ is small for a term that occurs mostly in small 
documents. Thus, the noise of a term t is large if t occurs in 
n(t) large documents and the noise is smaller if t occurs in 
small documents. Alternatively, we can assume a constant 
containment and a term-dependent occurrence. If we as-
sume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as 
the average probability that t represents a document. The 
common assumption is that the average containment or oc-
currence probability is proportional to n(t). However, here 
is additional potential: The statistical laws (see [3] on Luhn 
and Zipf) indicate that the average probability could follow 
a normal distribution, i. e. small probabilities for small n(t) 
and large n(t), and larger probabilities for medium n(t).
# Para 381 4
For the monotonous case we investigate here, the noise of 
a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and 
the noise of a term with n(t) = N is close to 1− e−λ. In the 
next section, we relate the value e−λ to information theory.
# Para 385 2
3.3 The probability of a maximal informative 
signal
# Para 387 4
The probability e−1 is special in the sense that a signal 
with that probability is a signal with maximal information as 
derived from the entropy definition. Consider the definition 
of the entropy contribution H(t) of a signal t.
# Para 391 1
H(t) := P(t) · − ln P(t)
# Para 392 1
We form the first derivation for computing the optimum.
# Para 393 1
− ln P(t) + P(t) · P(t)
# Para 394 2
= −(1+lnP(t)) 
For obtaining optima, we use:
# Para 396 1
0 = −(1 + ln P(t))
# Para 397 3
The entropy contribution H(t) is maximal for P(t) = e−1. 
This result does not depend on the base of the logarithm as 
we see next:
# Para 400 1
−1 
# Para 401 1
P(t) · ln b ·P(t)
# Para 402 1
1	1 + ln P(t) 
# Para 403 1
ln b + log P(t)	ln b	)
# Para 404 3
We summarise this result in the following theorem: 
TheOrem 3. The probability of a maximal informa-
tive signal: The probability Pmax = e−1 ≈ 0.37 is the prob-
# Para 407 2
ability of a maximal informative signal. The entropy of a 
maximal informative signal is Hmax = e−1.
# Para 409 2
PrOOf. The probability and entropy follow from the deriva-
tion above.
# Para 411 6
The complement of the maximal noise probability is e−λ 
and we are looking now for a generalisation of the entropy 
definition such that e−λ is the probability of a maximal in-
formative signal. We can generalise the entropy definition 
by computing the integral of λ+ln P(t), i. e. this derivation 
is zero for e−λ. We obtain a generalised entropy:
# Para 417 1
J −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t))
# Para 418 6
The generalised entropy corresponds for λ = 1 to the classi-
cal entropy. By moving from disjoint to independent docu-
ments, we have established a link between the complement 
of the noise probability of a term that occurs in all docu-
ments and information theory. Next, we link independent 
documents to probability theory.
# Para 424 1
4. THE LINK TO PROBABILITY THEORY
# Para 425 3
We review for independent documents three concepts of 
probability theory: possible worlds, binomial distribution 
and Poisson distribution.
# Para 428 1
4.1 Possible Worlds
# Para 429 5
Each conjunction of document events (for each document, 
we consider two document events: the document can be 
true or false) is associated with a so-called possible world. 
For example, consider the eight possible worlds for three 
documents (N = 3).
# Para 434 1
Pin (tN is noisy) = lim
# Para 435 1
N→∞
# Para 436 1
lim
# Para 437 1
N→∞
# Para 438 2
∂H(t) 
∂P(t)
# Para 440 2
∂H(t)  
∂P(t)
# Para 442 1
= − logb P(t)+
# Para 443 1
= −C
# Para 444 1
230
# Para 445 9
world w	conjunction
w7	d1 ∧ d2 ∧ d3
ws	d1 ∧ d2 ∧ ¬d3
w5	d1 ∧ ¬d2 ∧ d3
w4	d1 ∧ ¬d2 ∧ ¬d3
w3	¬d1 ∧ d2 ∧ d3
w2	¬d1 ∧ d2 ∧ ¬d3
w1	¬d1 ∧ ¬d2 ∧ d3
w0	¬d1 ∧ ¬d2 ∧ ¬d3
# Para 454 3
With each world w, we associate a probability µ(w), which 
is equal to the product of the single probabilities of the doc-
ument events.
# Para 457 13
world w	µ(w)
	probability
w7	(λ·N)0
	(N)3
ws	·	(1 − N)1
	rrN)2	
w5	\N)2·	(1 − N)1
w4	(N)1 ·	2
		(1 − N)
w3	(N)2·	(1 − N)1
w2	(λ	(1 − N)2
w1	(N)1 ·	
w0	(λ	N)3
# Para 470 5
The sum over the possible worlds in which k documents are 
true and N−k documents are false is equal to the probabil-
ity function of the binomial distribution, since the binomial 
coefficient yields the number of possible worlds in which k 
documents are true.
# Para 475 1
4.2 Binomial distribution
# Para 476 3
The binomial probability function yields the probability 
that k of N events are true where each event is true with 
the single event probability p.
# Para 479 1
P(k) := binom(N, k, p) := (N k / pk (1 − p)N− k
# Para 480 5
The single event probability is usually defined as p := λ/N, 
i. e. p is inversely proportional to N, the total number of 
events. With this definition of p, we obtain for an infinite 
number of documents the following limit for the product of 
the binomial coefficient and p k :
# Para 485 2
lim N k 
N→∞k p
# Para 487 1
N · (N−1) · ... · (N−k +1)
# Para 488 1
k!
# Para 489 2
The limit is close to the actual value for k &lt;&lt; N. For large 
k, the actual value is smaller than the limit.
# Para 491 2
The limit of (1−p)N−k follows from the limit limN→∞(1+ W= 
ex.
# Para 493 1
N−k
# Para 494 1
lim (1 −p)N −k = lim (1 − λN)
# Para 495 1
N→∞	N→
# Para 496 2
Again, the limit is close to the actual value for k &lt;&lt; N. For 
large k, the actual value is larger than the limit.
# Para 498 1
4.3 Poisson distribution
# Para 499 2
For an infinite number of events, the Poisson probability 
function is the limit of the binomial probability function.
# Para 501 1
Ak
# Para 502 2
binom(N, k, p) = k! · e−λ 
P(k) = poisson(k, λ) := k! · e−λ
# Para 504 4
The probability poisson(0, 1) is equal to e−1, which is the 
probability of a maximal informative signal. This shows 
the relationship of the Poisson distribution and information 
theory.
# Para 508 4
After seeing the convergence of the binomial distribution, 
we can choose the Poisson distribution as an approximation 
of the independent term noise probability. First, we define 
the Poisson noise probability:
# Para 512 2
DefInItIOn 4. The Poisson term noise probability: 
P,oi(t is noisy|c) := e−λ ·
# Para 514 5
For independent documents, the Poisson distribution ap-
proximates the probability of the disjunction for large n(t), 
since the independent term noise probability is equal to the 
sum over the binomial probabilities where at least one of 
n(t) document containment events is true.
# Para 519 1
Pin (t is noisy | c) = n(t) (n(t)1 pk (1 − p)N−k
# Para 520 1
1. k J
# Para 521 1
1
# Para 522 1
Pin (t is noisy | c) ≈ P,oi (t is noisy | c)
# Para 523 7
We have defined a frequency-based and a Poisson-based prob-
ability of being noisy, where the latter is the limit of the 
independence-based probability of being noisy. Before we 
present in the final section the usage of the noise proba-
bility for defining the probability of being informative, we 
emphasise in the next section that the results apply to the 
collection space as well as to the the document space.
# Para 530 1
k
# Para 531 1
 N) = e—λ
# Para 532 2
5. THE COLLECTION SPACE AND THE 
DOCUMENT SPACE
# Para 534 9
Consider the dual definitions of retrieval parameters in 
table 1. We associate a collection space D × T with a col-
lection c where D is the set of documents and T is the set 
of terms in the collection. Let ND := |D| and NT := |T| 
be the number of documents and terms, respectively. We 
consider a document as a subset of T and a term as a subset 
of D. Let nT(d) := |{t|d ∈ t}| be the number of terms that 
occur in the document d, and let nD(t) := | {d|t ∈ d}| be the 
number of documents that contain the term t.
# Para 543 7
In a dual way, we associate a document space L × T with 
a document d where L is the set of locations (also referred 
to as positions, however, we use the letters L and l and not 
P and p for avoiding confusion with probabilities) and T is 
the set of terms in the document. The document dimension 
in a collection space corresponds to the location (position) 
dimension in a document space.
# Para 550 4
The definition makes explicit that the classical notion of 
term frequency of a term in a document (also referred to as 
the within-document term frequency) actually corresponds 
to the location frequency of a term in a document. For the
# Para 554 1
lime
# Para 555 1
—
# Para 556 1
λ(1−
# Para 557 1
N→
# Para 558 1
=lim
# Para 559 1
N→∞
# Para 560 1
(λ)k = λk
# Para 561 1
N	k!
# Para 562 1
lim
# Para 563 1
N→∞
# Para 564 1
λk
# Para 565 1
k!
# Para 566 1
n(t)�
# Para 567 1
k=1
# Para 568 1
231
# Para 569 9
space	collection	document
dimensions	documents and terms	locations and terms
document/location frequency	nD(t, c): Number of documents in which term t occurs in collection c	nL(t, d): Number of locations (positions) at which term t occurs in document d
	ND(c): Number of documents in collection c	NL(d): Number of locations (positions) in docu-ment d
term frequency	nT(d,c): Number of terms that document d con- tains in collection c	nT(l,d): Number of terms that location l contains in document d
	NT(c): Number of terms in collection c	NT(d): Number of terms in document d
noise/occurrence containment	P(t1c) (term noise) P(d1c) (document)	P(t1d) (term occurrence) P(l1d) (location)
informativeness conciseness	— ln P(t1c) — ln P(d1c)	— ln P(t1d) — ln P(l 1d)
P(informative) P(concise)	ln(P(t1c))/ ln(P(tm in, c)) ln(P(d1c))/ ln(P(dmin1c))	ln(P(t1d))/ ln(P(tm in, d)) ln(P(l1d))/ln(P(lm in1d))
# Para 578 1
Table 1: Retrieval parameters
# Para 579 3
actual term frequency value, it is common to use the max-
imal occurrence (number of locations; let lf be the location 
frequency).
# Para 582 2
tf(t, d) := lf(t, d) :=  Pf, , (t occurs 1d) =  nL (t, d) 
Pf,,(t. occurs1d) nL(t,, ,d)
# Para 584 4
A further duality is between informativeness and concise-
ness (shortness of documents or locations): informativeness 
is based on occurrence (noise), conciseness is based on con-
tainment.
# Para 588 9
We have highlighted in this section the duality between 
the collection space and the document space. We concen-
trate in this paper on the probability of a term to be noisy 
and informative. Those probabilities are defined in the col-
lection space. However, the results regarding the term noise 
and informativeness apply to their dual counterparts: term 
occurrence and informativeness in a document. Also, the 
results can be applied to containment of documents and lo-
cations.
# Para 597 2
6. THE PROBABILITY OF BEING INFOR-
MATIVE
# Para 599 6
We showed in the previous sections that the disjointness 
assumption leads to frequency-based probabilities and that 
the independence assumption leads to Poisson probabilities. 
In this section, we formulate a frequency-based definition 
and a Poisson-based definition of the probability of being 
informative and then we compare the two definitions.
# Para 605 2
DefInItIOn 5. The frequency-based probability of be-
ing informative:
# Para 607 3
We define the Poisson-based probability of being informa-
tive analogously to the frequency-based probability of being 
informative (see definition 5).
# Para 610 1
DefInItIOn 6. The Poisson-based probability of be-
# Para 611 1
ing informative:
# Para 612 4
For λ &gt;&gt; 1, we can alter the noise and informativeness Pois-
son by starting the sum from 0, since eλ &gt;&gt; 1. Then, the 
minimal Poisson informativeness is poisson(0, λ) = e−λ. We 
obtain a simplified Poisson probability of being informative:
# Para 616 1
λ — ln En (to \k 
# Para 617 1
Ppoj(t is informative1c) �
# Para 618 3
ln En(t) λk 
k=0 k!  
λ
# Para 621 5
The computation of the Poisson sum requires an optimi-
sation for large n(t). The implementation for this paper 
exploits the nature of the Poisson density: The Poisson den-
sity yields only values significantly greater than zero in an 
interval around λ.
# Para 626 15
Consider the illustration of the noise and informative-
ness definitions in figure 1. The probability functions dis-
played are summarised in figure 2 where the simplified Pois-
son is used in the noise and informativeness graphs. The 
frequency-based noise corresponds to the linear solid curve 
in the noise figure. With an independence assumption, we 
obtain the curve in the lower triangle of the noise figure. By 
changing the parameter p := λ/N of the independence prob-
ability, we can lift or lower the independence curve. The 
noise figure shows the lifting for the value λ := ln N � 
9.2. The setting λ = ln N is special in the sense that the 
frequency-based and the Poisson-based informativeness have 
the same denominator, namely ln N, and the Poisson sum 
converges to λ. Whether we can draw more conclusions from 
this setting is an open question.
# Para 641 2
We can conclude, that the lifting is desirable if we know 
for a collection that terms that occur in relatively few doc-
# Para 643 1
— ln (e−λ	n(t) λk I
# Para 644 1
Ek=1 k!
# Para 645 1
— ln(e−λ • λ)
# Para 646 1
λ —ln Ek=1 kk 
# Para 647 1
=
# Para 648 1
λ — lnλ
# Para 649 1
For the sum expression, the following limit holds:
# Para 650 2
lim	n(t)�	λk	=eλ—1
n(t)→∞	k=1	k!	
# Para 652 1
Ppoj(t is informative1c) :=
# Para 653 1
— ln n(t)
# Para 654 1
N 
# Para 655 1
—ln 1
# Para 656 1
N
# Para 657 1
— logN	N)= 1 — logN n(t) = 1 — ln In N
# Para 658 1
N	N
# Para 659 1
Pf, ,(t is informative1c) :=
# Para 660 1
λ
# Para 661 1
= 1
# Para 662 1
232
# Para 663 1
0.8
# Para 664 1
0.6
# Para 665 1
0.4
# Para 666 1
0.2
# Para 667 1
0
# Para 668 1
1
# Para 669 6
frequency
independence: 1/N
independence: ln(N)/N
poisson: 1000
poisson: 2000
poisson: 1 000,2000
# Para 675 1
frequency
# Para 676 2
independence: 1/N 
0.8	independence: ln(N)/N
# Para 678 1
poisson: 1000
# Para 679 2
poisson: 2000 
poisson: 1000,2000
# Para 681 1
0.6
# Para 682 1
0.4
# Para 683 1
0.2
# Para 684 1
0
# Para 685 1
1
# Para 686 1
0	2000 4000 6000 8000 10000
# Para 687 1
n(t): Number of documents with term t 
# Para 688 1
0	2000 4000 6000 8000 10000
# Para 689 1
n(t): Number of documents with term t
# Para 690 1
Figure 1: Noise and Informativeness
# Para 691 10
Probability function		Noise	Informativeness
Frequency PfreQ	Def Interval	n(t)/N	ln(n(t)/N)/ln(1/N)
		1/N &lt; PfreQ &lt; 1.0	0.0 &lt; PfreQ &lt; 1.0
Independence Pin	Def Interval	1 — (1 — p)n (t)	ln(1 — (1 — p)n(t))/ ln(p)
		p &lt; Pin &lt; 1 — e—λ	ln(p) &lt; Pin &lt; 1.0
Poisson Ppoi	Def Interval Def	e—λEn(t) λk	(λ — ln En(t) \k )/(λ — ln λ)
Poisson Ppoi simplified	Interval	k=1 k!	k=1
		e—λ • λ &lt; Ppoi &lt; 1 — e—λ	(λ — ln(eλ — 1))/(λ — ln λ) &lt; Ppoi &lt; 1.0
		e—λ Ek=0 kk	(λ — ln Ek=0 kk )/λ
		e—λ &lt; Ppoi &lt; 1.0	0.0 &lt; Ppoi &lt; 1.0
# Para 701 1
Figure 2: Probability functions
# Para 702 5
uments are no guarantee for finding relevant documents, 
i. e. we assume that rare terms are still relatively noisy. On 
the opposite, we could lower the curve when assuming that 
frequent terms are not too noisy, i. e. they are considered as 
being still significantly discriminative.
# Para 707 6
The Poisson probabilities approximate the independence 
probabilities for large n(t); the approximation is better for 
larger λ. For n(t) &lt; λ, the noise is zero whereas for n(t) &gt; λ 
the noise is one. This radical behaviour can be smoothened 
by using a multi-dimensional Poisson distribution. Figure 1 
shows a Poisson noise based on a two-dimensional Poisson:
# Para 713 1
λkλk
# Para 714 1
poisson(k, λ1, λ2) := π • e—λ1•k� +(1—π)•e- λ2 • 2
# Para 715 1
k!
# Para 716 6
The two dimensional Poisson shows a plateau between λ1 = 
1000 and λ2 = 2000, we used here π = 0.5. The idea be-
hind this setting is that terms that occur in less than 1000 
documents are considered to be not noisy (i.e. they are in-
formative), that terms between 1000 and 2000 are half noisy, 
and that terms with more than 2000 are definitely noisy.
# Para 722 10
For the informativeness, we observe that the radical be-
haviour of Poisson is preserved. The plateau here is ap-
proximately at 1/6, and it is important to realise that this 
plateau is not obtained with the multi-dimensional Poisson 
noise using π = 0.5. The logarithm of the noise is nor-
malised by the logarithm of a very small number, namely 
0.5 • e—1000 + 0.5 • e—2000. That is why the informativeness 
will be only close to one for very little noise, whereas for a 
bit of noise, informativeness will drop to zero. This effect 
can be controlled by using small values for π such that the 
# Para 732 4
noise in the interval [λ1; λ2] is still very little. The setting 
π = e—2000/6 leads to noise values of approximately e —2000/6 
in the interval [λ1; λ2], the logarithms lead then to 1/6 for 
the informativeness.
# Para 736 6
The indepence-based and frequency-based informativeness 
functions do not differ as much as the noise functions do. 
However, for the indepence-based probability of being infor-
mative, we can control the average informativeness by the 
definition p := λ/N whereas the control on the frequency- 
based is limited as we address next.
# Para 742 7
For the frequency-based idf, the gradient is monotonously 
decreasing and we obtain for different collections the same 
distances of idf-values, i. e. the parameter N does not affect 
the distance. For an illustration, consider the distance be-
tween the value idf(tn+1) of a term tn+1 that occurs in n+1 
documents, and the value idf(tn) of a term tn that occurs in 
n documents.
# Para 749 1
idf(tn+1) — idf(tn) = ln  n
# Para 750 1
n+ 1
# Para 751 1
The first three values of the distance function are:
# Para 752 3
idf(t2) — idf(t1) = ln(1/(1	+ 1))	=	0.69
idf(t3) — idf(t2) = ln(1/(2	+ 1))	=	0.41
idf(t4) — idf(t3) = ln(1/(3	+ 1))	=	0.29
# Para 755 3
For the Poisson-based informativeness, the gradient decreases 
first slowly for small n(t), then rapidly near n(t) R� λ and 
then it grows again slowly for large n(t).
# Para 758 2
In conclusion, we have seen that the Poisson-based defini-
tion provides more control and parameter possibilities than
# Para 760 1
233
# Para 761 18
the frequency-based definition does. Whereas more control 
and parameter promises to be positive for the personalisa-
tion of retrieval systems, it bears at the same time the dan-
ger of just too many parameters. The framework presented 
in this paper raises the awareness about the probabilistic 
and information-theoretic meanings of the parameters. The 
parallel definitions of the frequency-based probability and 
the Poisson-based probability of being informative made 
the underlying assumptions explicit. The frequency-based 
probability can be explained by binary occurrence, constant 
containment and disjointness of documents. Independence 
of documents leads to Poisson, where we have to be aware 
that Poisson approximates the probability of a disjunction 
for a large number of events, but not for a small number. 
This theoretical result explains why experimental investiga-
tions on Poisson (see [7]) show that a Poisson estimation 
does work better for frequent (bad, noisy) terms than for 
rare (good, informative) terms.
# Para 779 8
In addition to the collection-wide parameter setting, the 
framework presented here allows for document-dependent 
settings, as explained for the independence probability. This 
is in particular interesting for heterogeneous and structured 
collections, since documents are different in nature (size, 
quality, root document, sub document), and therefore, bi-
nary occurrence and constant containment are less appro-
priate than in relatively homogeneous collections.
# Para 787 1
7. SUMMARY
# Para 788 18
The definition of the probability of being informative trans-
forms the informative interpretation of the idf into a proba-
bilistic interpretation, and we can use the idf -based proba-
bility in probabilistic retrieval approaches. We showed that 
the classical definition of the noise (document frequency) in 
the inverse document frequency can be explained by three 
assumptions: the term within-document occurrence prob-
ability is binary, the document containment probability is 
constant, and the document containment events are disjoint. 
By explicitly and mathematically formulating the assump-
tions, we showed that the classical definition of idf does not 
take into account parameters such as the different nature 
(size, quality, structure, etc.) of documents in a collection, 
or the different nature of terms (coverage, importance, po-
sition, etc.) in a document. We discussed that the absence 
of those parameters is compensated by a leverage effect of 
the within-document term occurrence probability and the 
document containment probability.
# Para 806 12
By applying an independence rather a disjointness as-
sumption for the document containment, we could estab-
lish a link between the noise probability (term occurrence 
in a collection), information theory and Poisson. From the 
frequency-based and the Poisson-based probabilities of be-
ing noisy, we derived the frequency-based and Poisson-based 
probabilities of being informative. The frequency-based prob-
ability is relatively smooth whereas the Poisson probability 
is radical in distinguishing between noisy or not noisy, and 
informative or not informative, respectively. We showed how 
to smoothen the radical behaviour of Poisson with a multi-
dimensional Poisson.
# Para 818 5
The explicit and mathematical formulation of idf- and 
Poisson-assumptions is the main result of this paper. Also, 
the paper emphasises the duality of idf and tf, collection 
space and document space, respectively. Thus, the result 
applies to term occurrence and document containment in a 
# Para 823 9
collection, and it applies to term occurrence and position 
containment in a document. This theoretical framework is 
useful for understanding and deciding the parameter estima-
tion and combination in probabilistic retrieval models. The 
links between indepence-based noise as document frequency, 
probabilistic interpretation of idf, information theory and 
Poisson described in this paper may lead to variable proba-
bilistic idf and tf definitions and combinations as required 
in advanced and personalised information retrieval systems.
# Para 832 7
Acknowledgment: I would like to thank Mounia Lalmas, 
Gabriella Kazai and Theodora Tsikrika for their comments 
on the as they said “heavy” pieces. My thanks also go to the 
meta-reviewer who advised me to improve the presentation 
to make it less “formidable” and more accessible for those 
“without a theoretic bent”. This work was funded by a 
research fellowship from Queen Mary University of London.
# Para 839 1
8. REFERENCES
# Para 840 3
[1] A. Aizawa. An information-theoretic perspective of 
tf-idf measures. Information Processing and 
Management, 39:45–65, January 2003.
# Para 843 4
[2] G. Amati and C. J. Rijsbergen. Term frequency 
normalization via Pareto distributions. In 24th 
BCS-IRSG European Colloquium on IR Research, 
Glasgow, Scotland, 2002.
# Para 847 2
[3] R. K. Belew. Finding out about. Cambridge University 
Press, 2000.
# Para 849 3
[4] A. Bookstein and D. Swanson. Probabilistic models 
for automatic indexing. Journal of the American 
Society for Information Science, 25:312–318, 1974.
# Para 852 2
[5] I. N. Bronstein. Taschenbuch der Mathematik. Harri 
Deutsch, Thun, Frankfurt am Main, 1987.
# Para 854 2
[6] K. Church and W. Gale. Poisson mixtures. Natural 
Language Engineering, 1(2):163–190, 1995.
# Para 856 4
[7] K. W. Church and W. A. Gale. Inverse document 
frequency: A measure of deviations from poisson. In 
Third Workshop on Very Large Corpora, ACL 
Anthology, 1995.
# Para 860 4
[8] T. Lafouge and C. Michel. Links between information 
construction and information gain: Entropy and 
bibliometric distribution. Journal of Information 
Science, 27(1):39–49, 2001.
# Para 864 4
[9] E. Margulis. N-poisson document modelling. In 
Proceedings of the 15th Annual International ACM 
SIGIR Conference on Research and Development in 
Information Retrieval, pages 177–189, 1992.
# Para 868 2
[10] S. E. Robertson and S. Walker. Some simple effective 
approximations to the 2-poisson model for
# Para 870 4
probabilistic weighted retrieval. In Proceedings of the 
17th Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, 
pages 232–241, London, et al., 1994. Springer-Verlag.
# Para 874 3
[11] S. Wong and Y. Yao. An information-theoric measure 
of term specificity. Journal of the American Society 
for Information Science, 43(1):54–61, 1992.
# Para 877 4
[12] S. Wong and Y. Yao. On modeling information 
retrieval with probabilistic inference. ACM 
Transactions on Information Systems, 13(1):38–68, 
1995.
# Para 881 1
234
