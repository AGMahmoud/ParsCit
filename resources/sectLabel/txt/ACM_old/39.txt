Automated Rich Presentation of a Semantic Topic
Lie Lu and Zhiwei Li
Microsoft Research Asia
{llu, zli}@microsoft.com
ABSTRACT
To have a rich presentation of a topic, it is not only expected that +L+ many relevant multimodal information, including images, text, +L+ audio and video, could be extracted; it is also important to +L+ organize and summarize the related information, and provide +L+ users a concise and informative storyboard about the target topic. +L+ It facilitates users to quickly grasp and better understand the +L+ content of a topic. In this paper, we present a novel approach to +L+ automatically generating a rich presentation of a given semantic +L+ topic. In our proposed approach, the related multimodal informa- +L+ tion of a given topic is first extracted from available multimedia +L+ databases or websites. Since each topic usually contains multiple +L+ events, a text-based event clustering algorithm is then performed +L+ with a generative model. Other media information, such as the +L+ representative images, possibly available video clips and flashes +L+ (interactive animates), are associated with each related event. A +L+ storyboard of the target topic is thus generated by integrating each +L+ event and its corresponding multimodal information. Finally, to +L+ make the storyboard more expressive and attractive, an incidental +L+ music is chosen as background and is aligned with the storyboard. +L+ A user study indicates that the presented system works quite well +L+ on our testing examples.
Categories and Subject Descriptors
H.5.3 [Information Interfaces and Presentation]: Group and +L+ Organization Interfaces - Organizational design; H.3.1 [Informa- +L+ tion Storage and Retrieval]: Content Analysis and Indexing - +L+ abstracting methods.
General Terms
Algorithms, Design, Management, Experimentation, Theory
Keywords
Rich presentation, multimodality, multimedia authoring, story- +L+ board, events clustering, multimedia fusion
1. INTRODUCTION
In the multimedia field, a major objective of content analysis is to +L+ discover the high-level semantics and structures from the low- +L+ level features, and thus to facilitate indexing, browsing, searching, +L+ and managing the multimedia database. In recent years, a lot of
Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that +L+ copies bear this notice and the full citation on the first page. To copy +L+ otherwise, or republish, to post on servers or to redistribute to lists, +L+ requires prior specific permission and/or a fee.
MM’05, November 6–11, 2005, Singapore.
Copyright 2005 ACM 1-59593-044-2/05/0011...$5.00.
technologies have been developed for various media types, +L+ including images, video, audio and etc. For example, various +L+ approaches and systems have been proposed in image content +L+ analysis, such as semantic classification [1], content-based image +L+ retrieval [2] and photo album management [3]. There are also a lot +L+ of research focuses on video analysis, such as video segmentation +L+ [4], highlight detection [5], video summarization [6][7], and video +L+ structure analysis [8], applied in various data including news +L+ video, movie and sports video. Since audio information is very +L+ helpful for video analysis, many research works on audio are also +L+ developed to enhance multimedia analysis, such as audio +L+ classification [9], and audio effect detection in different audio +L+ streams [10]. Most recently, there are more and more approaches +L+ and systems integrating multimodal information in order to +L+ improve analysis performance [11][12].
The main efforts of the above mentioned research have focused on +L+ understanding the semantics (including a topic, an event or the +L+ similarity) from the multimodal information. That is, after the +L+ multimedia data is given, we want to detect the semantics implied +L+ in these data. In this paper, we propose a new task, Rich +L+ Presentation, which is an inverse problem of the traditional +L+ multimedia content analysis. That is, if we have a semantic topic, +L+ how can we integrate its relevant multimodal information, +L+ including image, text, audio and video, to richly present the target +L+ topic and to provide users a concise and informative storyboard? +L+ In this paper, the so-called “semantic topic” is a generic concept. +L+ It could be any keyword representing an event or events, a +L+ person’s name, or anything else. For example, “World Cup 2002” +L+ and “US election” could be topics, as well as “Halloween” and +L+ “Harry Potter”. In this paper, our task is to find sufficient +L+ information on these topics, extract the key points, fuse the +L+ information from different modalities, and then generate an +L+ expressive storyboard.
Rich presentation can be very helpful to facilitate quickly +L+ grasping and better understanding the corresponding topic. +L+ People usually search information from (multimedia) database or +L+ the Internet. However, what they get is usually a bulk of +L+ unorganized information, with many duplicates and noise. It is +L+ tedious and costs a long time to get what they want by browsing +L+ the search results. If there is a tool to help summarize and +L+ integrate the multimodal information, and then produce a concise +L+ and informative storyboard, it will enable users to quickly figure +L+ out the overview contents of a topic that they want to understand. +L+ Rich presentation provides such a tool, and thus it could have +L+ many potential applications, such as education and learning, +L+ multimedia authoring, multimedia retrieval, documentary movie +L+ production, and information personalization.
In this paper, we will present the approach to rich presentation. In +L+ order to produce a concise and informative storyboard to richly +L+ present a target topic, we need to answer the following questions. +L+ 1) How to extract the relevant information regarding the target
745
topic? 2) How to extract the key points from the relevant +L+ information and build a concise and informative storyboard? 3) +L+ How to fuse all the information from different modality? and 4) +L+ how to design the corresponding rendering interface?
A Target Topic
Rich Presentation
Fig. 1 The system framework of rich presentation of a target +L+ semantic topic. It is mainly composed of three steps, relevant +L+ multimodal information extraction, media analysis, and rich +L+ presentation generation.
In this paper, we propose a number of novel approaches to deal +L+ with the above issues and also present an example system. Fig. 1 +L+ illustrates the proposed system framework of rich presentation. It +L+ is mainly composed of three steps, relevant multimodal informa- +L+ tion extraction, media analysis including multiple events cluster- +L+ ing, representative media detection and music rhythm analysis; +L+ and the final storyboard generation and music synchronization.
In the proposed system, given the semantic topic, the relevant +L+ information, including text, image, video and music, is first +L+ extracted from the available multimedia database or the web data- +L+ base. User interaction is also allowed to provide extra relevant +L+ material or give relevant feedback. Then, the information is +L+ summarized, with an event clustering algorithm, to give a concise +L+ representation of the topic and figure out the overview of the +L+ contents. Other multimedia materials, such as representative +L+ images (or image sequences) and geographic information, are +L+ subsequently associated with each event. In the next step, all the +L+ above information is integrated to generate a storyboard, in which +L+ each event is presented as one or multiple slides. An incidental +L+ music, which is also possibly relevant to the topic, is finally +L+ synchronized with the storyboard to improve its expressiveness +L+ and attractiveness. Thus, with these steps, a concise and +L+ informative rich presentation regarding the target topic is gener- +L+ ated.
The rest of the paper is organized as follows. Section 2 discusses +L+ the relevant information extraction corresponding to the target +L+ topic. Section 3 presents our approach to the topic representation, +L+ including multiple events clustering, event description, and +L+ representative media selection. Section 4 describes the approach +L+ to rich presentation generation, including storyboard generation, +L+ incidental music analysis and synchronization. Experiments and +L+ evaluations are presented in the Section 5. Conclusions are given +L+ in the Section 6.
2. OBTAINING RELEVANT INFORMATION
To obtain the multimodal information which is relevant to the +L+ input topic (keyword), generally, we could search them from +L+ various databases which have been indexed with the “state-of-the- +L+ art” multimedia analysis techniques. However, in current stage, +L+ there is lack of such publicly available multimedia databases. The +L+ public search engine like MSN or Google indexes all the Internet +L+ web-pages and can return a lot of relevant information, but the +L+ search results usually contain much noise. We could also build a +L+ private database for this system to provide more relevant and +L+ clean results, but it will be too much expensive to collect and +L+ annotate sufficient multimedia data for various topics. In order to +L+ obtain relatively accurate and sufficient data for an arbitrary topic, +L+ in our system, we chose to collect the relevant multimodal +L+ information of the given topic from the news websites such as +L+ MSNBC, BBC and CNN, instead of building an available +L+ database from the scratch. These news websites are usually well +L+ organized and managed; and contain various kinds of high quality +L+ information including text, image and news video clips. Although +L+ the news websites are used as the information sources in our +L+ system, other various multimedia databases can be also easily +L+ incorporated into the system if they are available.
Instead of directly submitting the topic as a query and getting the +L+ returned results by using the search function provided by the +L+ websites, in our system, we crawled the news documents from +L+ these websites in advance and then build a full-text index. It +L+ enables us to quickly obtain the relevant documents, and also en- +L+ able us to use some traditional information retrieval technologies, +L+ such as query expansion [13], to remove the query ambiguousness +L+ and get more relevant documents.
In our approach, user interaction is also allowed to provide more +L+ materials relevant to the topic, or give relevant feedback on the +L+ returned results. For example, from the above websites, we can +L+ seldom find a music clip relevant to the target topic. In this case, +L+ users could provide the system a preferred music, which will be +L+ further used as incidental music to accompany with the storyboard +L+ presentation. Users could also give some feedbacks on the +L+ obtained documents. For example, if he gives a thumb-up to a +L+ document, the relevant information of the document needs to be +L+ presented in the final storyboard. On the other side, users could +L+ also thumb-down a document to remove the related information.
3. TOPIC REPRESENTATION
A semantic topic is usually a quite broad concept and it usually +L+ contains multiple events. For example, in the topic “Harry Potter”, +L+ the publication of each book and the release of each movie could +L+ be considered as an event; while in the topic “World Cup 2002”, +L+ each match could also be taken as an event. For each event, there +L+ are usually many documents reporting it. Therefore, in order to +L+ generate an informative and expressive storyboard to present the +L+ topic, it would be better to decompose the obtained information +L+ and cluster the documents into different events.
However, event definition is usually subjective, different +L+ individuals may have different opinions. It is also confusing in +L+ which scale an event should be defined. Also take “World Cup” +L+ as an example, in a larger scale, “World Cup 2002” and “World +L+ Cup 2006” could also be considered as a big event. Therefore, +L+ due to the above vagueness, in this paper, we do not strictly define
Relevant multimodal information Retrieval
User
Interaction
Music
Text
Relevant Media
Rhythm Analysis
•	Onset/Beat Sequence
•	Strength confidence
Multiple Events Clustering
•	Event summary (4w + time)
•	Geographic information
Media Association
•	Representative images
•	Relevant video clips
Storyboard Generation
Event presentation, multimodal information fusion, layout design
Storyboard
Music and storyboard synchronization
746
each event of the target topic. Following our previous works on +L+ news event detection [14], an event is assumed as some similar +L+ information describing similar persons, similar keywords, similar +L+ places, and similar time duration. Therefore, in our system, an +L+ event is represented by four primary elements: who (persons), +L+ when (time), where (locations) and what (keywords); and event +L+ clustering is to group the documents reporting similar primary +L+ elements. As for the scale of event, in the paper, it could be +L+ adaptively determined by the time range of the obtained +L+ documents or the required event number.
In this section, we present a novel clustering approach based on a +L+ generative model proposed in [14], instead of using traditional +L+ clustering methods such as K-means. After event clusters are +L+ obtained, the corresponding event summary is then extracted and +L+ other representative media is associated with each event.
3.1 Multiple Event Clustering
To group the documents into different events, essentially, we need +L+ to calculate p(ej I xi), which represents the probability that a docu- +L+ ment xi belongs to an event ej. Here, as mentioned above, an +L+ event ej (and thus the document xi describing the event) is +L+ represented by four primary elements: who (persons), when (time), +L+ where (locations) and what (keywords). That is,
Event / Docment = {persons, locations, keywords, time}
Assuming that a document is always caused by an event [14] and +L+ the four primary elements are independent, to calculate the +L+ probability p(ej I xi), in our approach, we first determine the likeli- +L+ hood that the document xi is generated from event ej, p(xi I ej) +L+ which could be further represented by the following generative +L+ model,
p(xi | ej) =p(namei | ej)p(loci | ej)p(keyi | ej)p(timei | ej) (1)
where namei, loci, keyi, and timei are the feature vectors +L+ representing persons, locations, keywords and time in the +L+ document xi, respectively. In our approach, the above entities are +L+ extracted by the BBN NLP tools [15]. The tool can extract seven +L+ types of entities, including persons, organizations, locations, date, +L+ time, money and percent. In our approach, the obtained organiza- +L+ tion entity is also considered as a person entity; and all the words +L+ except of persons, locations, and other stop-words are taken as +L+ keywords.
In more detail, namei (similarly, loci and keyi) is a vector &lt;ci1, +L+ ci2, ..., ciNp&gt;, where cin is the occurrence frequency of the personn +L+ appears in the document xi, and personn is the nth person in the +L+ person vocabulary, which is composed of all the persons appeared +L+ in all the obtained documents (similarly, we can define keyword +L+ vocabulary and location vocabulary). Assuming Np is the size of +L+ person vocabulary, p(nameiI ej) could be further expressed by
Np(namei | ej) = n p(personn | ej )cin (2)
n=1
Since the person, location and keyword are discrete variables +L+ represented by words, and the probability of the location and +L+ keyword can be also defined similarly as that of the person in (2), +L+ in the flowing sections, we will not discriminate them and +L+ uniformly represent the probability p(personn | ej) (correspond- +L+ ingly, the p(locationn | ej) and p(keywordn | ej)) as p(wn | ej), which +L+ denotes the probability that the word wn appears in the event ej
On the other hand, the time of an event usually lasts a continuous +L+ duration. It is also observed, especially in the news domain, that +L+ the documents about an event usually increases at the beginning +L+ stage of the event and then decreases at the end. Therefore, in +L+ our approach, a Gaussian model N(uj, aj) is utilized to roughly +L+ represent the probability p(timei | ej), where uj and aj is the mean +L+ and standard deviation, respectively.
To this end, in order to estimate the probability p(ej I xi), we need +L+ to estimate the parameters 6 = {p(wn | ej), uj, σj, 1!�j5K}, assuming +L+ K is the number of events (the selection of K is discussed in +L+ section 3.2). In our approach, the Maximum Likelihood is used to +L+ estimate the model parameters, as,
θ* = argmaxθ log(p(X |θ)) =
M	K
=argmax θ ∑ log(∑ p(ej)p(xi | ej
ia	j=1
where X represents the corpus of the obtained documents; M and +L+ K are number of documents and events, respectively.
Since it is difficult to derive a close formula to estimate the +L+ parameters, in our approach, an Expectation Maximization (EM) +L+ algorithm is applied to maximize the likelihood, by running E-step +L+ and M-step iteratively. A brief summary of these two steps is +L+ listed as follows, and more details can be found in [14].
•	In E-step, the posterior probability p(ej | xi) is estimated as:
p(ej | xi)(t+1) =  p(xi | ej)(t)p(ej)(t)	(4)
p( xi
where the upper script (t) indicate the tth iteration.
•	In M-step, the model parameters are updated, as,
where tf(i,n) is the term frequency of the word wn in the +L+ document xi and N is the corresponding vocabulary size. It +L+ is noted that, in (5), the Laplace smoothing [ 16] is applied to +L+ prevent zero probability for the infrequently occurring word.
At last, the prior of each event is updated as:
M
∑p (
p(ej)(t+1) = i=1(8)
M
arg +L+ The algorithm can increase the log-likelihood consistently with +L+ the iterations; and then converge to a local maximum. Once the +L+ parameters are estimated, we can simply assign each document to +L+ an event, as following
yi =argmaxj(p(ej |xi))	(9)
where yi is the event label of the document xi.
i=1
M

t+1)
(
(
)
(t
u
i=1
ej
+1)
| xi
(6)
∑p
timei

tf (i, n)
p(wn | ej)(t+1) = 	Mi=1jN	(5)
i=1	s=1
1+N+∑(p(e
 I x)	&apos;
∑s))
tf 0
,
M
uj
σ2(t+1) =  i=1 	/7)
j	M
l
(
∑p
)
|xi
i
ej
=1
∑ p(ej | xi)(t+1) ⋅ (timei −

t+1) 2
)
(
t+1)
M
|θ))
maxθ log(∏ p(xi
(3)
,θ))

ej
| xi
) (t+1)
747
The advantage of this generative approach is that it not only +L+ considers the temporal continuity of an event, it also can deal with +L+ the issue that some events overlap in some time durations. In this +L+ case, the Gaussian model of the event time can also be overlapped +L+ through this data-driven parameter estimation. From this view, +L+ the event clustering is also like a Gaussian mixture model (GMM) +L+ estimation in the timeline.
3.2 Determining the Number of Events
In the above approach to event clustering, the event number K is +L+ assumed known (as shown in (3)-(8)). However, the event number +L+ is usually very difficult to be determined a priori. In our approach, +L+ an intuitive way is adopted to roughly estimate the event number +L+ based on the document distribution along with the timeline.
As mentioned above, it is assumed that each document is caused +L+ by an event, and the document number of an event changes with +L+ the development of the event. According to this property, each +L+ peak (or the corresponding contour) of the document distribution +L+ curve might indicate one event [14], as the Fig. 2 shows. Thus, we +L+ can roughly estimate the event number by simply counting the +L+ peak number. However, the curve is quite noisy and there +L+ inevitably exist some noisy peaks in the curve. In order to avoid +L+ the noisy peaks, in our approach, only the salient peaks are +L+ assumed to be relevant to the event number.
To detect the salient peaks, we first smooth the document curve +L+ with a half-Hamming (raised-cosine) window, and then remove +L+ the very small peaks with a threshold. Fig.2 illustrates a +L+ smoothed document distribution with the corresponding threshold, +L+ collected on the topic “US Election” in four months. In +L+ experiments, the threshold is adaptively set as Yd-σd/2, where Yd +L+ and ad are the mean and standard deviation of the curve, +L+ respectively.
After the smoothing and tiny peaks removal, we further detect the +L+ valleys between every two contingent peaks. Thus, the range of +L+ an event (which is correlated to the corresponding peak) can be +L+ considered as the envelope in the two valleys. As shown in Fig2, +L+ the duration denoted by Li+Ri is a rough range of the event +L+ correlated to the peak Pi. Assuming an important event usually +L+ has more documents and has effects in a longer duration, the +L+ saliency of each peak is defined as,
Si =( P )(Li +Ri) (10)
Pavr Davr
where Pi is the ith peak, Li and Ri is the duration from the ith peak +L+ to the previous and next valley; Pavr is the average peak value and +L+ Davr is average duration between two valleys in the curve. Si is the +L+ saliency value of the peak Pi. It could also be considered as the +L+ normalized area under peak Pi, and thus, it roughly represents the +L+ document number of the corresponding event.
In our approach, the top K salient peaks are selected to determine +L+ the event number:
K=argmaxk{∑;1Si/∑N1S ≤η}	(11)
where S; is the sorted saliency value from large to small, N is
total number of detected peaks and ii is a threshold. In our +L+ experiments, ii is set as 0.9, which roughly means that at least +L+ 90% documents will be kept in the further initialization of event
clustering. This selection scheme is designed to guarantee there is +L+ no important information is missed in presentation. After the +L+ event number and initial clusters (the most salient peaks with their +L+ corresponding range) are selected, the event parameters could be +L+ initialized and then updated iteratively.
0 20 40 60 80 100 120
Fig.2 Peak saliency definition. It also illustrates the smoothed +L+ document distribution (document number per day) with the +L+ corresponding threshold for tiny peak removal. Each peak Pi is +L+ assumed to be correlated with each event.
It is noted that some technology such as Bayesian Information +L+ Criteria (BIC) or minimum description length (MDL) [17] could +L+ be used to estimate the optimal event number, by searching +L+ through a reasonable range of the event number to find the one +L+ which maximizes the likelihood in (3). However, these algo- +L+ rithms take long time, and it is usually not necessary to estimate +L+ the exact event number in our scenario of rich presentation. +L+ Actually, in our system, the most important point of event cluster- +L+ ing is that the clustered documents ‘really’ represent the same +L+ event, rather than the event number, as observed in the experi- +L+ ments. Moreover, in the step of synchronization between the +L+ music and storyboard (in the section 4.2), the number of presented +L+ events may be further refined, based on the user’s preference, in +L+ order to match the presentation duration with the music duration.
3.3 Event Description
After obtaining the events and the corresponding documents, we +L+ not only need a concise event summary, but also need to extract +L+ some representative media to describe each event.
3.3.1 Event Summary
A simple way to summarize an event is to choose some +L+ representative words on the persons, locations and keywords of +L+ the event. For example, for the event ej, the ‘leading actor’ could +L+ be chosen as the person with the maximum p(personn | ej), while +L+ the major location could be selected based on p(locationn | ej). +L+ However, such brief description might have a bad readability. +L+ Therefore, in order to increase the readability of the summary, in +L+ our system, we also provide an alterative way. That is, we choose +L+ a candidate document to represent an event. For example, the +L+ document with the highest p(xi| ej) is a good candidate represen- +L+ tative of the event ej. However, a document might be too long to +L+ be shown on the storyboard. Therefore, in our system, only the +L+ “title-brow” (the text between the news title and news body) of +L+ the document, which usually exists and is usually a good +L+ overview (summary) of the document based on our observation +L+ (especially true in our case of news document), is selected to +L+ describe the event.
20 +L+ 15 +L+ 10 +L+ 5 +L+ 0
Peaks relevant to event
P;-1	P;+1
L;	R;
P;
#Doc +L+ Threshold
&apos;
748
IV
I
III
II
Fig. 3 The event template of the Storyboard, which illustrates (I) the representative media, (II)geographic information, (III) event summary, +L+ and (IV) a film strip giving an overview of the events in the temporal order.
3.3.2 Extracting Representative Media
In the obtained documents describing an event, there are usually +L+ many illustrational images, with possible flashes and video clips. +L+ These media information is also a good representative of the +L+ corresponding event. However, since the obtained documents are +L+ directly crawled from the news websites, they usually contain +L+ many noisy multimedia resources, such as the advertisements. +L+ Moreover, there also possible exist some duplicate images in +L+ different documents describing the same event. Therefore, to +L+ extract the representative media from the documents, we need to +L+ remove noisy media and possible duplicate images. Before this, +L+ we also performed a pre-filtering to remove all the images smaller +L+ than 50 pixels in height or width.
•	Noisy Media Detection. In our approach, a simple but +L+ efficient rule is used to remove the noisy media resources. +L+ We find almost all advertisements are provided by other +L+ agencies rather than these news websites themselves. That is, +L+ the hosts of advertisement resources are from different +L+ websites. Thus, in our approach, we extract the host names +L+ from the URLs of all multimedia resources, and remove +L+ those resources with different host name.
•	Duplicate Detection. A number of image signature schemes
can be adopted here to accomplish duplicate detection. In +L+ our implementation, each image is converted into grayscale, +L+ and down-sampled to 8 × 8. That is, a 64-byte signature for +L+ each image is obtained. Then the Euclidean distance of the +L+ 64-byte signature are taken as the dissimilarity measure. +L+ Images have sufficiently small distance are considered as +L+ duplicates.
Once removing the noisy resources and duplicate images, we +L+ simply select the 1-4 large images from the top representative +L+ documents (with the top largest p(xi|ej)), and take them as +L+ representative media of the corresponding event. The exact +L+ number of the selected images is dependent on the document +L+ number (i.e., the importance) of the event and the total image
number the event has. It is noted that, in our current system, we +L+ only associates images with each event. However, other media +L+ like video and flashes can be chosen in a similar way.
4. RICH PRESENTATION GENERATION
In the proposed system, the above obtained information, including +L+ event summary and representative media, are fused to generate a +L+ concise and informative storyboard, in order to richly present the +L+ target topic. In this section, we will first describe the storyboard +L+ generation for the target topic, by presenting each event with the +L+ multimodal information. Then, we present the approach to +L+ synchronizing the storyboard with an incidental music.
4.1 Storyboard Generation
In our approach, a storyboard of a target topic is generated by +L+ presenting each event of the topic slide by slide. To describe an +L+ event, we have obtained the corresponding information including +L+ the person, time, location, event summary and other relevant +L+ images. Therefore, to informatively present each event, we need +L+ first to design an event template (i.e., an interface) to integrate all +L+ the information.
Fig. 3 illustrates the event template used in our proposed system, +L+ with an example event in the topic ‘US Election”. First, the +L+ template presents the representative images in the largest area +L+ (part I), since the pictures are more vivid than the words. As for +L+ each representative picture, the title and date of the document from +L+ which it is extracted is also illustrated. In the Fig.3, there are 4 +L+ pictures extracted from 3 documents. Then, the corresponding +L+ event summaries of these three documents are presented (part III), +L+ where each paragraph refers to the summary of one document. If a +L+ user is interested in one document, he can click on the correspond- +L+ ing title to read more details. Moreover, the geographic informa- +L+ tion of the event is shown with a map in the top-left corner (part +L+ II), to give users a view of the event location. The map is obtained +L+ from “MapPoint Location” service [18], which can return a
749
corresponding map based on user’s location query. However, the +L+ mapping is usually difficult, especially when the event location is +L+ confusing so that the representative location is not accurately +L+ detected. For example, the event shown in the Fig 1 is mapped to +L+ Washington D.C. rather than New York where the republic +L+ convention is held, since Washington is the most frequently +L+ mentioned places in the documents. Finally, a film strip (part IV) +L+ is also presented, arranging each event in the temporal order, +L+ where each event is simply represented by a cluster of images, +L+ with the current event highlighted. It enables users to have a quick +L+ overview of the past and the future in the event sequence.
By connecting various events slide by slide, we could get an +L+ informative storyboard regarding the target topic. In order to +L+ catch the development process of a topic, the events are ordered +L+ by their timestamps in the generated storyboard.
4.2 Synchronizing with Music
To make the storyboard more expressive and attractive, and to +L+ provide a more relaxing way to read information, in the proposed +L+ system, we will accompany the storyboard with an incidental +L+ music and align the transitions between event slides with the +L+ music beats, following the idea in music video generation [19][20]. +L+ Sometimes, music could also provide extra information about the +L+ target topic. For example, when the target topic is a movie, the +L+ corresponding theme song could be chosen for the rich presenta- +L+ tion. In this sub-section, we will present our approach to music +L+ analysis and synchronization with the storyboard.
4.2.1 Music Rhythm Analysis
In the proposed system, we detect the onset sequences instead of +L+ the exact beat series to represent music rhythm. This is because +L+ the beat information is sometimes not obvious, especially in light +L+ music which is usually selected as incidental music. The strongest +L+ onset in a time window could be assumed as a “beat”. This is +L+ reasonable since there are some beat positions in a time window +L+ (for example, 5 seconds); thus, the most possible position of a beat +L+ is the position of the strongest onset.
The process of onset estimation is illustrated in Fig. 4. After FFT +L+ is performed on each frame of 16ms-length, an octave-scale filter- +L+ bank is used to divide the frequency domain into six sub-bands, +L+ including [0, co0 /26), [co0 /26, co0 /25), ..., [co0 /22, co0 /2], where co0 +L+ refers to the sampling rate.
Onset Curve
Fig. 4 The process of onset sequence estimation
After the amplitude envelope of each sub-band is extracted by +L+ using a half-Hamming window, a Canny operator is used for onset +L+ sequence detection by estimating its difference function,
Di (n) = Ai (n) ⊗ C(n) (12)
where Di(n) is the difference function in the ith sub-band, Ai(n) is +L+ the amplitude envelope of the ith sub-band, and C(n) is the Canny +L+ operator with a Gaussian kernel,
C(n) = i e .2/2σ2 n∈
σ 2 c c
where Lc is the length of the Canny operator and a is used to +L+ control the operator’s shape, which are set as 12 and 4 in our +L+ implementation, respectively.
Finally, the sum of the difference curves of these six sub-bands is +L+ used to extract onset sequence. Each peak is considered as an +L+ onset, and the peak value is considered as the onset strength.
Based on the obtained onsets, an incidental music is further +L+ segmented into music sub-clips, where a strong onset is taken as +L+ the boundary of a music sub-clip. These music sub-clips are then +L+ used as the basic timeline for the synchronization in the next step. +L+ Thus, to satisfy the requirement that the event slide transitions of +L+ the storyboard should occur at the music beats, we just need to +L+ align the event slide boundaries and music sub-clip boundaries.
To give a more pleasant perception, the music sub-clip should not +L+ be too short or too long, also it had better not always keep the +L+ same length. In our implementation, the length of music sub-clips +L+ is randomly selected in a range of [tmin, tmax] seconds. Thus, the +L+ music sub-clips can be extracted in the following way: given the +L+ previous boundary, the next boundary is selected as the strongest +L+ onset in the window which is [tmin, tmax] seconds away from the +L+ previous boundary. In the proposed system, users can manually +L+ specify the range of the length of the music sub-clip. The default +L+ range in the system is set as [12, 18] seconds, in order to let users +L+ have enough time to read all the information on each event slide.
4.2.2 Alignment Scheme
To synchronize the transitions between different event slides and +L+ the beats of the incidental music, as mentioned above, we actually +L+ need to align the slide boundaries and music sub-clip boundaries. +L+ To satisfy this requirement, a straightforward way is to set the +L+ length of each event slide be equal to the corresponding length of +L+ the sub-music clip.
However, as Fig. 5 illustrates, the number of event slides is +L+ usually not equal to the number of music sub-clip. In this case, in +L+ our proposed system, we provide two schemes to solve this +L+ problem.
1) Music Sub-clip Based. In this scheme, only the top N important +L+ events of the target topic are adaptively chosen and used in the +L+ rich presentation, where N is supposed as the number of music +L+ sub-clip in the corresponding incidental music, as the Fig.5 shows. +L+ Although a formal definition of event importance is usually hard +L+ and subjective, in our approach, the importance score of an event +L+ is simply measured by the number of documents reporting it, +L+ assuming that the more important the event, the more the +L+ corresponding documents. The assumption is quite similar as that +L+ in the definition of (10).
Acoustic Music Data +L+ FFT
Difference curve
Sub-Band 1
Envelope +L+ Extractor
...	...	...
.
.
.
.
.
.
Difference curve
Sub-Band N
Envelope +L+ Extractor
]
(13)
750
2) Specified Event Number Based. In this scheme, users can +L+ specify the number of the event he wants to learn. For example, a +L+ user could choose to show the top 30 important events or all the +L+ events. Thus, to accommodate all the events in the music duration, +L+ we will repeat the incidental music if it is needed and then fade out +L+ the music at the end.
Fig. 5 Music and storyboard synchronization: a music sub-slip +L+ based scheme, that is, only the top important events are presented +L+ to match the number of music sub-clips.
4.2.3 Rendering
After the alignment between storyboard and incidental music, in +L+ our system, fifteen common transition effects, such as cross-fade, +L+ wipe and dissolve, are also randomly selected to connect the event +L+ slides, producing a better rich presentation in final rendering.
5. EVALUATIONS
In this section, we evaluate the performance of the proposed +L+ approach to rich presentation and its key component, event +L+ clustering. In the experiments, we randomly select 8 topics of +L+ different types, including Earthquake, Halloween, Air Disaster, +L+ US Election, Nobel Prize, Britney Spears, David Beckham, and +L+ Harry Potter, from some hot news topics in the end of 2004 and +L+ beginning of 2005. Once the topic is selected, the topic name is +L+ used as a query and the relevant documents are collected from +L+ CNN, MSNBC and BBC. More details about the selected topics +L+ and the corresponding documents are shown in the Table 1, which +L+ lists the topic name, the time range of the collected documents, +L+ and the number of documents and its corresponding events.
Table 1. A list of testing topics in the rich presentation evaluations
No.	Topic	Time	#doc	#event
1	Earthquake	1995-2004	976	17
2	Halloween	1995-2004	762	9
3	Air Disaster	1995-2004	210	13
4	US Election	1995-2004	2486	—
5	Britney Spears	2000-2004	1311	—
6	Nobel Prize	1995-2004	186	—
7	David Beckham	1995-2004	877	—
8	Harry Potter	2000-2004	841	—
Total	——	——	7649	—
It is noted that, in the table, only 3 topics have labeled events, +L+ while another 5 topics have not. This is because that, the labeling +L+ work of a topic is very subjective and usually hard for individuals +L+ to manually decide the event number of a given topic. Therefore, +L+ we only label the topics which are easily to be annotated based on
the criterion in Topic Detection and Tracking (TDT) project [21]. +L+ For example, Halloween is a topic which is reported once a year, +L+ thus, each year&apos;s documents can be regarded as an event; as for +L+ Earthquake and Air Disaster, their events lists could be found +L+ from corresponding official websites. In the annotation, we +L+ remove the events which do not have or have few (less than 4) +L+ relevant documents, and also remove the documents not belonging +L+ to any events.
After parsing the obtained documents, for each topic, we usually +L+ can obtain 3.8 images per document in average. With further +L+ duplicate detection, only 1.6 images per document are remained. +L+ Moreover, from each document, we could also obtain about 3.0 +L+ unique location entities and 2.8 unique name entities. Other words +L+ except of these entities are taken as keywords. Fig.6 shows a real +L+ representation of an example document with extracted entities in +L+ the XML format, from which the event clustering is performed.
&lt;URL&gt;http://news.bbc.co.uk/1/hi/world/americas/4071845.stm &lt;/URL&gt; +L+ &lt;Abstract&gt;The US battleground state of Ohio has certified the victory +L+ of President George W Bush&apos;s in last month&apos;s poll. &lt;/Abstract&gt;
&lt;Date&gt; 2004/12/6 &lt;/Date&gt;
&lt;NLPRESULT&gt;
&lt;LOCATION&gt;
&lt;entity&gt; Ohio &lt;/entity&gt; &lt;freq&gt;4&lt;/freq&gt;
&lt;entity&gt; US &lt;/entity&gt; &lt;freq&gt; 2 &lt;/freq&gt;
&lt;/LOCATION&gt;
&lt;PERSON&gt;
&lt;entity&gt; Bush &lt;/entity&gt; &lt;freq&gt; 3 &lt;/freq&gt;
&lt;entity&gt;David Cobb&lt;/entity&gt; &lt;freq&gt;1&lt;/freq&gt;
... +L+ &lt;/PERSON&gt;
...
&lt;DATE&gt;
&lt;entity&gt; 6 December, 200&lt;/entity&gt; &lt;freq&gt; 1 &lt;/freq&gt;
&lt;entity&gt; Friday &lt;/entity&gt; &lt;freq&gt; 2 &lt;/freq&gt;
...
&lt;/DATE&gt;
&lt;KEYWORDS&gt;
...
&lt;entity&gt; recount &lt;/entity&gt; &lt;freq&gt;7&lt;/freq&gt;
&lt;entity&gt; elect &lt;/entity&gt; &lt;freq&gt;3&lt;/freq&gt;
&lt;entity&gt; America &lt;/entity&gt; &lt;freq&gt;3&lt;/freq&gt;
&lt;entity&gt; poll &lt;/entity&gt; &lt;freq&gt;3&lt;/freq&gt;
... +L+ &lt;/KEYWORDS&gt;
&lt;/NLPRESULT&gt;
Fig. 6. XML representation of a document on “US Election” with +L+ extracted entities
5.1 Event Clustering
As mentioned above, the evaluation of the approach to event +L+ clustering is evaluated on three topics, including Earthquake, Hal- +L+ loween, and Air Disaster, for which the corresponding event num- +L+ bers are determined and the documents are labeled using a similar +L+ method in the TDT project. However, in the proposed appraoch, +L+ we actually do not estimate the optimal event number, but use a +L+ much larger one. Therefore, in order to better evaluate the +L+ performance of the event clustering algorithm and compare with +L+ its counterpart, we use the event number in the ground truth to +L+ initialize the cluster number in the proposed clustering algorithm.
Event +L+ Slide List
Music +L+ Sub-Clip
E1
S1
E2
S2
E3
S3
E4
S4
E5
S5
E6
E7
E8
751
In the experiments, K-means, which is another frequently used +L+ clustering algorithm (as well in TDT [22]), is adopted to compare +L+ with the proposed approach. The comparison results of two +L+ clustering approaches are illustrated in Table 2, with precision and +L+ recall for each topic.
Table 2. The performance comparison between our approach and +L+ K-means on the event clustering
	Precision		Recall	
	K-means	Ours	K-means	Ours
Earthquake	0.74	0.87	0.63	0.74
Halloween	0.88	0.93	0.72	0.81
Air Disaster	0.57	0.68	0.55	0.61
Average	0.73	0.83	0.63	0.72
From Table 2, it can be seen that the results of our approach are +L+ significantly better than those of K-means, both on precision and +L+ recall. On the three testing topics, the average precision of our +L+ approach is up to 0.83 and the average recall achieves 0.72, which +L+ is 10% and 9% higher than those of K-means, respectively. By +L+ tracing the process of K-means, we find that K-means usually +L+ assigns documents far away from each other on the timeline into +L+ the same cluster, since the time information affects little in K- +L+ means. It also indicates the advantages of our approach with time +L+ modeling.
The algorithms also show different performance on different kind +L+ topics. As for the “Air disaster”, its performance is not as good as +L+ that of the other two, since the features (words and time) of its +L+ events are more complicated and intertwined in the feature space.
As for the topics (4-8 in Table I) which could not have an +L+ objective evaluation, the clustering performance on these topics +L+ could be indirectly reflected by the subjective evaluation of the +L+ rich presentation presented in section 5.2. This is because users +L+ will be more satisfied when the grouped documents shown in each +L+ event slide really belong to the same event; while users are not +L+ satisfied if the documents from different events are mixed in one +L+ event slide.
5.2 Rich Presentation
It is usually difficult to find a quantitative measure for rich +L+ presentation, since the assessment of the goodness of rich presen- +L+ tation is a strong subjective task. In this paper, we carry out a pre- +L+ liminary user study to evaluate the performance of the proposed +L+ rich presentation schemes.
To indicate the performance of rich presentation, we design two +L+ measures in the experiments, including ‘informativeness’ and +L+ ‘enjoyablity’, following the criteria used in the work [7]. Here, the +L+ informativeness measures whether the subjects satisfy with the +L+ information obtained from the rich presentation; while enjoyablity +L+ indicates if users feel comfortable and enjoyable when they are +L+ reading the rich presentation. In evaluating the informativeness, +L+ we also provide the documents from which the rich presentation is +L+ generated. They are used as baseline, based on which the subjects +L+ can more easily evaluate if the important overview information +L+ contained in the documents is conveyed by the rich presentation. +L+ Moreover, in order to reveal the subjects’ opinion on the design of +L+ the storyboard template, like the one shown in Fig 3, we also ask +L+ the subjects to evaluate the ‘interface design’.
In the user study, 10 volunteered subjects including 8 males and 2 +L+ females are invited. The subjects are around 20-35 years old, have +L+ much experience on computer manipulation, and usually read +L+ news on web in their leisure time. We ask them to give a +L+ subjective score between 1 and 5 for each measure of the rich +L+ presentation of each testing topic (an exception is ‘interface +L+ design’, which is the same for each rich presentation). Here, the +L+ score ‘1’ to ‘5’ stands for unsatisfied (1), somewhat unsatisfied (2), +L+ acceptable (3), satisfied (4) and very satisfied (5), respectively.
In experiments, we first check with the ‘interface design’ measure. +L+ We find 7 out of 10 subjects satisfy with the event template design +L+ and the left three also think it is acceptable. The average score is +L+ up to 3.9. An interesting observation is that, some subjects like +L+ the template design very much at the first glance, but they feel a +L+ little boring after they finish all the user study since every slide in +L+ the rich presentation of each topic has the same appearance. It +L+ hints us that we had better design different templates for different +L+ topics to make the rich presentation more attractive.
As for the other two measures, we average the score across all the +L+ subjects to represent the performance for each topic, and list the +L+ detailed results in Table 3. It can be seen that the average score of +L+ both enjoyablity and informativeness achieves 3.7, which indicates +L+ that most subjects satisfy the provided overview information of the +L+ target topic, and they enjoy themselves when reading these rich +L+ presentations.
Table 3. The evaluation results of rich presentation on each topic
No.	Topic	Informative	Enjoyable
1	Earthquake	4.3	3.2
2	Halloween	3.6	4.0
3	Air Disaster	4.0	3.4
4	US Election	4.1	4.0
5	Britney Spears	3.6	4.1
6	Nobel Prize	3.3	3.4
7	David Beckham	3.4	4.0
8	Harry Potter	3.3	3.4
Average		3.7	3.7
In the experiments, we find informativeness is highly depended on +L+ the correlation between the presented documents and the target +L+ topic. If the presented information is consistent with the topic, +L+ subjects usually give a high score for informativeness, such as +L+ those on Earthquake and US Election; otherwise, they will give a +L+ low score, like those on David Beckham and Nobel Prize. It +L+ indicates that it is quite important to provide users clean +L+ information of the target topic with less noise. However, in +L+ current system, the documents are crawled from web and +L+ inevitably contain many noises. It affects much on the perform- +L+ ance of informativeness in the current system. We need to consider +L+ how to prone the information of the target topic in the future +L+ works.
We also find that the enjoyablity score is usually related with +L+ informativeness. If the subjects do not get enough information +L+ from the rich presentation, they will be not enjoyable as well, such +L+ as the topics of Nobel Prize and Harry Potter. Enjoyablity is also +L+ topic-related, the subjects usually feel unconformable when they +L+ are facing with miserable topics, such as Earthquake and Air +L+ Disaster, although their informativeness is quite high. On the
752
contrary, users give a high score for enjoyablity on the interesting +L+ topics, such as Britney Spears and David Beckham, although their +L+ informative score is not high. This is because that there are +L+ usually many funny and interesting pictures in the presentation of +L+ these topics. Another finding is that users usually fell unenjoyable +L+ if the images and summaries in one event slide are not consistent +L+ with each other. From this view, the high enjoyablity score in our +L+ experiments also indicates that our event clustering algorithm +L+ works promisingly
6. CONCLUSIONS
To facilitate users to quickly grasp and go through the content of a +L+ semantic topic, in this paper, we have proposed a novel approach +L+ to rich presentation to generate a concise and informative +L+ storyboard for the target topic, with many relevant multimodal +L+ information including image, text, audio and video. In this +L+ approach, the related multimodal information of a given topic is +L+ first extracted from news databases. Then, the events are clustered, +L+ and the corresponding information, such as representative images, +L+ geographic information, and event summary, is obtained. The +L+ information is composed into an attractive storyboard which is +L+ finally synchronized with incidental music. A user study indicates +L+ that the presented system works well on our testing examples.
There is still some room for improving the proposed approach. +L+ First, the proposed approach could be extended to other +L+ multimedia databases or more general websites. For example, +L+ some standard multimedia database like NIST TRECVID could +L+ provide a nice platform for the implementation and evaluation of +L+ event detection and rich presentation. Second, to integrate more +L+ relevant multimedia information (such as video clips and flashes) +L+ and more accurate information regarding the target topic is highly +L+ expected by users. Thus, more advanced information retrieval/ +L+ extraction techniques and other multimedia analysis techniques are +L+ needed to be exploited and integrated, such as relevance ranking, +L+ mapping schemes, important or representative video clips +L+ detection and video clip summarization. We also need to design a +L+ much natural way to incorporate video clips in the event template. +L+ Third, we also consider designing various storyboard templates for +L+ different kind of topics. For example, each topic may be belonging +L+ to different clusters such as politics, sports and entertainments, +L+ each of which can have a representative template. Forth, +L+ appropriate user interaction will be added to further make the +L+ storyboard more interactive and easy to control. Finally, a +L+ thorough evaluation will be implemented to evaluate the effect of +L+ each component in the framework and storyboard template.
7. REFERENCES
[1] A. Vailaya, M.A.T. Figueiredo, A. K. Jain, and H.-J. Zhang.
“Image classification for content-based indexing”. IEEE
Transactions on Image Processing, Vol. 10, Iss.1, 2001
[2] F. J., M.-J. Li, H.-J. Zhang, and B. Zhang. “An effective +L+ region-based image retrieval framework”. Proc. ACM +L+ Multimedia’02, pp. 456-465, 2002
[3] J. Platt “AutoAlbum: Clustering Digital Photographs using +L+ Probabilistic Model Merging” Proc. IEEE Workshop on +L+ Content-Based Access of Image and Video Libraries, pp. 96– +L+ 100, 2000.
[4] A. Hanjalic, R. L. Lagendijk, J. Biemond, “Automated high- +L+ level movie segmentation for advanced video-retrieval
systems”, IEEE Trans on Circuits and Systems For Video +L+ Technology, Vol. 9, No. 4, pp. 580-588, 1999.
[5] J. Assfalg and et al, “Semantic annotation of soccer videos: +L+ automatic highlights identification,&quot; CVIU&apos;03, vol. 92, pp. +L+ 285-305, 2003.
[6] A. Ekin, A. M. Tekalp, and R. Mehrotra, &quot;Automatic soccer +L+ video analysis and summarization,&quot; IEEE Trans. on Image +L+ Processing, 12(7), pp. 796-807, 2003.
[7] Y. -F. Ma, L. Lu, H. -J. Zhang, and M.-J Li. “A User +L+ Attention Model for Video Summarization”. ACM +L+ Multimeida’02, pp. 533-542, 2002.
[8] L. Xie, P. Xu, S.F. Chang, A. Divakaran, and H. Sun, +L+ &quot;Structure analysis of soccer video with domain knowledge +L+ and hidden markov models,&quot; Pattern Recognition Letters, +L+ vol. 25(7), pp. 767-775, 2004.
[9] L. Lu, H. Jiang, H. J. Zhang, “A Robust Audio Classification +L+ and Segmentation Method,” Proc. ACM Multimedia’01, pp. +L+ 203-211, 2001
[10] R. Cai, L. Lu, H.-J. Zhang, and L.-H. Cai, “Highlight Sound +L+ Effects Detection in Audio Stream,” Proc. ICME’03 Vol.3, +L+ pp.37-40, 2003.
[11] Y. Rui, A. Gupta, and A. Acero, “Automatically Extracting +L+ Highlights for TV Baseball Programs”, Proc. ACM Multi- +L+ media’00, pp. 105-115, 2000.
[12] C. Snoek, and M. Worring. “Multimodal Video Indexing: A +L+ Review of the State-of-the-art”. Multimedia Tools and +L+ Applications, Vol. 25, No. 1 pp. 5 – 35, 2005
[13] E.M. Voorhees, “Query expansion using lexical-semantic +L+ relations” Proc. ACM SIGIR Conference on Research and +L+ Development in Information Retrieval , pp 61 - 69, 1994
[14] Z.-W. Li, M.-J. Li, and W.-Y. Ma. &quot;A Probabilistic Model for +L+ Retrospective News Event Detection”, Proc. SIGIR +L+ Conference on Research and Development in Information +L+ Retrieval, 2005
[15] D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. “An +L+ Algorithm That Learns What’s in a Name”. Machine +L+ Learning, 34(1-3), 1999
[16] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. “Text +L+ Classification from Labeled and Unlabeled Documents using +L+ EM”. Machine Learning, 39(2-3), 2000
[17] T. Hastie, R. Tibshirani, and J. Friedman. “The Elements of +L+ Statistical Learning: Data Mining, Inference and Prediction”. +L+ Springer-Verlag, 2001
[18] MapPoint Web Service http://www.microsoft.com/mappoint/ +L+ products/ webservice/default.mspx
[19] X.-S. Hua, L. Lu, H.-J. Zhang. &quot;Automated Home Video +L+ Editing&quot;, Proc. ACM Multimedia’03, pp. 490-497, 2003
[20] J. Foote, M. Cooper, and A. Girgensohn. “Creating Music +L+ Videos Using Automatic Media Analysis”. ACM +L+ Multimedia’02, pp.553-560, 2002.
[21] Topic Detection and Tracking (TDT) Project: http://www. +L+ nist.gov/speech/tests/tdt/
[22] J. Allan, R. Papka, and V. Lavrenko. “On-line New Event +L+ Detection and Tracking”. Proc. SIGIR Conference on +L+ Research and Development in Information Retrieval 98, +L+ pp.37-45, 1998
753
