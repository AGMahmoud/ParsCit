A Two-Phase Sampling Technique for Information
Extraction from Hidden Web Databases
Y.L. Hedley, M. Younas, A. James
School of Mathematical and Information Sciences
Coventry University, Coventry CV1 5FB, UK
{y.hedley, m.younas, a.james}@coventry.ac.uk +L+ ABSTRACT
Hidden Web databases maintain a collection of specialised +L+ documents, which are dynamically generated in response to users’ +L+ queries. However, the documents are generated by Web page +L+ templates, which contain information that is irrelevant to queries. +L+ This paper presents a Two-Phase Sampling (2PS) technique that +L+ detects templates and extracts query-related information from the +L+ sampled documents of a database. In the first phase, 2PS queries +L+ databases with terms contained in their search interface pages and +L+ the subsequently sampled documents. This process retrieves a +L+ required number of documents. In the second phase, 2PS detects +L+ Web page templates in the sampled documents in order to extract +L+ information relevant to queries. We test 2PS on a number of real- +L+ world Hidden Web databases. Experimental results demonstrate +L+ that 2PS effectively eliminates irrelevant information contained in +L+ Web page templates and generates terms and frequencies with +L+ improved accuracy.
Categories and Subject Descriptors
H.3.5 [Information Storage and Retrieval]: Online Information +L+ Services – Web-based services.
General Terms
Algorithms, Experimentation.
Keywords
Hidden Web Databases, Document Sampling, Information +L+ Extraction.
1. INTRODUCTION
An increasing number of databases on the Web maintain a +L+ collection of documents such as archives, user manuals or news +L+ articles. These databases dynamically generate documents in +L+ response to users’ queries and are referred to as Hidden Web +L+ databases [5]. As the number of databases proliferates, it has +L+ become prohibitive for specialised search services (such as +L+ search.com) to evaluate databases individually in order to answer +L+ users’ queries.
Current techniques such as database selection and categorisation
Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that +L+ copies bear this notice and the full citation on the first page. To copy +L+ otherwise, or republish, to post on servers or to redistribute to lists, +L+ requires prior specific permission and/or a fee.
WIDM’04, November 12–13, 2004, Washington, DC, USA.
Copyright 2004 ACM 1-58113-978-0/04/0011...$5.00. +L+ M. Sanderson
Department of Information Studies
University of Sheffield, Sheffield, S1 4DP, UK
m.sanderson@sheffield.ac.uk
have been employed to enhance the effectiveness of information +L+ retrieval from databases [2, 5, 10, 11, 15]. In the domain of the +L+ Hidden Web, knowledge about the contents of databases is often +L+ unavailable. Existing approaches such as in [2, 10, 15] acquire +L+ knowledge through sampling documents from databases. For +L+ instance, query-based sampling [2] queries databases with terms +L+ that are randomly selected from those contained in the sampled +L+ documents. The techniques in [10, 15] sample databases with +L+ terms obtained from Web logs to retrieve additional topic terms. +L+ A major issue associated with existing techniques is that they also +L+ extract information irrelevant to queries. That is, information +L+ extracted is often found in Web page templates, which contain +L+ navigation panels, search interfaces and advertisements. +L+ Consequently, the accuracy of terms and frequencies generated +L+ from sampled documents has been reduced.
In addition, approximate string matching techniques are adopted +L+ by [13] to extract information from Web pages, but this approach +L+ is limited to textual contents only. Alternatively, the approaches +L+ proposed in [3, 4] analyse Web pages in tree-like structures. +L+ However, such an approach requires Web pages with well- +L+ conformed HTML tag trees. Furthermore, [3] discovers +L+ dynamically generated objects from Web pages, which are +L+ clustered into groups of similar structured pages based on a set of +L+ pre-defined templates, such as exception page templates and +L+ result page templates.
In this paper, we propose a sampling and extraction technique, +L+ which is referred to as Two-Phase Sampling (2PS). 2PS aims to +L+ extract information relevant to queries in order to acquire +L+ information contents of underlying databases. Our technique is +L+ applied in two phases. First, it randomly selects a term from those +L+ found in the search interface pages of a database to initiate the +L+ process of sampling documents. Subsequently, 2PS queries the +L+ database with terms randomly selected from those contained in +L+ the sampled documents. Second, 2PS detects Web page templates +L+ and extracts query-related information from which terms and +L+ frequencies are generated to summarise the database contents.
Our approach utilises information contained in search interface +L+ pages of a database to initiate the sampling process. This differs +L+ from current sampling techniques such as query-based sampling, +L+ which performs an initial query with a frequently used term. +L+ Furthermore, 2PS extracts terms that are relevant to queries thus +L+ generating statistics (i.e., terms and frequencies) that represent +L+ database contents with improved accuracy. By contrast, the +L+ approaches in [2, 10, 15] extract all terms from sampled +L+ documents, including those contained in Web page templates. +L+ Consequently, information that is irrelevant to queries is also +L+ extracted.
1
Figure 1. The Two-Phase Sampling (2PS) technique.
2PS is implemented as a prototype system and tested on a number +L+ of real-world Hidden Web databases, which contain computer +L+ manuals, healthcare archives and news articles. Experimental +L+ results show that our technique effectively detects Web page +L+ templates and generates terms and frequencies (from sampled +L+ documents) that are relevant to the queries.
The remainder of the paper is organised as follows. Section 2 +L+ introduces current approaches to the discovery of information +L+ contents of Hidden Web databases. Related work on the +L+ information extraction from Web pages or dynamically generated +L+ documents is also discussed. Section 3 describes the proposed +L+ 2PS technique. Section 4 presents experimental results. Section 5 +L+ concludes the paper.
2. RELATED WORK
A major area of current research into the information retrieval of +L+ Hidden Web databases focuses on the automatic discovery of +L+ information contents of databases, in order to facilitate their +L+ selection or categorisation. For instance, the technique proposed +L+ in [6] analyses the hyperlink structures of databases in order to +L+ facilitate the search for databases that are similar in content. The +L+ approach adopted by [10, 15] examines the textual contents of +L+ search interface pages maintained by data sources to gather +L+ information about database contents.
A different approach is to retrieve actual documents to acquire +L+ such information. However, in the domain of Hidden Web +L+ databases, it is difficult to obtain all documents from a database. +L+ Therefore, a number of research studies [2, 10, 15] obtain +L+ information by retrieving a set of documents through sampling. +L+ For instance, query-based sampling [2] queries databases with +L+ terms that are randomly selected from those contained in the +L+ sampled documents. The techniques in [10, 15] sample databases +L+ with terms extracted from Web logs to obtain additional topic +L+ terms. These techniques generate terms and frequencies from +L+ sampled documents, which are referred to as Language Models +L+ [2], Textual Models [10, 15] or Centroids [11].
A key issue associated with the aforementioned sampling +L+ techniques is that they extract information that is often irrelevant +L+ to queries, since information contained in Web page templates +L+ such as navigation panels, search interfaces and advertisements is +L+ also extracted. For example, a language model generated from the +L+ sampled documents of the Combined Health Information +L+ Database (CHID) contains terms (such as ‘author’ and ‘format’) +L+ with high frequencies. These terms are not relevant to queries but +L+ are used for descriptive purposes. Consequently, the accuracy of +L+ terms and frequencies generated from sampled documents has
been reduced. The use of additional stop-word lists has been +L+ considered in [2] to eliminate irrelevant terms - but it is +L+ maintained that such a technique can be difficult to apply in +L+ practice.
Existing techniques in information extraction from Web pages are +L+ of varying degrees of complexity. For instance, approximate +L+ string matching techniques are adopted by [13] to extract texts +L+ that are different. This approach is limited to finding textual +L+ similarities and differences. The approaches proposed in [3, 4] +L+ analyse textual contents and tag structures in order to extract data +L+ from Web pages. However, such an approach requires Web pages +L+ that are produced with well-conformed HTML tag-trees. +L+ Computation is also needed to convert and analyse Web pages in +L+ a tree-like structure. Moreover, [3] identifies Web page templates +L+ based on a number of pre-defined templates, such as exception +L+ page templates and result page templates.
Our technique examines Web documents based on textual +L+ contents and the neighbouring tag structures rather than analysing +L+ their contents in a tree-like structure. We also detect information +L+ contained in different templates through which documents are +L+ generated. Therefore, it is not restricted to a pre-defined set of +L+ page templates.
Furthermore, we focus on databases that contain documents such +L+ as archives and new articles. A distinct characteristic of +L+ documents found in such a domain is that the content of a +L+ document is often accompanied by other information for +L+ supplementary or navigation purposes. The proposed 2PS +L+ technique detects and eliminates information contained in +L+ templates in order to extract the content of a document. This +L+ differs from the approaches in [1, 4], which attempt to extract a +L+ set of data from Web pages presented in a particular pattern. For +L+ example, the Web pages of a bookstore Web site contain +L+ information about authors followed by their associated list of +L+ publications. However, in the domain of document databases, +L+ information contained in dynamically generated Web pages is +L+ often presented in a structured fashion but irrelevant to queries.
Other research studies [9, 8, 12] are specifically associated with +L+ the extraction of data from query forms in order to further the +L+ retrieval of information from the underlying databases.
3. TWO-PHASE SAMPLING
This section presents the proposed technique for extracting +L+ information from Hidden Web document databases in two phases, +L+ which we refer to as Two-Phase Sampling (2PS). Figure 1 depicts +L+ the process of sampling a database and extracting query-related
2
information from the sampled documents. In phase one, 2PS +L+ obtains randomly sampled documents. In phase two, it detects +L+ Web page templates. This extracts information relevant to the +L+ queries and then generates terms and frequencies to summarise +L+ the database content. The two phases are detailed in section 3.1 +L+ and 3.2.
3.1 Phase One: Document Sampling
In the first phase we initiate the process of sampling documents +L+ from a database with a randomly selected term from those +L+ contained in the search interface pages of the database. This +L+ retrieves top N documents where N represents the number of +L+ documents that are the most relevant to the query. A subsequent +L+ query term is then randomly selected from terms extracted from +L+ the sampled documents. This process is repeated until a required +L+ number of documents are sampled. The sampled documents are +L+ stored locally for further analysis.
Figure 2 illustrates the algorithm that obtains a number of +L+ randomly sampled documents. tq denotes a term extracted from +L+ the search interface pages of a database, D. qtp represents a query
term selected from a collection of terms, Q, qtp e Q, 1 &lt;_ p &lt;_ m;
where m is the distinct number of terms extracted from the search +L+ interface pages and the documents that have been sampled. R +L+ represents the set of documents randomly sampled from D. tr is a +L+ term extracted from di. di represents a sampled document from D,
di e D, 1 &lt;_ i &lt;_ n, where n is the number of document to sample.
Algorithm SampleDocument
Extract tq from search interface pages of D, Q = tq +L+ For i = 1 to n
Randomly select qtp from Q
If (qtp has not been selected previously)
Execute the query with qtp on D
j = 0
While j &lt;= N
If (di o R)
Retrieve di from D
Extract tr from di,
R = di +L+ Q = tr +L+ Increase j by 1
End if
End while
End if
End for
Figure 2. The algorithm for sampling documents from a
database.
2PS differs from query-based sampling in terms of selecting an +L+ initial query. The latter selects an initial term from a list of +L+ frequently used terms. 2PS initiates the sampling process with a +L+ term randomly selected from those contained in the search +L+ interface pages of the database. This utilises a source of +L+ information that is closely related to its content. Moreover, 2PS +L+ analyses the sampled documents in the second phase in order to +L+ extract query-related information. By contrast, query-based
sampling does not analyse their contents to determine whether +L+ terms are relevant to queries.
3.2 Phase Two: Document Content Extraction +L+ and Summarisation
The documents sampled from the first phase are further analysed +L+ in order to extract information relevant to the queries. This is then +L+ followed by the generation of terms and frequencies to represent +L+ the content of the underlying database. This phase is carried out +L+ through the following processes.
3.2.1 Generate Document Content Representations
The content of each sampled document is converted into a list of +L+ text and tag segments. Tag segments include start tags, end tags +L+ and single tags specified in HyperText Markup Language +L+ (HTML). Text segments are text that resides between two tag +L+ segments. The document content is then represented by text +L+ segments and their neighbouring tag segments, which we refer to +L+ as Text with Neighbouring Adjacent Tag Segments (TNATS). The +L+ neighbouring adjacent tag segments of a text segment are defined +L+ as the list of tag segments that are located immediately before and +L+ after the text segment until another text segment is reached. The +L+ neighbouring tag segments of a text segment describe how the +L+ text segment is structured and its relation to the nearest text +L+ segments. Assume that a document contains n segments, a text +L+ segment, txs, is defined as: txs = (txi, tg-lstj, tg-lstk), where txi is
the textual content of the ith text segment, 1 &lt;_ i &lt;_ n; tg-lstj
represents p tag segments located before txi and tg-lstk represents +L+ q tag segments located after txi until another text segment is
reached. tg-lstj = (tg1, ..., tgp), 1 &lt;_ j &lt;_ p and tg-lstk = (tg1, ..., tgq),
1 &lt;_ k &lt;_ q.
Figure 3. A template-generated document from CHID.
Figure 3 shows a template-generated document retrieved from the +L+ CHID database. The source code for this document is given in +L+ Figure 4. For example, text segment, ‘1. Equipos Mas Seguros: +L+ Si Te Inyectas Drogas.’, can be identified by the text (i.e., ‘1. +L+ Equipos Mas Seguros: Si Te Inyectas Drogas.’) and its +L+ neighbouring tag segments. These include the list of tags located +L+ before the text (i.e., &lt;/TITLE&gt;, &lt;/HEAD&gt;, &lt;BODY&gt;, &lt;HR&gt;, +L+ &lt;H3&gt;, &lt;B&gt; and &lt;I&gt;) and the neighbouring tags located after the +L+ text (i.e., &lt;/I&gt;, &lt;/B&gt;, &lt;/H3&gt;, &lt;I&gt; and &lt;B&gt;). Thus, this segment is +L+ then represented as (‘1. Equipos Mas Seguros: Si Te Inyectas +L+ Drogas.’, (&lt;/TITLE&gt;, &lt;/HEAD&gt;, &lt;BODY&gt;, &lt;HR&gt;, &lt;H3&gt;, &lt;B&gt; +L+ ,&lt;I&gt;), (&lt;/I&gt;, &lt;/B&gt;, &lt;/H3&gt;, &lt;I&gt;, &lt;B&gt;)). Figure 5 shows the content
3
representation of the CHID document (given in Figure 3) +L+ generated based on TNATS. Given a sampled document, d, with n +L+ text segments, the content of d is then represented as: Content(d)
= {txs1, ..., txsn}, where txsi represents a text segment, 1 &lt;_ i &lt;_ n.
Figure 4. The source code for the CHID document.
Figure 5. The content representation of the CHID document
using TNATS.
3.2.2 Detect Templates
In the domain of Hidden Web databases, documents are often +L+ presented to users through one or more templates. Templates are +L+ typically employed in order to describe document contents or to +L+ assist users in navigation. For example, information contained in +L+ the document (as shown in Figure 3) can be classified into the two +L+ following categories:
(i) Template-Generated Information. This includes information +L+ such as navigation panels, search interfaces and +L+ advertisements. In addition, information may be given to +L+ describe the content of a document. Such information is +L+ irrelevant to a user’s query. For example, navigation links +L+ (such as ‘Next Doc’ and ‘Last Doc’) and headings (such +L+ ‘Subfile’ and ‘Format’) are found in the document.
(ii) Query-Related Information. This information is retrieved in +L+ response to a user’s query, i.e., ‘1. Equipos Mas Seguros: +L+ Si Te Inyectas Drogas. ...’.
The 2PS technique detects Web page templates employed by +L+ databases to generate documents in order to extract information
that is relevant to queries. Figure 6 describes the algorithm that +L+ detects information contained in Web page templates from n +L+ sampled documents. di represents a sampled document from the
database D, di, e D, 1 &lt;_ i &lt;_ n. Content(di) denotes the content
representation of di.
Algorithm DetectTemplate +L+ For i = 1 to n
If T = 0
If S = 0
S = di
Else if S ;t� 0
While l &lt;= s AND T = 0
Compare (Content(di),Content(dl)) +L+ If Content(di) = Content(dl)
wptk = Content(di) n Content(dl),
Store wptk, T = wptk
Delete (Content(di) n Content(dl)) from
Content(di), Content(dl) +L+ Gk = di, Gk = dl
Delete dl from S
End if
End while
If T = 0
S = di +L+ End if
End if
Else if T ;t� 0
While k &lt;= r AND di o Gk
Compare (Content(wptk), Content(di)) +L+ If Content(wptk) = Content(di)
Delete (Content(wptk) n Content(di)) from
Content(di)
Gk = di
End if +L+ End while
If S ;t� 0 AND di o Gk
While l &lt;= s AND di o Gk
Compare (Content(di),Content(dl)) +L+ If Content(di) = Content(dl)
wptk = Content(di) n Content(dl)
Store wptk, T = wptk
Delete (Content(di) n Content(dl)) from
Content(di), Content(dl) +L+ Gk = di, Gk = dl
Delete dl from S
End if
End while
End if
If di o Gk
S = di +L+ End if
End if
End for
Figure 6. The algorithm for detecting and eliminating the
information contained in Web page templates.
...
&lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;CHID Document +L+ &lt;/TITLE&gt;&lt;/HEAD&gt;
&lt;BODY&gt;
&lt;HR&gt;&lt;H3&gt;&lt;B&gt;&lt;I&gt; 1. Equipos Mas Seguros: Si Te Inyectas +L+ Drogas.
&lt;/I&gt;&lt;/B&gt;&lt;/H3&gt;
&lt;I&gt;&lt;B&gt;Subfile: &lt;/B&gt;&lt;/I&gt;
AIDS Education&lt;BR&gt;
&lt;I&gt;&lt;B&gt;Format (FM): &lt;/B&gt;&lt;/I&gt;
08 - Brochure.
&lt;BR&gt;
...
...
‘CHID Document’, (&lt;HTML&gt;, &lt;HEAD&gt;, &lt;TITLE&gt;), +L+ (&lt;/TITLE&gt;, &lt;/HEAD&gt;, &lt;BODY&gt;, &lt;HR&gt;, &lt;H3&gt;, &lt;B&gt;, +L+ &lt;I&gt;);
‘1. Equipos Mas Seguros: Si Te Inyectas Drogas.’, +L+ (&lt;/TITLE&gt;, &lt;/HEAD&gt;, &lt;BODY&gt;, &lt;HR&gt;, &lt;H3&gt;, &lt;B&gt;, +L+ &lt;I&gt;), (&lt;/I&gt;, &lt;/B&gt;, &lt;/H3&gt;, &lt;I&gt;, &lt;B&gt;);
‘Subfile:’, (&lt;/I&gt;, &lt;/B&gt;, &lt;/H3&gt;, &lt;I&gt;, &lt;B&gt;), (&lt;/B&gt;, &lt;/I&gt;); +L+ ‘AIDS Education’, (&lt;/B&gt;, &lt;/I&gt;), (&lt;BR&gt;, &lt;I&gt;, &lt;B&gt;);
‘Format (FM):’, (&lt;BR&gt;, &lt;I&gt;, &lt;B&gt;), (&lt;/B&gt;, &lt;/I&gt;);
...
4
Similar to the representation for the contents of sampled +L+ documents, the content of a Web page template, wpt, is +L+ represented as Content(wpt) = {txs1, ..., txsq}, where q is the
number of text segments, txsj, 1 ≤ j ≤ q. T represents a set of
templates detected. T = {wpt1, ..., wptr}, where r is the distinct
number of templates, wptk, 1 ≤ k ≤ r. Gk represents a group of
documents generated from wptk. Furthermore, S represents the +L+ sampled documents from which no templates have yet been +L+ detected. Thus, S = {d1, ..., ds}, where s is the number of
temporarily stored document, dl, 1 ≤ l ≤ s.
The process of detecting templates is executed until all sampled +L+ documents are analysed. This results in the identification of one +L+ or more templates. For each template, two or more documents are +L+ assigned to a group associated with the template from which the +L+ documents are generated. Each document contains text segments +L+ that are not found in their respective template. These text +L+ segments are partially related to their queries. In addition to a set +L+ of templates, the content representations of zero or more +L+ documents in which no matched patterns are found are stored.
3.2.3 Extract Query-Related Information
This process analyses a group of documents associated with each +L+ template from which documents are generated. It further identifies +L+ any repeated patterns from the remaining text segments of the +L+ documents in order to extract query-related information.
We compute cosine similarity [14] given in (1) to determine the +L+ similarities between the text segments of different documents that +L+ are associated the template where the documents are generated. +L+ The textual content of each text segment is represented as a vector +L+ of terms with weights. The weight of a term is obtained by its +L+ occurrence in the segment.
from the document content (given in Figure 4) as a result of +L+ eliminating information contained in the Web page template.
3.2.4 Generate Content Summary
Frequencies are computed for the terms extracted from randomly +L+ sampled documents. These summarise the information content of +L+ a database, which we refer to as Content Summary.
Algorithm ExtractQueryInfo
For each (da ∈ Gk)
For each (db ∈ Gk), da ≠ db
Compare (Content(da),Content(db)) +L+ If Content(da) = Content(db)
Delete (Content(da) ∩ Content(db)) from
Content(da), Content(db) +L+ End if
End for
End for
For each (di ∈ Gk)
Extract txm of txsm from Content(di) +L+ End for
For each (dl ∈ S)
Extract txn of txsn from Content(dl) +L+ End for
Figure 7. The algorithm for extracting query-related
information from template-generated documents.
1. Equipos Mas Seguros: Si Te Inyectas Drogas. +L+ AIDS Education
...
t
(	,	)	(	)	(	) 2	(	)
txs txs	tw tw	tw	tw
i	j	ik	jk	ik
=	∗	∗
∑	∑	∑ jk
k=1	k=1
(1)	Figure 8. The query-related information extracted from the
CHID document.
COSINE
t
t
1
=
k
2
.
where txsi and txsj represent two text segments in a document; twik +L+ is the weight of term k in txsi, and twjk is the weight of term k in +L+ txsj . This is only applied to text segments with identical adjacent +L+ tag segments. Two segments are considered to be similar if their +L+ similarity exceeds a threshold value. The threshold value is +L+ determined experimentally.
The algorithm that extracts information relevant to queries is +L+ illustrated in Figure 7. da and db represent the sampled documents
from the database, D, da, db ∈ Gk, where Gk denotes a group of
documents associated with the template, wptk, from which the +L+ documents are generated. txm represents the textual content of a
text segment, txsm, contained in di, di ∈ Gk. txn represents the +L+ textual content of a text segment, txsn, contained in dl, dl ∈ S. S
represents the sampled documents from which no templates are +L+ detected.
The results of the above algorithm extract text segments with +L+ different tag structures. It also extracts text segments that have +L+ identical adjacent tag structures but are significantly different in +L+ their textual contents. Figure 8 shows the information extracted
Previous experiments in [2] demonstrate that a number of +L+ randomly sampled documents (i.e., 300 documents) sufficiently +L+ represent the information content of a database.
In the domain of Hidden Web databases, the inverse document +L+ frequency (idf), used in traditional information retrieval, is not +L+ applicable, since the total number of documents in a database is +L+ often unknown. Therefore, document frequency (df), collection +L+ term frequency (ctf) and average term frequency (avg_tf) initially +L+ used in [2] are applied in this paper. We consider the following +L+ frequencies to compute the content summary of a Hidden Web +L+ database.
•	Document frequency (df): the number of documents in the
collection of documents sampled that contain term t, +L+ where d is the document and f is the frequency
•	Collection term frequency (ctf): the occurrence of a term
in the collection of documents sampled, where c is the +L+ collection, t is the term and f is the frequency
•	Average term frequency (avg_tf): the average frequency
of a term obtained from dividing collection term +L+ frequency by document frequency (i.e., avg_tf = ctf / df)
5
Table 1. 3 Hidden Web databases used in the experiments
Database	URL	Subject	Content	Template
Help Site	www.help-site.com	Computer manuals	Homogeneous	Multiple templates
CHID	www.chid.nih.gov	Healthcare articles	Homogeneous	Single template
Wired News	www.wired.com	General news articles	Heterogeneous	Single template
The content summary of a document database is defined as +L+ follows. Assume that a Hidden Web database, D, is sampled with +L+ N documents. Each sampled document, d, is represented as a +L+ vector of terms and their associated weights [14]. Thus d = (w1, +L+ ..., wm), where wi is the weight of term ti, and m is the number of +L+ distinct terms in d e D, 1 &lt;_ i ≤ m. Each wi is computed using term +L+ frequency metric, avg_tf (i. e., wi = ctfi/dfi). The content summary +L+ is then denoted as CS(D), which is generated from the vectors of +L+ sampled documents. Assume that n is the number of distinct terms +L+ in all sampled documents. CS(D) is, therefore, expressed as a +L+ vector of terms: CS(D)= {w1, ..., wn}, where wi is computed by +L+ adding the weights of ti in the documents sampled from D and +L+ dividing the sum by the number of sampled documents that
contain ti, 1 &lt;_ i ≤ n.
4. EXPERIMENTAL RESULTS
This section reports on a number of experiments conducted to +L+ assess the effectiveness of the 2PS technique in terms of: (i) +L+ detecting Web page templates, and (ii) extracting relevant +L+ information from the documents of a Hidden Web databases +L+ through sampling. The experimental results are compared with +L+ those from query-based sampling (abbreviated as QS). We +L+ compare 2PS with QS as it is a well-established technique and has +L+ also been widely adopted by other relevant studies [5, 10, 11, 15].
Experiments are carried out on three real-world Hidden Web +L+ document databases including Help Site, CHID and Wired News, +L+ which provide information about user manuals, healthcare +L+ archives and news articles, respectively. Table 1 summarises +L+ these databases in terms of their subjects, contents and templates +L+ employed. For instance, Help Site and CHID contain documents +L+ relating to subjects on computing and healthcare, respectively. +L+ Their information contents are homogeneous in nature. By +L+ contrast, Wired News contains articles that relate to different +L+ subjects of interest.
Where the number of templates is concerned, CHID and Wired +L+ News generate documents from one Web page template. Help +L+ Site maintains a collection of documents produced by other +L+ information sources. Subsequently, different Web page templates +L+ are found in Help Site sampled documents.
The experiment conducted using QS initiates the first query to a +L+ database with a frequently used term to obtain a set of sampled +L+ documents. Subsequent query terms are randomly selected from +L+ those contained in the sampled documents. It extracts terms +L+ (including terms contained in Web page templates) and updates +L+ the frequencies after each document is sampled. By contrast, 2PS +L+ initiates the sampling process with a term contained in the search +L+ interface pages of a database. In addition, 2PS analyses the +L+ sampled documents in the second phase in order to extract query- +L+ related information, from which terms and frequencies are +L+ generated.
Experimental results in [2] conclude that QS obtains +L+ approximately 80% of terms from a database, when 300 +L+ documents are sampled and top 4 documents are retrieved for +L+ each query. These two parameters are used to obtain results for +L+ our experiments in which terms and frequencies are generated for +L+ QS and 2PS after 300 documents have been sampled. The results +L+ generated from QS provide the baseline for the experiments.
Three sets of samples are obtained for each database and 300 +L+ documents are retrieved for each sample. First, we manually +L+ examine each set of sampled documents to obtain the number of +L+ Web page templates used to generate the documents. This is then +L+ compared with the number of templates detected by 2PS. The +L+ detection of Web page templates from the sampled documents is +L+ important as this determines whether irrelevant information is +L+ effectively eliminated.
Next, we compare the number of relevant terms (from top 50 +L+ terms) retrieved using 2PS with the number obtained by QS. +L+ Terms are ranked according to their ctf frequencies to determine +L+ their relevancy to the queries. This frequency represents the +L+ occurrences of a term contained in the sampled documents. Ctf +L+ frequencies are used to demonstrate the effectiveness of +L+ extracting query-related information from sampled documents +L+ since the terms extracted from Web page templates are often +L+ ranked with high ctf frequencies.
Table 2. The number of templates employed by databases and
the number detected by 2PS
Databases		Number of templates	
		Employed	Detected
Help Site	Sample 1	17	15
	Sample 2	17	16
	Sample 3	19	17
CHID	Sample 1	1	1
	Sample 2	1	1
	Sample 3	1	1
Wired News	Sample 1	1	1
	Sample 2	1	1
	Sample 3	1	1
Experimental results for QS and 2PS are summarised as follows. +L+ Firstly, Table 2 gives the number of Web page templates +L+ employed by the databases and the number detected by 2PS. It +L+ shows that 2PS effectively identifies the number of templates +L+ found in the sampled documents. However, a small number of +L+ templates are not detected from Help Site. For instance, 2PS does +L+ not detect two of the templates from the first set of sampled +L+ documents, since the two templates are very similar in terms of +L+ content and structure.
6
Table 3 summarises the number of relevant terms (from top 50 +L+ terms ranked according to their ctf frequencies) obtained for the +L+ three databases. These terms are retrieved using 2PS and QS. We +L+ determine the relevancy of a term by examining whether the term +L+ is found in Web page templates. Table 3 gives the number of +L+ retrieved terms that do not appear in Web page templates. The +L+ results show that 2PS obtains more relevant terms. For instance, +L+ in the first set of documents sampled from CHID using 2PS, the +L+ number of relevant terms retrieved is 47. By comparison, the +L+ number of terms obtained for QS is 20.
The results generated from CHID and Wired News demonstrate +L+ that 2PS retrieves more relevant terms, as a large number of terms +L+ contained in the templates have been successfully eliminated from +L+ the top 50 terms. However, the elimination of template terms is +L+ less noticeable for Help Site. Our observation is that template +L+ terms attain high frequencies since the CHID and Wired News +L+ databases generate documents using a single Web page template. +L+ By comparison, a larger number of Web page templates are found +L+ in the documents sampled from Help Site. As a result, terms +L+ contained in the templates do not attain high frequencies as those +L+ found in the templates employed by CHID and Wired News.
Table 4 and 5 show the results of the top 50 terms ranked +L+ according to their ctf frequencies retrieved from the first set of +L+ sampled documents of the CHID database. Table 4 shows the top +L+ 50 terms retrieved for QS whereby terms contained in Web page +L+ templates are not excluded. As a result, a number of terms (such +L+ as ‘author’, ‘language’ and ‘format’) have attained much higher +L+ frequencies. By contrast, Table 5 lists the top 50 terms retrieved +L+ using 2PS. Our technique eliminates terms (such as ‘author’ and +L+ ‘format’) and obtains terms (such as ‘treatment’, ‘disease’ and +L+ ‘immunodeficiency’) in the higher rank.
Table 3. The number of relevant terms retrieved (from top 50
terms) according to ctf frequencies
Databases		Number of relevant terms	
		QS	2PS
Help Site	Sample 1	46	48
	Sample 2	47	48
	Sample 3	46	48
CHID	Sample 1	20	47
	Sample 2	19	47
	Sample 3	20	47
Wired News	Sample 1	14	42
	Sample 2	10	43
	Sample 3	11	39
5. CONCLUSION
This paper presents a sampling and extraction technique, 2PS, +L+ which utilises information that is contained in the search interface +L+ pages and documents of a database in the sampling process. This +L+ technique extracts information relevant to queries from the +L+ sampled documents in order to generate terms and frequencies +L+ with improved accuracy. Experimental results demonstrate that +L+ our technique effectively eliminates information contained in +L+ Web page templates, thus attaining terms and frequencies that are +L+ of a higher degree of relevancy. This can also enhance the +L+ effectiveness of categorisation in which such statistics are used to +L+ represent the information contents of underlying databases.
We obtain promising results by applying 2PS in the experiments +L+ on three databases that differ in nature. However, experiments on +L+ a larger number of Hidden Web databases are required in order to +L+ further assess the effectiveness of the proposed technique.
Table 4. Top 50 terms and frequencies ranked according to ctf generated from CHID when QS is applied
Rank	Term	Rank	Term	Rank	Term
1	hiv	18	document	35	lg
2	aids	19	disease	36	ve
3	information	20	published	37	yr
4	health	21	physical	38	ac
5	prevention	22	subfile	39	corporate
6	education	23	audience	40	mj
7	tb	24	update	41	description
8	accession	25	verification	42	www
9	number	26	major	43	cn
10	author	27	pamphlet	44	pd
11	persons	28	chid	45	english
12	language	29	human	46	national
13	sheet	30	date	47	public
14	format	31	abstract	48	immunodeficiency
15	treatment	32	code	49	virus
16	descriptors	33	ab	50	org
17	availability	34	fm		
7
Table 5. Top 50 terms and frequencies ranked according to ctf generated from CHID when 2PS is applied
Rank	Term	Rank	Term	Rank	Term
1	hiv	18	education	35	testing
2	aids	19	virus	36	programs
3	information	20	org	37	services
4	health	21	notes	38	clinical
5	prevention	22	nt	39	people
6	tb	23	cdc	40	hepatitis
7	persons	24	service	41	community
8	sheet	25	box	42	world
9	treatment	26	research	43	listed
10	disease	27	department	44	professionals
11	human	28	positive	45	training
12	pamphlet	29	tuberculosis	46	diseases
13	www	30	control	47	accession
14	http	31	drug	48	network
15	national	32	discusses	49	general
16	public	33	ill	50	std
17	immunodeficiency	34	organizations		
6. REFERENCES
[1] Arasu, A. and Garcia-Molina, H. Extracting Structured Data +L+ from Web Pages. In Proceedings of the 2003 ACM SIGMOD +L+ International Conference on Management, 2003, 337-348.
[2] Callan, J. and Connell, M. Query-Based Sampling of Text +L+ Databases. ACM Transactions on Information Systems +L+ (TOIS), Vol. 19, No. 2, 2001, 97-130.
[3] Caverlee, J., Buttler, D. and Liu, L. Discovering Objects in +L+ Dynamically-Generated Web Pages. Technical report, +L+ Georgia Institute of Technology, 2003.
[4] Crescenzi, V., Mecca, G. and Merialdo, P. ROADRUNNER: +L+ Towards Automatic Data Extraction from Large Web Sites, +L+ In Proceedings of the 27th International Conference on Very +L+ Large Data Bases (VLDB), 2001, 109-118.
[5] Gravano, L., Ipeirotis, P. G. and Sahami, M. QProber: A +L+ System for Automatic Classification of Hidden-Web +L+ Databases. ACM Transactions on Information Systems +L+ (TOIS), Vol. 21, No. 1, 2003.
[6] Heß, M. and Drobnik, O. Clustering Specialised Web- +L+ databases by Exploiting Hyperlinks. In Proceedings of the +L+ Second Asian Digital Library Conference, 1999.
[7] Hedley, Y.L., Younas, M., James, A. and Sanderson M. +L+ Query-Related Data Extraction of Hidden Web Documents. +L+ In Proceedings of the 27th Annual International ACM SIGIR +L+ Conference, 2004, 558-559.
[8] Lage, J. P., da Silva, A. S., Golgher, P. B. and Laender, A. +L+ H. F. Automatic Generation of Agents for Collecting Hidden
Web Pages for Data Extraction. Data &amp; Knowledge +L+ Engineering, Vol. 49, No. 2, 2004, 177-196.
[9] Liddle, S.W., Yau, S.H. and Embley, D. W. On the +L+ Automatic Extraction of Data from the Hidden Web. In +L+ Proceedings of the 20th International Conference on +L+ Conceptual Modeling, (ER) Workshops, 2001, 212-226.
[ 10] Lin, K.I. and Chen, H. Automatic Information Discovery +L+ from the Invisible Web. International Conference on +L+ Information Technology: Coding and Computing (ITCC), +L+ 2002, 332-337.
[ 11 ] Meng, W., Wang, W., Sun, H. and Yu, C. Concept
Hierarchy Based Text Database Categorization.
International Journal on Knowledge and Information +L+ Systems, Vol. 4, No. 2, 2002, 132-150.
[ 12] Raghavan, S. and Garcia-Molina, H. Crawling the Hidden +L+ Web. In Proceedings of the 27th International Conference on +L+ Very Large Databases (VLDB), 2001, 129-138.
[ 13] Rahardjo, B. and Yap, R. Automatic Information Extraction +L+ from Web Pages, In Proceedings of the 24th Annual +L+ International ACM SIGIR Conference, 2001, 430-431.
[ 14] Salton, G. and McGill, M. Introduction to Modern +L+ Information Retrieval. New York, McCraw-Hill, 1983.
[ 15] Sugiura, A. and Etzioni, O. Query Routing for Web Search +L+ Engines: Architecture and Experiments. In Proceedings of +L+ the 9th International World Wide Web Conference: The +L+ Web: The Next Generation, 2000, 417-430.
8
