An Intensional Approach to the Specification of Test Cases
for Database Applications
David Willmor
School of Computer Science
University of Manchester
Oxford Road, Manchester, UK
d.willmor@cs.manchester.ac.uk +L+ ABSTRACT
When testing database applications, in addition to creating +L+ in-memory fixtures it is also necessary to create an initial +L+ database state that is appropriate for each test case. Cur- +L+ rent approaches either require exact database states to be +L+ specified in advance, or else generate a single initial state +L+ (under guidance from the user) that is intended to be suit- +L+ able for execution of all test cases. The first method allows +L+ large test suites to be executed in batch, but requires con- +L+ siderable programmer effort to create the test cases (and +L+ to maintain them). The second method requires less pro- +L+ grammer effort, but increases the likelihood that test cases +L+ will fail in non-fault situations, due to unexpected changes +L+ to the content of the database. In this paper, we propose a +L+ new approach in which the database states required for test- +L+ ing are specified intensionally, as constrained queries, that +L+ can be used to prepare the database for testing automati- +L+ cally. This technique overcomes the limitations of the other +L+ approaches, and does not appear to impose significant per- +L+ formance overheads.
Categories and Subject Descriptors
D.2.5 [Software Engineering]: Testing and Debugging— +L+ Testing tools
General Terms
Experimentation, Verification
Keywords
databases, software testing, database testing
1. INTRODUCTION
Modern information systems are typically organised as +L+ collections of independent application programs that com- +L+ municate with one another by means of a central database. +L+ The database records the state of the organisation that the
Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee.
ICSE’06, May 20–28, 2006, Shanghai, China.
Copyright 2006 ACM 1-59593-085-X/06/0005 ...$5.00.
Suzanne M. Embury
School of Computer Science
University of Manchester
Oxford Road, Manchester, UK
s.m.embury@cs.manchester.ac.uk
information system supports, while the application programs +L+ implement the business processes that manipulate the state. +L+ To take a simple but ubiquitous example, a database sys- +L+ tem might record details of customers, products and sales, +L+ while the application programs associated with it handle op- +L+ erations such as new product purchases and update of the +L+ product catalogue, as well as supporting decision making +L+ by generating reports regarding the most profitable product +L+ lines, names and addresses of loss-making customers, etc.
In order to test such application programs, it is necessary +L+ to create test fixtures that simulate the presence of the rest +L+ of the information system. Fixtures for traditional test cases +L+ typically consist of in-memory objects and data structures +L+ that provide the inputs to the program being tested. This +L+ kind of fixture is also needed when testing database appli- +L+ cations (especially when performing unit testing); however, +L+ since it is unrealistic (and often incorrect) to execute test +L+ cases against an empty database, we need to create addi- +L+ tional fixture elements within the database itself.
Current practice in the software industry is to maintain +L+ one or more test databases that can be used for testing in- +L+ dividual programs. These databases can be artificially gen- +L+ erated (e.g., using tools such as DBMonster1 and DataFac- +L+ tory2) or they may be subsets of the live database, taken +L+ as a snapshot at some recent point in time. Copies of the +L+ live data sets have the advantage that they are more likely +L+ to be representative of the patterns of data encountered in +L+ practice, while artificial data sets have the advantage that +L+ they can be made to embody specific characteristics (such +L+ as particular data skew patterns or volumes), which may be +L+ useful for load and stress testing.
Both approaches, however, suffer from several disadvan- +L+ tages. The most significant problem occurs when none of +L+ the available test databases are suitable starting points for a +L+ particular test case. For example, suppose a particular test +L+ case executes a program which purges inactive customers, +L+ with the aim of verifying that the business rule forbidding +L+ deletion of customers with negative balances is correctly en- +L+ forced. If none of the test databases contains any inactive +L+ customers with negative balances, then the test case can- +L+ not be executed successfully. For a one-off test run, testing +L+ personnel can choose a database that is close to what is re- +L+ quired, and manually update it so that it is suitable for use +L+ with the test case. But if a complete test suite is to be exe- +L+ cuted (possibly including test cases which themselves make +L+ modifications to the database state) then in the worst case
1http://DBMonster.kernelpanic.pl +L+ 2http://www.quest.com/datafactory
102
this manual intervention will be required in between every +L+ test case execution. This is clearly undesirable if test suites +L+ are large or time-consuming to execute, or if the test suite +L+ is to be run in batch (as in the case of overnight regression +L+ testing, for example).
Current research in testing for database systems proposes +L+ two approaches to this problem. One of these is to include +L+ within the test case description a full (extensional) specifica- +L+ tion of the database state against which it is to be run (and +L+ of the database state that should be produced if the test has +L+ executed successfully) [13, 14]. This solution is exemplified +L+ by DBUnit3, an extension of the JUnit testing framework4 +L+ that is designed for testing database applications written in +L+ Java. Each DBUnit test case is accompanied by an XML +L+ file describing the data set required for the test. Before each +L+ test run, DBUnit clears the database state and inserts the +L+ data described by the XML file.
This approach has the advantage of simplicity, but it places +L+ a considerable burden on testing personnel, especially when +L+ complex database states are required. It is also inefficient, +L+ since the database must be continually destroyed and recre- +L+ ated between tests, even when significant parts of the database +L+ might have been reused by the succeeding tests. Moreover, +L+ maintenance of a large suite of such tests is extremely chal- +L+ lenging, since any small change to the database schema may +L+ require corresponding changes to many test cases.
The second approach that has been explored in the liter- +L+ ature is more efficient in that it requires the creation of only +L+ one database state per test suite (rather than one per test +L+ case). It is exemplified by the AGENDA database testing +L+ toolkit [6, 7], which can automatically generate a database +L+ state given information about the schema, some data gen- +L+ eration functions for individual attributes and some user- +L+ selected heuristics describing the kind of database state re- +L+ quired. The AGENDA tool also generates test cases from a +L+ simple analysis of the program being verified. The user must +L+ then add preconditions to each test case that are checked +L+ just before it is executed and that will prevent a case from +L+ being executed against an inappropriate database state. This +L+ approach successfully relieves the user of the need to specify +L+ complete database states in full detail, but at a cost. The +L+ user must accept that some of the test cases may not be +L+ executed because the database state fails the precondition, +L+ even when it would require only a small change to bring the +L+ database into a suitable state for the test. Since only one +L+ database state is created per test suite, this problem of failed +L+ tests is likely to become more severe as the size of the test +L+ suite grows. There is also a potential inefficiency involved +L+ in generating test descriptions and inputs, and in creating +L+ the additional log tables and constraints/triggers needed by +L+ the AGENDA tool, for test cases that are not in fact going +L+ to be executed.
Ideally, we would prefer to be able to combine the advan- +L+ tages of both these approaches, to give a form of database +L+ test case that is quick and natural to specify, and which +L+ maximises the number of cases within the suite that can be +L+ executed while minimising the number of full test databases +L+ that need to be maintained. Our thesis is that this can +L+ be achieved by allowing testing personnel to describe the +L+ database states involved in their test cases intensionally, in
3http://www.dbunit.org +L+ 4http://www.junit.org
the form of declarative conditions that the input database +L+ must satisfy, and by providing a testing harness that can +L+ automatically adjust the input database so that the test +L+ conditions are satisfied [19].
In this paper, we present a language for specifying such +L+ intensional database tests, and describe its semantics and +L+ operational behaviour (Section 2). We present an algorithm +L+ for automatically modifying database states so that test pre- +L+ conditions are satisfied (Section 3), thus ensuring that all +L+ test cases can be executed without requiring any human +L+ intervention. We further describe how we have extended the +L+ JUnit testing framework to allow intensional database tests +L+ to be specified and executed in practice (Section 4). Finally, +L+ we present the results of an evaluation of the performance +L+ of the techniques (Section 5) and conclude (Section 6).
2. SPECIFYING INTENSIONAL TESTS
A conventional test case is typically modelled as a triple +L+ &lt; p, i, o &gt;, which denotes a test that executes program p +L+ with inputs (e.g., parameters) denoted by i. If no faults are +L+ encountered during the test execution, the output that will +L+ be produced is o. In the case of test cases for database ap- +L+ plications, we must add two further elements—the specifica- +L+ tion of the database state against which p is to be executed, +L+ and some statement of the database state that should result +L+ from the execution of p if it is operating correctly according +L+ to its specification.
For example, consider the example program mentioned +L+ in Section 1 that prunes inactive customer details from the +L+ database. For this test case, we require a database state that +L+ contains at least one inactive customer. This could easily +L+ be stated as a predicate logic condition over the database, +L+ assuming the obvious mapping between stored relations and +L+ predicates, e.g.:
(3custNo, lastOrderOn, a, b, c)
customer(custNo, a, b, c, lastOrderOn) n
lastOrderOn &lt; today — 90
The program in question does not access any parts of the +L+ database other than the customer table. Therefore, we do +L+ not care what values the other tables contain and need not +L+ mention them in the intensional specification of the test.
This approach works equally well for observing the results +L+ of the test. For example, when testing the customer pruning +L+ behaviour, we might require that no inactive customer with +L+ a non-negative balance should exist in the database after +L+ the test:
-((3custNum, lastOrderDate, a, b, c)
customer(custNum, a, bal, c, lastOrderDate) n
lastOrderDate &lt; today — 90 n bal &gt; 0)
Effectively, the test case describes a set of valid (i.e., fault- +L+ free) state transition for the database, as a classic pre/post- +L+ condition pair.
This first-order-logic style of database specification does +L+ not work so well when we consider the testing problem in +L+ more depth, however. The problem is that we need to do +L+ more than test the input database for compliance with the +L+ requirements of the test case; we also need to extract in- +L+ formation from it to be used to instantiate other elements
103
of the test case. For example, suppose we wish to test a +L+ program that deletes details of individual customers. Such +L+ programs typically require some input from the user, identi- +L+ fying the specific customer record that is to be deleted (e.g., +L+ by supplying the relevant customer code as a parameter). +L+ This could be achieved by requiring the tester to embed the +L+ customer code into the test case elements, as literal values. +L+ Alternatively, we could search for a suitable customer that +L+ already exists in the database, using a standard database +L+ query, and use the values from that in specifying the inputs +L+ for the test case. This would minimise the amount of work +L+ required to prepare the database for test execution (since we +L+ would be using data already present in the database), and it +L+ would also mean that test cases can be written very quickly, +L+ since the user does not need to specify every last detail of +L+ the data to be used.
Under this approach, the specification of the input database +L+ state now has a dual role: it must state the condition that +L+ determines whether the database state is suitable for execu- +L+ tion of the test case and it must also return bindings for the +L+ free variables that appear in the remaining components of +L+ the test case. For the latter purpose, we would prefer to use +L+ a straightforward query language, while for the former we +L+ require the ability to place conditions on the data. With a +L+ simple extension of a standard query language such as SQL, +L+ we can combine both these purposes in a single statement. +L+ For example, the following statement:
ANY :cn GENERATED BY
SELECT custNo FROM customer
WHERE lastOrderDate &lt; today() - 90 +L+ AND balance &lt; 0
retrieves the customer code of some record that meets the +L+ given conditions (an inactive customer with negative bal- +L+ ance) from the database, and binds it to the variable : cn. +L+ It also places a cardinality constraint on the result of the +L+ query, that at least one such binding must exist (implied by +L+ the use of the keyword ANY).
The variable : cn can then be used to specify other ele- +L+ ments of the test case. The obvious usage in this example is +L+ in specifying the inputs to the program being tested, but it +L+ can also be used in describing the expected outputs of the +L+ program. In this example test case, the correct behaviour +L+ of the DeleteCustomer program is to reject the deletion +L+ of : cn, since customers with a negative balance cannot be +L+ purged from the database. We might therefore give the fol- +L+ lowing specification of the desired output database state:
AT LEAST 1 :cn2 GENERATED BY +L+ SELECT custNo FROM customer +L+ WHERE custNo = :cn
Of course, not all test cases are best specified in terms of +L+ values retrieved from the database. For example, suppose +L+ that we wish to write test cases for a program that adds new +L+ customers to the database. The inputs to this program are +L+ the details of the new customer, and the precondition for one +L+ particular test case states that no customer should exist that +L+ has the same customer code as that of the customer being +L+ created. We cannot retrieve the customer details from the +L+ database in this case, as they have not yet been stored in it. +L+ Again, we could force the user to include the required values +L+ as literals in the test case, but ideally we would like to give
&lt;CONDITION&gt;::= &lt;TYPE&gt; &lt;BINDINGLIST&gt;
GENERATED BY &lt;SELECT&gt; +L+ &lt;TYPE&gt;::= ANY I NO I AT LEAST &lt;i&gt; I
AT MOST &lt;i&gt; EXACTLY &lt;i&gt; |
ALL I FIRST
&lt;i&gt;::= {0-9}
&lt;BINDINGLIST&gt;
::=&lt;BINDING&gt; { ‘,’ &lt;BINDINGLIST&gt; } +L+ &lt;BINDING&gt;::= {A-Z I a-z}
&lt;SELECT&gt; ::= ...
Figure 1: Simplified BNF Grammar for SQL Exten- +L+ sions
more support to the process of test case generation. One +L+ way to achieve this is to allow user-defined data generator +L+ functions to be incorporated within queries as though they +L+ were relations. For example, the following expression states +L+ our requirements for this test case, while also binding the +L+ variables needed for input to the program:
ANY :cn, :name, :addr, :bal GENERATED BY +L+ SELECT gc.custno, gc.name, gc.addr, 0 +L+ FROM genCustomerDetails() AS gc
WHERE gc.custno NOT IN (
SELECT custno +L+ FROM customer +L+ WHERE balance &gt; 0)
Here, the data generator function getCustomerDetails ( ) +L+ is used as if it were a normal relation, whereas in fact the +L+ results it returns are computed on the fly. In fact, several +L+ of the main commercial database management systems al- +L+ ready allow user-defined functions to be embedded in queries +L+ in this way, so this does not require a further extension of +L+ SQL. Figure 1 shows the minimal extensions that are needed +L+ to support all the kinds of constrained query shown above +L+ using the SQL99 standard [17].
2.1 Test Case Semantics
Clearly, the semantics of these intensional database test +L+ cases is more complex than for traditional extensional tests. +L+ However, we can define their semantics formally in terms +L+ of a mapping from intensional tests to sets of equivalent +L+ extensional database test cases. We first present a formal +L+ definition of the structure of our intensional test cases:
DefInItIOn 1. An intensional database test case is a quin- +L+ tuple &lt; p, i, DBi, o, DBo &gt;, where:
•	p is the program to be executed in the test,
•	i is a tuple of n variables and literals that describes the +L+ inputs to be given to program p, where n is the number +L+ of parameters expected by p,
•	DBi is a set of constrained queries that together specify +L+ the initial database state.
•	o is a tuple of m variables and literal that describes the +L+ expected outputs from the program p.
•	DBo is a set of constrained queries that together specify +L+ the conditions that must hold in the database state after +L+ execution of p if no fault has been encountered.
104
A constrained query has the form &lt; Q, min, max, vars &gt;, +L+ where Q is a standard relational algebra query, min and +L+ max describe the constraints on the cardinality of the query +L+ result set, and vars is the list of variables bound by the +L+ query result.
A database test case is well-formed for use with a partic- +L+ ular database schema Σ iff:
9 for every variable v that occurs free in i, DBi, o and +L+ DBo, there exists a query in DBi that provides a bind- +L+ ing for v,
9 for every query &lt; q, n, m, vs &gt; in DBi U DBo, q is a +L+ well-formed query over Σ that returns k-tuples, where +L+ IvsI = k, and
9 there are no circular variable dependencies amongst +L+ the queries in DBi.
We can now define a semantics for the intensional database +L+ test cases as follows. Every intensional test case is equivalent +L+ to a set of extensional test cases. An extensional test case +L+ defines a specific test run, in terms of actual inputs and +L+ outputs, rather than expressions denoting sets of inputs and +L+ outputs. The set of all possible extensional test cases is +L+ given by:
PxGnxDBxGxDB
where P is the set of all programs, G is the set of all lit- +L+ erals, Gn is the set of all n-tuples formed from G and DB +L+ is the set of all database states (relative to all schemas)5. +L+ The components of each extensional test are the program +L+ to be tested, the input values, the initial database state, +L+ the expected output and the expected final database state, +L+ respectively.
An intensional test case is effectively a shorthand expres- +L+ sion for a set of extensional test cases that are all derived +L+ from the same equivalence partition of the test case inputs. +L+ An intensional database test &lt; p, i, DBi, o, DBo &gt;, where +L+ DBi = {&lt; qi, ni, mi, vi &gt;} and DBo = {&lt; qo, no, mo, vo &gt;}, +L+ is equivalent to the following set of extensional tests:
{&lt; p, i[vi/v], dbi, o[vi/v], dbo &gt; I
dbi E DB n
(ni &lt;_ Iqi(dbi)I &lt;_ mi) n
v E qi(dbi) n
dbo E DB n
(no &lt;_ I (qo [vi /v])(dbo)I &lt;_ mo)}
We use the notation exp[01/02] to express the substitution of +L+ the values in 01 by the corresponding values in 02 whereever +L+ they occur in exp. Therefore, this expression denotes the set +L+ of extensional tests where the input database satisfies the +L+ constraints imposed by the initial constrained query, and +L+ where the bindings from execution of that query (here ex- +L+ pressed as the tuple of variables v) are substituted into the
5For simplicity of presentation, we assume that all programs +L+ require the same number of inputs (n). In practice, n can +L+ be the largest number of inputs required by any program, +L+ and the unused values can be filled with nulls.
expressions defining the inputs, expected output and ex- +L+ pected final database state before they too are evaluated 6.
The idea underlying this notion of an intensional test is +L+ that when any of its corresponding extensional sets are ex- +L+ ecuted, the intensional test is itself deemed to have been +L+ executed. Thus, the use of intensional tests allows much +L+ greater freedom at test execution time, since we may choose +L+ any of the possible extensional tests, depending on which is +L+ closest to our starting environment. In the next section, we +L+ will consider the practical ramifications of this approach to +L+ testing, and describe how the semantics just described can +L+ be implemented in practice.
3. DATABASE PREPARATION
The execution of an intensional database test case con- +L+ sists of three distinct phases: 1) preparation of the environ- +L+ ment for test execution; 2) execution of the test with the +L+ prepared inputs; and 3) capture and storage of the results, +L+ for later analysis. Since all the work of finding bindings +L+ for the variables in the test case specification is done in the +L+ preparation phase, the final two phases are straightforward +L+ and differ little from standard testing procedures. When +L+ program execution is complete, the constrained query that +L+ determines whether the test has been successful or not is +L+ evaluated against the database, and the output from the +L+ program is checked against what is expected. In the case +L+ of test failure, the details of the actual extensional test that +L+ was executed are recorded, for diagnosis purposes.
The first phase, however, is more complex. If we were +L+ content to execute only those test cases which happen to +L+ be suitable for use with the initial database state, then the +L+ preparation phase would simply be a matter of executing +L+ the input constrained queries against the database and, if +L+ they are all successful, using the bindings thus produced +L+ to instantiate the remaining components of the test case. +L+ However, thanks to the declarative nature of our test case +L+ specifications, the testing framework can be pro-active in +L+ cases where the given database is not suitable for use by +L+ the test case, and can automatically generate a sequence of +L+ updates that will cause the constrained queries to produce +L+ the required number of bindings.
In fact, this problem is similar (though not identical) to +L+ one that has been studied by the database and artificial in- +L+ telligence communities for many years. It is known variously +L+ as the view update problem [9], the knowledge base update +L+ problem [12], and the transaction repair problem [10]. Many +L+ database systems have the capability to define views on top +L+ of the basic database. A view is a kind of virtual relation. +L+ To the user, it appears to be a normal relation, but it con- +L+ tains no stored data. Instead, the contents of the view are +L+ defined by a expression over other relations, and attempts +L+ to retrieve data from the view are converted into queries +L+ over these relations. To take a simple example for illustra- +L+ tion, we might create a view called Debtors which appears +L+ to be a relation of the same name containing all customers +L+ with a negative balance. Attempts to retrieve Debtors is
6For simplicity of presentation, we assume here that there +L+ is only one query in each of DBi and DBo. In practice, +L+ it may be necessary to include several queries, each pro- +L+ ducing different bindings and imposing different cardinality +L+ constraints. In this case, the constraints must be conjoined, +L+ and the full set of bindings can be retrieved by performing +L+ a natural join of all the queries, with join condition true.
105
converted into a query against the customer table with an +L+ added constraint on the balance.
If views are truly to act as normal relations then it should +L+ be possible to update them as well query them. But what +L+ does it mean to update a virtual relation? In this case, the +L+ view update must be converted into a sequence of updates +L+ on the stored relations that will cause the desired change in +L+ the contents of the view itself. This is a non-trivial problem +L+ for realistic view languages, and becomes even more difficult +L+ when we move into the context of knowledge bases, where +L+ virtual relations can be defined using rules over other rela- +L+ tions, and when we add integrity constraints that must be +L+ maintained by all updates [1, 2, 3, 4, 5, 8, 11].
Only in very narrow circumstances does a view update +L+ have a single translation into real updates [15, 18]. Various +L+ heuristics for selecting from amongst the possible transla- +L+ tions have been proposed (of which the most common is to +L+ choose the update that results in the smallest change to the +L+ existing data set [2]), but in real applications user input is +L+ needed in order to identify the translation that corresponds +L+ most closely to the real world state that the database should +L+ reflect [10].
In the case of intensional database tests, we have a query +L+ (the constrained query that describes our requirements for +L+ the test) that does not produce the correct number of an- +L+ swers when executed against the test database. We need to +L+ find a sequence of updates to the base data that will cause +L+ our query to produce the number of answers we need. How- +L+ ever, in this case, there is no requirement to find the set of +L+ updates that matches the state of reality — any sensible up- +L+ date that satisfies the query conditions will be acceptable. +L+ This simplifies the problem considerably, removing the need +L+ for complex search procedures and for any user input.
3.1 The Preparation Algorithm
One of the advantages of using a query-based language +L+ for test specification (as opposed to a predicate calculus- +L+ based language) is that we can make use of a very common +L+ and easy-to-analyse internal form for (relational) database +L+ queries, called relational algebra. This form provides a small +L+ number of operations on relations that can be combined to +L+ form complex queries. For example, the three most basic +L+ (and useful) relational algebra operators are:
9 The projection operator, πAttsR, which creates a re- +L+ lation from R by deleting all attributes not in Atts. +L+ For example, π[Country]Customer produces a relation +L+ that contains just the countries that appear in the +L+ Customer relation.
9 The selection operator, QcR, which creates a relation +L+ that contains all the rows from relation R that satisfy +L+ the condition c. For example, Qbal&lt;0 Customer returns +L+ a relation containing details of all customers with neg- +L+ ative balances.
9 The join operator, R ✶c S, which creates a relation +L+ containing rows from the cross product of R and S that +L+ satisfy the join condition c. The query Debtor ✶dNo=WNo +L+ Inactive returns details of all debtors who are also in- +L+ active.
Since the result of each relational algebra operator is itself +L+ a relation, together they form a closed algebra. This means
that we can form arbitrarily complex queries by applying +L+ operators to the results of other operators. For example, a +L+ query which retrieves the customer number of all customers +L+ with a negative balance would be written as:
π[custNo] (Qbalance&lt;0 Customer)
A common way to visualise such expressions is as a tree of +L+ operators. The tree for the above query is shown in Figure 2.
Figure 2: Relational Algebra Tree for Negative Bal- +L+ ance Query.
Our algorithm for preparing a database for testing is based +L+ around this notion of a relational algebra tree. We take the +L+ cardinality constraints from the test specification, and push +L+ them down through the nodes of the input database query +L+ tree, collecting up additional conditions as we go. When we +L+ reach a leaf node (i.e. a base relation), we make updates +L+ to the database so that the pushed-down constraints are +L+ satisfied for that relation.
At each stage, we collect up the different kinds of con- +L+ straint and push them further down into the tree. These +L+ constraint types are:
9 Min and Max, the upper and lower bounds on the de- +L+ sired cardinality of the result set.
9 SelC, the selection conditions on the relations that we +L+ are interested in.
9 UAtts, the collection of attributes that are used in the +L+ constrained query, and that must be populated in any +L+ new data that we insert.
We also build up a collection of queries that describe the +L+ data that has been prepared for testing so far, as we progress +L+ through the tree. We call these queries “bindings” (Bgs), +L+ since they give us values for the variables that occur within +L+ the selection and join conditions. At each stage, the bindings +L+ should contain one query for each leaf node that has so far +L+ been prepared.
It is easiest to see how this works by considering a simple +L+ example, such as that shown in Figure 2. Let us assume we +L+ have a constrained query that requires at least one customer +L+ with negative balance to exist, and that our database does +L+ not currently contain any such customers. We begin at the +L+ root node of the tree, with only the cardinality constraints +L+ extracted from the test specification:
Min = 1, Max = null, SelC = true,
UAtts = 0, Bgs = 0
The top node is a projection operator. Projection does not +L+ affect the cardinality of the result set, nor impose any condi- +L+ tions, but it does tell us something about the attributes used
106
Figure 3: Relational Algebra Tree Showing Multiple +L+ Joins
by the query. We therefore add the projection attributes to +L+ UAtts and push the constraints down to the next node:
Min = 1, Max = null, SelC = true,
UAtts = {custNo}, Bgs = 0
Next we must deal with the selection node. Selection nodes +L+ reduce the cardinality of their input, so we need to push +L+ down the selection conditions to ensure that any updates +L+ we may make affect the correct tuples. We also need to add +L+ any attributes appearing in the selection condition to UAtts:
Min = 1, Max = null, SelC = balance &lt; 0,
UAtts = {custNo, balance}, Bgs = 0
The final node is the leaf node, representing the Customer +L+ relation. We construct a query from the conditions on that +L+ relation and execute it, to find out how many answers are +L+ currently in the database. In this case, there are none, so +L+ we need to insert a new Customer record with at least +L+ the custNo and balance attributes populated, and with +L+ a negative balance. If there are any integrity constraints +L+ on this relation, then we need to make sure they are also +L+ satisfied by the new data.
We use the DBMonster data generator mentioned earlier +L+ to create the new data. It allows generation functions to +L+ be specified for attributes, and additional constraints to be +L+ placed on them. It will also maintain primary key, foreign +L+ key, non-null and domain constraints if configured appro- +L+ priately using the information present in the pushed-down +L+ constraints.
Of course, this is a very simple example. In general, we +L+ can expect to have to deal with more complicated queries +L+ involving several joins, such as that shown in Figure 3. This +L+ relational algebra tree is equivalent to the following con- +L+ strained query:
ANY :orderNo, :productNo GENERATED BY +L+ SELECT o.orderno, p.productno
FROM Order o, Orderdetail d, Product p +L+ WHERE o.orderno = d.orderno AND
d.productno = p.productno AND +L+ p.price &gt; 50
which requires that at least one order must exist that in- +L+ volves the purchase of at least one product that costs more
than £50. Joins complicate the process of preparing the +L+ database, because they introduce dependencies between the +L+ updates that take place at different leaf nodes. For example, +L+ imagine that we have processed the tree shown in Figure 3 as +L+ far as the leaf node representing the OrderDetail relation. +L+ Join operators further constrain the selection condition (by +L+ conjoining in their join condition), but add no other con- +L+ straints. So, by the time we reach this leaf node, SelC will +L+ have been set to:
o.orderno = d.orderno A d.productno = p.productno
We need to find out whether a suitable OrderDetail record +L+ exists within the database. However, in order to do this, +L+ we need to know something about what preparation actions +L+ were performed when the Product leaf node was processed. +L+ Maybe there were already plenty of £50-plus products in +L+ the catalogue, or maybe there were none and one had to +L+ be created. How is this information passed through to the +L+ OrderDetail node so that the correct tuple can be identi- +L+ fied or created?
In the current version of our algorithm, we have chosen +L+ to use the database itself to communicate these values. If +L+ there are many suitable Product records, then we can find +L+ one by querying the database directly once again. If a new +L+ product had to be created, then it will now be present in +L+ the database, so we can still retrieve it by querying. The +L+ information needed to construct these queries is present in +L+ the selection conditions that have been considered during +L+ the processing of the relational algebra tree up to this point. +L+ For example, in order to search for an OrderDetail tuple +L+ that is connected to a suitable Product, we need to issue +L+ the following query:
SELECT d.* FROM OrderDetail d, Product p +L+ WHERE d.productno = p.productno AND +L+ p.price &gt; 50
This query cannot be constructed from only the constraints +L+ pushed-down from the parent nodes of the leaf node; instead, +L+ we need to collect up the constraints imposed by all nodes +L+ visited before the current node, so that they are available for +L+ query formation. This is done using the Bgs data structure +L+ mentioned earlier.
Figure 4 presents the complete algorithm, showing the be- +L+ haviour required for each different type of operator. The al- +L+ gorithm is presented as a side-effecting function which takes +L+ the constrained query that is to be satisfied by the database, +L+ and a set of initial conditions that state the required cardi- +L+ nality bounds and initialise SelC to true, UAtts to 0 and Bgs +L+ to 0. The function returns a set of bindings, but these are +L+ discarded. The main task of the algorithm is carried out +L+ by the side-effecting updates that occur when leaf nodes are +L+ processed.
4. DOT-UNIT TESTING FRAMEWORK
The intensional database test language and accompanying +L+ preparation algorithm have been implemented within a test- +L+ ing tool, called DOT- Unit. This tool is part of a larger Data- +L+ Oriented Testing7 framework that is under development at +L+ the University of Manchester [20]. DOT-Unit has been im- +L+ plemented as an extension to the JUnit testing framework
7http://www.cs.man.ac.uk/—willmord/dot/
107
Projection operator
prepare(irattsQ, Min, Max, UAtts, SelC, Bgs)
= prepare(Q, Min, Max, UAtts U Atts, SelC, Bgs)
Selection operator
prepare(acQ, Min, Max, UAtts, SelC, Bgs)
= prepare(Q, Min, Max, UAtts, SelC n c, Bgs)
Join operator
prepare(Q1 ✶jc Q2, Min, Max, UAtts, SelC, Bgs) +L+ = prepare(Q2, Min, Max, UAtts, SelC n jc, +L+ prepare(Q1, Min, Max, UAtts, SelC, Bgs))
Relation (leaf node)
prepare(Rasv, Min, Max, UAtts, SelC, Bgs) +L+ Q = bindingQuery(v, SelC, Bgs)
Execute Q to produce result set RS
if IRSI &lt; Min then
Invoke DBMonster to create (Min - IRSI) more +L+ instances of R that satisfy the conditions in Q +L+ else if IRSI &gt; Max then
Delete the first (IRSI - Max) tuples in RS +L+ else
No preparation updates needed
return (Bgs U binding(v, Q))
Figure 4: The Database Preparation Algorithm
for the unit testing of Java applications [16]. We have sub- +L+ classed the standard JUnit TestCase class, to create a ded- +L+ icated DatabaseTestCase class for specifying and man- +L+ aging intensional database tests. DatabaseTestCase pro- +L+ vides facilities for specifying pre-conditions on database state, +L+ generating and manipulating the bindings that are produced +L+ by such pre-conditions, and evaluating post-conditions on +L+ the database state after the test has been completed. The +L+ standard JUnit methods for determining the results of test +L+ execution on the in-memory fixture can also be used.
Figure 5 shows an example DatabaseTestCase that in- +L+ cludes two individual tests. The first verifies that when a +L+ customer with a non-negative balance is deleted, all cus- +L+ tomers with that customer number really do disappear from +L+ the database. The second uses a data generation function to +L+ propose attribute values for a new customer record (includ- +L+ ing a unique customer number), and checks that after the +L+ program has executed only one customer with the generated +L+ customer number exists.
We use a prefixed colon to indicate variables that are +L+ shared amongst the test components — a notation that will +L+ be familiar to many database programmers, since it is com- +L+ monly used in various forms of embedded SQL. The shared +L+ variables acquire their values when the test harness evalu- +L+ ates the precondition (and performs any necessary database +L+ preparation steps). These values can then be accessed us- +L+ ing the binding method, and can be used in arbitrarily +L+ complex assert conditions, as well as in instantiating the +L+ post-condition query.
One of the main advantages of using the JUnit framework +L+ as the basis for the implementation of DOT-Unit is that it +L+ allows us to integrate our tool seamlessly into existing de- +L+ velopment environments, such as Eclipse8. Thus, DOT-Unit +L+ tests are executed in exactly the same way as a standard JU- +L+ nit test case, and the results are displayed using the same +L+ interface components. This allows testing of database and +L+ non-database components to be interleaved in a convenient +L+ and natural manner.
8http://www.eclipse.org
5. EVALUATION
The practicality of this intensional test case approach de- +L+ pends largely on the performance overhead imposed by the +L+ database preparation algorithm. If the time required to ex- +L+ ecute each individual test case is significantly higher using +L+ our approach than with DBUnit, say, then fewer tests will +L+ be able to be executed in the time available and the benefits +L+ of faster test development and fewer spurious test failures +L+ will be negated.
To gain a handle on the degree of performance overhead +L+ to be expected from DOT-Unit, we made use of an exist- +L+ ing extensional DB test suite that we created for earlier +L+ work [20]. This suite was designed for mp3cd browser9, an +L+ open-source Java/JDBC program that stories information +L+ about mp3 files in a MySQL 5.0 database10. The schema +L+ of the database consists of 6 relations with 22 attributes, 7 +L+ primary key constraints and 6 foreign key constraints. We +L+ created an equivalent intensional test suite, consisting of 20 +L+ test cases, from the extensional suite by converting each test +L+ case into DOT-Unit pre- and post-conditions. We also re- +L+ placed each hard-coded test parameter in the original tests +L+ into constrained query bindings.
We wanted to investigate two specific aspects of the per- +L+ formance of DOT-Unit. First, we wanted to compare its +L+ performance with that of DBUnit over the equivalent test +L+ cases as the database size grows. Second, we wanted to gain +L+ some idea of what aspects of DB preparation and testing +L+ were dominating the performance of DOT-Unit. The re- +L+ sults of the experiments we performed are presented below. +L+ All experiments were run on a Pentium-M 2.0GHz machine, +L+ with 1Gb RAM, running Ubuntu Linux.
5.1 Comparison with DBUnit
At first sight, the extensional approach, as exemplified +L+ by DBUnit, would seem to be the more efficient method +L+ of the two, as the testing harness does not need to spend +L+ any time figuring out what updates need to be made prior +L+ to each test—it only needs to execute them. This does
9http://mp3cdbrowser.sourceforge.net/mp3cd/ +L+ 10http://www.mysql.com
108
public class ProgramTest extends DatabaseTestCase {
public void testDeleteCustomer() {
preCondition(&quot;ANY :cn GENERATED BY SELECT custNo FROM customer WHERE balance &gt; 0;&quot;);
Program p = new Program(); +L+ p.deleteCustomer(binding(&quot;:cn&quot;));
postCondition(&quot;NO :cn2 GENERATED BY SELECT custno FROM customer WHERE custNo = :cn;&quot;);
}
public void testNewCustomer() {
preCondition(&quot;ANY :cn, :name, :addr GENERATED BY SELECT gc.custNo, gc.name, gc.addr FROM +L+ genCustomerDetails() AS gc WHERE gc.custNo NOT IN (SELECT custNo FROM customer);&quot;);
Program p = new Program();
boolean b = p.newCustomer(binding(&quot;:cn&quot;), binding(&quot;:name&quot;), binding(&quot;:addr&quot;));
assertTrue(b);
postCondition(&quot;EXACTLY 1 :cn, :name, :addr GENERATED BY SELECT custno, name, addr
FROM customer;&quot;);
}
}
Figure 5: Example DOT-Unit Test Case
not happen by accident, but because a human programmer +L+ has spent time earlier, deciding exactly what the database +L+ should look like for each test case. However, when writing +L+ DBUnit tests, it is common to try to reuse database de- +L+ scriptions for multiple test cases where possible, to reduce +L+ the amount of programming and maintenance time. In this +L+ case, some redundant updates will be made before each test +L+ case - updates that our extensional approach will not bother +L+ to make. It is also the case that DBUnit makes its updates +L+ blindly, whether they are needed or not, whereas the inten- +L+ sional approach will be able to reuse much of the existing +L+ database state for each new test case.
Given this, it seems likely that the performance of DBUnit +L+ will be better when the database state required for each +L+ test case is relatively small, but that the situation will be +L+ reversed when the database state grows much larger. In +L+ order to gauge the point at which this change occurs, we +L+ ran our two test suites (extensional and intensional) with +L+ databases of varying sizes, and measured the execution time +L+ taken to execute the whole test suite.
In each case, we generated initial database states of vary- +L+ ing sizes at random - either populating the database directly +L+ (for the intensional test cases) or generating XML descrip- +L+ tions of the required state (for the extensional test cases). +L+ The results are shown in Figure 6.
Figure 6: Comparison of Approaches as DB Size +L+ Increases
To our surprise, although the performance of DOT-Unit was +L+ initially worse than that of DBUnit, it overtook its com- +L+ petitor at a comparatively small database size of around 20 +L+ tuples per relation. Obviously, this experiment is a little +L+ unfair to DBUnit, since programmers are unlikely to create +L+ database descriptions consisting of 1000s of tuples per re- +L+ lation. However, tests of this scale will be needed at some +L+ point in the development cycle, in order to verify the be- +L+ haviour of the system on more realistic data sets.
In order to assess the behaviour of DOT-Unit more pre- +L+ cisely, consider the graph in Figure 7, which shows the re- +L+ sults at small databases sizes in more detail. It can be ob- +L+ served that the performance of DOT-Unit first improves and +L+ then begins to degrade again at a database size of around +L+ 50 tuples per relation.
Figure 7: Detailed Comparison of Approaches
One possible explanation for this initial improvement in per- +L+ formance is that, as the database size rises, so does the +L+ probability that the data needed for the test case is al- +L+ ready present in the database. For the very small states, +L+ a lot of preparation work is required to create the needed +L+ data, whereas less work is needed for a more fully populated +L+ database. As the database size increases further, however, +L+ the costs of making the queries needed to test the precondi- +L+ tions and formulate the preparation updates rises, pushing +L+ up the time required for the entire preparation step. This
109
behaviour may be a peculiarity of the particular test suite +L+ used, of course, and further, more extensive studies will be +L+ required in order to completely characterise the performance +L+ of the DOT-Unit test harness.
From these initial results, however, DOT-Unit appears to +L+ scale well relative to database size, and the execution times +L+ are of the same order of magnitude as those resulting from +L+ DBUnit. This suggests that the intensional approach may +L+ provide a good compromise between saving expensive pro- +L+ grammer time in developing new test cases and expenditure +L+ of cheaper processing time in executing the test cases.
5.2 Effect of Constraint Complexity
A further concern was the effect of increasing constraint +L+ complexity on the performance of DOT-Unit test cases. How +L+ much additional overhead is added for conditions involving +L+ a higher number of selection conditions and (most impor- +L+ tantly) joins? In order to assess this, we grouped the test +L+ cases into three groups, according to their complexity:
9 A: queries with one or more selections and no joins,
9 B: queries with one or more selections and a join be- +L+ tween two relations,
9 C: queries with one or more selections and joins be- +L+ tween three relations.
This gave a test suite with 5 test cases in each of these +L+ categories, which we executed against a randomly generated +L+ database state with 500 tuples per relation that does not +L+ satisfy any of the test case pre-conditions. Figure 8 shows +L+ the results obtained for the three complexity categories. We +L+ measured the average time taken to execute the test cases +L+ in each category, including a breakdown of where the time +L+ is spent in each case:
9 Test: the time required to execute the procedural as- +L+ pects of the test case;
9 Query: the time required to execute the query aspect +L+ of the test case condition;
9 Prepare the time required to execute the preparation +L+ aspect of the test case condition.
While the overall time required to execute the test cases rises +L+ as the complexity rises (unsurprisingly), the relative propor- +L+ tions of time spent in the various phases remains roughly the +L+ same. The preparation phase seems to account for slightly +L+ more than half of the time in each case, indicating that sig- +L+ nificant improvements could be achieved with a less-naive +L+ preparation algorithm.
6. CONCLUSIONS
We have presented a new approach to the specification +L+ of test cases for database systems that attempts to reduce +L+ the amount of manual intervention required in between test +L+ case runs while also minimising the number of spurious test +L+ failures due to inappropriate input database states. The ap- +L+ proach has the further advantage that it sits naturally on top +L+ of test data sets taken from live databases, and this allows +L+ testing to be carried out using realistic data sets without re- +L+ quiring significant programmer effort to tailor the data set to +L+ the test cases. In effect, the intensional approach we have
Figure 8: The Affect of Changing Constraint Com- +L+ plexity
described allows software developers to trade programmer +L+ time for test execution time
Our experience has indicated that intensional test cases +L+ are quick and natural to write for anyone who is familiar +L+ with SQL and database programming, although a study +L+ with an independent testing team would be necessary be- +L+ fore we can make any strong claims in this regard. How- +L+ ever, compared with what is involved in writing pure JDBC +L+ database test cases and DBUnit test cases, we found that +L+ the self-contained nature of the intensional test cases was a +L+ definite advantage. Writing DBUnit test cases requires the +L+ programmer to continually check that the test case is com- +L+ patible with the database description. Moreover, since it is +L+ common to try to reuse database descriptions for multiple +L+ test cases by combining their requirements into one database +L+ state, it becomes very easy to break one test case by chang- +L+ ing the database description in order to ready it for another. +L+ These problems do not arise with intensional testing, since +L+ all the information about the test case is present in a single +L+ file (the Java class file).
We designed this first version of the preparation algorithm +L+ for simplicity and correctness rather than efficiency, and as +L+ such it performs rather stupidly in many cases. We are cur- +L+ rently exploring options for improving the algorithm, includ- +L+ ing more intelligent selection of the order in which the rela- +L+ tional algebra tree is traversed, alternating between passing +L+ query bindings and passing literal value bindings as is most +L+ efficient, and making use of modifications to existing tuples +L+ as well as simply adding and deleting tuples (both of which +L+ are comparatively expensive operations). The complexity of +L+ the conditions we can handle is at present limited by the +L+ capabilities of DBMonster, and can be expanded by devel- +L+ opment of a custom data generation facility. We also need +L+ to expand the range of queries that can be handled, beyond +L+ simple select-project-join queries. For example, standard +L+ SQL also allows aggregation and ordering within queries— +L+ both of which offer challenges in terms of automatic prepa- +L+ ration.
A further problem with our current algorithm is that it +L+ may sometimes fail to find a solution to the database prepa- +L+ ration problem, even though one exists. This is due to the +L+ fact that updates are made at leaf nodes before the full set of +L+ constraints on those nodes has been encountered. It should
110
be possible to address the problem with more sophisticated +L+ querying techniques (this is an example of a fairly standard +L+ constrained search problem, after all), although this will add +L+ to the performance overhead. A thorough study of the trade- +L+ offs between spurious failures and more intelligent searching +L+ will need to be carried out before any concrete recommen- +L+ dations can be made.
Finally, we note that where it is important to test large +L+ numbers of frame constraints (i.e. aspects of the original +L+ database state that are not affected by the execution of the +L+ program under test), it may be easier to express the test case +L+ using DBUnit, rather than cluttering up the intensional test +L+ with many such constraints.
Our work presents a number of possible avenues for future +L+ work beyond the improvements mentioned above, of which +L+ the most urgent is the question of ordering of test cases +L+ within suites. This ordering can be in terms of reducing the +L+ cost of the modifications to database state or to maximise +L+ fault coverage. There is also the question of whether the +L+ modifications to database state should always persist be- +L+ tween test cases or under certain conditions discarded. For +L+ example, a test case may specify that a relation be empty +L+ and to satisfy the condition the content is discarded. How- +L+ ever, this relation may be required by later test cases and so +L+ by discarding its contents we increase the divide between the +L+ test state and the real world. This could be accomplished +L+ by either embedding the modifications inside of a transac- +L+ tion which can then be aborted or by using a hypothetical +L+ database engine.
7. ACKNOWLEDGMENTS
We thank Leonardo Mariani and the anonymous reviewers +L+ for comments on earlier drafts of this paper. David Willmor +L+ is supported by a research studentship from the UK Engi- +L+ neering and Physical Sciences Research Council.
8. REFERENCES
[1] M. Arenas, L. E. Bertossi, and J. Chomicki.
Consistent query answers in inconsistent databases. In +L+ Proceedings of the 18th ACM +L+ SIGACT-SIGMOD-SIGART Symposium on Principles +L+ of Database Systems (PODS), pages 68–79. ACM +L+ Press, 1999.
[2] L. E. Bertossi and J. Chomicki. Query answering in +L+ inconsistent databases. In J. Chomicki, R. van der +L+ Meyden, and G. Saake, editors, Logics for Emerging +L+ Applications of Databases, pages 43–83. Springer, +L+ 2003.
[3] P. Bohannon, M. Flaster, W. Fan, and R. Rastogi. A +L+ cost-based model and effective heuristic for repairing +L+ constraints by value modification. In Proceedings of +L+ the SIGMOD Conference, pages 143–154. ACM, 2005.
[4] L. Bravo and L. E. Bertossi. Logic programs for +L+ consistently querying data integration systems. In +L+ G. Gottlob and T. Walsh, editors, Proceedings of the +L+ 18th International Joint Conference on Artificial +L+ Intelligence (IJCAI), pages 10–15. Morgan Kaufmann, +L+ August 2003.
[5] A. Cali, D. Lembo, and R. Rosati. On the decidability +L+ and complexity of query answering over inconsistent +L+ and incomplete databases. In Proceedings of the 22nd +L+ ACM SIGACT-SIGMOD-SIGART Symposium on
Principles of Database Systems (PODS), pages +L+ 260–271. ACM, June 2003.
[6] D. Chays, S. Dan, P. G. Frankl, F. I. Vokolos, and +L+ E. J. Weber. A framework for testing database +L+ applications. In Proceedings of the International +L+ Symposium on Software Testing and Analysis +L+ (ISSTA), pages 147–157, August 2000.
[7] D. Chays, Y. Deng, P. G. Frankl, S. Dan, F. I. +L+ Vokolos, and E. J. Weyuker. An AGENDA for testing +L+ relational database applications. Software Testing, +L+ Verification and Reliability, 14(1):17–44, 2004.
[8] J. Chomicki and J. Marcinkowski. On the +L+ computational complexity of minimal-change integrity +L+ maintenance in relational databases. In L. E. Bertossi, +L+ A. Hunter, and T. Schaub, editors, Inconsistency +L+ Tolerance, volume 3300 of Lecture Notes in Computer +L+ Science, pages 119–150. Springer, 2005.
[9] S. S. Cosmadakis and C. H. Papadimitriou. Updates +L+ of relational views. Journal of the ACM,
31(4):742–760, 1984.
[10] S. M. Embury, S. M. Brandt, J. S. Robinson, +L+ I. Sutherland, F. A. Bisby, W. A. Gray, A. C. Jones, +L+ and R. J. White. Adapting integrity enforcement +L+ techniques for data reconciliation. Information +L+ Systems, 26(8):657–689, 2001.
[11] G. Greco, S. Greco, and E. Zumpano. A logical +L+ framework for querying and repairing inconsistent +L+ databases. IEEE Transactions on Knowledge and +L+ Data Engineering, 15(6):1389–1408, 2003.
[12] A. Guessoum and J. W. Lloyd. Updating knowledge +L+ bases. New Generation Computing, 8(1):71–89, 1990.
[13] F. Haftmann, D. Kossmann, and A. Kreutz. Efficient +L+ regression tests for database applications. In +L+ Proceedings of the 2nd Biennial Conference on +L+ Innovative Data Systems Research (CIDR), pages +L+ 95–106. Online Proceedings, January 2005.
[14] G. M. Kapfhammer and M. L. Soffa. A family of test +L+ adequacy criteria for database-driven applications. In +L+ Proceedings of the 11th ACM SIGSOFT Symposium +L+ on Foundations of Software Engineering, pages +L+ 98–107. ACM, September 2003.
[15] R. Langerak. View updates in relational databases +L+ with an independent scheme. ACM Transactions on +L+ Database Systems (TODS), 15(1):40–66, 1990.
[16] P. Louridas. Junit: Unit testing and coding in +L+ tandem. IEEE Software, 22(4):12 – 15, July-Aug 2005.
[17] J. Melton and A. R. Simon. SQL:1999 Understanding +L+ Relational Language Components. Morgan Kaufmann, +L+ 2002.
[18] H. Shu. Using constraint satisfaction for view update. +L+ Journal of Intelligent Information Systems, +L+ 15(2):147–173, 2000.
[19] D. Willmor and S. M. Embury. Exploring test +L+ adequacy for database systems. In Proceedings of the +L+ 3rd UK Software Testing Research Workshop +L+ (UKTest), pages 123–133. The University of Sheffield, +L+ September 2005.
[20] D. Willmor and S. M. Embury. A safe regression test +L+ selection technique for database–driven applications. +L+ In Proceedings of the 21st International Conference on +L+ Software Maintenance (ICSM), pages 421–430. IEEE +L+ Computer Society, September 2005.
111
