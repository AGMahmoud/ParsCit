CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy
4: Error results for each technique in the user trials.
5: Comparison of ratings distributions between Movie- 
Lens and user trial data.
Recommendation Accuracy
To evaluate the accuracy of the techniques, mean abso-
lute error was computed between the predicted rating 
and the users actual rating for each of the methods. 
Results of our analysis are presented in Figure 4 for 
four techniques. It must be noted that the y-axis on 
this graph is non-standard- the graph is intended to 
highlight relative differences between each technique. 
As expected, the average predictions– that is, predic-
tions based on the average user described earlier, exhib-
ited the worst performance, producing an accuracy of 
80.5%. It is important to distinguish between this “av-
erage user” technique, and the “average ratings” pre-
sented in Figure 4. Predictions based on an average 
user (column 1 in Figure 4) have high accuracy, this is 
not surprising if we take a look at the rating distribu-
tion graph in Figure 5. Users tended to rate only movies 
they liked, and the average user was constructed from 
a list of the most commonly rated movies, which as it 
turns out were generally the most highly rated movies. 
This may be attributed to the fact that the movies were 
not new and users tended to remember them in a good 
light. Our profile-based technique (column 3) with ma-
nipulation beats the benchmark (column 2) achieving a 
small relative increase of 1.05%. A single factor between 
groups ANOVA shows that these differences are signif-
icant in each case with p = 0.006, F = 3.87. This small 
increase is an important result because it indicates that 
manipulation does increase recommendation accuracy.
The most surprising result was that the dynamic feed-
back technique performed worse than the other profile- 
based techniques. After much analysis of the graphs 
it was determined that users tended to over-tweak the 
system to achieve desired results for their salient item 
sets. In doing this, many of the profile-based corre-
lations were overwritten and the resulting layout was 
overfitted to the specific item sets. When this occurs, 
the system loses the inherent benefits of collaborative 
filtering, causing accuracy to drop. A solution to this 
may be to ensure diversity within the salient item sets. 
To assess the effects of users interaction with the sys-
tem a pre and post study questionnaire was answered 
by each participant. Detailed discussion of this survey 
is not possible due to space restrictions. Briefly, par-
ticipants were asked a range of questions to assess their 
experience with graphical interfaces, recommenders and 
visualizations, they were then asked to rate 10 state-
ments about the PeerChooser interface. The salient 
findings from this survey were that 78% of users felt 
that the system provided a good explanation of collab-
orative filtering. and that more than 80% of partici-
pants felt they benefitted overall from interacting with 
the system.
CONCLUSIONS
This note has introduced PeerChooser, an interactive 
visualization system for collaborative filtering. A pre-
cise representation of the CF process is presented to the 
user allowing openness and explanation of the CF pro-
cess. Feedback is enabled through manipulation of the 
visualized data to facilitate expression of current mood 
and requirements. Results of our preliminary tests with 
real-world users were presented and discussed. Our re-
sults indicate that the visual-interactive approach can 
help produce better accuracy, more information, and an 
enhanced user experience with the recommender sys-
tem.
ACKNOWLEDGMENTS
This research was supported in part by Science Foundation Ireland 
under Grant No. 03/IN.3/I361, and the Intelligence Technology Inno-
vation Centre (ITIC) through its Knowledge Discovery and Dissemi-
nation Program, as NSF grant #IIS-0635492.
REFERENCES
1. R. A. Becker, S. G. Eick, and A. R. Wilks. Visualizing network 
data. IEEE Transactions on Visualization and Computer 
Graphics, 1(1):16-28, 1995.
2. J. L. Herlocker, J. A. Konstan, and J. Riedl. Explaining 
collaborative filtering recommendations. In Computer Supported 
Cooperative Work, pages 241-250, 2000.
3. B. N. Miller, I. Albert, S. K. Lam, J. A. Konstan, and J. Riedl. 
Movielens unplugged: experiences with an occasionally connected 
recommender system. In IUI ’03: Proceedings of the 8th 
international conference on Intelligent user interfaces, pages 
263-266, New York, NY, USA, 2003. ACM Press.
4. J. O’Donovan and B. Smyth. Trust in recommender systems. In 
IUI ’05: Proceedings of the 10th international conference on 
Intelligent user interfaces, pages 167-174. ACM Press, 2005.
5. P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl 
Grouplens: An open architecture for collaborative filtering of 
netnews. In Proceedings of ACM CSCW’94 Conference on 
Computer-Supported Cooperative Work, Sharing Information 
and Creating Meaning, pages 175-186, 1994.
6. R. Sinha and K. Swearingen. The role of transparency in 
recommender systems. In CHI ’02 extended abstracts on Human 
factors in computing systems, pages 830-831. ACM Press, 2002.
1088
