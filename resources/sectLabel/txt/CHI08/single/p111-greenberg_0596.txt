CHI 2008 Proceedings · Usability Evaluation Considered Harmful?	April 5-10, 2008 · Florence, Italy
interacts within its context of use.2 Similarly, we need to 
understand methods that evaluate cultural aspects of 
designs. We are seeing some of this at ACM CSCW and 
UBICOMP, where ethnographic approaches are now 
considered vital if we are to understand how our 
technologies can be embedded within social and work 
groups, and within our physical environment. Other fields, 
such as Communication and Culture, have their own 
methods that may be appropriated for our use.
This list is incomplete, and we hope that others within CHI 
will add to it. Overall, what we are arguing for is a change 
in culture in how we do our research and practice, and in 
how we train our professionals, where we encompass a 
broad range of methods and approaches to how we create 
and validate our work.
RELATED WORK
We are not the first to raise cautions about the doctrine of 
evaluations in CHI. Henry Lieberman seeded the debate in 
his Tyranny of Evaluation, where he damns usability 
evaluation and the insistence that CHI places on it [20]. 
Shumin Zhai challenged his position by saying that in spite 
of the concerns, our evaluation methods are better than 
doing nothing at all [33]. Cockton continued the debate in 
2007 [6], where he argued that the problem is not whether 
one should do evaluations, but that there is a lack of 
methods that are useful to various design stages, or to 
various practitioners (e.g., inventor vs. artist vs. designer vs. 
optimizer). Dan Olsen moderated a panel on evaluating 
interface systems research at UIST 2007. He raised 
concerns about how our expectations on measures of 
usability when evaluating interactive systems can create a 
usability trap, and offers alternate criterion for helping us 
judge systems [23].
Others raise concerns about the methods we use. Many 
standard textbooks offer the standard caveats to empirical 
testing, e.g., internal vs. external validity, statistical vs. 
practical significance, generalization, and so on [7]. 
Narrowing to the CHI arena, Stanley Dicks argues on the 
uses and misuses of usability testing in [26], while 
Barkhuus and Rode analyze the preponderance of usability 
evaluation in CHI and raise concerns about how such 
evaluations are now typically done [1]. Kaye and Sengers 
look at the evolution of evaluation in CHI: they stress the 
‘Damaged Merchandise’ controversy that had practitioners 
from different fields challenging the usefulness of methods, 
particularly between advocates of discount methods vs. 
formal quantitative methods [18]. Pinelle and Gutwin 
analyzed evaluation in CSCW from 1990-1998 and found 
that almost 1/3 of the systems were not formally evaluated, 
but perhaps more importantly that only about 1/4 included 
evaluation in a real-world setting [24]. As CSCW systems
2 http://www.scottberkun.com/essays/essay23.htm
are often culturally situated, this raises serious questions 
about the evaluations that ignore real world context.
Of course, there are many people who argue that other non- 
evaluation methods can contribute to design. For example, 
Tohidi et. al. consider sketches as an effective way of 
getting reflective user feedback [30], while Buxton more 
generally considers the role of sketching in the design 
process [3]. On the cultural side, Gaver et. al. are 
developing methods that probe cultural reactions and 
technology uptake by niche cultures [11].
Finally, several splinter groups within the CHI umbrella 
arose in part as a reaction to evaluation expectations. ACM 
UIST emphasizes novel systems, interaction techniques, 
and algorithms – while evaluation is desired, it is not 
required if the design is inspiring and well argued (although 
they too are debating about how they are falling in the 
evaluation trap [23]). ACM CSCW and UBICOMP 
incorporate and nurture ethnography and qualitative 
methods as part of its methodology corpus, for both realized 
the importance of culture to understanding technological 
innovations. ACM DIS and DUX favors case studies that 
emphasize design, design rationale, and the design process.
CONCLUSION
We recapitulate our main message:
the choice of evaluation methodology – if any – must arise from 
and be appropriate for the actual problem or research question 
under consideration.
We should begin with the situation we are examining and 
the question we are trying to solve. We should choose a 
method that truly informs us about that situation or answers 
that question. More often than not, usability evaluation will 
be that method; this is why CHI has embraced it. Yet we 
should be open to other non-empirical methods – design 
critiques, design alternatives, case studies, cultural probes, 
reflection, design rationale – as being perhaps more 
appropriate for some of our situations.
It would be just as inappropriate to drop usability testing 
altogether in favor of the approaches that we are 
advocating. Zhai argues that usability evaluation is the best 
game in town [33], and we qualify by saying that this is true 
in many, but not all cases. For some cases, other methods 
are more appropriate. However, in all cases a combination 
of methods – from empirical to non-empirical to reflective 
– will likely help triangulate and enrich the discussion of a 
system’s validity. It is just a matter of balance, but then, 
that is the true essence of evaluation anyhow!
Our concerns may appear novel to young CHI practitioners, 
but those who have been around will have heard them 
before and will likely have their own opinion. Regardless of 
who you are, consider how you can help enrich CHI. Join 
the debate. Change your development practices as a 
researcher and practitioner. Reconsider how you judge the 
papers while refereeing. Teach our new professionals that 
HCI 0 Usability Evaluation; it is far more than that.
119
