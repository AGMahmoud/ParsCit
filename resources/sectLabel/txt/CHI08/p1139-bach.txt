CHI 2008 Proceedings · Tangibles: Input &amp; Output	April 5-10, 2008 · Florence, Italy
You Can Touch, but You Can’t Look:
Interacting with In-Vehicle Systems
&apos;Kenneth Majlund Bach, &apos;Mads Gregers Jæger, &apos;,2Mikael B. Skov and &apos;Nils Gram Thomassen
1Department of Computer Science	2Department of Information Systems
Aalborg University	The University of Melbourne
Selma Lagerlöfs Vej 300, 9220 Aalborg East	111 Barry Street, Carlton, 3006
Denmark		Victoria, Australia
{kenned, hunter, dubois, nmyt}@cs.aau.dk
ABSTRACT
Car drivers are nowadays offered a wide array of in-vehicle 
systems i.e. route guidance systems, climate controls, music 
players. Such in-vehicle systems often require the driver’s 
visual attention, but visual workload has shown significant 
less eyes-on-the-road time and affects driving performance. 
In this paper, we illustrate and compare three different 
interaction techniques for in-vehicle systems. We refer to 
them as tactile, touch, and gesture interaction. The focus of 
the techniques is the effects on drivers while driving cars. 
We evaluated the interaction techniques with 16 subjects in 
two settings. Our results showed that gesture interaction has 
a significant effect on the number of driver eye glances 
especially eye fixations of more seconds. However, gesture 
interaction still required rapid eye glances for hand/eye 
coordination. On the other hand, touch interaction leads to 
fast and efficient task completion while tactile interaction 
seemed inferior to the two other interaction techniques.
Author Keywords
In-vehicle systems, visual attention, gesture interaction, 
touch interaction, tactile interaction, driving, eye glances
ACM Classification Keywords
H.5.2 [User Interfaces]: Interaction styles
INTRODUCTION
Designing for mobility is rather challenging as mobile users 
and mobile use are characterized by great diversity and 
done for different purposes. As designers we need to enable 
users to interact with technologies during travelling, trips, 
journeys, hikes, visits, or rides. An increasingly important 
area for human-computer interaction within mobility is the
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
or republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee.
CHI 2008, April 5–10, 2008, Florence, Italy.
Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00
design of in-vehicle systems. With significant advances in 
technology, car drivers are nowadays offered a wide array 
of in-vehicle systems i.e. route guidance systems, climate 
controls, music players. As emerging vehicles constantly 
offer additional and more advanced in-vehicle systems, the 
side effects of in-vehicle systems become more significant. 
Several research studies illustrate that interacting with such 
in-vehicle systems while driving highly challenges drivers’ 
attention on the primary task of driving [12, 16, 22].
Visual attention stands out as the most important attention 
property when driving vehicles. Not surprisingly, research 
on visual workload has shown significant less eyes-on-the-
road time when drivers interact with in-vehicle systems 
with high visual demands and this directly affects driving 
performances [11, 24].
However, many vehicles are today shipped with a built in 
touch screens which highly depends on visual attention. 
Such touch-based screens are typically being employed to 
enable the driver to interact with novel functions as well as 
established functions. Consequently, simple control 
operations (e.g. using a fan speed dial) have become more 
complicated requiring a number of discrete steps 
interactions (e.g. mode selection, option choice, adjustment 
setting). Taking the immediate safety critical aspects of 
driving into account, Green recommends that we need to 
identify and investigate new interaction techniques that are 
more suited for in-vehicle systems [10].
This paper compares three different interaction techniques 
for in-vehicle systems. Inspired by previous research (i.e. 
[1, 2, 19, 24]), we identify and refer to tactile, touch, and 
gesture interaction for in-vehicle systems. The interaction 
techniques are illustrated and manifested through three 
different music players and are compared on their effects on 
primary and secondary driving tasks performances and on 
visual attention of the driver. The paper is organized as 
follows. First, we illustrate related work on in-vehicle 
systems interaction. Secondly, we illustrate the interaction 
techniques. Thirdly, we outline an experiment where the 
techniques are compared against each other and findings are 
presented and illustrated. Finally, we discuss our results.
1139
CHI 2008 Proceedings · Tangibles: Input &amp; Output	April 5-10, 2008 · Florence, Italy
RELATED WORK
Most in-vehicle systems research focus on how to minimize 
driver workload and how to reduce visual attention on such 
systems [2]. As touch sensitive interfaces are finding their 
way into more vehicles (i.e. where drivers typically can 
control media centres, climate controls etc.), a characteristic 
assumption is that we need to look past the capabilities of 
tactile feedback and search for new techniques that require 
no (or very limited) visual attention. Traditionally, the car 
requires little visual attention to master i.e. gear selection, 
throttle control, or steering wheels.
Touch-based and tactile-based interaction techniques have 
been investigated on their abilities to support drivers when 
interacting with in-vehicle systems. Several studies explore 
opportunities and limitations of these techniques [5, 17, 24]. 
Bellotti et al. compared a haptic-based force-feedback 
knobble against a touch-based interface on a configurable 
touch screen. They found that the tactile interface 
performed better on visual demanding tasks and also in 
relation to eyes-off-the-road frequency and duration [5]. 
Tsimhoni and Green identified a similar result where touch- 
based interaction required higher visual attention, but it also 
involved lowered driving performance [24]. They noticed 
that drivers wandered more in their lane and departed from 
their lane more frequently. Also, drivers made shorter 
glances at the interface, had a higher number of glances, but 
waited longer between glances. If driving conditions got 
difficult and thus visual demands for driving were high, 
task completion times increased. Finally, Noy et al. found 
that manipulation tasks performed on a touch screen were 
significantly more mentally demanding than radio tuning 
tasks simply because it required significant visual attention 
as a consequence of the lack of tactile and kinaesthetic 
feedback [17].
While touch-based and tactile-based techniques seem to 
suffer from a number of inherent limitations in terms of 
reducing visual attention, two other techniques potentially 
provide attention low interaction – i.e. speech recognition 
and gesture-based interaction. Speech recognition stands 
out as a potentially useful technique for in-vehicle systems 
interaction as it provides both hands-free and eye attention 
free interaction (at least in theory). Barón and Green [4] 
state, however, that speech recognition is highly cognitive 
demanding and it has been characterized as both impractical 
and flawed in regard to in-vehicle interaction [8].
Gesture-based interaction, on the other hand, could provide 
a suitable alternative to speech recognition as it overcomes 
some of the inherent impracticalities of spoken language 
while driving. Alpen and Minardo explored gesture-based 
interaction with in-vehicle systems [1]. In an experiment 
using driving simulators, subjects performed entertainment 
tasks, (e.g. find a song, search presets, adjust volume) with 
both a gesture interface on the windshield and a 
conventional radio. Subjects in the gesture condition made 
fewer driving errors than radio interface subjects, however 
not significantly fewer errors. Further, subjects preferred
the gesture interface to the radio, because it allowed them to 
keep eyes and attention on the driving. Subjects did not 
have to reach and touch anything and gesture interaction 
allowed them to be less accurate while being successful.
Pirhonen et al. compared a gesture-based interface to a 
touch-based interface for an on-the-go MP3 music player 
[19]. They stressed the importance for mobile users to focus 
their visual attention on the world around them and not on 
the device (drivers are typically in a similar situation). They 
found that overall workload was significantly reduced as 
well as overall task completion time for the gesture-based 
interface. Further, gesture interaction facilitated the primary 
task (of walking) as the walking speed was closer to the 
participants’ preferred walking speed. No difference was 
found in the number of errors made between the two types 
of interfaces.
IN-VEHICLE SYSTEM INTERACTION
Based on previous research on interaction techniques for in- 
vehicle systems and inspired by Pirhonen et al. [19], we 
present tactile, touch, and gesture interaction as alternatives 
to in-vehicle interaction techniques. The three techniques 
(tactile, touch and gesture) are represented by individual 
systems in order to demonstrate and compare the qualities 
and problems regarding attention.
One type of well-established in-vehicle systems is common 
car stereos or music players. For this experiment, we opted 
to use music players as case systems to represent the three 
interaction techniques (i.e. [1, 19]). The different types of 
interaction are not unique to the domain of music players 
and were chosen as case systems to allow direct comparison 
of the interaction techniques. The terms tactile interaction, 
touch interaction and gesture interaction will be used when 
discussing the three music players that manifest the 
respective interaction techniques.
Tactile Interaction
The tactile interaction was represented by an off-the-shelf 
car stereo with CD functionality. The product was a xZound 
CDX5 and is illustrated in figure 1.
Figure 1. The conventional car stereo (Xzound CDX5) used for 
the tactile interaction. Play/pause is located at the lower 
middle part (1), skip forward/back is located in the lower right 
(2), and the volume is controlled via the knob to the left (3).
Prior to selecting the specific model standard functionalities 
were investigated in car stereos to ensure that any model we 
would select was both representative and intuitive. The car 
stereo was selected due to its traditional looks and its 
conventional interface. The car stereo operates via tactile 
interaction with either the buttons at the lower part of the
1140
CHI 2008 Proceedings · Tangibles: Input &amp; Output	April 5-10, 2008 · Florence, Italy
unit or via turning the volume controller knob seen on the 
left. Thus, the user of the car stereo usually experience 
tactile feedback when pressing buttons or turning knobs. 
The display presents information regarding playing mode, 
track number, and volume level.
Touch Interaction
The touch interaction technique was manifested in a touch 
screen with a touch-to-push interface (see figure 2).
Figure 2. Overview of the touch screen based music player 
manifesting touch interaction. Play/pause is located in the 
middle (1), skip forward/skip back is located on the sides (2), 
and volume control is located to the right (3). The figure shows 
the system in playing mode – if the pause button is touched, 
the icon changes to a “play icon” and the music stops. 
Diagonal size is approximately 7”.
By touching the buttons on the interfaces, the user controls 
the basic functionality of the system. Buttons are triggered 
by button release only, i.e. nothing happens if buttons are 
pressed and held or if buttons are missed. The player 
features only basic functionality of the kind also found on 
the car stereo, i.e. play/pause, skip back/forward, and adjust 
volume up/down. The interface also displays the artist and 
song number for the track currently being played.
Gesture Interaction
The gesture interaction was implemented through a touch 
screen based “drawing canvas” as illustrated in figure 3.
Figure 3. A touch screen serves as the canvas for gesture 
interaction. The hand is drawing a line from left to right 
indicating a skip forward operation.
The user controls the player by using a finger to draw 
gestures anywhere on the canvas. The input is then matched 
against a set of predefined gestures and any command 
assigned to that specific gesture is executed. The system 
features the same basic functionality as the two other
systems, i.e. play/pause, skip back/forward, and adjust 
volume up/down. Track numbers are read out loud is 
included dubbed “Get Song Number”. In addition a set of 
earcons provide auditory feedback, for instance when a 
gesture could not be recognized. Figure 4 presents the 
implemented gestures, which are inspired by Pirhonen et al. 
[19], and their respective function.
1 2
Play / pause
Skip Forward 
Skip Backward 
Volume Up 
Volume Down
Get Song Number
Figure 4. The set of input gestures for the gesture interaction 
based music player. A dot indicates a gesture start, e.g. play or 
pause is executed by tapping the canvas twice.
EXPERIMENT
The main objective for the experiment was to compare the 
three interaction techniques as illustrated above. In this 
section, we outline the rationale behind the experiment and 
illustrate key elements of the experiment.
Subjects
16 people (1 female, M=28, SD=5.7) participated in our 
experiment. All subjects carried valid driver’s licenses and 
their driving experience ranged from app. 100 km per year 
to 20,000 km per year (M=4,732 km, SD=5,966 km). All 
subjects stated that they were in a fair health condition 
including having normal or corrected-to-normal vision. 7 of 
the 8 subjects in the controlled track sessions had been to a 
similar facility while training for their driver’s licenses 
whereas 6 of 8 subjects in the simulated driving sessions 
indicated that they had previously used steering wheel and 
pedals for computer games and had tried the selected game.
Design
We utilized a combined within-subject and between-subject 
design using interface type (tactile, touch, and gesture) as 
within independent variable and driving setting (controlled 
and simulated) as between independent variable. Subjects 
were randomly assigned to exactly one of the two driving 
settings where they each participated in three sessions; one 
for each of the interface types. The order of interface types 
was counter-balanced between subjects to minimize carry-
over effects. Thus, in total we conducted 48 (3x2x8) testing 
sessions (3=interaction techniques, 2=settings, 8=subjects).
1141
CHI 2008 Proceedings · Tangibles: Input &amp; Output	April 5-10, 2008 · Florence, Italy
The primary dependent measures were primary driving task 
performances (longitudinal and lateral control), secondary 
driving task performances (interaction errors and task 
completion times), and eye glance behavior. Additionally, 
satisfaction was measured as a secondary measure. The 
dependent measures are further elaborated and discussed in 
the data analysis section.
Driving Settings
We integrated two different driving settings for the testing 
of the interaction techniques. We refer to them as controlled 
driving and simulated driving (i.e. [2]). This allowed us to 
identify and explore similarities and differences between 
the different driving situations and to address problems that 
might be associated with either the controlled driving or the 
simulated driving. This paper primarily addresses the three 
interaction techniques; specific findings and discussion of 
the driving settings can be found in [3].
Controlled Driving
The controlled driving took place at a driving facility in 
Aalborg, Denmark normally used for training purposes (see 
figure 5). Subjects drove on a 3.2 km test track and there 
were no other road users on the facility at the time of the 
experiment. The track was equipped with various driving 
features to mimic low-density rural driving conditions i.e. 
simple curve exercises and simple slalom areas. Also, the 
track had a speeding camera and traffic lights. The vehicle 
used for the experiment was a 3-door Opel Astra.
Figure 5. Vehicle setup with the touch interaction and gesture 
interaction players running off a touch screen (left). The car 
stereo (tactile interaction) is placed just below. Subject driving 
the vehicle used in the experiment (right).
Besides swapping the original car stereo with the one 
needed for the experiment, a 17” flat touch screen (of which 
only a 7” section was used) was attached to the centre 
console. The tactile car stereo was placed in the middle of 
the centre console and the touch screen was mounted on the 
top part of the same console.
Simulated Driving
The simulated driving took place in the HCI laboratory at 
Aalborg University decorated as a medium-fidelity driving 
simulator. The driving simulator consisted of a set of force 
feedback steering wheel and pedals to control a vehicle in a 
PlayStation based simulation (Gran Turismo 3).
Figure 6. Frame from the laboratory video feed showing a 
participant driving the simulator while using gesture 
interaction (left). Setup in the driving simulator with force- 
feedback steering wheel and car seats (right).
A similar car and circuit was selected for the simulation. To 
enhance the driving experience, the windshield image was 
projected onto a 2 x 2-meter canvas directly in front of the 
driver. Driver and passenger car seats were integrated to 
promote a more realistic driving environment. The touch 
screen and car stereo were placed in the same position as in 
they were in the controlled driving setting.
Tasks
Thirty smaller assignments were used in all 48 sessions. 
The tasks were introduced to the subjects verbally readout 
by the test manager. All tasks could be completed by using 
the basic functionality offered by each of the interaction 
technique.
Procedure
The procedures for the two experiment settings were made 
as identical as possible. All participants were introduced to 
the three systems prior to the testing sessions. This involved 
an introduction to the basic functionality of the systems and 
the gestures for the gesture-based interaction. Moreover, the 
subjects were asked to complete some smaller exercises to 
ensure that the functionality was understood correctly.
The controlled driving subjects were introduced to the 
vehicle and the driving facility. They were asked to drive 
around the course a few times to familiarize themselves 
with both the facility and the car. During this practice, the 
test manager would introduce the track and explain the 
procedure and give general driving instructions. Subjects 
were asked to drive casually around the circuit at a speed of 
app. 50 km/h. In addition, they were told to drive as they 
would normally do and to adhere to a few simple driving 
tasks that were scattered around the circuit, i.e. the traffic 
lights and the speeding camera where they were asked to 
get as close to 60 km/h as they could. These tasks were 
presented in an attempt to create a higher sense of realism. 
The vehicle was equipped with two cameras to capture eye 
movements and interaction of the subjects. A GPS device 
was positioned in the car to provide information about the 
vehicle speed. The GPS data combined with the laptop data 
log would also prove useful as a statistical analysis tool 
concerning the vehicle speed and position on the circuit for 
each interaction event.
1142
CHI 2008 Proceedings · Tangibles: Input &amp; Output	April 5-10, 2008 · Florence, Italy
We used a similar approach in simulated driving where the 
subjects were given time to adjust to the steering sensitivity 
and the simulator driving sensation. For the evaluation, the 
subjects were told to drive between 50 and 70 km/h and to 
stay in the middle of the road. The driving simulator used 
four cameras to capture eye movements and interaction of 
the subjects.
Data Analysis
We chose to integrate several parameters for the assessment 
of the interaction techniques. From a literature review on 
in-vehicle systems research, we identified and illustrated a 
number of relevant dependent measures for assessing the 
quality of the three interaction techniques and their effects 
on the driver and the driving [2]. The included measures in 
the experiment were:
1. Primary driving task performance
2. Secondary driving task performance 
3. Eye glance behaviour
Further, we measured subjective attitudes of the subjects 
towards the interaction techniques.
1) We measured primary driving task performance through 
lateral and longitudinal control errors (inspired by i.e. [1, 
16, 25]). Lateral control incidents were defined as attention 
related loss of vehicle lateral control (lane excursions, 
steering wheel input, etc.) while longitudinal control errors 
denote incidents where subjects had problems controlling 
vehicle velocity (speed maintenance, acceleration, etc.).
Lateral and longitudinal control errors were identified by 
reviewing video recordings and logs. Two authors of this 
paper both reviewed all 48 video recordings separately by 
identifying, classifying, and reporting incidents. Their lists 
of incidents were then compared and merged into one final 
incidents list. An inter-rate reliability test of this analysis 
(weighted Cohen’s Kappa) gave a=0.92. This corresponds 
to an excellent agreement according to Fleiss [7].
2) We integrated secondary driving task performances to 
include task effectiveness (task completion and interaction 
errors) and task efficiency (total task completion time). 
Task effectiveness is a common measure when conducting 
in-vehicle research (i.e. [1, 15, 16, 27]) as well as efficiency 
(i.e. [5, 24]).
Interaction errors were identified from the video recordings 
based on different criteria due to the nature of the systems 
representing each interaction technique. For the tactile 
interaction technique, interaction errors were defined as 
unsuccessful attempts to press a specific button or selecting 
a wrong control for the assignment (e.g. using the volume 
control when asked to skip one track forward). For the 
touch and gesture interaction techniques, interaction errors 
were automatically logged when touching any non-button 
part of the surface (touch) or producing a non-recognized 
gesture (gesture).
The task completion times for touch and gesture interaction 
were measured through the data log entries of the sessions 
as the laptop running the two players also logged task data. 
As tactile interaction offered no such data, it was necessary 
to manually measure task completion times through video 
analysis. Two authors reviewed all 48 video recordings 
independently by measuring the task completion times for 
all 1440 assignments. Due to the nature of this data, no 
inter-rate reliability tests were conducted, but an average 
was calculated from the list of the two authors. To get the 
total task completion times (including the time it took for 
participants to move their hand to and from the system), a 
constant was added to the values from the interaction log. 
The constant was calculated using a stopwatch on a sample 
of 20 assignments for both systems and was set to 660 
milliseconds.
3) Eye glance is probably the most common predicator of 
driver attention and is an accepted measure for in-vehicle 
system evaluations as the link between visual attention and 
driving performance (i.e. [8, 21, 26]. We divided eye 
glances into three categories according to their duration 
(less than 0.5 seconds, 0.5-2.0 seconds and above 2.0 
seconds). These categories are motivated in the following.
Eye fixations (when the eyes dwell on something) are 
typically glances over 0.5 seconds and can be used as an 
indication of what a driver is attentive towards [26]. This 
defines the boundary between two categories; below and 
above the eye fixation time of 0.5 seconds. Also, research 
suggests that drivers are very reluctant to continue without 
roadway information for more than 2 seconds also known 
as the “2-second rule” [21, 27]. This defines the boundary 
between two others of our categories; eye glances of more 
than 2 seconds.
We classified eye glances according to whether they were 
below 0.5 seconds, between 0.5-2.0 seconds, or above 2.0 
seconds using video analysis. Two authors independently of 
each other classified eye glances in the 48 video recordings. 
Their classification were then compared and an inter-rate 
reliability test (weighted Cohen’s Kappa) showed a=0.71 
suggesting substantial agreement according to Fleiss [7]. 
Due to the number of disagreements, a third author would 
review all incidents (where the two authors disagreed) and 
classify them according to the categories.
The questionnaire included questions on the subjective 
preferences towards the interaction techniques. We used a 
5-point scale on perceived workload, concurrent driving 
performance, and ease of use. Their answers served as input 
for the subsequent interview sessions.
RESULTS
This section presents the results from the data analysis for 
each measure. We present results in the following order: 
primary driving task performance, secondary driving task 
performance, eye glance behaviour, driving settings impact, 
and satisfaction. All results were subjected to one-way
1143
CHI 2008 Proceedings · Tangibles: Input &amp; Output	April 5-10, 2008 · Florence, Italy
repeated-measures ANOVA tests and Tukey HSD post hoc 
tests at a 5% confidence level.
Primary Driving Task Performance
We included two variables for measuring primary driving 
performance namely lateral control (lane excursions, 
steering wheel input, etc) and longitudinal control (speed 
maintenance, acceleration). We identified 182 incidents of 
lateral or longitudinal control errors in the 48 sessions.
compared to the tactile and the gesture subjects. This can 
partly be explained by the fact that one specific task was 
much easier to solve using the touch interaction system’s 
informative display compared to the tactile interaction 
(smaller display) or gesture interaction (no display), which 
is also reflected in the deviation values for average per task 
values. However, omitting this particular task would not 
alter the statistical significant difference between the task 
completion times of the three interaction techniques.
Tactile
Touch	Gesture
3.88	3.63
162.48
83.5
Task Completion 
Time
Interaction Errors
142.81
3.88
120 
100 
80 
60
40
0
200
20
180
160
140
14 
12 
10
8
6 
4 
2 
0
4
0
3
2
1
2.13
1.81	1.75
1.56
3
1.13
Lateral 
Longitudinal
Tactile	Touch	Gesture 3
Figure 7. Lateral control and longitudinal control errors for 
each interaction technique.
While numbers of longitudinal errors were relatively alike 
for the three interaction techniques, we found a significant 
difference between the numbers for lateral control, F(2, 45) 
= 9.52, p &lt; .001. A Tukey HSD post hoc test revealed that 
the tactile interaction resulted in significantly more lateral 
control errors than both the touch and gesture interaction 
sessions (p &lt; .01). Four subjects did not make any errors 
while using touch interaction compared to three subjects for 
gesture interaction and none for tactile interaction.
Secondary Driving Task Performance
In addition to primary driving performance, we adapted a 
number of secondary driving performance measures namely 
interaction errors and task completion times. 1440 tasks 
(16 participants x 30 tasks x 3 interfaces) were assigned 
during the experiment. The result shows only marginal 
difference in interaction errors between the three interaction 
types and no significant differences were identified, F(2, 
45) = 0.03, p &gt; .97 (see figure 8).
Task efficiency was expressed by the total task completion 
time used for each of the systems (for 16 x 30 assignments) 
in seconds. The time spent on task execution is significantly 
different between the three systems according to a one-way 
repeated measures ANOVA, F(2, 45) = 65.53, p &lt; .0001. 
Post hoc tests showed that the total task completion times 
was significantly lower for the touch interaction subjects
Figure 8. Task completion times and interaction errors for the 
three interaction techniques.
Eye Glance Behavior
Eye glances were categorized into three categories (under 
0.5 seconds, 0.5 – 2.0 seconds, above 2.0 seconds) inspired 
by [19, 24, 25]. From the 48 driving sessions, we identified 
2657 eye glances. Using the tactile interaction, subjects had 
the most eye glances with 1,120. The touch interaction 
produced 1021 (roughly 9% less eye glances compared to 
the tactile interaction) while the gesture interaction 
produced 516 eye glances (which corresponds to 54% less 
eye glances compared to the tactile interaction). Figure 9 
illustrated number of eye glances for each of the three eye 
glance categories.
The gesture interaction accounted for approximately 19 % 
of all eye glances, while tactile and touch interaction 
accounted for 42% and 39%. A one-way repeated-measures 
ANOVA showed significant difference between the number 
of eye glances for the interaction technique, F(2, 45) = 38.4, 
p &lt; .0001. This indicates that the gesture interaction led to 
fewer eye glances. A post-hoc test showed significant 
difference at the 1% level between the gesture interaction 
and the two other techniques. The gesture subjects varied 
highly on numbers of eye glances ranging from 9 to 96 eye 
glances to complete the 30 tasks.
Category one eye glances (less than 0.5 seconds) were less 
common when using tactile and touch interaction (with 60 
and 64 respectively) compared to gesture interaction, which
1144
CHI 2008 Proceedings · Tangibles: Input &amp; Output	April 5-10, 2008 · Florence, Italy
produced 229 category one eye glances. This difference 
was statistical significant, F(2, 45) = 7.08, p &lt; .01. When 
using gesture interaction, it was common for the subjects to 
use quick eye glances (below the fixation limit) to 
coordinate their hand with the gesture canvas. Almost half 
of the gesture interaction eye glances (44%) were category 
one. Somewhat surprisingly, this type of eye glances was 
not common for tactile or touch interaction, but it seemed 
that their hand/eye coordination often involved fixation 
time on the displays.
Figure 9. Number of eye glances for the three interaction 
technique on eye glance categories.
All three techniques yielded more category two glances (0.5 
- 2.0 seconds) compared the other glance categories. This is 
perhaps not surprising, but a repeated measures ANOVA 
showed significant difference between the techniques on 
these eye glances, F(2, 45) = 62.14, p &lt; .0001. A Tukey 
HSD post-hoc test showed significant difference at the 1% 
level between the gesture interaction and the two other 
techniques, but now significant difference between tactile 
and touch interaction. Perhaps not surprisingly, gesture 
interaction did not yield any eye glances of category 3 
(above 2 seconds), which is interesting due to their impact 
on driving performance. In contrast, the tactile and touch 
interaction techniques yielded 58 and 65 total eye glances. 
This difference was significant, F(2, 45) = 10.43, p &lt; .001.
The number of tasks solved with zero eye glances also 
differed across the interaction techniques, with 176 (out of 
480 possible) for the gesture interaction compared to 4 and 
35 for the touch interaction and tactile interaction 
respectively. If the three categories were given estimates for 
average duration (based on Green [9, 10] type 1 = 0.2 
seconds, type 2 = 0.8 seconds, type 3 = 2.4 seconds), the 
subjects would remove their eyes from the road for 60 
seconds using the tactile interaction technique compared to 
55 seconds with touch interaction and only 17 seconds for 
gesture interaction.
Controlled versus Simulated Driving
We chose to conduct the experiment both in a controlled 
driving environment and a simulated driving facility. This 
was chosen to reduce some of the known limitations of two 
settings. Not surprisingly, the settings illustrated a number 
of similarities and differences. Task completion times and 
interaction errors showed quite similar patterns for the two 
settings. Also, eye glances produced similar results with the 
exception of eye glance over 2.0 seconds where controlled 
driving subjects would have significantly more eye glances 
than simulated driving subjects. This is perhaps surprising 
given the safety critical condition for the controlled driving.
Longitudinal control was, however, quite different between 
the two settings. Controlled driving subjects had significant 
fewer longitudinal control errors than simulated driving 
subjects. It seemed that the lack of vehicle movement made 
it difficult to maintain the desired speed. Further findings 
on the impact of the settings can be found in [2].
Satisfaction
Overall, our subjects preferred gesture interaction to both 
touch and tactile interaction. However, all 16 test subjects 
indicated that they would use any of the three interaction 
techniques during real world driving. When asked to 
prioritize the three interaction techniques, gesture was 
ranked first by 10 participants, touch was preferred by 5 
while one participant preferred the tactile technique). The 
tactile interaction technique ranked third with 13 of the 16 
subjects (gesture interaction was ranked third by 3 subjects 
and the touch interaction was not ranked third by any 
subject).
The gesture based music player was characterized as being 
intuitive and very easy and fast to use while the tactile 
interaction based player (car stereo) was criticized for its 
small buttons and poor layout. The gesture interaction 
technique was generally described as very pleasant and less 
demanding and distracting than the other two interaction 
techniques.
DISCUSSION
In-vehicle systems research has shown that we need new 
ways of interacting with in-vehicle systems as conventional 
interaction techniques decrease driving performance (i.e. 
[10, 15, 16, 23]). We investigated three different interaction 
techniques in their abilities to support drivers interacting 
with in-vehicle systems while driving. In the following, we 
will discus our findings in terms of the three interaction 
techniques and reflect our findings against other research 
studies. In this discussion, we will pay special attention to 
the gesture interaction technique and we will also discuss 
possibilities of using multimodal interfaces.
Tactile Interaction
Tactile interaction was less intuitive and efficient than we 
had expected. With the highest average task completion 
times, the most eye glances, and the highest number of in-
complete assignments, tactile interaction was inferior to the
62.67
55.75
4.06
4
0
3.63
3.65
&gt;2.0 
&lt;0.5 
0.5-2.0
14.31
17.94
6 
4 
2 
0
20 
18 
16 
14 
12
10
8
Tactile	Touch	Gesture
80 
70 
60 
50 
40 
30 
20 
10 
0
1145
CHI 2008 Proceedings · Tangibles: Input &amp; Output	April 5-10, 2008 · Florence, Italy
two other interaction techniques. Having said that, tactile 
interaction illustrated a few strengths and opportunities not 
directly visible in the findings section. The tactile feedback 
was much appreciated by the subjects (though they did not 
stress this to a be lack for the other interaction techniques) 
who used the sensation of the physical buttons on the car 
stereo to “scan” for the appropriate buttons. However, it did 
not lead to fewer eye glances than with touch interaction.
The subjects generally used time performing the tasks when 
using tactile interaction, though there were certain tasks 
which benefited immensely from the tactile input style. 
This was for instance the case with volume adjustment, 
where a knob on the car stereo allowed the participants to 
quickly scroll to an appropriate volume level, which is in 
line with Tsimhoni and Green [24]. Gonzàlez et al. present 
a similar but different tactile interaction technique where 
drivers interact with in-vehicle systems through a thumb- 
based interaction technique [9]. Their objective was to have 
eyes on the road and hands on the wheels and they found 
that the thumb-based interaction technique could be a 
promising mean for in-vehicle systems input.
Touch Interaction
Touch interaction presented itself as the fastest and easiest 
interaction technique. It produced better results (although 
not significantly better) compared to the tactile interaction 
on eye glances and total task completion times. As touch 
interaction featured no tactile feedback (except for that of 
touching the screen surface) we initially expected that it 
would require substantially more eye glances in order to 
accommodate the same set of tasks compared to tactile 
interaction. This would be in line with the findings of [25]. 
However, we did discover this pattern presumably due to a 
rather simple interface compared to the more complex car 
stereos.
It may be argued that touch interaction benefited too much 
from the superior display capabilities allowing reasonable 
few buttons to be arranged in a manner that would facilitate 
interaction better than the smaller and more complicated car 
stereo. This is partly true. But touch-based screens make it 
possible to customize and alter the appearance of the 
interface; something that traditional tactile interface cannot 
simply provide. Button size and interface density certainly 
influence the accuracy and effectiveness of touch display 
[20]. Kristoffersen and Ljungberg found that touch screens 
had very high visual demands and was not suitable for use 
in attention limited situations [14]. This was confirmed by 
our study as touch-based interaction subjects produced 
almost as many eye glances as tactile interaction and in fact 
yielding more long duration eye glances than tactile 
interaction.
Gesture Interaction
Gesture interaction presented an interesting alternative for 
interacting with in-vehicle systems. As with the two other 
techniques it has its pronounced merits and shortcomings. 
As hypothesized, gesture interaction excels by its low
visual demand. Our subjects used significantly fewer eye 
glances to perform assignments when using the gesture- 
based interaction. Most interestingly, we found no long 
duration eye glances (above 2 seconds), which are 
particularly devastating in terms of driving performance [6]. 
However, gesture interaction did not prove to be an eyes- 
free interaction technique as subjects often needed quick 
glances to support the coordination. This was also found by 
Alpern and Minardo who concluded that gesture interaction 
will not work in practice without eye glances [1].
We find it interesting that gesture interaction did not lead to 
significantly higher driving performance (in terms of fewer 
lateral or longitudinal errors), as subjects used only half the 
amount of eye glances compared to the other interaction 
techniques. Our findings seem to question the relationship 
between eyes-off-the-road-time and driving performance 
(i.e. [8, 21, 26]). It could be argued that the mental 
workload associated with gesture interaction (remembering 
the right gesture, remembering the system status, etc.) 
equals out the advantage gained by the limited eye glances, 
as it is known to be the case with speech recognition [4, 8]. 
But we have only limited results illustrating this. Perhaps 
somewhat surprisingly, gesture interaction did not result in 
more errors made than the other two conditions and the 
subjects were able to complete just as many tasks as with 
the tactile and touch interaction interfaces.
Our findings illuminate some limitations with the gesture 
interaction technique. One serious limitation was the lack of 
passive feedback, i.e. feedback on the current status of the 
systems and variable values. The implementation of gesture 
interaction for this experiment sought to remedy this 
deficiency by using a set of earcons as auditory feedback. 
We were inspired by Pirhonen et al. [19] who combined 
gesture input and earcons for a mobile music player. They 
found that this type of auditory feedback helped users in 
gaining an understanding of the current system status. We 
found, on the hand, that non-persistent feedback (such as 
earcons) was easily misunderstood, ignored, or missed. This 
happened several times, i.e. as subjects would fail to realize 
that they had paused the song or kept trying to increase the 
volume even though they were at the highest volume level.
Task completion time analysis revealed that using gesture 
interaction was generally more time consuming than touch 
interaction and was significantly slower on the adjusting the 
volume. In general, gesture interaction worked less well 
when it came to quickly adjusting properties, at least in the 
way gesture interaction was implemented in our study. To 
increase volume, subjects would have to input the “volume 
up” gesture several times. Some subjects suggested the 
implementation of more gestures, but again this could 
compromise how well drivers remember all gestures.
Subjects were mostly satisfied with the gesture interaction 
technique mainly due the fact that it allowed them to keep a 
focus on the road. Alpern and Minardo support this finding 
that drivers recognize the qualities of gesture interaction in
1146
CHI 2008 Proceedings · Tangibles: Input &amp; Output	April 5-10, 2008 · Florence, Italy
driving [1]. They also found that subjects felt more in 
control of the vehicle when using gesture interaction.
Perspectives on a Multimodal Interaction Technique
The benefits of low visual demand from gesture interaction 
are somewhat questioned by the identified disadvantages 
and limitations. Nevertheless, reducing visual demand is in 
itself important enough to suggest incorporating gesture 
interaction into vehicles. Our subjects felt that the best way 
to optimize gesture interaction was use it in combination 
with elements from the two other interaction techniques. 
Combining gestures with tactile and passive visual elements 
could create a powerful and diverse interaction platform.
One way to achieve such a platform would be to combine 
gesture interaction with touch interaction. Here you could 
have a touch screen interface, which could receive gestures 
on top of the button-layer interface allowing the user to 
select either input technique for the basic functions and 
resort to traditional interaction when it comes to advanced 
functions. This alternative also offers some potential in 
terms of disabling advanced resource demanding interaction 
when the vehicle is moving perhaps even turning off the 
screen accepting only gesture input. Such a combination of 
interaction technique could be further augmented by 
offering tactile elements, such as a knob-type input device 
like the one found on the car stereo.
According to Wickens [26], an advantage of using multiple 
interfaces and modalities lies in the possibility to prey on 
the concept of minimal attention user interfaces. Pascoe et 
al. stated that minimal attention is achieved through the use 
of modes of interaction that does not interfere with the 
mode that the user is already employing [18]. In this regard, 
an interface could accommodate for inaccessible or 
inopportune perceptual channels by offering multimodality. 
Which modalities should be used at any given time would 
then either be up to the driver to decide or possibly based 
on context or situation. The challenge is to discover how 
the benefits from each interaction technique elements can 
be utilized without bringing with them the disadvantages of 
their original interaction technique.
Limitations
Our experiment suffers from a number of limitations, which 
could form further research with interaction techniques for 
in-vehicle systems. First, the three techniques differed not 
only in terms of their input characteristics, but also in terms 
of output. This was discussed briefly in the previous, but 
the way feedback was provided to the subjects in the three 
systems could certainly have influenced our findings. We 
need further studies to address ways to combine the input 
and the output. Secondly, for practical reasons we had to 
place the touch and gesture interfaces in a slightly different 
position than the tactile interface. Placement of in-vehicle 
systems is definitely important as drivers remove their eyes 
from the road to the system. The lower placement of the 
tactile interface could probably be a disadvantage in terms 
of eye glance durations and perhaps other issues. Thirdly,
the tactile interface provided more functionality than the 
touch and gesture interfaces being an off-the-shelf product. 
It is acknowledged that increased system complexity can 
reduce user efficiency and effectiveness. Thus, the extra 
functionality could potentially influence the findings of the 
tactile sessions in a negative way.
CONCLUSION
Research on in-vehicle systems has illustrated a need for 
new interaction techniques for in-vehicle systems as more 
conventional techniques decrease driving performance. 
Studies on visual workload has shown significant less eyes-
on-the-road time if drivers interact with in-vehicle systems 
with high visual demands and this directly affects driving 
performances. In this paper, we compared three different 
interaction techniques for in-vehicle systems. Our focus 
was to explore their effects on driver performance as well 
as eye glance behaviour. Hence, we aim to follow a “you 
can touch, but you cannot look” mantra.
Our results indicated that gesture interaction can reduce eye 
glances on simple secondary task interaction performances 
especially longer eye fixation glances. However, we found 
no effect of eye glances to the driving errors, i.e. on lateral 
control and longitudinal control. But the gesture interaction 
was not fully attention free as subjects sometimes had to 
make eye/hand coordination. The touch interaction was the 
fastest interaction technique, but also led to the most long 
duration eye glances (above 2 seconds). Finally, we found 
no significant differences on interaction errors.
As our experiment suffers from a number of limitations in 
terms of i.e. the differences on implemented feedback and 
placement of interfaces during the experiment, new studies 
could focus on how the interaction techniques perform with 
similar feedback forms. Furthermore, further studies could 
focus on how we can combine the interaction techniques to 
create powerful and adaptable configurations.
AKNOWLEDGEMENTS
The work behind this paper received financial support from 
the Danish Research Agency (grants no. 2106-04-0022 and 
274-07-0157). We thank all the participating test subjects. 
We would also like to thank Bang &amp; Olufsen for the 
collaboration especially Jannie Friis Kristensen. Finally, we 
want to thank several anonymous reviewers for comments 
on drafts of this paper.
REFERENCES
1. Alpern, M. and Minardo, K. (2003). Developing a Car 
Gesture Interface for Use as a Secondary Task in CHI 
2003: New Horizons. Human-Computer Interaction 
Institute (HCII), Carnegie Mellon University.
2. Bach, K. M., Jæger, M. G., Skov, M. B., and
Thomassen, N. G. (2007). A Classification of In-Vehicle 
Systems Research: Understanding, Measuring and 
Evaluating Attention, HCI Lab Technical Report no. 
2007/2
1147
CHI 2008 Proceedings · Tangibles: Input &amp; Output	April 5-10, 2008 · Florence, Italy
3. Bach, K. M., Jæger, M. G., Skov, M. B., and
Thomassen, N. G. (2008). Evaluating In-Vehicle 
Systems: Controlled Driving versus Simulated Driving. 
HCI Lab Technical Report no. 2008/1
4. Barón, A. and Green, P. (2006). Safety and Usability of 
Speech Interfaces for In-Vehicle Tasks while Driving: A 
Brief Literature Review. The University of Michigan 
Transportation Research Institute (UMTRI).
5. Bellotti, F., Gloria, A. De, Montanari, R., Dosio, N. and 
Morreale, D. (2005). COMUNICAR: Designing a 
Multimedia, Context-Aware Human-Machine Interface 
for Cars in Cognition, Technology &amp; Work, Vol. 7, No. 
1. Springer, pp. 36-45
6. Dugarry, A. (2004). Advanced Driver Assistance 
Systems Information Management and Presentation. 
Cranfield University.
7. Fleiss, J. L., Levin, B. and Paik, M. Cho (2003). 
Statistical methods for rates and proportions, 3rd ed.. 
New York, John Wiley.
8. Gellatly, A. William (1997). The Use of Speech 
Recognition Technology in Automotive Applications. 
Faculty of the Virginia Polytechnic Institute and State.
9. González, I. E., Wobbrock, J. O., Chau, D. H., Faulring, 
A. and Myers, B. A. (2007) Eyes on the road, hands on 
the wheel: Thumb-based interaction techniques for input 
on steering wheels. Proceedings of Graphics Interface 
2007. Montréal, Québec (May 28-30, 2007). Waterloo, 
Ontario: Canadian Human-Computer Communications 
Society, pp. 95-102
10.Green, P. (2000). Crashes Induced by Driver
Information Systems and What Can Be Done to Reduce 
Them in Society of Automotive Engineers. University 
of Michigan Transportation Research Institue (UMTRI).
11.Green, P. (1999). Visual and Task Demands of Driver 
Information Systems. The University of Michigan 
Transportation Research Institute (UMTRI).
12.Green, P. (2004). Driver Distraction, Telematics Design, 
and Workload Managers: Safety Issues and Solutions in 
SAE publication P-387. Society of Automotive 
Engineers, Inc., Pennsylvania, USA, pp. 165-180
13.Jones, C. Martyn and Jonsson, I. (2005). Automatic 
Recognition of Affective Cues in the Speech of Car 
Drivers to Allow Appropriate Responses in Proceedings 
of OZCHI 2005. Heriot-Watt University.
14.Kristoffersen, S. and Lungberg, F. (1999). Making Place 
to make IT Work: Empirical Explorations of HCI for 
Mobile CSCW in Proceedings of the international ACM 
SIGGROUP conference on Supporting group work. 
ACM Press, pp. 276-285
15.Lansdown, T. C., Brook-Carter, N. and Kersloot, T. 
(2002). Primary Task Disruption from Multiple In- 
Vehicle Systems in ITS Journal, Vol. 7. Taylor &amp; 
Francis Group, , pp. 151-168
16.Lansdown, T. C., Brook-Carter, N. and Kersloot, T. 
(2004). Distraction from Multiple In-Vehicle Secondary 
Tasks: Vehicle Performance and Mental Workload 
Implications in Ergonomics, Vol. 47, No. 1,. Taylor &amp; 
Francis Group, pp. 91–104
17.Noy, Y. Ian, Lemoine, T. L., Klachan, C. and Burns, P. 
C. (2004). Task Interruptability and Duration as 
Measures of Visual Distraction in Applied Ergonomics, 
Vol. 35. Elsevier Ltd. , pp. 207-213
18.Pascoe, J., Ryan, N. and Morse, D. (2000). Using While 
Moving: HCI Issues in Fieldwork Environments in 
ACM Transactions on Computer-Human Interaction, 
Vol. 7, No. 3,. University of Kent at Canterbury, pp. 
417-437
19.Pirhonen, A., Brewster, S. and Holguin, C. (2002). 
Gestural and Audio Metaphors as a Means of Control 
for Mobile Devices in CHI Letters, Vol. No. 4, Issue 
No. 1, pp. 291-298.
20.Potter, R. L., Weldon, L. J. and Shneiderman, B. (1988) 
Improving the accuracy of touch screens: An 
experimental evaluation of three strategies. Proceedings 
of the ACM Conference on Human Factors in
Computing Systems (CHI &apos;88). Washington, D.C. (May 
15-19, 1988). New York: ACM Press, pp. 27-32
21.Rockwell, T. H. (1988). Spare visual capacity in driving 
– revisited: New empirical results for an old idea in 
Gale, A.G., et al. Ed.: Vision in Vehicles II, Elsevier 
Science, North Holland, pp. 317-324.
22.Stevens, A. (2000). Safety of Driver Interaction with In- 
Vehicle Information Systems in Proceeding of the 
Institution of Mechanical Engineers - Part D - Journal of 
Automobile Engineering, Vol. 214, Issue 6. Professional 
Engineering Publising, pp. 639-644
23.Strayer, D. L., Drews, F. A. and Crouch, D. J. (2006). A 
Comparison of the Cell Phone Driver and the Drunk 
Driver in Human Factors, Vol. 48, No. 2. Human 
Factors and Ergonomics Society, pp. 381-391
24.Tsimhoni, O. and Green, P. (2001). Visual Demand of 
Driving and the Execution of Display-Intensive, In- 
Vehicle Tasks in Proceedings of the Human Factors and 
Ergonomics Society 45th Annual Meeting 2001.
25.Tsimhoni, O., Smith, D. and Green, P. (2004). Address 
Entry While Driving: Speech Recognition Versus a 
Touch-Screen Keyboard in Human Factors, Vol. 46, No. 
4. Human Factors and Ergonomics Society, pp. 600-610
26.Wickens, C. D. and Hollands, J. G. (2000). Engineering 
Psychology and Human (3. ed.). Prentice Hall.
27.Zwahlen, H. T., Adams, C. and DeBald, D. (1988). 
Safety Aspects of CRT Touch Panel Controls on 
Automobiles in Gale, A.G., et al. Ed.: Vision in 
Vehicles II,. Elsevier Science, North Holland, pp. 335- 
344
1148
